# Dashboard Deployment Guide

This document explains how to deploy the LLM Contract Analysis Dashboard on GitHub Pages and Streamlit Cloud.

## Files Created

### 1. Enhanced Pipeline (`run_pipeline_enhanced.py`)
- Integrates contract classification into the main pipeline
- Adds contract analysis as a pipeline stage
- Exports analysis results for dashboard consumption
- Supports novel pattern discovery

### 2. Streamlit Dashboard (`streamlit_dashboard.py`)
- Interactive web dashboard for contract analysis
- Visualizes violation patterns and novel discoveries
- Provides research insights and recommendations
- Supports data export and reporting

### 3. Deployment Configuration
- `.streamlit/config.toml` - Streamlit configuration
- `requirements_dashboard.txt` - Dashboard dependencies
- `.github/workflows/deploy-dashboard.yml` - GitHub Actions workflow

## Deployment Options

### Option 1: Streamlit Cloud (Recommended)

1. **Push to GitHub:**
   ```bash
   git add .
   git commit -m "Add contract classification dashboard"
   git push origin main
   ```

2. **Deploy on Streamlit Cloud:**
   - Visit [share.streamlit.io](https://share.streamlit.io)
   - Connect your GitHub repository
   - Select `streamlit_dashboard.py` as the main file
   - Use `requirements_dashboard.txt` for dependencies

3. **Access your dashboard:**
   - URL will be: `https://[app-name].streamlit.app`

### Option 2: GitHub Pages (Static)

1. **Enable GitHub Pages:**
   - Go to repository Settings â†’ Pages
   - Select "GitHub Actions" as source

2. **Deploy automatically:**
   - Push changes to trigger the workflow
   - GitHub Actions will build and deploy
   - Access at: `https://[username].github.io/[repository-name]`

### Option 3: Local Development

1. **Install dependencies:**
   ```bash
   pip install -r requirements_dashboard.txt
   ```

2. **Run dashboard:**
   ```bash
   streamlit run streamlit_dashboard.py
   ```

3. **Access locally:**
   - Open browser to `http://localhost:8501`

## Running Enhanced Pipeline

### Basic Usage

```bash
# Run full enhanced pipeline
python run_pipeline_enhanced.py --step full

# Run only contract analysis
python run_pipeline_enhanced.py --step analysis --max-posts 500

# Generate comprehensive report
python run_pipeline_enhanced.py --generate-report
```

### Configuration

Edit `pipeline_config.yaml` to customize:

```yaml
pipeline_steps:
  data_acquisition: true
  keyword_filtering: true
  llm_screening: true
  contract_analysis: true  # Enable contract analysis

llm_screening:
  mode: 'agentic'  # Use agentic for better classification
  model: 'gpt-4-turbo-2024-04-09'
  temperature: 0.1

contract_analysis:
  enable_novel_discovery: true
  min_confidence: 0.3
  save_results: true
  export_format: 'json'
```

## Dashboard Features

### 1. Overview Dashboard
- Total posts and violation statistics
- Contract type distribution charts
- Severity analysis
- Research value distribution

### 2. Violation Analysis
- Detailed violation explorer
- Filtering by severity, confidence, and type
- Novel vs. known pattern distinction
- Interactive violation cards

### 3. Novel Discoveries
- Categorized novel pattern explorer
- Discovery timeline
- Pattern descriptions and evidence
- Research value assessment

### 4. Research Insights
- High-value post identification
- Research recommendations
- Statistical summaries
- Export capabilities

### 5. Raw Data View
- Sortable data tables
- CSV export functionality
- Detailed violation breakdowns

## Data Sources

The dashboard can consume data from:

1. **Live Database** (if modules available)
   - Connects to MongoDB collections
   - Real-time analysis results

2. **Exported JSON Files**
   - `contract_analysis_results_*.json`
   - Generated by enhanced pipeline

3. **Sample Data** (for demo)
   - Automatically generated if no data available
   - Realistic patterns for demonstration

## Customization

### Adding New Visualizations

```python
# In streamlit_dashboard.py
def _show_custom_analysis(self):
    st.header("ðŸ”§ Custom Analysis")
    
    # Your custom visualizations
    fig = px.scatter(
        data,
        x='confidence',
        y='research_value',
        color='severity'
    )
    st.plotly_chart(fig)
```

### Modifying Filters

```python
# Add new filter in sidebar
framework_filter = st.multiselect(
    "Frameworks",
    ["langchain", "openai-python", "anthropic"],
    default=[]
)
```

### Custom Themes

Edit `.streamlit/config.toml`:

```toml
[theme]
primaryColor = "#your-color"
backgroundColor = "#your-bg-color"
secondaryBackgroundColor = "#your-secondary-bg"
textColor = "#your-text-color"
```

## Environment Variables

For full functionality, set:

```bash
# Database connection
export MONGODB_URI="mongodb://localhost:27017"
export DATABASE_NAME="llm_contracts_research"

# API keys (for analysis)
export OPENAI_API_KEY="your-openai-key"
export GITHUB_TOKEN="your-github-token"
```

## Troubleshooting

### Dashboard Won't Load
- Check `requirements_dashboard.txt` installation
- Verify data files exist or sample data generates
- Check Streamlit logs for errors

### No Data Displayed
- Run enhanced pipeline first: `python run_pipeline_enhanced.py`
- Check for `contract_analysis_results_*.json` files
- Verify database connection if using live data

### Performance Issues
- Limit data size with filters
- Use sampling for large datasets
- Consider data preprocessing

### GitHub Pages Deployment Fails
- Check GitHub Actions logs
- Verify workflow permissions
- Ensure all required files are committed

## Security Considerations

1. **API Keys:** Never commit API keys to repository
2. **Database:** Use read-only connections for dashboard
3. **Data Privacy:** Sanitize sensitive data before visualization
4. **Access Control:** Use private repositories if needed

## Monitoring and Analytics

### Performance Metrics
- Dashboard load times
- Data processing speed
- User interaction patterns

### Usage Analytics
- Most viewed sections
- Export frequency
- Filter usage patterns

### Error Tracking
- Failed data loads
- Visualization errors
- Export failures

## Future Enhancements

1. **Real-time Updates**
   - WebSocket connections
   - Live data streaming
   - Automatic refresh

2. **Advanced Filters**
   - Date range selection
   - API provider filtering
   - Custom query builder

3. **Machine Learning Integration**
   - Automated pattern detection
   - Prediction capabilities
   - Clustering analysis

4. **Collaboration Features**
   - Annotation tools
   - Shared views
   - Export templates

## Support

For issues with deployment or usage:

1. Check the [Issues](../../issues) page
2. Review deployment logs
3. Verify configuration files
4. Test with sample data first

## Contributing

To contribute to the dashboard:

1. Fork the repository
2. Create feature branch
3. Test changes locally
4. Submit pull request
5. Update documentation