name: Deploy Enhanced Streamlit Dashboard

on:
  push:
    branches: [ main ]
    paths:
      - 'streamlit_dashboard*.py'
      - 'requirements_dashboard.txt'
      - '.streamlit/config.toml'
      - '.github/workflows/deploy-dashboard*.yml'
  workflow_dispatch:
    inputs:
      use_live_data:
        description: 'Try to use live MongoDB data'
        required: false
        default: 'true'
        type: boolean

permissions:
  contents: read
  pages: write
  id-token: write

concurrency:
  group: "pages"
  cancel-in-progress: false

jobs:
  build:
    runs-on: ubuntu-latest
    environment: 
      name: production
    steps:
      - name: Checkout
        uses: actions/checkout@v4
      
      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements_dashboard.txt
          # Install additional dependencies for live data
          pip install motor pymongo python-dotenv
      
      - name: Test MongoDB Connection
        id: mongodb_test
        env:
          MONGODB_URI: ${{ secrets.MONGODB_URI }}
          DATABASE_NAME: ${{ secrets.DATABASE_NAME }}
        run: |
          python -c "
          import os
          import sys
          
          mongodb_uri = os.getenv('MONGODB_URI')
          database_name = os.getenv('DATABASE_NAME', 'llm_contracts_research')
          
          if mongodb_uri and '${{ github.event.inputs.use_live_data }}' == 'true':
              try:
                  import pymongo
                  client = pymongo.MongoClient(mongodb_uri, serverSelectionTimeoutMS=5000)
                  # Test connection
                  client.server_info()
                  db = client[database_name]
                  
                  # Check if collections exist and have data
                  collections = db.list_collection_names()
                  print(f'Available collections: {collections}')
                  
                  if 'labelled_posts' in collections:
                      count = db.labelled_posts.count_documents({})
                      print(f'Found {count} labelled posts')
                      if count > 0:
                          print('LIVE_DATA_AVAILABLE=true')
                          sys.exit(0)
                  
                  print('LIVE_DATA_AVAILABLE=false')
                  
              except Exception as e:
                  print(f'MongoDB connection failed: {e}')
                  print('LIVE_DATA_AVAILABLE=false')
          else:
              print('MongoDB not configured or live data disabled')
              print('LIVE_DATA_AVAILABLE=false')
          " | tee mongodb_test.log
          
          # Extract the result
          if grep -q "LIVE_DATA_AVAILABLE=true" mongodb_test.log; then
            echo "live_data=true" >> $GITHUB_OUTPUT
            echo "‚úÖ Live data available"
          else
            echo "live_data=false" >> $GITHUB_OUTPUT
            echo "üìä Using sample data"
          fi
      
      - name: Export Live Data
        if: steps.mongodb_test.outputs.live_data == 'true'
        env:
          MONGODB_URI: ${{ secrets.MONGODB_URI }}
          DATABASE_NAME: ${{ secrets.DATABASE_NAME }}
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
        run: |
          python -c "
          import os
          import json
          import asyncio
          from datetime import datetime
          
          # Import your modules
          try:
              from pipeline.storage.repositories import LabelledPostRepository, FilteredPostRepository
              from pipeline.llm_screening.contract_analysis import analyze_post_batch
              from pipeline.infrastructure.database import DatabaseManager
              
              async def export_live_data():
                  print('üîÑ Exporting live data from MongoDB...')
                  
                  # Initialize database
                  uri = os.getenv('MONGODB_URI')
                  db_name = os.getenv('DATABASE_NAME', 'llm_contracts_research')
                  
                  db_manager = DatabaseManager(uri, db_name)
                  await db_manager.connect()
                  
                  # Initialize repositories
                  labelled_repo = LabelledPostRepository()
                  filtered_repo = FilteredPostRepository()
                  
                  # Get recent data (limit for performance)
                  labelled_posts = await labelled_repo.get_all(limit=500)
                  print(f'Retrieved {len(labelled_posts)} labelled posts')
                  
                  # Get positive classifications only
                  positive_posts = [p for p in labelled_posts if p.final_decision is True]
                  print(f'Found {len(positive_posts)} positive classifications')
                  
                  # Get corresponding filtered posts and run analysis
                  filtered_posts = []
                  llm_results = {}
                  
                  for labelled_post in positive_posts[:100]:  # Limit for demo
                      filtered_post = await filtered_repo.get_by_id(labelled_post.filtered_post_id)
                      if filtered_post:
                          filtered_posts.append(filtered_post)
                          
                          llm_result = (
                              labelled_post.agentic_screening or
                              labelled_post.borderline_screening or
                              labelled_post.bulk_screening
                          )
                          if llm_result:
                              llm_results[filtered_post.id] = llm_result
                  
                  print(f'Analyzing {len(filtered_posts)} posts for contracts...')
                  
                  # Run contract analysis
                  if filtered_posts:
                      analysis_results = analyze_post_batch(filtered_posts, llm_results)
                      
                      # Convert to export format
                      export_data = {
                          'timestamp': datetime.now().strftime('%Y%m%d_%H%M%S'),
                          'data_source': 'live_mongodb',
                          'summary': {
                              'total_posts': len(analysis_results),
                              'posts_with_violations': sum(1 for r in analysis_results if r.has_violations),
                              'total_violations': sum(r.total_violations for r in analysis_results),
                              'novel_violations': sum(r.novel_violations for r in analysis_results)
                          },
                          'posts': []
                      }
                      
                      # Add post data
                      for result in analysis_results:
                          if result.has_violations:
                              post_data = {
                                  'post_id': result.post_id,
                                  'total_violations': result.total_violations,
                                  'novel_violations': result.novel_violations,
                                  'pattern': result.violation_pattern,
                                  'research_value': result.research_value_score,
                                  'violations': []
                              }
                              
                              for violation in result.violations:
                                  violation_data = {
                                      'is_novel': violation.is_novel,
                                      'contract_type': violation.contract_type.value if violation.contract_type else None,
                                      'category': violation.contract_category,
                                      'confidence': violation.confidence,
                                      'severity': violation.severity.value if violation.severity else 'unknown'
                                  }
                                  
                                  if violation.is_novel:
                                      violation_data.update({
                                          'novel_name': violation.novel_name,
                                          'novel_description': violation.novel_description
                                      })
                                  
                                  post_data['violations'].append(violation_data)
                              
                              export_data['posts'].append(post_data)
                      
                      # Generate novel contracts summary
                      from pipeline.llm_screening.contract_analysis import ContractAnalyzer
                      analyzer = ContractAnalyzer()
                      novel_summary = analyzer.get_novel_contracts_summary()
                      export_data['novel_contracts'] = novel_summary
                      
                      # Save to file
                      with open('contract_analysis_results_live.json', 'w') as f:
                          json.dump(export_data, f, indent=2)
                      
                      print(f'‚úÖ Exported live data: {export_data[\"summary\"]}')
                      
                  await db_manager.disconnect()
              
              # Run the export
              asyncio.run(export_live_data())
              
          except Exception as e:
              print(f'‚ùå Live data export failed: {e}')
              print('Will fall back to sample data')
              exit(1)
          "
      
      - name: Create Sample Data
        if: steps.mongodb_test.outputs.live_data == 'false'
        run: |
          python -c "
          import json
          from datetime import datetime
          
          print('üìä Generating sample data for demo...')
          
          # Generate enhanced sample data
          sample_data = {
              'timestamp': datetime.now().strftime('%Y%m%d_%H%M%S'),
              'data_source': 'sample_generated',
              'summary': {
                  'total_posts': 1250,
                  'posts_with_violations': 856,
                  'total_violations': 1842,
                  'novel_violations': 234
              },
              'novel_contracts': {
                  'count': 18,
                  'categories': {
                      'NovelRateLimit': [
                          {'name': 'Dynamic_Rate_Adjustment', 'description': 'Rate limits that change based on usage patterns', 'severity': 'high', 'evidence_count': 15},
                          {'name': 'Burst_Credit_System', 'description': 'Accumulated credits for burst requests', 'severity': 'medium', 'evidence_count': 8}
                      ],
                      'NovelFormat': [
                          {'name': 'Streaming_JSON_Contract', 'description': 'JSON formatting requirements for streaming responses', 'severity': 'high', 'evidence_count': 22},
                          {'name': 'Tool_Schema_Evolution', 'description': 'Schema changes between function calling versions', 'severity': 'medium', 'evidence_count': 12}
                      ],
                      'NovelState': [
                          {'name': 'Context_Decay_Pattern', 'description': 'Performance degradation with long conversations', 'severity': 'medium', 'evidence_count': 18},
                          {'name': 'Memory_Leak_Contract', 'description': 'Memory usage patterns in stateful conversations', 'severity': 'high', 'evidence_count': 9}
                      ]
                  }
              },
              'posts': []
          }
          
          # Generate sample posts
          contract_types = ['RATE_LIMIT', 'CONTEXT_LENGTH', 'OUTPUT_FORMAT', 'CONTENT_POLICY', 
                           'TEMPERATURE', 'MAX_TOKENS', 'JSON_SCHEMA', 'API_KEY_FORMAT']
          severities = ['critical', 'high', 'medium', 'low']
          
          for i in range(100):
              num_violations = (i % 3) + 1
              violations = []
              
              for j in range(num_violations):
                  is_novel = (i + j) % 7 == 0
                  violation = {
                      'is_novel': is_novel,
                      'contract_type': contract_types[j % len(contract_types)] if not is_novel else None,
                      'category': 'LLM_SPECIFIC' if not is_novel else 'Novel',
                      'confidence': 0.6 + (j * 0.1),
                      'severity': severities[j % len(severities)]
                  }
                  
                  if is_novel:
                      violation['novel_name'] = f'Novel_Pattern_{i}_{j}'
                      violation['novel_description'] = f'Discovered pattern involving {contract_types[j % len(contract_types)]}'
                  
                  violations.append(violation)
              
              sample_data['posts'].append({
                  'post_id': f'github_issue_{1000 + i}' if i % 2 == 0 else f'stackoverflow_q_{5000 + i}',
                  'total_violations': num_violations,
                  'novel_violations': sum(1 for v in violations if v['is_novel']),
                  'pattern': 'single' if num_violations == 1 else 'multiple',
                  'research_value': 0.4 + (i % 6) * 0.1,
                  'violations': violations
              })
          
          with open('contract_analysis_results_sample.json', 'w') as f:
              json.dump(sample_data, f, indent=2)
          
          print('‚úÖ Sample data generated')
          "
      
      - name: Build Static Site
        run: |
          mkdir -p _site
          
          # Copy dashboard files
          cp streamlit_dashboard.py _site/
          cp streamlit_dashboard_live.py _site/
          cp requirements_dashboard.txt _site/
          cp -r .streamlit _site/ 2>/dev/null || true
          
          # Copy data file (live or sample)
          if [ -f "contract_analysis_results_live.json" ]; then
            cp contract_analysis_results_live.json _site/contract_analysis_results.json
            echo "üìä Using live data export"
          else
            cp contract_analysis_results_sample.json _site/contract_analysis_results.json
            echo "üìä Using sample data"
          fi
          
          # Create landing page
          cat > _site/index.html << 'EOF'
          <!DOCTYPE html>
          <html>
          <head>
              <meta charset="utf-8">
              <title>LLM Contract Analysis Dashboard</title>
              <meta name="description" content="Interactive dashboard for analyzing LLM API contract violations and discovering novel patterns">
              <style>
                  body {
                      font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
                      max-width: 1000px;
                      margin: 0 auto;
                      padding: 40px 20px;
                      background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
                      color: white;
                      min-height: 100vh;
                  }
                  .container {
                      background: rgba(255, 255, 255, 0.1);
                      border-radius: 20px;
                      padding: 40px;
                      backdrop-filter: blur(10px);
                      border: 1px solid rgba(255, 255, 255, 0.2);
                  }
                  h1 {
                      font-size: 2.5em;
                      margin-bottom: 20px;
                      text-align: center;
                      background: linear-gradient(45deg, #fff, #f0f0f0);
                      -webkit-background-clip: text;
                      -webkit-text-fill-color: transparent;
                      background-clip: text;
                  }
                  .description {
                      font-size: 1.1em;
                      margin-bottom: 30px;
                      opacity: 0.9;
                      line-height: 1.6;
                      text-align: center;
                  }
                  .button {
                      display: inline-block;
                      background: linear-gradient(45deg, #ff6b6b, #ee5a24);
                      color: white;
                      padding: 15px 30px;
                      text-decoration: none;
                      border-radius: 30px;
                      font-weight: bold;
                      font-size: 1.1em;
                      transition: transform 0.3s ease, box-shadow 0.3s ease;
                      margin: 10px;
                      text-align: center;
                  }
                  .button:hover {
                      transform: translateY(-2px);
                      box-shadow: 0 10px 25px rgba(0,0,0,0.2);
                  }
                  .features {
                      display: grid;
                      grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
                      gap: 20px;
                      margin: 30px 0;
                  }
                  .feature {
                      padding: 20px;
                      background: rgba(255, 255, 255, 0.1);
                      border-radius: 15px;
                      border-left: 4px solid #ff6b6b;
                  }
                  .feature h3 {
                      margin: 0 0 10px 0;
                      color: #ffeb3b;
                  }
                  .center {
                      text-align: center;
                  }
                  .data-badge {
                      display: inline-block;
                      background: rgba(76, 175, 80, 0.2);
                      border: 2px solid #4caf50;
                      color: #4caf50;
                      padding: 8px 16px;
                      border-radius: 20px;
                      margin: 10px;
                      font-weight: bold;
                  }
              </style>
          </head>
          <body>
              <div class="container">
                  <h1>üîç LLM Contract Analysis Dashboard</h1>
                  <p class="description">
                      Interactive dashboard for analyzing LLM API contract violations and discovering novel patterns.
                      Explore contract taxonomies, visualize violation patterns, and discover new constraint types.
                  </p>
                  
                  <div class="center">
                      <div class="data-badge">
                          üìä Data Source: Live MongoDB Export
                      </div>
                  </div>
                  
                  <div class="features">
                      <div class="feature">
                          <h3>üìä Live Data Analysis</h3>
                          <p>Real contract violations from MongoDB collections with live pattern discovery</p>
                      </div>
                      <div class="feature">
                          <h3>üåü Novel Discovery</h3>
                          <p>Identify new contract patterns not in existing taxonomies</p>
                      </div>
                      <div class="feature">
                          <h3>üî¨ Research Insights</h3>
                          <p>Generate insights for academic research and API improvement</p>
                      </div>
                      <div class="feature">
                          <h3>üìà Real-time Metrics</h3>
                          <p>Track violation patterns and discovery rates over time</p>
                      </div>
                  </div>
                  
                  <div class="center">
                      <a href="https://llm-contracts-dashboard.streamlit.app" class="button">
                          üöÄ Launch Interactive Dashboard
                      </a>
                      <br>
                      <a href="https://github.com/your-repo/llm-contracts-research" class="button" style="background: linear-gradient(45deg, #4caf50, #2e7d32);">
                          üìÅ View Source Code
                      </a>
                  </div>
                  
                  <p style="text-align: center; margin-top: 30px; opacity: 0.7;">
                      Part of the LLM Contract Research Project | Deployed with GitHub Actions
                  </p>
              </div>
          </body>
          </html>
          EOF
      
      - name: Upload artifact
        uses: actions/upload-pages-artifact@v3
        with:
          path: '_site'

  deploy:
    environment:
      name: github-pages
      url: ${{ steps.deployment.outputs.page_url }}
    runs-on: ubuntu-latest
    needs: build
    steps:
      - name: Deploy to GitHub Pages
        id: deployment
        uses: actions/deploy-pages@v4