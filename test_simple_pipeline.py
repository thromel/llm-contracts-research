#!/usr/bin/env python3
"""
Simple test script for the LLM screening pipeline.

This will test the basic components and help identify any issues.
"""

import asyncio
import logging
import os
import sys
from datetime import datetime

# Add the pipeline to the path
sys.path.insert(0, '.')

# Setup logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


async def test_imports():
    """Test that all basic imports work."""
    logger.info("üîç Testing imports...")

    try:
        # Test basic imports
        from pipeline.common.config import PipelineConfig, get_development_config
        logger.info("‚úÖ Config imports successful")

        from pipeline.common.database import MongoDBManager
        logger.info("‚úÖ Database imports successful")

        from pipeline.llm_screening.bulk_screener import BulkScreener
        logger.info("‚úÖ Bulk screener imports successful")

        from pipeline.llm_screening.borderline_screener import BorderlineScreener
        logger.info("‚úÖ Borderline screener imports successful")

        from pipeline.llm_screening.screening_orchestrator import ScreeningOrchestrator
        logger.info("‚úÖ Screening orchestrator imports successful")

        from pipeline.preprocessing.keyword_filter import KeywordPreFilter
        logger.info("‚úÖ Keyword filter imports successful")

        return True

    except Exception as e:
        logger.error(f"‚ùå Import failed: {str(e)}")
        import traceback
        traceback.print_exc()
        return False


async def test_config():
    """Test configuration loading."""
    logger.info("‚öôÔ∏è Testing configuration...")

    try:
        from pipeline.common.config import get_development_config, ScreeningMode, LLMProvider

        config = get_development_config()
        logger.info(f"‚úÖ Config loaded: {config.screening_mode}")

        # Test config properties
        logger.info(f"Database: {config.database.database_name}")
        logger.info(f"Screening mode: {config.screening_mode}")

        return True

    except Exception as e:
        logger.error(f"‚ùå Config test failed: {str(e)}")
        import traceback
        traceback.print_exc()
        return False


async def test_models():
    """Test data models."""
    logger.info("üìã Testing data models...")

    try:
        from pipeline.common.models import RawPost, FilteredPost, LLMScreeningResult, Platform

        # Test creating a raw post
        raw_post = RawPost(
            platform=Platform.GITHUB,
            source_id="test-123",
            url="https://github.com/test/repo/issues/1",
            title="Test Issue",
            body_md="Test body content",
            created_at=datetime.utcnow(),
            score=5,
            tags=["test"],
            author="test_user",
            acquisition_timestamp=datetime.utcnow(),
            acquisition_version="1.0.0"
        )

        logger.info(f"‚úÖ Raw post created: {raw_post.title}")

        # Test creating a screening result
        screening_result = LLMScreeningResult(
            decision="relevant",
            confidence=0.85,
            rationale="Contains API usage patterns",
            model_used="mock-model"
        )

        logger.info(f"‚úÖ Screening result created: {screening_result.decision}")

        return True

    except Exception as e:
        logger.error(f"‚ùå Models test failed: {str(e)}")
        import traceback
        traceback.print_exc()
        return False


async def test_screener_initialization():
    """Test screener initialization without API calls."""
    logger.info("ü§ñ Testing screener initialization...")

    try:
        from pipeline.llm_screening.bulk_screener import BulkScreener
        from pipeline.common.config import get_development_config

        config = get_development_config()

        # Mock database manager for testing
        class MockDBManager:
            async def connect(self):
                pass

            async def disconnect(self):
                pass

        mock_db = MockDBManager()

        # Test bulk screener initialization
        bulk_screener = BulkScreener(
            api_key="mock-key-12345",
            db_manager=mock_db,
            base_url="http://localhost:8000/v1",
            model="mock-model"
        )

        logger.info("‚úÖ Bulk screener initialized")

        # Test borderline screener
        from pipeline.llm_screening.borderline_screener import BorderlineScreener

        borderline_screener = BorderlineScreener(
            api_key="mock-key-12345",
            db_manager=mock_db,
            model="mock-model"
        )

        logger.info("‚úÖ Borderline screener initialized")

        return True

    except Exception as e:
        logger.error(f"‚ùå Screener initialization failed: {str(e)}")
        import traceback
        traceback.print_exc()
        return False


async def test_agentic_imports():
    """Test agentic screener imports."""
    logger.info("ü§ñ Testing agentic imports...")

    try:
        from pipeline.llm_screening.agentic_screener import (
            AgenticScreeningOrchestrator,
            ContractViolationAnalysis,
            TechnicalErrorAnalysis,
            ContextRelevanceAnalysis,
            FinalDecision
        )

        logger.info("‚úÖ Agentic screener imports successful")

        # Test creating analysis objects
        violation_analysis = ContractViolationAnalysis(
            has_violation=True,
            violation_type="rate_limit",
            confidence=0.9,
            evidence=["Rate limit error message"],
            violation_severity="high"
        )

        logger.info(
            f"‚úÖ Violation analysis created: {violation_analysis.violation_type}")

        return True

    except Exception as e:
        logger.error(f"‚ùå Agentic imports failed: {str(e)}")
        import traceback
        traceback.print_exc()
        return False


async def test_keyword_filter():
    """Test keyword filter functionality."""
    logger.info("üîç Testing keyword filter...")

    try:
        from pipeline.preprocessing.keyword_filter import KeywordPreFilter

        # Mock database manager
        class MockDBManager:
            async def connect(self):
                pass

            async def disconnect(self):
                pass

        mock_db = MockDBManager()

        # Initialize keyword filter
        keyword_filter = KeywordPreFilter(mock_db)

        # Test keyword matching
        test_text = "I'm getting a rate limit error when calling the OpenAI API"

        # This would normally check against keywords, but we'll just test initialization
        logger.info("‚úÖ Keyword filter initialized")

        return True

    except Exception as e:
        logger.error(f"‚ùå Keyword filter test failed: {str(e)}")
        import traceback
        traceback.print_exc()
        return False


async def main():
    """Run all tests."""
    logger.info("üöÄ Starting LLM Screening Pipeline Component Tests")

    tests = [
        ("Basic Imports", test_imports),
        ("Configuration", test_config),
        ("Data Models", test_models),
        ("Screener Initialization", test_screener_initialization),
        ("Agentic Imports", test_agentic_imports),
        ("Keyword Filter", test_keyword_filter),
    ]

    results = {}

    for test_name, test_func in tests:
        logger.info(f"\n{'='*50}")
        logger.info(f"Running: {test_name}")
        logger.info(f"{'='*50}")

        try:
            result = await test_func()
            results[test_name] = result
            status = "‚úÖ PASSED" if result else "‚ùå FAILED"
            logger.info(f"{test_name}: {status}")
        except Exception as e:
            logger.error(f"{test_name}: ‚ùå FAILED with exception: {str(e)}")
            results[test_name] = False

    # Summary
    logger.info(f"\n{'='*50}")
    logger.info("TEST SUMMARY")
    logger.info(f"{'='*50}")

    passed = sum(1 for result in results.values() if result)
    total = len(results)

    for test_name, result in results.items():
        status = "‚úÖ PASSED" if result else "‚ùå FAILED"
        logger.info(f"{test_name}: {status}")

    logger.info(f"\nOverall: {passed}/{total} tests passed")

    if passed == total:
        logger.info(
            "üéâ All component tests passed! Basic pipeline structure is working.")
        logger.info("üí° Next steps:")
        logger.info("   1. Set up MongoDB connection")
        logger.info("   2. Configure API keys")
        logger.info("   3. Test with real data")
    else:
        logger.info("‚ö†Ô∏è Some tests failed. Check logs for details.")


if __name__ == "__main__":
    asyncio.run(main())
