[
  {
    "repository": "huggingface/transformers",
    "issue_number": 35959,
    "title": "Cannot import name 'LRScheduler' from 'torch.optim.lr_scheduler'",
    "body": "Run the following command and get an error\n\n<img width=\"361\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/9565b42c-6aba-486b-8713-f05766ca5acb\" />\n\nRuntimeError: Failed to import transformers.trainer because of the following error (look up to see its traceback):\ncannot import name 'LRScheduler' from 'torch.optim.lr_scheduler' \n\nHow can I fix this issue? Any suggestion welcome!\n\nAttach pytorch and transformer version information\n\n<img width=\"411\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/94eabc8a-f650-41bb-bbf4-7e2f21560730\" />",
    "state": "closed",
    "created_at": "2025-01-29T13:47:41+00:00",
    "closed_at": "2025-01-29T14:10:09+00:00",
    "updated_at": "2025-01-29T14:10:10+00:00",
    "author": "maverick-2030",
    "author_type": "User",
    "comments_count": 1,
    "reactions_count": 0,
    "labels": "",
    "milestone": null,
    "closed_by": "Rocketknight1",
    "resolution_time_hours": 0.37444444444444447,
    "first_comments": [
      {
        "author": "Rocketknight1",
        "created_at": "2025-01-29T14:10:09+00:00",
        "body": "Hi @maverick-2030, our support for Torch 1.x is very limited at this point. I recommend upgrading to 2.x and seeing if that resolves the issue!"
      }
    ],
    "url": "https://github.com/huggingface/transformers/issues/35959"
  },
  {
    "repository": "huggingface/transformers",
    "issue_number": 35956,
    "title": "KeyError: 'tags' arised while huggingface-hub==0.23.4,transformers==4.41.2",
    "body": "### System Info\n\n@ArthurZucker @Rocketknight1 @stevhliu \n\nMy code worked perfectly when huggingface-hub==0.27.0,transformers==4.47.1 but failed when huggingface-hub==0.23.4,transformers==4.41.2(Experimented with 2 different environments,one is production environment which caused this issue,another one is a new one)\nActually there where no issues previously.It will be much helpful if someone share a solution without upgrading the versions,as if i change any versions there it can affect the rest of the codes.In code tried model is llam3 7b instruct and a locally finetuned model inorder to generate response from llm using pipeline,the dataset wa in a private repository and tried to utilize it using from datasets import load_dataset\n\npython version = 3.10.12,ubuntu 22.04\n\nDetailed Error:\nTraceback (most recent call last):\n  File \"/home/paperspace/product/backend/dataingestion/test_ingestion.py\", line 245, in process_llm_dataset\n    output_file_path, processed_df = Benchmark.prepare_and_generate_responses(request)\n  File \"/home/paperspace/product/backend/dataingestion/test_ingestion.py\", line 146, in prepare_and_generate_responses\n    dataset = load_dataset(\n  File \"/home/paperspace/anaconda3/envs/eval_env/lib/python3.10/site-packages/datasets/load.py\", line 2129, in load_dataset\n    builder_instance = load_dataset_builder(\n  File \"/home/paperspace/anaconda3/envs/eval_env/lib/python3.10/site-packages/datasets/load.py\", line 1849, in load_dataset_builder\n    dataset_module = dataset_module_factory(\n  File \"/home/paperspace/anaconda3/envs/eval_env/lib/python3.10/site-packages/datasets/load.py\", line 1564, in dataset_module_factory\n    ).get_module()\n  File \"/home/paperspace/anaconda3/envs/eval_env/lib/python3.10/site-packages/datasets/load.py\", line 944, in get_module\n    data_files = DataFilesDict.from_patterns(\n  File \"/home/paperspace/anaconda3/envs/eval_env/lib/python3.10/site-packages/datasets/data_files.py\", line 721, in from_patterns\n    else DataFilesList.from_patterns(\n  File \"/home/paperspace/anaconda3/envs/eval_env/lib/python3.10/site-packages/datasets/data_files.py\", line 624, in from_patterns\n    resolve_pattern(\n  File \"/home/paperspace/anaconda3/envs/eval_env/lib/python3.10/site-packages/datasets/data_files.py\", line 388, in resolve_pattern\n    for filepath, info in fs.glob(pattern, detail=True, **glob_kwargs).items()\n  File \"/home/paperspace/anaconda3/envs/eval_env/lib/python3.10/site-packages/huggingface_hub/hf_file_system.py\", line 406, in glob\n    path = self.resolve_path(path, revision=kwargs.get(\"revision\")).unresolve()\n  File \"/home/paperspace/anaconda3/envs/eval_env/lib/python3.10/site-packages/huggingface_hub/hf_file_system.py\", line 179, in resolve_path\n    repo_and_revision_exist, err = self._repo_and_revision_exist(repo_type, repo_id, revision)\n  File \"/home/paperspace/anaconda3/envs/eval_env/lib/python3.10/site-packages/huggingface_hub/hf_file_system.py\", line 127, in _repo_and_revision_exist\n    self._api.repo_info(repo_id, revision=revision, repo_type=repo_type, timeout=HF_HUB_ETAG_TIMEOUT)\n  File \"/home/paperspace/anaconda3/envs/eval_env/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py\", line 114, in _inner_fn\n    return fn(*args, **kwargs)\n  File \"/home/paperspace/anaconda3/envs/eval_env/lib/python3.10/site-packages/huggingface_hub/hf_api.py\", line 2491, in repo_info\n    return method(\n  File \"/home/paperspace/anaconda3/envs/eval_env/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py\", line 114, in _inner_fn\n    return fn(*args, **kwargs)\n  File \"/home/paperspace/anaconda3/envs/eval_env/lib/python3.10/site-packages/huggingface_hub/hf_api.py\", line 2366, in dataset_info\n    return DatasetInfo(**data)\n  File \"/home/paperspace/anaconda3/envs/eval_env/lib/python3.10/site-packages/huggingface_hub/hf_api.py\", line 799, in __init__\n    self.tags = kwargs.pop(\"tags\")\nKeyError: 'tags'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/paperspace/product/backend/dataingestion/test_ingestion.py\", line 261, in <module>\n    process_llm_dataset(request)\n  File \"/home/paperspace/product/backend/dataingestion/test_ingestion.py\", line 250, in process_llm_dataset\n    raise HTTPException(status_code=500, detail=str(e))\nfastapi.exceptions.HTTPException: 500: 'tags'\n\n### Who can help?\n\n@ArthurZucker @Rocketknight1 @stevhliu \n\n\n### Information\n\n- [ ] The official example scripts\n- [ ] My own modified scripts\n\n### Tasks\n\n- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\n- [ ] My own task or dataset (give details below)\n\n### Reproduction\n\nuse keivalya/MedQuad-MedicalQnADataset from huggingface using \" from datasets import load_dataset\" pass the question part only to the llm(llama3 7 b instruct or any finetuned form of it) inorder to generate answer.But remember one thing in my case i have transformed that dataset by extracting the question and answer and saved it as a jsonl file with structure like this {\"question\": \"What is the capital of France?\",\"answer\": \"The capital of France is Paris.\"} this transformed dataset is actually loaded using from datasets import load_dataset;then utilizing from transformers import pipeline inorder to use the llama 3 7b instruct.\n\n\n### Expected behavior\n\nExpecting the code properly works properly not only in[ huggingface-hub==0.27.0,transformers==4.47.1] but also in [huggingface-hub==0.23.4,transformers==4.41.2]",
    "state": "closed",
    "created_at": "2025-01-29T11:46:52+00:00",
    "closed_at": "2025-01-29T11:47:17+00:00",
    "updated_at": "2025-01-29T11:47:17+00:00",
    "author": "HarikrishnanK9",
    "author_type": "User",
    "comments_count": 0,
    "reactions_count": 0,
    "labels": "bug",
    "milestone": null,
    "closed_by": "HarikrishnanK9",
    "resolution_time_hours": 0.006944444444444444,
    "first_comments": [],
    "url": "https://github.com/huggingface/transformers/issues/35956"
  },
  {
    "repository": "huggingface/transformers",
    "issue_number": 35949,
    "title": "lr_scheduler needs to be at the epoch level",
    "body": "### System Info\n\nIn the page: https://huggingface.co/docs/transformers/training, \n\nnote the following blurb.\n\nIsn't lr_scheduler updated per epoch? The code below calls lr_scheduler.step() per batch in stead of per epoch. So the line needs to be moved out to the outer for loop.\n\n```\nfrom tqdm.auto import tqdm\n\nprogress_bar = tqdm(range(num_training_steps))\n\nmodel.train()\nfor epoch in range(num_epochs):\n    for batch in train_dataloader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        outputs = model(**batch)\n        loss = outputs.loss\n        loss.backward()\n\n        optimizer.step()\n        lr_scheduler.step()\n        optimizer.zero_grad()\n        progress_bar.update(1)\n```\n\n### Who can help?\n\n_No response_\n\n### Information\n\n- [x] The official example scripts\n- [ ] My own modified scripts\n\n### Tasks\n\n- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\n- [ ] My own task or dataset (give details below)\n\n### Reproduction\n\nFollow the steps in the page above.\n\n### Expected behavior\n\n```\nfrom tqdm.auto import tqdm\n\nprogress_bar = tqdm(range(num_training_steps))\n\nmodel.train()\nfor epoch in range(num_epochs):\n    for batch in train_dataloader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        outputs = model(**batch)\n        loss = outputs.loss\n        loss.backward()\n\n        optimizer.step()\n        optimizer.zero_grad()\n        progress_bar.update(1)\n    lr_scheduler.step()\n```",
    "state": "closed",
    "created_at": "2025-01-29T00:05:14+00:00",
    "closed_at": "2025-01-29T09:15:01+00:00",
    "updated_at": "2025-01-29T09:15:01+00:00",
    "author": "guidothekp",
    "author_type": "User",
    "comments_count": 1,
    "reactions_count": 0,
    "labels": "bug",
    "milestone": null,
    "closed_by": "guidothekp",
    "resolution_time_hours": 9.163055555555555,
    "first_comments": [
      {
        "author": "Ryoo72",
        "created_at": "2025-01-29T02:49:38+00:00",
        "body": "Learning rate scheduler updates can be either per-epoch or per-batch depending on configuration. However, modern large models often train on massive datasets with very few epochs (typically 1-3 epochs). In such cases with limited epochs, epoch-based scheduling can be problematic as it provides very few adjustment points for the learning rate. To address this, many modern training approaches calculate the total number of training steps upfront and update the learning rate based on global steps instead of epochs. This is why you'll often see lr_scheduler.step() called within the batch loop rather than the epoch loop in modern training code."
      }
    ],
    "url": "https://github.com/huggingface/transformers/issues/35949"
  },
  {
    "repository": "huggingface/transformers",
    "issue_number": 35879,
    "title": "ModernBERT fails to work without FlashAttention !",
    "body": "### System Info\n\n# Issue: ModernBERT Without Flash Attention on Tesla V100\n\n## Problem Description\n\nI am trying to use **ModernBERT** without **Flash Attention**, as it requires an **Ampere GPU or newer** (see error below), while I only have a **Tesla V100 16GB**.\n\n![Error Screenshot](https://cdn-uploads.huggingface.co/production/uploads/63ad8d4120176b2d2162112b/5XxXxd0fXzCkWaqg1EMKv.png)\n*Figure 1: Error message indicating that Flash Attention requires an Ampere or newer GPU.*\n\nTo bypass **Flash Attention**, I explored the **transformers** source code and found that ModernBERT allows alternative attention implementations like **SDPA** or **Eager**. This can be set using:\n\n```python\nmodel.config._attn_implementation = ['flash_attention_2', 'eager', 'sdpa'][2]\n```\n\nHowever, this leads to the following unexpected error:\n\n```python\nTypeError: ModernBertUnpaddedRotaryEmbedding.forward() got an unexpected keyword argument 'position_ids'\n```\n\nThe error originates from the **ModernBertAttention** module, specifically when applying the **rotary embeddings function** on the `qkv` tensor.\n\n![Stack Trace Screenshot](https://cdn-uploads.huggingface.co/production/uploads/63ad8d4120176b2d2162112b/gX9eaF5DeHpnCW9ErF9t4.png)\n*Figure 2: Stack trace showing the unexpected keyword argument error in ModernBertUnpaddedRotaryEmbedding.*\n\n---\n\n## Reproduction Code\n\nHere is the full code to reproduce the issue:\n\n```python\n# import flash_attn_2_cuda as flash_attn_cud\nfrom transformers import AutoTokenizer, AutoModel\n\n# Model name\nmodel_name = \"answerdotai/ModernBERT-base\"  \n\n# Load tokenizer and model\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModel.from_pretrained(model_name).to('cuda')\n\n# Force SDPA Attention instead of Flash Attention 2\nmodel.config._attn_implementation = ['flash_attention_2', 'eager', 'sdpa'][2]\n\n# Test the model\ntext = \"The capital of France is [MASK].\"\ninputs = tokenizer(text, return_tensors=\"pt\").to(\"cuda\")\noutputs = model(**inputs)\n```\n\n---\n\n## Error Location\n\nAfter investigating the source code, the error occurs at **line 180** in the following file:  \nðŸ”— [Transformers GitHub - ModernBERT](https://github.com/huggingface/transformers/blob/main/src/transformers/models/modernbert/modeling_modernbert.py#L180)\n\n![Source Code Screenshot](https://cdn-uploads.huggingface.co/production/uploads/63ad8d4120176b2d2162112b/v3L77EW6s98T3qCmpmFrw.png)\n*Figure 3: Source code snippet highlighting the line where the error occurs in ModernBertAttention.*\n\n---\n\n## Thoughts and Next Steps\n\nThis issue restricts ModernBERT's usability for those without Ampere GPUs, limiting **accessibility** to **Open Source models**. It would be helpful if:\n\n- The model could gracefully fall back to **SDPA** or **Eager Attention** without errors.\n- The **position_ids** issue in `ModernBertUnpaddedRotaryEmbedding` was addressed.\n\nI hope this post helps locate the issue and contributes to making **Open Source AI more accessible**! ðŸš€\n\n\n### Who can help?\n\n@tomaarsen @ArthurZucker \n\n### Information\n\n- [x] The official example scripts\n- [ ] My own modified scripts\n\n### Tasks\n\n- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\n- [ ] My own task or dataset (give details below)\n\n### Reproduction\n\nI have provided the full reproduction code in the description \n\n### Expected behavior\n\nThe expected behavior was to get the logits and not an Error :)",
    "state": "closed",
    "created_at": "2025-01-24T14:52:09+00:00",
    "closed_at": "2025-01-24T14:59:59+00:00",
    "updated_at": "2025-01-24T15:00:01+00:00",
    "author": "benhachy",
    "author_type": "User",
    "comments_count": 1,
    "reactions_count": 0,
    "labels": "bug",
    "milestone": null,
    "closed_by": "benhachy",
    "resolution_time_hours": 0.13055555555555556,
    "first_comments": [
      {
        "author": "benhachy",
        "created_at": "2025-01-24T14:59:59+00:00",
        "body": "Thanks to Tom Aarsen, the error has been resolved. Check out the discussion here if you're looking to run ModernBert on GPUs older than the Ampere series:\n\nhttps://huggingface.co/answerdotai/ModernBERT-base/discussions/56"
      }
    ],
    "url": "https://github.com/huggingface/transformers/issues/35879"
  },
  {
    "repository": "huggingface/transformers",
    "issue_number": 35862,
    "title": "Adding special tokens by default",
    "body": "### System Info\n\n- `transformers` version: 4.47.1\n- Platform: Linux-6.1.85+-x86_64-with-glibc2.35\n- Python version: 3.11.11\n- Huggingface_hub version: 0.27.1\n- Safetensors version: 0.5.2\n- Accelerate version: 1.2.1\n- Accelerate config: \tnot found\n- PyTorch version (GPU?): 2.5.1+cu121 (True)\n- Tensorflow version (GPU?): 2.17.1 (True)\n- Flax version (CPU?/GPU?/TPU?): 0.10.2 (gpu)\n- Jax version: 0.4.33\n- JaxLib version: 0.4.33\n- Using distributed or parallel set-up in script?: False\n- Using GPU in script?: False\n- GPU type: Tesla T4\n\n### Who can help?\n\n@ArthurZucker \n\n### Information\n\n- [ ] The official example scripts\n- [ ] My own modified scripts\n\n### Tasks\n\n- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\n- [ ] My own task or dataset (give details below)\n\n### Reproduction\n\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\ncheckpoint = \"HuggingFaceTB/SmolLM2-135M\" # same behavior with gpt2\ndevice = \"cuda\" # for GPU usage or \"cpu\" for CPU usage\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\nmodel = AutoModelForCausalLM.from_pretrained(checkpoint).to(device)\ninputs = tokenizer(\"Gravity is\", return_tensors=\"pt\", return_special_tokens_mask=True)\n```\noutputs:\n```cmd\n{'input_ids': tensor([[22007,  6463,   314]]), 'attention_mask': tensor([[1, 1, 1]]), 'special_tokens_mask': tensor([[0, 0, 0]])}\n```\n\n### Expected behavior\n\nI'd expect the tokenized to add the `<|endoftext|>` / `bos` by default, setting `add_special_tokens=True` doesn't change the bahavior as it's already the default. Am I missing something?",
    "state": "closed",
    "created_at": "2025-01-23T21:33:32+00:00",
    "closed_at": "2025-01-26T13:55:50+00:00",
    "updated_at": "2025-01-26T13:55:52+00:00",
    "author": "MostHumble",
    "author_type": "User",
    "comments_count": 1,
    "reactions_count": 0,
    "labels": "bug",
    "milestone": null,
    "closed_by": "MostHumble",
    "resolution_time_hours": 64.37166666666667,
    "first_comments": [
      {
        "author": "MostHumble",
        "created_at": "2025-01-26T13:55:50+00:00",
        "body": "Seems like this has been a design choice [#3311](https://github.com/huggingface/transformers/issues/3311#issuecomment-696129801)"
      }
    ],
    "url": "https://github.com/huggingface/transformers/issues/35862"
  },
  {
    "repository": "huggingface/transformers",
    "issue_number": 35856,
    "title": "DPO LoRA loss incorrect with Gradient Accumulation in 4.48.1",
    "body": "### System Info\n\n- `transformers` version: 4.48.1\n- Platform: Linux-6.8.0-49-generic-x86_64-with-glibc2.35\n- Python version: 3.11.10\n- Huggingface_hub version: 0.26.3\n- Safetensors version: 0.4.5\n- Accelerate version: 1.2.1\n- Accelerate config:    not found\n- PyTorch version (GPU?): 2.5.1+cu124 (True)\n- Tensorflow version (GPU?): not installed (NA)\n- Flax version (CPU?/GPU?/TPU?): not installed (NA)\n- Jax version: not installed\n- JaxLib version: not installed\n- Using distributed or parallel set-up in script?: <fill in>\n- Using GPU in script?: <fill in>\n- GPU type: NVIDIA A40\n\n\n### Who can help?\n\n\nSee loss charts https://wandb.ai/axolotl-ai/transformers-4_48_1\n\nLikely caused by https://github.com/huggingface/transformers/pull/35651\n\n### Information\n\n- [x] The official example scripts\n- [ ] My own modified scripts\n\n### Tasks\n\n- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\n- [ ] My own task or dataset (give details below)\n\n### Reproduction\n\nchecking out the latest TRL and running:\n```bash\npython trl/scripts/dpo.py     --dataset_name trl-lib/ultrafeedback_binarized     --model_name_or_path Qwen/Qwen2-0.5B-Instruct     --learning_rate 5.0e-6     --num_train_epochs 1     --per_device_train_batch_size 2     --gradient_accumulation_steps 8     --gradient_checkpointing     --logging_steps 1     --eval_strategy steps     --eval_steps 50     --output_dir Qwen2-0.5B-DPO     --no_remove_unused_columns     --use_peft     --lora_r 32     --lora_alpha 16 --torch_dtype bfloat16 --bf16  --attn_implementation flash_attention_2 --report_to wandb\n```\ninstalling older versions of transformers up to 4.48.0 fixes the loss values.\nusing gradient accumulation of 1 on 4.48.1 seems fine.\n\n### Expected behavior\n\nthe loss values using ga=8 should have similar loss values of ga=1\n\n<img width=\"460\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/92b3c2cd-2439-48a4-95e6-fdaeb9ba4fd1\" />",
    "state": "closed",
    "created_at": "2025-01-23T15:12:53+00:00",
    "closed_at": "2025-01-23T22:22:35+00:00",
    "updated_at": "2025-01-24T00:35:24+00:00",
    "author": "winglian",
    "author_type": "User",
    "comments_count": 5,
    "reactions_count": 0,
    "labels": "bug",
    "milestone": null,
    "closed_by": "winglian",
    "resolution_time_hours": 7.161666666666667,
    "first_comments": [
      {
        "author": "Rocketknight1",
        "created_at": "2025-01-23T15:34:15+00:00",
        "body": "hi @winglian, can you check if this is fixed on `main`? We'll be releasing a patch in a couple of days to fix that issue"
      },
      {
        "author": "winglian",
        "created_at": "2025-01-23T15:37:19+00:00",
        "body": "also broken on main still. Looks like it's maybe a trl issue."
      },
      {
        "author": "farzadab",
        "created_at": "2025-01-23T22:16:03+00:00",
        "body": "I am using `transformers` directly without `trl` and I'm still seeing a similar issue, so it's not specific to `trl`.\nThe reported `train/loss` is not divided correctly by `gradient_accumulation_steps`.\n\nI know the loss was fine on `4.44`, but I have tried the following versions and they are all identically incorrect: `4.47.1`, `4.48.0`, `4.48.1`.\nThe problematic PR (https://github.com/huggingface/transformers/pull/35651) mentioned at the top is only present in `4.48.1`, so it can't have been the cause of the issue.\n\nIn the following experiment, the 3 runs are approximately identical except that they use `gradient_accumulation_steps` of 1, 4, and 6. If I divide the train_loss by gradient_accumulation_steps, the issue is solved.\n![Image](https://github.com/user-attachments/assets/a340bafb-33d5-4b1a-a51d-f3acadcafec4)\n\nFor reference, I am using FSDP. Let me know if you need me to run any more experiments."
      },
      {
        "author": "winglian",
        "created_at": "2025-01-23T22:22:09+00:00",
        "body": "@farzadab see the merged trl fix here https://github.com/huggingface/trl/pull/2615/files"
      },
      {
        "author": "farzadab",
        "created_at": "2025-01-24T00:34:46+00:00",
        "body": "Thanks Wing, I can confirm that does solve the issue on my side too!\n\nI still feel like the cause of the issue is just sloppy design on the `transformers` side, not something that should be patched-up downstream. Is it not possible for someone in TRL wanting to handle loss_kwargs (whatever that is) themselves?\n\nSpecifically referring to this line:\n![Image](https://github.com/user-attachments/assets/b26dfac3-d765-49c1-b014-a24b23c02e87)\n\nIt would've been less bug-prone to set `PreTrainedModel.accepts_loss_kwargs = False` and then set it to True only on the models that do use it."
      }
    ],
    "url": "https://github.com/huggingface/transformers/issues/35856"
  },
  {
    "repository": "huggingface/transformers",
    "issue_number": 35853,
    "title": "How to load a model directly into the GPU memoryï¼Ÿ",
    "body": "I have enough GPU memory, but not enough CPU memory.When I use the \n\"from_pretrained\" function, the program gets killed due to insufficient memory.",
    "state": "closed",
    "created_at": "2025-01-23T09:47:04+00:00",
    "closed_at": "2025-01-23T15:18:59+00:00",
    "updated_at": "2025-01-23T15:19:01+00:00",
    "author": "LiBai531",
    "author_type": "User",
    "comments_count": 1,
    "reactions_count": 0,
    "labels": "",
    "milestone": null,
    "closed_by": "Rocketknight1",
    "resolution_time_hours": 5.531944444444444,
    "first_comments": [
      {
        "author": "Rocketknight1",
        "created_at": "2025-01-23T15:18:59+00:00",
        "body": "Hi @LiBai531, try `device_map=\"auto\"` when loading. Closing since this isn't a bug, though!"
      }
    ],
    "url": "https://github.com/huggingface/transformers/issues/35853"
  },
  {
    "repository": "huggingface/transformers",
    "issue_number": 35839,
    "title": "is it expected that the same token has two different token ids in T5TokenizerFast",
    "body": "I am using the T5TokenizerFast from \"black-forest-labs/FLUX.1-dev\"\n```\ntokenizer_two = T5TokenizerFast.from_pretrained(\n    \"black-forest-labs/FLUX.1-dev\",\n    subfolder=\"tokenizer_2\",\n)\n\nprint(tokenizer_two.encode(\"person\"))\nOutput:\n[568, 1]\n\nprint(tokenizer_two.convert_tokens_to_ids(\"person\"))\nOutput:\n6075\n\nprint(tokenizer_two.decode([6075]))\nOutput:\nperson\n\nprint(tokenizer_two.decode([568]))\nOutput:\nperson\n\ntext_inputs = tokenizer_two(\n    \"A photo of a person.\",\n    padding=\"max_length\",\n    max_length=tokenizer_two.model_max_length,\n    truncation=True,\n    return_tensors=\"pt\",\n)\nprint(text_inputs.input_ids) \n\nOutput:\ntensor([  71, 1202,   13,    3,    9,  568,    5,    1,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0])\n\n```\n\nSince I am new to this tokenizer, is it expected that the token \"person\" has two different token id 568 and 6075? Thanks.\n\ntransformers version: 4.46.3",
    "state": "closed",
    "created_at": "2025-01-22T13:01:19+00:00",
    "closed_at": "2025-01-22T17:21:07+00:00",
    "updated_at": "2025-01-22T17:21:08+00:00",
    "author": "garychan22",
    "author_type": "User",
    "comments_count": 1,
    "reactions_count": 0,
    "labels": "",
    "milestone": null,
    "closed_by": "Rocketknight1",
    "resolution_time_hours": 4.33,
    "first_comments": [
      {
        "author": "Rocketknight1",
        "created_at": "2025-01-22T17:21:07+00:00",
        "body": "Hi @garychan22, this is usually caused by one of the token variants including whitespace or a sentencepiece underline, which might be masked by the `decode()` method. Try finding those two tokens in `tokenizer.vocab`. It is not a bug!"
      }
    ],
    "url": "https://github.com/huggingface/transformers/issues/35839"
  },
  {
    "repository": "huggingface/transformers",
    "issue_number": 35811,
    "title": "Adding Audio-MAE Model for Underwater Audio to Hugging Face Transformers Library",
    "body": "### Feature request\n\nInclude the Audio-MAE model pretrained on underwater audio data in the Hugging Face Transformers library.\n\n### Motivation\n\nI have trained the Audio-MAE model on 30M underwater sound samples from the Orcasound dataset using 8 H100 GPUs for over a month. While exploring the Hugging Face Transformers library, I noticed that there is an implementation for Vision Transformer (ViT) MAE, but no implementation for Audio-MAE. Given the growing interest in underwater audio-based models, I believe that including a pretrained Audio-MAE model trained on 30 million underwater sound samples could be valuable for a wide range of audio-related applications, particularly in underwater acoustics.\n\n### Your contribution\n\nI would like to inquire whether a pretrained Audio-MAE model, with weights trained on 30 million underwater sound samples, would be considered for inclusion in the Transformers library. If needed, I am happy to contribute the model or further assist with its integration.",
    "state": "closed",
    "created_at": "2025-01-21T10:43:35+00:00",
    "closed_at": "2025-01-22T04:49:07+00:00",
    "updated_at": "2025-01-22T04:49:07+00:00",
    "author": "Ahmed-Telili",
    "author_type": "User",
    "comments_count": 1,
    "reactions_count": 0,
    "labels": "Feature request",
    "milestone": null,
    "closed_by": "Ahmed-Telili",
    "resolution_time_hours": 18.092222222222222,
    "first_comments": [
      {
        "author": "Rocketknight1",
        "created_at": "2025-01-21T14:31:15+00:00",
        "body": "Hi @Ahmed-Telili, thanks for the message, and the model sounds interesting!\n\nWe generally advise users that they don't really need our permission to upload models - you can create a \"custom code\" model and upload it to a Transformers repo even if the main library doesn't support your modeling code. See the guide [here](https://huggingface.co/docs/transformers/en/custom_models)"
      }
    ],
    "url": "https://github.com/huggingface/transformers/issues/35811"
  },
  {
    "repository": "huggingface/transformers",
    "issue_number": 35806,
    "title": "Errors when returning the tensors with PyTorch using AutoImageProcessor",
    "body": "### System Info\n\n- `transformers` version: 4.48.1\n- Platform: macOS-15.2-arm64-arm-64bit\n- Python version: 3.12.8\n- Huggingface_hub version: 0.27.0\n- Safetensors version: 0.4.5\n- Accelerate version: not installed\n- Accelerate config: not found\n- PyTorch version (GPU?): 2.3.1.post100 (False)\n- Tensorflow version (GPU?): not installed (NA)\n- Flax version (CPU?/GPU?/TPU?): not installed (NA)\n- Jax version: not installed\n- JaxLib version: not installed\n- Using distributed or parallel set-up in script?: No\n\n### Who can help?\n\n_No response_\n\n### Information\n\n- [ ] The official example scripts\n- [x] My own modified scripts\n\n### Tasks\n\n- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\n- [x] My own task or dataset (give details below)\n\n### Reproduction\n\nI am using the class `AutoImageProcessor` to process images before putting to any Vision Transformer models. Here it shows how the error can be reproduced:\n\n```\nfrom PIL import Image\nfrom transformers import AutoImageProcessor\nimport requests\n\n# Load the image processor\nimage_processor = AutoImageProcessor.from_pretrained('facebook/hiera-large-224-hf')\n\n# Load the image ( two cats, online)\nimage = Image.open(requests.get('http://images.cocodataset.org/val2017/000000039769.jpg', stream=True).raw)\n\n# pass through if I need a return as an numpy.ndarray object\nparsed_image_np = image_processor(images=image)\n# raise an error if I need a return in as an PyTorch.Tensor object\nparsed_image_pt = image_processor(images=image, return_tensors='pt')\n```\n\nThe stacktrace is shown below:\n```\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nFile /opt/anaconda3/envs/cv/lib/python3.12/site-packages/transformers/feature_extraction_utils.py:193, in BatchFeature.convert_to_tensors(self, tensor_type)\n    192 if not is_tensor(value):\n--> 193     tensor = as_tensor(value)\n    195     self[key] = tensor\n\nFile /opt/anaconda3/envs/cv/lib/python3.12/site-packages/transformers/feature_extraction_utils.py:150, in BatchFeature._get_is_as_tensor_fns.<locals>.as_tensor(value)\n    149 if isinstance(value, np.ndarray):\n--> 150     return torch.from_numpy(value)\n    151 else:\n\nTypeError: expected np.ndarray (got numpy.ndarray)\n\nDuring handling of the above exception, another exception occurred:\n\nValueError                                Traceback (most recent call last)\nCell In[10], line 1\n----> 1 image_processor(images=image, return_tensors='pt')\n\nFile /opt/anaconda3/envs/cv/lib/python3.12/site-packages/transformers/image_processing_utils.py:41, in BaseImageProcessor.__call__(self, images, **kwargs)\n     39 def __call__(self, images, **kwargs) -> BatchFeature:\n     40     \"\"\"Preprocess an image or a batch of images.\"\"\"\n---> 41     return self.preprocess(images, **kwargs)\n\nFile /opt/anaconda3/envs/cv/lib/python3.12/site-packages/transformers/utils/generic.py:854, in filter_out_non_signature_kwargs.<locals>.decorator.<locals>.wrapper(*args, **kwargs)\n    845         cls_prefix = \"\"\n    847     warnings.warn(\n    848         f\"The following named arguments are not valid for `{cls_prefix}{func.__name__}`\"\n    849         f\" and were ignored: {invalid_kwargs_names}\",\n    850         UserWarning,\n    851         stacklevel=2,\n    852     )\n--> 854 return func(*args, **valid_kwargs)\n\nFile /opt/anaconda3/envs/cv/lib/python3.12/site-packages/transformers/models/bit/image_processing_bit.py:321, in BitImageProcessor.preprocess(self, images, do_resize, size, resample, do_center_crop, crop_size, do_rescale, rescale_factor, do_normalize, image_mean, image_std, do_convert_rgb, return_tensors, data_format, input_data_format)\n    315 images = [\n    316     to_channel_dimension_format(image, data_format, input_channel_dim=input_data_format)\n    317     for image in all_images\n    318 ]\n    320 data = {\"pixel_values\": images}\n--> 321 return BatchFeature(data=data, tensor_type=return_tensors)\n\nFile /opt/anaconda3/envs/cv/lib/python3.12/site-packages/transformers/feature_extraction_utils.py:79, in BatchFeature.__init__(self, data, tensor_type)\n     77 def __init__(self, data: Optional[Dict[str, Any]] = None, tensor_type: Union[None, str, TensorType] = None):\n     78     super().__init__(data)\n---> 79     self.convert_to_tensors(tensor_type=tensor_type)\n\nFile /opt/anaconda3/envs/cv/lib/python3.12/site-packages/transformers/feature_extraction_utils.py:199, in BatchFeature.convert_to_tensors(self, tensor_type)\n    197         if key == \"overflowing_values\":\n    198             raise ValueError(\"Unable to create tensor returning overflowing values of different lengths. \")\n--> 199         raise ValueError(\n    200             \"Unable to create tensor, you should probably activate padding \"\n    201             \"with 'padding=True' to have batched tensors with the same length.\"\n    202         )\n    204 return self\n\nValueError: Unable to create tensor, you should probably activate padding with 'padding=True' to have batched tensors with the same length.\n```\n\nHowever, even if I put `padding=True` while processing the image, the error still occurs.\n\n### Expected behavior\n\nI would expect a dictionary would be returned by the last line (`parsed_image_pt = image_processor(images=image, return_tensors='pt')`)",
    "state": "closed",
    "created_at": "2025-01-21T05:33:23+00:00",
    "closed_at": "2025-01-21T15:35:38+00:00",
    "updated_at": "2025-01-21T15:35:40+00:00",
    "author": "stephenhky",
    "author_type": "User",
    "comments_count": 3,
    "reactions_count": 0,
    "labels": "bug",
    "milestone": null,
    "closed_by": "zucchini-nlp",
    "resolution_time_hours": 10.0375,
    "first_comments": [
      {
        "author": "zucchini-nlp",
        "created_at": "2025-01-21T09:01:19+00:00",
        "body": "@stephenhky hey!\n\nThe code doesn't throw an error for me in v4.48.1. From the stacktrace `TypeError: expected np.ndarray (got numpy.ndarray)`, seems like a version mismatch between numpy and torch. Can you try to update your `numpy`?"
      },
      {
        "author": "stephenhky",
        "created_at": "2025-01-21T14:38:47+00:00",
        "body": "@zucchini-nlp Thanks. I updated my `numpy`, and it is working now!"
      },
      {
        "author": "zucchini-nlp",
        "created_at": "2025-01-21T15:35:38+00:00",
        "body": "Closing as resolved then :)"
      }
    ],
    "url": "https://github.com/huggingface/transformers/issues/35806"
  },
  {
    "repository": "huggingface/transformers",
    "issue_number": 35803,
    "title": "Handle python 3.11 for huggingface[\"video\"] and dependencies",
    "body": "### Feature request\n\nCurrently, the only thing preventing the installation of:\n- huggingface[dev]\n- huggingface[all]\n- huggingface[video]\n\nOn python 3.11 is the `av==9.2.0` requirement. That requirement cites issue with `av==10.0.0` as reason for pinning this version, however the latest version is now `14.0.1`, it would be nice to revisit that pin, especially now that it forces users on python 3.10.\n\n### Motivation\n\nIt would allow contributors and users of huggingface[video] to use python 3.11.\n\n### Your contribution\n\nI've looked into the issue with av==9.2.0, the original failure is Cython generating bad C:\n```\n      av/logging.pyx:351:28: Cannot assign type 'void (void *, int, const char *, va_list) except * nogil' to 'av_log_callback' (alias of 'void (*)(void *, int, const char *, va_list) noexcept nogil'). Exception values are incompatible. Suggest adding 'noexcept' to the type of 'log_callback'.\n```\n\nThis can be circumvented by forcing an earlier version of Cython:\n```\n$ echo \"cython<3.0\" >> c.txt\n$ PIP_CONSTRAINT=c.txt pip install av==9.2.0\n```\nBut that leads to this error:\n`src/av/stream.c:2999:29: error: â€˜struct AVStreamâ€™ has no member named â€˜codecâ€™`\nWhich, looking into it, means that ffmpeg is too recent for this av code, that's where I give up because I realised that asking users to downgrade ffmpeg was probably not the way to go.\n\nI'm attempting to run the tests on python 3.10 w/ av==9.2.0 vs python 3.11 w/ av==14.0.1, but `make test` takes absolute ages even on 32 cores and 64Gb RAM and it just seems to fail randomly so clearly I am under equiped for this undertaking. Any tips to have a better times with running tests? Is there a way to only run those using `av` besides `grep \"import av\"`?",
    "state": "closed",
    "created_at": "2025-01-20T22:51:37+00:00",
    "closed_at": "2025-01-21T20:16:19+00:00",
    "updated_at": "2025-01-21T20:16:19+00:00",
    "author": "CalOmnie",
    "author_type": "User",
    "comments_count": 6,
    "reactions_count": 0,
    "labels": "Feature request",
    "milestone": null,
    "closed_by": "Rocketknight1",
    "resolution_time_hours": 21.411666666666665,
    "first_comments": [
      {
        "author": "Rocketknight1",
        "created_at": "2025-01-21T14:20:53+00:00",
        "body": "Hi @CalOmnie, you can call individual tests with `pytest tests/.../test_file.py -k 'test_name'`. Once you identify a failing/passing test, you can just check that specific test, rather than running everything with `make test`, which will definitely take hours!"
      },
      {
        "author": "CalOmnie",
        "created_at": "2025-01-21T14:50:46+00:00",
        "body": "Hey @Rocketknight1 , I was aware of that option, but my issue was \"how do I test every files that import `av`?\" I can get a list of modules importing it with `grep -RIn \"import av\"`, but then I have to repeat that for every result to capture the modules importing the modules importing `av` etc. Is there a better approach?"
      },
      {
        "author": "Rocketknight1",
        "created_at": "2025-01-21T15:08:34+00:00",
        "body": "Hmmn, I don't think we have a clean way to do that. However, for framework incompatibility issues like this, you probably don't need to run **every** test that uses `av`, right? I assumed that the Cython incompatibility would occur in most cases when `av` is used, so you don't need complete coverage to tell if it's been resolved or not"
      },
      {
        "author": "CalOmnie",
        "created_at": "2025-01-21T15:11:03+00:00",
        "body": "Alright, so if I understand correctly I can just select an arbitrary subset of functions that use `av` and only run the tests for those?"
      },
      {
        "author": "Rocketknight1",
        "created_at": "2025-01-21T15:37:46+00:00",
        "body": "Yes, I think so! Once you have a proposed solution, we can a full set of slow tests on it later"
      }
    ],
    "url": "https://github.com/huggingface/transformers/issues/35803"
  },
  {
    "repository": "huggingface/transformers",
    "issue_number": 35787,
    "title": "Significant Increase in Training Loss after Upgrading from Transformers 4.47.1 to 4.48.0",
    "body": "### System Info\n\n- huggingface_hub version: 0.27.1\n- Platform: Linux-5.15.0-1029-nvidia-x86_64-with-glibc2.35\n- Python version: 3.10.12\n- Running in iPython ?: No\n- Running in notebook ?: No\n- Running in Google Colab ?: No\n- Running in Google Colab Enterprise ?: No\n- Token path?: /raid/MLP/. cache/huggingface/token\n- Has saved token ?: True\n- Who am I ?: mjkmain\n- Configured git credential helpers: store\n- FastAI: N/A\n- Tensorflow: N/A\n- Torch: 2.4.1\n- Jinja2: 3.1.4\n- Graphviz: N/A\n- keras: N/A\n- Pydot: N/A\n- Pillow: 11.0.0\n- hf_transfer: 0.1.9\n- gradio: 5.9.1\n- tensorboard: N/A\n- numpy: 1.26.4\n- pydantic: 2.10.4\n- aiohttp: 3.11.9\n- ENDPOINT: https:/ /huggingface.co\n- HF_HUB_CACHE: /raid/MLP/â€¢ cache/huggingface/hub\n- HF_ASSETS_CACHE: /raid/MLP/. cache/huggingface/assets\n- HF_TOKEN_PATH: /raid/MLP/. cache/huggingface/token\n- HF_STORED_TOKENS_PATH: /raid/MLP/. cache/huggingface/stored_tokens\n- HF_HUB_OFFLINE: False\n- HF_HUB_DISABLE_TELEMETRY: False\n- HF_HUB_DISABLE_PROGRESS_BARS: None\n- HF_HUB_DISABLE_SYMLINKS_WARNING: False\n- HF_HUB_DISABLE_EXPERIMENTAL_WARNING: False\n- HF_HUB_DISABLE_IMPLICIT_TOKEN: False\n- HF_HUB_ENABLE_HF_TRANSFER: False\n- HF_HUB_ETAG_TIMEOUT: 10\n- HF_HUB_DOWNLOAD_TIMEOUT: 10\n\n### Who can help?\n\n@ArthurZucker, @muellerzr \n\n## Issue Description\nAfter upgrading the Transformers library from version `4.47.1` to `4.48.0`, Iâ€™ve observed a drastic increase in loss values during training. Under the same training script and configurations, the loss values in `4.47.1` are around 2.48 - 2.54, while in `4.48.0` they suddenly jump to 40+ (and sometimes even higher).\n\nBelow are sample logs from the first few training steps. The only change is the Transformers version; everything else remains identical:\n\n- ### Transformers v4.47.1:\n```\n{'loss': 2.4859, 'grad_norm': 1.453125, 'learning_rate': 1.0660980810234542e-07, 'epoch': 0.0}\n{'loss': 2.5428, 'grad_norm': 1.3359375, 'learning_rate': 2.1321961620469084e-07, 'epoch': 0.0}\n{'loss': 2.5043, 'grad_norm': 1.234375, 'learning_rate': 3.1982942430703626e-07, 'epoch': 0.0}\n{'loss': 2.4861, 'grad_norm': 1.3203125, 'learning_rate': 4.264392324093817e-07, 'epoch': 0.0}\n```\n\n- ### Transformers v4.48.0:\n```\n{'loss': 40.6004, 'grad_norm': 34.5, 'learning_rate': 1.0660980810234542e-07, 'epoch': 0.0}\n{'loss': 42.416, 'grad_norm': 34.0, 'learning_rate': 2.1321961620469084e-07, 'epoch': 0.0}\n{'loss': 41.237, 'grad_norm': 32.5, 'learning_rate': 3.1982942430703626e-07, 'epoch': 0.0}\n{'loss': 42.2229, 'grad_norm': 39.0, 'learning_rate': 4.264392324093817e-07, 'epoch': 0.0}\n```\n\nIn the same setup using `4.48.0`, if I change only the gradient accumulation steps from 16 to 1, the loss behaves similarly to `4.47.1`, as shown below:\n```\n{'loss': 2.3792, 'grad_norm': 8.25, 'learning_rate': 6.666666666666668e-09, 'epoch': 0.0}\n{'loss': 2.4288, 'grad_norm': 5.96875, 'learning_rate': 1.3333333333333335e-08, 'epoch': 0.0}\n{'loss': 2.5774, 'grad_norm': 6.1875, 'learning_rate': 2e-08, 'epoch': 0.0}\n{'loss': 2.4495, 'grad_norm': 6.96875, 'learning_rate': 2.666666666666667e-08, 'epoch': 0.0}\n{'loss': 2.8155, 'grad_norm': 7.53125, 'learning_rate': 3.3333333333333334e-08, 'epoch': 0.0}\n```\n\nAny insights into changes between 4.47.1 and 4.48.0 that might cause this behavior?\n\nThank you for your time, and I appreciate any help or pointers to relevant changes or fixes!\n\n\n\n\n\n\n\n\n### Information\n\n- [ ] The official example scripts\n- [x] My own modified scripts\n\n### Tasks\n\n- [x] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\n- [ ] My own task or dataset (give details below)\n\n### Reproduction\n\n## Environment\n\n- Transformers versions tested: 4.47.1 (good), 4.48.0 (issue)\n- Tested models: meta-llama/Llama-3.2-1B\n- Batch size: 1\n- Optimizer: adamw_bnb_8bit (from bitsandbytes)\n- Gradient checkpointing: True\n- **Gradient accumulation steps: 16 (issue observed); 1 (issue disappears)**\n- Number of devices: 8 (distributed training)\n\n## Steps to Reproduce\n\nInstall Transformers `4.47.1` and run the training script with the above parameters (gradient accumulation steps = 16) â€” observe normal loss values around 2-3.\nUpgrade to Transformers `4.48.0` (no other changes in code or environment) and rerun the same training script (gradient accumulation steps = 16) â€” notice a large increase in loss values (40+).\nStill using Transformers `4.48.0`, change gradient accumulation steps to 1 â€” observe that loss now returns to normal levels (around 2-3).\n\n### Expected behavior\n\nLoss values should remain consistent (as in 4.47.1) if there are no major changes in hyperparameters, data, or environment aside from the Transformers library version. ",
    "state": "closed",
    "created_at": "2025-01-20T10:28:01+00:00",
    "closed_at": "2025-01-23T02:29:11+00:00",
    "updated_at": "2025-01-23T02:29:11+00:00",
    "author": "mjkmain",
    "author_type": "User",
    "comments_count": 1,
    "reactions_count": 0,
    "labels": "bug",
    "milestone": null,
    "closed_by": "mjkmain",
    "resolution_time_hours": 64.01944444444445,
    "first_comments": [
      {
        "author": "ArthurZucker",
        "created_at": "2025-01-20T14:30:53+00:00",
        "body": "Patch is coming today for #35651 which should fix this I hope"
      }
    ],
    "url": "https://github.com/huggingface/transformers/issues/35787"
  },
  {
    "repository": "huggingface/transformers",
    "issue_number": 35775,
    "title": "LLaVA-OneVision image features and image tokens mismatch",
    "body": "### System Info\n\n- `transformers` version: 4.48.0\n- Platform: Linux-5.15.0-1067-nvidia-x86_64-with-glibc2.35\n- Python version: 3.11.11\n- Huggingface_hub version: 0.27.1\n- Safetensors version: 0.5.2\n- Accelerate version: 1.2.1\n- Accelerate config:    - compute_environment: LOCAL_MACHINE\n        - distributed_type: FSDP\n        - mixed_precision: bf16\n        - use_cpu: False\n        - debug: False\n        - num_processes: 1\n        - machine_rank: 0\n        - num_machines: 1\n        - rdzv_backend: static\n        - same_network: True\n        - main_training_function: main\n        - enable_cpu_affinity: False\n        - fsdp_config: {'fsdp_activation_checkpointing': True, 'fsdp_auto_wrap_policy': 'TRANSFORMER_BASED_WRAP', 'fsdp_backward_prefetch': 'BACKWARD_PRE', 'fsdp_cpu_ram_efficient_loading': True, 'fsdp_forward_prefetch': False, 'fsdp_offload_params': False, 'fsdp_sharding_strategy': 'FULL_SHARD', 'fsdp_state_dict_type': 'SHARDED_STATE_DICT', 'fsdp_sync_module_states': True, 'fsdp_transformer_layer_cls_to_wrap': '', 'fsdp_use_orig_params': True}\n        - downcast_bf16: no\n        - tpu_use_cluster: False\n        - tpu_use_sudo: False\n        - tpu_env: []\n- PyTorch version (GPU?): 2.5.1 (True)\n- Tensorflow version (GPU?): not installed (NA)\n- Flax version (CPU?/GPU?/TPU?): not installed (NA)\n- Jax version: not installed\n- JaxLib version: not installed\n- Using distributed or parallel set-up in script?: False\n- Using GPU in script?: True\n- GPU type: NVIDIA H100 80GB HBM3\n\n### Who can help?\n\n@amyeroberts @qubvel @zucchini-nlp \n\n### Information\n\n- [ ] The official example scripts\n- [x] My own modified scripts\n\n### Tasks\n\n- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\n- [x] My own task or dataset (give details below)\n\n### Reproduction\n\n```python\nfrom transformers import AutoProcessor, LlavaOnevisionForConditionalGeneration\nfrom datasets import load_dataset\nimport torch\nprocessor = AutoProcessor.from_pretrained(\"llava-hf/llava-onevision-qwen2-0.5b-ov-hf\")\nmodel = LlavaOnevisionForConditionalGeneration.from_pretrained(\"llava-hf/llava-onevision-qwen2-0.5b-ov-hf\", torch_dtype=torch.float16, device_map=\"auto\")\ndataset = load_dataset(\"lmms-lab/docvqa\", 'DocVQA')\n\nd = dataset['test'][2482]\nquestion = d['question']\nimage = d['image']\nconversation = [\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"image\"},\n            {\"type\": \"text\", \"text\": question},\n        ],\n    },\n]\nprompt = processor.apply_chat_template(conversation, add_generation_prompt=True)\ninputs = processor(images=image, text=prompt, return_tensors=\"pt\").to(\"cuda\")\nwith torch.no_grad():\n    outputs = model(**inputs)\n```\n\nTraceback as follows:\n```\nTraceback (most recent call last):\n  File \"<stdin>\", line 2, in <module>\n  File \"/mnt/home/miniforge3/envs/vek/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/home/miniforge3/envs/vek/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/home/miniforge3/envs/vek/lib/python3.11/site-packages/transformers/models/llava_onevision/modeling_llava_onevision.py\", line 688, in forward\n    raise ValueError(\nValueError: Image features and image tokens do not match: tokens: 7332, features 7261\n```\n\n### Expected behavior\n\nExpected: Output correctly without errors.\n\nThis is a follow-up issue of https://github.com/huggingface/transformers/issues/34625, where the behavior is the same but for different reasons. The reproduction example is a slight modification of the one provided by @chchch0109.\n",
    "state": "closed",
    "created_at": "2025-01-19T17:13:41+00:00",
    "closed_at": "2025-01-24T08:10:28+00:00",
    "updated_at": "2025-01-24T08:10:28+00:00",
    "author": "sheryc",
    "author_type": "User",
    "comments_count": 2,
    "reactions_count": 0,
    "labels": "bug",
    "milestone": null,
    "closed_by": "zucchini-nlp",
    "resolution_time_hours": 110.94638888888889,
    "first_comments": [
      {
        "author": "sheryc",
        "created_at": "2025-01-19T21:19:15+00:00",
        "body": "I found the cause: the Processor and the model's vision token unpadding differs by a rounding function. Sometimes they give different results because of precision issues. I added the rounding function in LlavaOnevisionProcessor to match the behavior of the model. PR: https://github.com/huggingface/transformers/pull/35779."
      },
      {
        "author": "Happy-Corpse",
        "created_at": "2025-01-20T08:04:52+00:00",
        "body": "I got the same issue: ValueError: Image features and image tokens do not match: tokens: 4589, features 4588 when using Llava-v1.6-vicuna-7b-hf (llava-next model) and the transformers version is 4.47.0. I followed PR: #35779 and modified processing_llava_next.py shown in the following code. But it doesn't work.  \n\noriginal_aspect_ratio = width / height\ncurrent_aspect_ratio = current_width / current_height\nif original_aspect_ratio > current_aspect_ratio:\n    new_height = int(round(height * current_width / width, 7))\n    padding = (current_height - new_height) // 2\n    current_height -= padding * 2\nelse:\n    new_width = int(round(width * current_height / height, 7))\n    padding = (current_width - new_width) // 2\n    current_width -= padding * 2\n"
      }
    ],
    "url": "https://github.com/huggingface/transformers/issues/35775"
  },
  {
    "repository": "huggingface/transformers",
    "issue_number": 35772,
    "title": "transformers.image_transforms.normalize documents and checks for the wrong type for std and mean arguments",
    "body": "### System Info\n\n- `transformers` version: 4.49.0.dev0\n- Platform: Linux-6.8.0-51-generic-x86_64-with-glibc2.39\n- Python version: 3.11.11\n- Huggingface_hub version: 0.27.1\n- Safetensors version: 0.5.1\n- Accelerate version: 1.2.1\n- Accelerate config:    not found\n- PyTorch version (GPU?): 2.5.1+cu124 (True)\n- Tensorflow version (GPU?): not installed (NA)\n- Flax version (CPU?/GPU?/TPU?): not installed (NA)\n- Jax version: not installed\n- JaxLib version: not installed\n- Using distributed or parallel set-up in script?: Neither\n- Using GPU in script?: No\n- GPU type: NVIDIA GeForce GTX 1060 6GB\n\n### Who can help?\n\n_No response_\n\n### Information\n\n- [x] The official example scripts\n- [ ] My own modified scripts\n\n### Tasks\n\n- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\n- [ ] My own task or dataset (give details below)\n\n### Reproduction\n\n```\nimport numpy as np\nfrom transformers.image_transforms import normalize\n\nmock_img = np.random.rand(3, 10, 10)\n\n# Simulating getting the std and mean from text\nstd = map(float, \"1.5 0. 2.\".split())\nmean = map(float, \"1.5 0. 2.\".split())\n# According to the doc, this setup should work, but it does not since \"map\" does not implement `len`\n\nnormalize(mock_img, std, mean)\n```\n\n### Expected behavior\n\nUse the `Sequence` type instead of Iterable to let users know that `len` needs to be implemented.",
    "state": "closed",
    "created_at": "2025-01-19T12:45:56+00:00",
    "closed_at": "2025-01-20T15:00:47+00:00",
    "updated_at": "2025-01-20T15:00:47+00:00",
    "author": "CalOmnie",
    "author_type": "User",
    "comments_count": 2,
    "reactions_count": 0,
    "labels": "bug",
    "milestone": null,
    "closed_by": "Rocketknight1",
    "resolution_time_hours": 26.2475,
    "first_comments": [
      {
        "author": "CalOmnie",
        "created_at": "2025-01-19T12:52:12+00:00",
        "body": "Question: Am I following the right process to submit very small fix that do not impact the runtime (e.g. typing and documentations)? I noticed a few other mistypes while reading the code and I'd like to do it properly going forward."
      },
      {
        "author": "Rocketknight1",
        "created_at": "2025-01-20T15:00:16+00:00",
        "body": "Hi @CalOmnie, yes, this was good! For small fixes when the issue is clear, like bugs or obviously incorrect types, you can just submit a PR. We recommend opening an issue first when the change might be controversial or need discussion (for example, adding a whole new model class, or changing the behaviour of existing tools)"
      }
    ],
    "url": "https://github.com/huggingface/transformers/issues/35772"
  },
  {
    "repository": "huggingface/transformers",
    "issue_number": 35768,
    "title": "Aria processor does not work with images",
    "body": "### System Info\n\n- `transformers` version: 4.48.0\n- Platform: Linux-5.4.0-174-generic-x86_64-with-glibc2.31\n- Python version: 3.9.20\n- Huggingface_hub version: 0.26.2\n- Safetensors version: 0.4.5\n- Accelerate version: 1.0.1\n- Accelerate config:    not found\n- PyTorch version (GPU?): 2.5.1+cu124 (True)\n- Tensorflow version (GPU?): not installed (NA)\n- Flax version (CPU?/GPU?/TPU?): not installed (NA)\n- Jax version: not installed\n- JaxLib version: not installed\n- Using distributed or parallel set-up in script?: <fill in>\n- Using GPU in script?: <fill in>\n- GPU type: NVIDIA A10\n\n### Who can help?\n\n@zucchini-nlp @aymeric-roucher \n\n### Information\n\n- [ ] The official example scripts\n- [x] My own modified scripts\n\n### Tasks\n\n- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\n- [x] My own task or dataset (give details below)\n\n### Reproduction\n\n```py\n>>> import numpy as np\n>>> from transformers import AutoProcessor\n>>> processor = AutoProcessor.from_pretrained(\"rhymes-ai/Aria\")\n>>> processor(text=\"<|img|>\", images=[np.zeros((3, 224, 224))])\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"/home/cyrus/miniconda3/envs/vllm/lib/python3.9/site-packages/transformers/models/aria/processing_aria.py\", line 129, in __call__\n    sample = sample.replace(self.tokenizer.image_token, self.tokenizer.image_token * num_crops)\n  File \"/home/cyrus/miniconda3/envs/vllm/lib/python3.9/site-packages/transformers/tokenization_utils_base.py\", line 1108, in __getattr__\n    raise AttributeError(f\"{self.__class__.__name__} has no attribute {key}\")\nAttributeError: LlamaTokenizerFast has no attribute image_token\n```\n\nThe processor for Aria model doesn't work because of missing `image_token` attribute.\n\n### Expected behavior\n\nSimilar to other models, we should add `image_token` directly to the processor and use that instead of referring to `tokenizer.image_token`.",
    "state": "closed",
    "created_at": "2025-01-19T02:41:03+00:00",
    "closed_at": "2025-01-20T08:57:02+00:00",
    "updated_at": "2025-01-20T08:57:03+00:00",
    "author": "DarkLight1337",
    "author_type": "User",
    "comments_count": 4,
    "reactions_count": 0,
    "labels": "bug",
    "milestone": null,
    "closed_by": "DarkLight1337",
    "resolution_time_hours": 30.26638888888889,
    "first_comments": [
      {
        "author": "zucchini-nlp",
        "created_at": "2025-01-20T08:20:10+00:00",
        "body": "I'll take a look if the configs on hub side were updated, since the conversion script adds an `image_token` \n\nOke, I see this commit (https://huggingface.co/rhymes-ai/Aria/commit/3e774c984a6e0699a566d9c83b9d562df329a42e) updates the hub files to align with transformers. Feel free to close if the issue is resolved"
      },
      {
        "author": "DarkLight1337",
        "created_at": "2025-01-20T08:36:24+00:00",
        "body": "Can confirm it is fixed now. Is it preferred to have `image_token` in the processor or tokenizer instance?"
      },
      {
        "author": "zucchini-nlp",
        "created_at": "2025-01-20T08:53:59+00:00",
        "body": "@DarkLight1337 More preferred in tokenizer as we've been trying to add all multimodal special tokens there. Especially useful for models with bunch of special tokens to wrap each image grid by row and column\n\nBut I agree that currently not all models save it in tokenizer, the feature was introduced a couple releases ago"
      },
      {
        "author": "DarkLight1337",
        "created_at": "2025-01-20T08:57:02+00:00",
        "body": "Thanks for the info! I'll close this issue now."
      }
    ],
    "url": "https://github.com/huggingface/transformers/issues/35768"
  },
  {
    "repository": "huggingface/transformers",
    "issue_number": 35740,
    "title": "Giving dictionary to Audio Classification Pipeline overwrites dictionary data",
    "body": "### System Info\n\n- `transformers` version: 4.48.0\n- Platform: macOS-14.6-arm64-arm-64bit\n- Python version: 3.12.4\n- Huggingface_hub version: 0.27.1\n- Safetensors version: 0.5.2\n- Accelerate version: 1.2.1\n- Accelerate config: \tnot found\n- PyTorch version (GPU?): 2.5.1 (False)\n- Tensorflow version (GPU?): not installed (NA)\n- Flax version (CPU?/GPU?/TPU?): not installed (NA)\n- Jax version: not installed\n- JaxLib version: not installed\n- Using distributed or parallel set-up in script?: None/NA\n\n### Who can help?\n\n@Rocketknight1 \n\n### Information\n\n- [ ] The official example scripts\n- [x] My own modified scripts\n\n### Tasks\n\n- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\n- [x] My own task or dataset (give details below)\n\n### Reproduction\n\n```python\nfrom transformers import pipeline\nimport torch\nimport numpy as np\n\nmodel_name = 'audeering/wav2vec2-large-robust-12-ft-emotion-msp-dim'\n#model_name = 'pollner/distilhubert-finetuned-ravdess'\ntop_k = 5\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nclassification_pipeline = pipeline(\n                \"audio-classification\",\n                model=model_name,\n                # top_k=top_k, \n                # function_to_apply='none',\n                device=device,\n            )\n\n# dummy signal\nsampling_rate = 16000\nsignal = np.zeros((sampling_rate), dtype=np.float32)\n\ninput_signal = {\n    'array': signal,\n    'sampling_rate': sampling_rate\n}\nprint(input_signal)\nprint(classification_pipeline(input_signal))\nprint(input_signal)\n```\n\n### Expected behavior\n\nI would imagine, but could be wrong, that the internal logic should not be mutating inputs, especially because this occurs when inputting a dictionary but not when just inputting a numpy array.",
    "state": "closed",
    "created_at": "2025-01-16T20:32:13+00:00",
    "closed_at": "2025-01-17T15:41:57+00:00",
    "updated_at": "2025-01-17T15:42:21+00:00",
    "author": "wilke0818",
    "author_type": "User",
    "comments_count": 2,
    "reactions_count": 0,
    "labels": "bug",
    "milestone": null,
    "closed_by": "Rocketknight1",
    "resolution_time_hours": 19.162222222222223,
    "first_comments": [
      {
        "author": "Rocketknight1",
        "created_at": "2025-01-17T14:58:51+00:00",
        "body": "Yes, thank you for the report! A fix is open at #35754"
      },
      {
        "author": "Rocketknight1",
        "created_at": "2025-01-17T15:42:19+00:00",
        "body": "@wilke0818 a fix has been merged! Can you install the latest version from `main` and confirm that it fixes the issue for you?"
      }
    ],
    "url": "https://github.com/huggingface/transformers/issues/35740"
  },
  {
    "repository": "huggingface/transformers",
    "issue_number": 35723,
    "title": "Break point in Transformers V4.48.0 and python 3.9",
    "body": "### System Info\n\n- `transformers` version: 4.48.0\n- Platform: Linux-5.15.0-25-generic-x86_64-with-glibc2.35\n- Python version: 3.9.0\n- Huggingface_hub version: 0.27.1\n- Safetensors version: 0.5.2\n- Accelerate version: 1.2.1\n\n\n### Who can help?\n\n @gante  @Rocketknight1\n\n### Information\n\n- [ ] The official example scripts\n- [ ] My own modified scripts\n\n### Tasks\n\n- [x] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\n- [x] My own task or dataset (give details below)\n\n### Reproduction\n\n\nInstall Python 3.9\nInstall Transformers >= v4.48.0\ninstall fastchat accelerate \nI was trying to attach delta weights of Vicuna-13B to existing Llama 13B weights in local , for this the advised was below:\n```\npython3 -m fastchat.model.apply_delta \\\n    --base-model-path /path/to/llama-13b \\\n    --target-model-path /path/to/output/vicuna-13b \\\n    --delta-path lmsys/vicuna-13b-delta-v1.1\n```\ndoing this raised the following error:\n```\nRuntimeError: Failed to import transformers.generation.streamers because of the following error (look up to see its traceback):\nunsupported operand type(s) for |: 'type' and 'NoneType'\n```\n\n\n### Expected behavior\n\nWork in Python 3.9",
    "state": "closed",
    "created_at": "2025-01-16T07:59:20+00:00",
    "closed_at": "2025-01-16T15:10:01+00:00",
    "updated_at": "2025-01-16T15:10:01+00:00",
    "author": "prajeet26",
    "author_type": "User",
    "comments_count": 2,
    "reactions_count": 0,
    "labels": "bug",
    "milestone": null,
    "closed_by": "gante",
    "resolution_time_hours": 7.178055555555556,
    "first_comments": [
      {
        "author": "jianbohuang",
        "created_at": "2025-01-16T09:20:39+00:00",
        "body": "[watch this](https://github.com/huggingface/transformers/pull/35103)"
      },
      {
        "author": "gante",
        "created_at": "2025-01-16T11:35:34+00:00",
        "body": "#35725 fixes it :)"
      }
    ],
    "url": "https://github.com/huggingface/transformers/issues/35723"
  },
  {
    "repository": "huggingface/transformers",
    "issue_number": 35711,
    "title": "Gemma 2 crashes with CUDA error on long context input (again)",
    "body": "### System Info\n\n- `transformers` version: 4.48.0\n- Platform: Linux-4.18.0-553.6.1.el8.x86_64-x86_64-with-glibc2.35\n- Python version: 3.10.12\n- Huggingface_hub version: 0.27.1\n- Safetensors version: 0.4.5\n- Accelerate version: 1.2.1\n- Accelerate config: \tnot found\n- PyTorch version (GPU?): 2.5.0a0+e000cf0ad9.nv24.10 (True)\n- Tensorflow version (GPU?): not installed (NA)\n- Flax version (CPU?/GPU?/TPU?): not installed (NA)\n- Jax version: not installed\n- JaxLib version: not installed\n- Using distributed or parallel set-up in script?: <fill in>\n- Using GPU in script?: <fill in>\n- GPU type: Tesla V100-SXM2-32GB\n\n\n### Who can help?\n\n_No response_\n\n### Information\n\n- [ ] The official example scripts\n- [x] My own modified scripts\n\n### Tasks\n\n- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\n- [ ] My own task or dataset (give details below)\n\n### Reproduction\n\nWhen I try to generate text with long prompt using gemma-2-2b-it, it crashes with CUDA assertion error. Short code to reproduce the error:\n```\nimport os\nos.environ[\"CUDA_LAUNCH_BLOCKING\"]=\"1\"\n\nimport torch\nfrom transformers import pipeline\n\nmodel_name = \"google/gemma-2-2b-it\"\npipe = pipeline(\"text-generation\", model=model_name, device=\"cuda\", torch_dtype=torch.float16)\n\nmessages = [\n\t{\"role\": \"user\", \"content\": 1000*\"Tell me something interesting. \"}\n]\nprompt = pipe.tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\npipe(prompt, max_new_tokens=100, pad_token_id=pipe.tokenizer.eos_token_id, eos_token_id=[pipe.tokenizer.eos_token_id], do_sample=True)\n```\nThis prints assertion failures:\n\n> /opt/pytorch/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [8310,0,0], thread: [0,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n>/opt/pytorch/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [8310,0,0], thread: [1,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n>...\n>  File /usr/local/lib/python3.10/dist-packages/transformers/models/gemma2/modeling_gemma2.py:224 in forward\n>    key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n>\n>  File /usr/local/lib/python3.10/dist-packages/transformers/cache_utils.py:1721 in update\n>    return update_fn(\n>\n>  File /usr/local/lib/python3.10/dist-packages/transformers/cache_utils.py:1694 in _static_update\n>    k_out[:, :, cache_position] = key_states\n>\n>RuntimeError: CUDA error: device-side assert triggered\n>Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n\nThe problem seems to be that in [Gemma2Attention.forward](https://github.com/huggingface/transformers/blob/12dfd99007dd803b3dfab5fe2ec66260b8007a23/src/transformers/models/gemma2/modeling_gemma2.py#L223) when [HybridCache.update](https://github.com/huggingface/transformers/blob/12dfd99007dd803b3dfab5fe2ec66260b8007a23/src/transformers/cache_utils.py#L1709) is called, the \"sliding_window\" parameter is not forwarded in the cache_kwargs argument and therefore the _static_update method is called instead of the _sliding update.\n\nThis issue seem similar to #31848 but it suggest extracting sliding_window from cache_kwargs as currently done but the caller does not add this value to cache_kwargs.\n\nAlso, I thought this model has at least 8k context window though no clear indication on model card, and yet sliding_window size is 4096.\n\n\n### Expected behavior\n\nIf I replace\n```\n            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n```\nwith\n```\n            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position, \"sliding_window\": self.sliding_window}\n```\ntext generation works.\nIf someone familiar with the codebase says it's a legit fix, I'm happy to create a pull request but don't know the codebase well enough. @ArthurZucker, do you think it is a good fix?",
    "state": "closed",
    "created_at": "2025-01-15T14:32:07+00:00",
    "closed_at": "2025-01-17T09:00:25+00:00",
    "updated_at": "2025-01-17T09:00:26+00:00",
    "author": "akkiss",
    "author_type": "User",
    "comments_count": 3,
    "reactions_count": 0,
    "labels": "bug",
    "milestone": null,
    "closed_by": "akkiss",
    "resolution_time_hours": 42.471666666666664,
    "first_comments": [
      {
        "author": "ArthurZucker",
        "created_at": "2025-01-16T14:57:23+00:00",
        "body": "cc @Cyrilvallez who's working on fixing generation with hybrid cache! But your fix is most probably legit! ðŸ¤— "
      },
      {
        "author": "Cyrilvallez",
        "created_at": "2025-01-16T15:00:11+00:00",
        "body": "Indeed @akkiss your fix is legit, but it's not the only issue ðŸ¥² see https://github.com/huggingface/transformers/pull/35681 for details!"
      },
      {
        "author": "akkiss",
        "created_at": "2025-01-17T09:00:25+00:00",
        "body": "Great, I see #35681 contains this modification, can't wait for it to be merged ðŸ˜‰ "
      }
    ],
    "url": "https://github.com/huggingface/transformers/issues/35711"
  },
  {
    "repository": "huggingface/transformers",
    "issue_number": 35702,
    "title": "Training with Accelerate will crash if using TrainingArguments",
    "body": "### System Info\n\n- `transformers` version: 4.47.1\n- Platform: Linux-4.18.0-372.9.1.el8.x86_64-x86_64-with-glibc2.35\n- Python version: 3.11.10\n- Huggingface_hub version: 0.27.0\n- Safetensors version: 0.4.5\n- Accelerate version: 1.2.1\n- Accelerate config:    not found\n- PyTorch version (GPU?): 2.5.1+cu124 (True)\n- Tensorflow version (GPU?): 2.18.0 (True)\n- Flax version (CPU?/GPU?/TPU?): not installed (NA)\n- Jax version: not installed\n- JaxLib version: not installed\n- Using distributed or parallel set-up in script?: no\n- Using GPU in script?: no\n- GPU type: *edited out*\n\n### Who can help?\n\n@muellerzr @SunMarc\n\n### Information\n\n- [ ] The official example scripts\n- [x] My own modified scripts\n\n### Tasks\n\n- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\n- [ ] My own task or dataset (give details below)\n\n### Reproduction\n\nThe training code `main.py`:\n\n```python\n# run with:\n# accelerate launch main.py\n\nimport torch\nfrom transformers import TrainingArguments, HfArgumentParser\nfrom accelerate import Accelerator\naccelerator = Accelerator()\n\nif __name__ == \"__main__\":\n    parser = HfArgumentParser(TrainingArguments)\n    \n    # it does not matter whether the path exists or not\n    d = {\"output_dir\": \"/path/to/output/dir\"}\n    args = parser.parse_dict(d)\n\n    # it does not matter what the model is\n    model = torch.nn.Linear(4, 4)\n\n    # this causes crash\n    model = accelerator.prepare(model)\n```\n\nRun the following command to reproduce the bug:\n```shell\n# The configuration of `accelerate` does not affect the bug.\naccelerate launch main.py\n```\n\nThe stack trace:\n```\nTraceback (most recent call last):\n  File \"/root/main.py\", line 20, in <module>\n    model = accelerator.prepare(model)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/conda/lib/python3.11/site-packages/accelerate/accelerator.py\", line 1294, in prepare\n    if self.distributed_type == DistributedType.DEEPSPEED:\n       ^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/conda/lib/python3.11/site-packages/accelerate/accelerator.py\", line 568, in distributed_type\n    return self.state.distributed_type\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/conda/lib/python3.11/site-packages/accelerate/state.py\", line 1125, in __getattr__\n    raise AttributeError(\nAttributeError: `AcceleratorState` object has no attribute `distributed_type`. This happens if `AcceleratorState._reset_state()` was called and an `Accelerator` or `PartialState` was not reinitialized.\nTraceback (most recent call last):\n  File \"/opt/conda/bin/accelerate\", line 8, in <module>\n    sys.exit(main())\n             ^^^^^^\n  File \"/opt/conda/lib/python3.11/site-packages/accelerate/commands/accelerate_cli.py\", line 48, in main\n    args.func(args)\n  File \"/opt/conda/lib/python3.11/site-packages/accelerate/commands/launch.py\", line 1168, in launch_command\n    simple_launcher(args)\n  File \"/opt/conda/lib/python3.11/site-packages/accelerate/commands/launch.py\", line 763, in simple_launcher\n    raise subprocess.CalledProcessError(returncode=process.returncode, cmd=cmd)\nsubprocess.CalledProcessError: Command '['/opt/conda/bin/python3.11', 'main.py']' returned non-zero exit status 1.\n```\n\n### Expected behavior\n\nThe training should not crash.",
    "state": "closed",
    "created_at": "2025-01-15T03:05:57+00:00",
    "closed_at": "2025-01-15T14:06:33+00:00",
    "updated_at": "2025-01-15T14:06:33+00:00",
    "author": "tiandeyu-cs",
    "author_type": "User",
    "comments_count": 3,
    "reactions_count": 0,
    "labels": "bug",
    "milestone": null,
    "closed_by": "tiandeyu-cs",
    "resolution_time_hours": 11.01,
    "first_comments": [
      {
        "author": "SunMarc",
        "created_at": "2025-01-15T13:20:05+00:00",
        "body": "cc @muellerzr "
      },
      {
        "author": "muellerzr",
        "created_at": "2025-01-15T13:59:32+00:00",
        "body": "If you're using the trainer you should **never** make your own accelerator. I disagree with this \"fix\" and should raise the error it does"
      },
      {
        "author": "tiandeyu-cs",
        "created_at": "2025-01-15T14:03:41+00:00",
        "body": "I see. Thanks for your explanation."
      }
    ],
    "url": "https://github.com/huggingface/transformers/issues/35702"
  },
  {
    "repository": "huggingface/transformers",
    "issue_number": 35678,
    "title": "[i18n-<languageCode>] Translating docs to <languageName>",
    "body": "<!--\nNote: Please search to see if an issue already exists for the language you are trying to translate.\n-->\n\nHi!\n\nLet's bring the documentation to all the <languageName>-speaking community ðŸŒ (currently 0 out of 267 complete)\n\nWho would want to translate? Please follow the ðŸ¤— [TRANSLATING guide](https://github.com/huggingface/transformers/blob/main/docs/TRANSLATING.md). Here is a list of the files ready for translation. Let us know in this issue if you'd like to translate any, and we'll add your name to the list.\n\nSome notes:\n\n* Please translate using an informal tone (imagine you are talking with a friend about transformers ðŸ¤—).\n* Please translate in a gender-neutral way.\n* Add your translations to the folder called `<languageCode>` inside the [source folder](https://github.com/huggingface/transformers/tree/main/docs/source).\n* Register your translation in `<languageCode>/_toctree.yml`; please follow the order of the [English version](https://github.com/huggingface/transformers/blob/main/docs/source/en/_toctree.yml).\n* Once you're finished, open a pull request and tag this issue by including #issue-number in the description, where issue-number is the number of this issue. Please ping @stevhliu and @MKhalusova for review.\n* ðŸ™‹ If you'd like others to help you with the translation, you can also post in the ðŸ¤— [forums](https://discuss.huggingface.co/).\n\n## Get Started section\n\n- [ ] [index.md](https://github.com/huggingface/transformers/blob/main/docs/source/en/index.md) https://github.com/huggingface/transformers/pull/20180\n- [ ] [quicktour.md](https://github.com/huggingface/transformers/blob/main/docs/source/en/quicktour.md) (waiting for initial PR to go through)\n- [ ] [installation.md](https://github.com/huggingface/transformers/blob/main/docs/source/en/installation.md).\n\n## Tutorial section\n- [ ] [pipeline_tutorial.md](https://github.com/huggingface/transformers/blob/main/docs/source/en/pipeline_tutorial.md)\n- [ ]  [autoclass_tutorial.md](https://github.com/huggingface/transformers/blob/main/docs/source/en/autoclass_tutorial.md)\n- [ ]  [preprocessing.md](https://github.com/huggingface/transformers/blob/main/docs/source/en/preprocessing.md)\n- [ ]  [training.md](https://github.com/huggingface/transformers/blob/main/docs/source/en/training.md)\n- [ ]  [accelerate.md](https://github.com/huggingface/transformers/blob/main/docs/source/en/accelerate.md)\n- [ ]  [model_sharing.md](https://github.com/huggingface/transformers/blob/main/docs/source/en/model_sharing.md)\n- [ ]  [multilingual.md](https://github.com/huggingface/transformers/blob/main/docs/source/en/multilingual.md)\n\n<!--\nKeep on adding more as you go ðŸ”¥\n-->",
    "state": "closed",
    "created_at": "2025-01-13T18:58:10+00:00",
    "closed_at": "2025-01-13T19:44:30+00:00",
    "updated_at": "2025-01-13T19:44:30+00:00",
    "author": "lawchingman",
    "author_type": "User",
    "comments_count": 0,
    "reactions_count": 0,
    "labels": "WIP",
    "milestone": null,
    "closed_by": "stevhliu",
    "resolution_time_hours": 0.7722222222222223,
    "first_comments": [],
    "url": "https://github.com/huggingface/transformers/issues/35678"
  },
  {
    "repository": "huggingface/transformers",
    "issue_number": 35663,
    "title": "About GA loss in the latest transformers version",
    "body": "### System Info\n\ntransformers 4.48.0\n\n### Who can help?\n\n@ArthurZucker and @muellerzr \n\n### Information\n\n- [ ] The official example scripts\n- [ ] My own modified scripts\n\n### Tasks\n\n- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\n- [ ] My own task or dataset (give details below)\n\n### Reproduction\n\nI feel deeply apologize for everyone because in my previous PR #35438 , I accidentally inherited the typo in #34915 that ought to be fixed in #35113 and #35121. This caused the training loss of models with `loss_kwargs` to become very large once the gradient accumulation is enabled. I think we should merge #35651 ASAP and provide a stable version with the correct implementation.\n\n\n### Expected behavior\n\nThe GA loss should be correctly scaled.",
    "state": "closed",
    "created_at": "2025-01-13T15:13:54+00:00",
    "closed_at": "2025-01-17T05:26:29+00:00",
    "updated_at": "2025-01-17T05:26:29+00:00",
    "author": "hiyouga",
    "author_type": "User",
    "comments_count": 2,
    "reactions_count": 0,
    "labels": "bug",
    "milestone": null,
    "closed_by": "hiyouga",
    "resolution_time_hours": 86.20972222222223,
    "first_comments": [
      {
        "author": "muellerzr",
        "created_at": "2025-01-13T15:30:39+00:00",
        "body": "These things happen, and getting back into the swing of things post holidays takes a minute so I overlooked testing the slow tests locally as well.\n\nI'll have some new tests today added which run as part of the daily CI + PR CI so we can be flagged sooner of it. ðŸ¤— \n\nWe appreciate all you've done with your investigations into it immensely! \n\nWe'll merge it soon + make it part of the patch this week"
      },
      {
        "author": "ArthurZucker",
        "created_at": "2025-01-16T15:10:04+00:00",
        "body": "Thanks @hiyouga for your honesty, and really don't worry! It is a lot more our responsibility than yours! ðŸ¤— Your contributions are most welcome, make sure to keep them coming!!! "
      }
    ],
    "url": "https://github.com/huggingface/transformers/issues/35663"
  },
  {
    "repository": "huggingface/transformers",
    "issue_number": 35653,
    "title": "TypeError: LlavaForConditionalGeneration.forward() got an unexpected keyword argument 'default_system'",
    "body": "### System Info\n\nI can use llama factory training normally, but encounter the following problems during inference with order: FORCE_TORCHRUN=1 CUDA_VISIBLE_DEVICES=0,1 llamafactory-cli train eval_llava_lora_sft.yaml\r\nsystem info:\r\n[rank0]: Traceback (most recent call last):\r\n[rank0]:   File \"/data/llamafactory/src/llamafactory/launcher.py\", line 23, in <module>\r\n[rank0]:     launch()\r\n[rank0]:   File \"/data/llamafactory/src/llamafactory/launcher.py\", line 19, in launch\r\n[rank0]:     run_exp()\r\n[rank0]:   File \"/data/llamafactory/src/llamafactory/train/tuner.py\", line 50, in run_exp\r\n[rank0]:     run_sft(model_args, data_args, training_args, finetuning_args, generating_args, callbacks)\r\n[rank0]:   File \"/data/llamafactory/src/llamafactory/train/sft/workflow.py\", line 127, in run_sft\r\n[rank0]:     predict_results = trainer.predict(dataset_module[\"eval_dataset\"], metric_key_prefix=\"predict\", **gen_kwargs)\r\n[rank0]:   File \"/home/amax/.conda/envs/llamafactory/lib/python3.12/site-packages/transformers/trainer_seq2seq.py\", line 259, in predict\r\n[rank0]:     return super().predict(test_dataset, ignore_keys=ignore_keys, metric_key_prefix=metric_key_prefix)   \r\n[rank0]:   File \"/home/amax/.conda/envs/llamafactory/lib/python3.12/site-packages/transformers/trainer.py\", line 4042, in predict\r\n[rank0]:     output = eval_loop(\r\n[rank0]:              ^^^^^^^^^^\r\n[rank0]:   File \"/home/amax/.conda/envs/llamafactory/lib/python3.12/site-packages/transformers/trainer.py\", line 4158, in evaluation_loop\r\n[rank0]:     losses, logits, labels = self.prediction_step(model, inputs, prediction_loss_only, ignore_keys=ignore_keys)\r\n[rank0]:   File \"/data/llamafactory/src/llamafactory/train/sft/trainer.py\", line 122, in prediction_step\r\n[rank0]:     loss, generated_tokens, _ = super().prediction_step(  # ignore the returned labels (may be truncated)\r\n[rank0]:   File \"/home/amax/.conda/envs/llamafactory/lib/python3.12/site-packages/transformers/trainer_seq2seq.py\", line 331, in prediction_step\r\n[rank0]:     generated_tokens = self.model.generate(**generation_inputs, **gen_kwargs)\r\n[rank0]:   File \"/home/amax/.conda/envs/llamafactory/lib/python3.12/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\r\n[rank0]:     return func(*args, **kwargs)\r\n[rank0]:   File \"/home/amax/.conda/envs/llamafactory/lib/python3.12/site-packages/transformers/generation/utils.py\", line 2215, in generate\r\n[rank0]:     result = self._sample(\r\n[rank0]:   File \"/home/amax/.conda/envs/llamafactory/lib/python3.12/site-packages/transformers/generation/utils.py\", line 3206, in _sample\r\n[rank0]:     outputs = self(**model_inputs, return_dict=True)\r\n[rank0]:   File \"/home/amax/.conda/envs/llamafactory/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\r\n[rank0]:     return self._call_impl(*args, **kwargs)\r\n[rank0]:   File \"/home/amax/.conda/envs/llamafactory/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\r\n[rank0]:     return forward_call(*args, **kwargs)\r\n[rank0]: TypeError: LlavaForConditionalGeneration.forward() got an unexpected keyword argument 'default_system'\r\n[rank0]:[W113 18:16:18.306520029 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())\r\nW0113 18:16:19.816000 226009 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 226042 closing signal SIGTERM\r\nE0113 18:16:19.850000 226009 site-packages/torch/distributed/elastic/multiprocessing/api.py:869] failed (exitcode: 1) local_rank: 1 (pid: 226043) of binary: /home/amax/.conda/envs/llamafactory/bin/python\r\nTraceback (most recent call last):\r\n  File \"/home/amax/.conda/envs/llamafactory/bin/torchrun\", line 8, in <module>\r\n    sys.exit(main())\r\n             ^^^^^^\r\n  File \"/home/amax/.conda/envs/llamafactory/lib/python3.12/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 355, in wrapper\r\n    return f(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^\r\n  File \"/home/amax/.conda/envs/llamafactory/lib/python3.12/site-packages/torch/distributed/run.py\", line 919, in main\r\n    run(args)\r\n  File \"/home/amax/.conda/envs/llamafactory/lib/python3.12/site-packages/torch/distributed/run.py\", line 910, in run\r\n    elastic_launch(\r\n  File \"/home/amax/.conda/envs/llamafactory/lib/python3.12/site-packages/torch/distributed/launcher/api.py\", line 138, in __call__\r\n    return launch_agent(self._config, self._entrypoint, list(args))\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/amax/.conda/envs/llamafactory/lib/python3.12/site-packages/torch/distributed/launcher/api.py\", line 269, in launch_agent\r\n    raise ChildFailedError(\r\ntorch.distributed.elastic.multiprocessing.errors.ChildFailedError:\r\n============================================================\r\n/data/llamafactory/src/llamafactory/launcher.py FAILED\r\n------------------------------------------------------------\r\nFailures:\r\n  <NO_OTHER_FAILURES>\r\n------------------------------------------------------------\r\nRoot Cause (first observed failure):\r\n[0]:\r\n  time      : 2025-01-13_18:16:19\r\n  host      : amax\r\n  rank      : 1 (local_rank: 1)\r\n  exitcode  : 1 (pid: 226043)\r\n  error_file: <N/A>\r\n  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n\n### Who can help?\n\n_No response_\n\n### Information\n\n- [ ] The official example scripts\n- [ ] My own modified scripts\n\n### Tasks\n\n- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\n- [ ] My own task or dataset (give details below)\n\n### Reproduction\n\n### model\r\nmodel_name_or_path: model/llava-1.5-7b-hf\r\nadapter_name_or_path: saves/llava-1.5-7b/lora/sft\r\n\r\n### method\r\nstage: sft\r\ndo_predict: true\r\nfinetuning_type: lora\r\n\r\n### dataset\r\neval_dataset: QaTa-COV19-v2\r\ntemplate: llava\r\ncutoff_len: 2048\r\nmax_samples: 50\r\noverwrite_cache: true\r\npreprocessing_num_workers: 16\r\n\r\n### output\r\noutput_dir: saves/llava-1.5-7b-eval\r\noverwrite_output_dir: true\r\n\r\n### eval\r\nper_device_eval_batch_size: 1\r\npredict_with_generate: true\r\nddp_timeout: 180000000\r\n\n\n### Expected behavior\n\nsuccessfully running the inference process",
    "state": "closed",
    "created_at": "2025-01-13T10:29:11+00:00",
    "closed_at": "2025-01-14T08:32:12+00:00",
    "updated_at": "2025-01-14T08:32:12+00:00",
    "author": "sysu19351160",
    "author_type": "User",
    "comments_count": 5,
    "reactions_count": 0,
    "labels": "bug,VLM",
    "milestone": null,
    "closed_by": "sysu19351160",
    "resolution_time_hours": 22.05027777777778,
    "first_comments": [
      {
        "author": "zucchini-nlp",
        "created_at": "2025-01-13T14:56:14+00:00",
        "body": "@sysu19351160 hey!\n\nWe usually run generation with the set of kwargs provided in the inputs and from what I see llama-factory is passing a `default_system` as key in `gen_kwargs` when calling `trainer.predict()`. I think the best solution here is to open an issue on llama-factory and remove any kwargs not used directly by `generate()` call\n\nhttps://github.com/hiyouga/LLaMA-Factory/blob/e3e2c8c689c54ebb2af264de808502e5a8ba0f2b/src/llamafactory/train/sft/workflow.py#L126"
      },
      {
        "author": "Rocketknight1",
        "created_at": "2025-01-13T16:38:24+00:00",
        "body": "# I HOPE THIS RESOLVED YOUR ISSUE"
      },
      {
        "author": "sysu19351160",
        "created_at": "2025-01-14T04:13:35+00:00",
        "body": "> [@sysu19351160](https://github.com/sysu19351160) hey!\n> \n> We usually run generation with the set of kwargs provided in the inputs and from what I see llama-factory is passing a `default_system` as key in `gen_kwargs` when calling `trainer.predict()`. I think the best solution here is to open an issue on llama-factory and remove any kwargs not used directly by `generate()` call\n> \n> https://github.com/hiyouga/LLaMA-Factory/blob/e3e2c8c689c54ebb2af264de808502e5a8ba0f2b/src/llamafactory/train/sft/workflow.py#L126\n\nI think your analysis is rational, but I am not sure about how to remove the useless kwargs from gen_kwargs. I would appreciate it if you could help me with more details."
      },
      {
        "author": "zucchini-nlp",
        "created_at": "2025-01-14T08:10:57+00:00",
        "body": "@sysu19351160 I'd recommend opening an issue in llama-factory for that question because the extra kwarg was added by their Argument Parser ðŸ¤— "
      },
      {
        "author": "sysu19351160",
        "created_at": "2025-01-14T08:28:44+00:00",
        "body": "> [@sysu19351160](https://github.com/sysu19351160) I'd recommend opening an issue in llama-factory for that question because the extra kwarg was added by their Argument Parser ðŸ¤—\n\nOK, thanks"
      }
    ],
    "url": "https://github.com/huggingface/transformers/issues/35653"
  },
  {
    "repository": "huggingface/transformers",
    "issue_number": 35649,
    "title": "PR #35438 introduced a new bug",
    "body": "### System Info\n\n(base) MBP-HD6JD9Q599-2052 :: ~/code/transformers â€¹main*â€º % transformers-cli env                                                   1 â†µ\r\n\r\nCopy-and-paste the text below in your GitHub issue and FILL OUT the two last points.\r\n\r\n- `transformers` version: 4.49.0.dev0\r\n- Platform: macOS-14.6.1-arm64-arm-64bit\r\n- Python version: 3.12.4\r\n- Huggingface_hub version: 0.27.1\r\n- Safetensors version: 0.4.3\r\n- Accelerate version: 1.2.1\r\n- Accelerate config:    not found\r\n- PyTorch version (GPU?): 2.5.1 (False)\r\n- Tensorflow version (GPU?): not installed (NA)\r\n- Flax version (CPU?/GPU?/TPU?): not installed (NA)\r\n- Jax version: not installed\r\n- JaxLib version: not installed\r\n- Using distributed or parallel set-up in script?: <fill in>\r\n\n\n### Who can help?\n\n@muellerzr @hiyouga @ArthurZucker @SunMarc \n\n### Information\n\n- [X] The official example scripts\n- [ ] My own modified scripts\n\n### Tasks\n\n- [X] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\n- [ ] My own task or dataset (give details below)\n\n### Reproduction\n\n```\r\nexport RUN_SLOW=True\r\npytest tests/trainer/test_trainer.py::TrainerIntegrationPrerunTest::test_gradient_accumulation_loss_alignment_with_loss_func\r\n\r\n======================================================= short test summary info =======================================================\r\nFAILED tests/trainer/test_trainer.py::TrainerIntegrationPrerunTest::test_gradient_accumulation_loss_alignment_with_loss_func - AssertionError: 3.0949999999999998 not less than 0.01 : Difference 3.0949999999999998 is not within 0.01\r\n=================================================== 1 failed, 2 warnings in 54.91s ====================================================\r\n```\n\n### Expected behavior\n\nTest Passed.",
    "state": "closed",
    "created_at": "2025-01-13T08:21:04+00:00",
    "closed_at": "2025-01-16T12:59:54+00:00",
    "updated_at": "2025-01-16T12:59:54+00:00",
    "author": "techkang",
    "author_type": "User",
    "comments_count": 7,
    "reactions_count": 0,
    "labels": "bug",
    "milestone": null,
    "closed_by": "SunMarc",
    "resolution_time_hours": 76.64722222222223,
    "first_comments": [
      {
        "author": "techkang",
        "created_at": "2025-01-13T08:22:00+00:00",
        "body": "PR link: https://github.com/huggingface/transformers/pull/35438"
      },
      {
        "author": "techkang",
        "created_at": "2025-01-13T08:48:03+00:00",
        "body": "I think the PR: https://github.com/huggingface/transformers/pull/35438 should be reverted and the proper way to fix the bug mentioned in the PR is as follows.\r\n\r\nIn the following code, `loss` is scaled when `num_items_in_batch is None`.\r\nhttps://github.com/huggingface/transformers/blob/main/src/transformers/trainer.py#L3712-L3713\r\n\r\nBut the true meaning of this code is to scale loss when GA bug fix is not performed. This is not identical to `num_items_in_batch is None` after recent PRs. So it should be changed to \r\n```\r\nif not self.model_accepts_loss_kwargs and self.compute_loss_func is None\r\n```"
      },
      {
        "author": "muellerzr",
        "created_at": "2025-01-13T09:20:52+00:00",
        "body": "Thanks! Would you like to make a PR for this? Else I can do so today"
      },
      {
        "author": "techkang",
        "created_at": "2025-01-13T09:24:30+00:00",
        "body": "@muellerzr Thanks for reply. I will open a PR today."
      },
      {
        "author": "hiyouga",
        "created_at": "2025-01-13T09:36:21+00:00",
        "body": "Hi @techkang , it has been an evidence that #35121 introduces bug making the loss of the Qwen2VL model incorrect through our rigorous experiments in #35438 . I think we should not only focus on the model with loss function but also pay attention to the models without `loss_kwargs`. There should be a solution that let both the two conditions work instead of simply reverting our fix.  cc @muellerzr "
      }
    ],
    "url": "https://github.com/huggingface/transformers/issues/35649"
  },
  {
    "repository": "huggingface/transformers",
    "issue_number": 35648,
    "title": "Unnecessary KV Cache Updates During Training Mode",
    "body": "### System Info\n\n- `transformers` version: 4.48.0\r\n- Platform: Linux-5.4.0-58-generic-x86_64-with-glibc2.31\r\n- Python version: 3.11.10\r\n- Huggingface_hub version: 0.26.3\r\n- Safetensors version: 0.4.5\r\n- Accelerate version: 1.1.1\r\n- Accelerate config:    not found\r\n- PyTorch version (GPU?): 2.5.1+cu118 (True)\r\n- Tensorflow version (GPU?): not installed (NA)\r\n- Flax version (CPU?/GPU?/TPU?): not installed (NA)\r\n- Jax version: not installed\r\n- JaxLib version: not installed\r\n- Using distributed or parallel set-up in script?: <fill in>\r\n- Using GPU in script?: <fill in>\r\n- GPU type: NVIDIA A100-PCIE-40GB\n\n### Who can help?\n\n@ArthurZucker @muellerz @SunMarc \n\n### Information\n\n- [ ] The official example scripts\n- [X] My own modified scripts\n\n### Tasks\n\n- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\n- [X] My own task or dataset (give details below)\n\n### Reproduction\n\nI've identified an issue regarding unnecessary KV cache updates during training, which affects all current LLM models in the library and impacts both memory efficiency and torch.compile compatibility.\r\n\r\nTaking `src/transformers/models/llama/modeling_llama.py` as an example:\r\n\r\n1. The `past_key_values` is set as a `DynamicCache` object even during training:\r\nhttps://github.com/huggingface/transformers/blob/15bd3e61f8d3680ca472c9314ad07584d20f7b81/src/transformers/models/llama/modeling_llama.py#L547-L548\r\n\r\n2. This leads to unnecessary memory allocation for storing KV cache during the forward pass:\r\n\r\nhttps://github.com/huggingface/transformers/blob/15bd3e61f8d3680ca472c9314ad07584d20f7b81/src/transformers/models/llama/modeling_llama.py#L273-L276\r\n\r\n3. Additionally, this causes issues with `torch.compile`, resulting in multiple recompilations due to layer_idx guards (my internal test) :\r\n```\r\n[rank6]:V0113 13:51:42.251000 31893 site-packages/torch/_dynamo/guards.py:2813] [10/6] [__recompiles] Recompiling function forward in /weka-jd/prod/deepseek/permanent/shared/chengxin/workspace/research_project/lingua/apps/main/hf_transformers/modeling_transformer.py:315\r\n[rank6]:V0113 13:51:42.251000 31893 site-packages/torch/_dynamo/guards.py:2813] [10/6] [__recompiles]     triggered by the following guard failure(s):\r\n[rank6]:V0113 13:51:42.251000 31893 site-packages/torch/_dynamo/guards.py:2813] [10/6] [__recompiles]     - 10/5: L['self']._modules['self_attn'].layer_idx == 5              \r\n[rank6]:V0113 13:51:42.251000 31893 site-packages/torch/_dynamo/guards.py:2813] [10/6] [__recompiles]     - 10/4: L['self']._modules['self_attn'].layer_idx == 4              \r\n[rank6]:V0113 13:51:42.251000 31893 site-packages/torch/_dynamo/guards.py:2813] [10/6] [__recompiles]     - 10/3: L['self']._modules['self_attn'].layer_idx == 3              \r\n[rank6]:V0113 13:51:42.251000 31893 site-packages/torch/_dynamo/guards.py:2813] [10/6] [__recompiles]     - 10/2: L['self']._modules['self_attn'].layer_idx == 2              \r\n[rank6]:V0113 13:51:42.251000 31893 site-packages/torch/_dynamo/guards.py:2813] [10/6] [__recompiles]     - 10/1: L['self']._modules['self_attn'].layer_idx == 1              \r\n[rank6]:V0113 13:51:42.251000 31893 site-packages/torch/_dynamo/guards.py:2813] [10/6] [__recompiles]     - 10/0: L['self']._modules['self_attn'].layer_idx == 0              \r\n```\r\n\r\n### Workaround\r\nA temporary solution is to explicitly set `use_cache=False` during training:\r\n```\r\nloss = model.forward(input_ids, labels, use_cache=False)\r\n```\n\n### Expected behavior\n\nno kv cache during training",
    "state": "closed",
    "created_at": "2025-01-13T06:10:29+00:00",
    "closed_at": "2025-01-16T02:31:15+00:00",
    "updated_at": "2025-01-16T02:31:15+00:00",
    "author": "Hannibal046",
    "author_type": "User",
    "comments_count": 8,
    "reactions_count": 0,
    "labels": "bug",
    "milestone": null,
    "closed_by": "Hannibal046",
    "resolution_time_hours": 68.34611111111111,
    "first_comments": [
      {
        "author": "Hannibal046",
        "created_at": "2025-01-13T06:17:51+00:00",
        "body": "Or we could change from:\r\nhttps://github.com/huggingface/transformers/blob/15bd3e61f8d3680ca472c9314ad07584d20f7b81/src/transformers/models/llama/modeling_llama.py#L532\r\n\r\nto:\r\n```python\r\nif use_cache is None:\r\n   use_cache = False if self.training else self.config.use_cache\r\n```"
      },
      {
        "author": "Rocketknight1",
        "created_at": "2025-01-13T16:17:48+00:00",
        "body": "cc @gante for cache as well"
      },
      {
        "author": "gante",
        "created_at": "2025-01-14T19:24:03+00:00",
        "body": "@Hannibal046 ðŸ‘‹ \n\n@Rocketknight1 and I chatted offline and we can't think of a reason why the cache should stay active while training. ~I'm going to update to the pattern as you wrote -- if the user doesn't manually specify the cache, only use the config file value (which is `True` by default) if not training~ (see next comment)"
      },
      {
        "author": "gante",
        "created_at": "2025-01-15T11:52:21+00:00",
        "body": "@Hannibal046 @Rocketknight1 \n\nCorrection: we may indeed want to pass `past_key_values` with `self.training=True`, in fine-tuning routines like prefix tuning (thank you @BenjaminBossan for reminding me). As such, the change will NOT be implemented.\n\nIn fact, I'm going to add a tiny integration test that will fail if we decide to disallow caches at training time."
      },
      {
        "author": "Hannibal046",
        "created_at": "2025-01-15T12:19:24+00:00",
        "body": "Hi @gante,\n\nThanks for the quick response! While I'm not entirely certain about why prefix tuning requires caching during training, I believe there's an issue with the current past_key_values implementation. \n\nFor reference, in previous Hugging Face versions (such as transformers v4.30.2), past_key_values was consistently set to None during training:\nhttps://github.com/huggingface/transformers/blob/66fd3a8d626a32989f4569260db32785c6cbf42a/src/transformers/models/llama/modeling_llama.py#L517-L519\n\nhttps://github.com/huggingface/transformers/blob/66fd3a8d626a32989f4569260db32785c6cbf42a/src/transformers/models/llama/modeling_llama.py#L199-L201"
      }
    ],
    "url": "https://github.com/huggingface/transformers/issues/35648"
  },
  {
    "repository": "huggingface/transformers",
    "issue_number": 35639,
    "title": "Breaking change in v4.48.0 and Python 3.9",
    "body": "### System Info\n\nPython 3.9 and Transformers v4.48.0 \n\n### Who can help?\n\n_No response_\n\n### Information\n\n- [ ] The official example scripts\n- [ ] My own modified scripts\n\n### Tasks\n\n- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\n- [ ] My own task or dataset (give details below)\n\n### Reproduction\n\nv4.48.0 introduced a breaking change that happens on import of TextIteratorStreamer\r\n\r\nSteps to reproduce\r\n\r\n1. Install Python 3.9\r\n2. Install Transformers >= v4.48.0\r\n\r\nRun following:\r\n\r\n```python\r\nfrom transformers import TextIteratorStreamer\r\n```\r\n\r\nReceive the following stack\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/tmp/test/lib/python3.9/site-packages/transformers/utils/import_utils.py\", line 1817, in _get_module\r\n    return importlib.import_module(\".\" + module_name, self.__name__)\r\n  File \"/usr/lib64/python3.9/importlib/__init__.py\", line 127, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 1030, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 1007, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 986, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 680, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap_external>\", line 850, in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 228, in _call_with_frames_removed\r\n  File \"/tmp/test/lib/python3.9/site-packages/transformers/generation/streamers.py\", line 231, in <module>\r\n    class AsyncTextIteratorStreamer(TextStreamer):\r\n  File \"/tmp/test/lib/python3.9/site-packages/transformers/generation/streamers.py\", line 285, in AsyncTextIteratorStreamer\r\n    self, tokenizer: \"AutoTokenizer\", skip_prompt: bool = False, timeout: float | None = None, **decode_kwargs\r\nTypeError: unsupported operand type(s) for |: 'type' and 'NoneType'\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"<frozen importlib._bootstrap>\", line 1055, in _handle_fromlist\r\n  File \"/tmp/test/lib/python3.9/site-packages/transformers/utils/import_utils.py\", line 1806, in __getattr__\r\n    value = getattr(module, name)\r\n  File \"/tmp/test/lib/python3.9/site-packages/transformers/utils/import_utils.py\", line 1805, in __getattr__\r\n    module = self._get_module(self._class_to_module[name])\r\n  File \"/tmp/test/lib/python3.9/site-packages/transformers/utils/import_utils.py\", line 1819, in _get_module\r\n    raise RuntimeError(\r\nRuntimeError: Failed to import transformers.generation.streamers because of the following error (look up to see its traceback):\r\nunsupported operand type(s) for |: 'type' and 'NoneType'\r\n```\n\n### Expected behavior\n\nWork in Python 3.9",
    "state": "closed",
    "created_at": "2025-01-12T15:42:31+00:00",
    "closed_at": "2025-01-15T12:45:45+00:00",
    "updated_at": "2025-01-27T15:09:54+00:00",
    "author": "davidmezzetti",
    "author_type": "User",
    "comments_count": 9,
    "reactions_count": 0,
    "labels": "bug",
    "milestone": null,
    "closed_by": "Rocketknight1",
    "resolution_time_hours": 69.05388888888889,
    "first_comments": [
      {
        "author": "Rocketknight1",
        "created_at": "2025-01-13T16:05:41+00:00",
        "body": "The cause here is the use of `X | None` types without a `from __future__ import annotations` line at the top of the file. Opening a PR now"
      },
      {
        "author": "davidmezzetti",
        "created_at": "2025-01-15T10:09:31+00:00",
        "body": "Thank you @Rocketknight1! I really appreciate it!\n"
      },
      {
        "author": "Rocketknight1",
        "created_at": "2025-01-15T12:46:08+00:00",
        "body": "@davidmezzetti the fix should be merged now, but you'll need to install from main or wait until the next release to use it!"
      },
      {
        "author": "davidmezzetti",
        "created_at": "2025-01-20T10:30:36+00:00",
        "body": "@Rocketknight1 @gante Thank you for the updates.\n\nDo you plan to release a 4.48.1 release or is this waiting until 4.49? As you can see here, there are a couple other projects in the same situation. \n\nWhile Python 3.9 isn't as common and there is a workaround, the bug is pretty bad in that simply importing the module raises the error. "
      },
      {
        "author": "davidmezzetti",
        "created_at": "2025-01-23T13:50:44+00:00",
        "body": "Pinging @ArthurZucker too.\n\nIt looks like there is another 3.9 compatibility related bug: https://github.com/huggingface/transformers/pull/35843\n\n"
      }
    ],
    "url": "https://github.com/huggingface/transformers/issues/35639"
  },
  {
    "repository": "huggingface/transformers",
    "issue_number": 35635,
    "title": "set_initialized_submodules too slow when loading big model like DeepSeekV3",
    "body": "### System Info\n\ntransformers==4.44.0\n\n### Who can help?\n\n_No response_\n\n### Information\n\n- [ ] The official example scripts\n- [ ] My own modified scripts\n\n### Tasks\n\n- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\n- [ ] My own task or dataset (give details below)\n\n### Reproduction\n\nLoad a model that has 40000+ nn.Modules ï¼ˆetc. DeepSeekV3ï¼‰ï¼Œthen you will find it cost too much time on set_initialized_submodules.\n\n### Expected behavior\n\nThis function is O(N^2) and should be optimized.",
    "state": "closed",
    "created_at": "2025-01-12T09:11:48+00:00",
    "closed_at": "2025-01-23T21:36:09+00:00",
    "updated_at": "2025-01-23T21:36:10+00:00",
    "author": "hongchuan666",
    "author_type": "User",
    "comments_count": 2,
    "reactions_count": 0,
    "labels": "bug",
    "milestone": null,
    "closed_by": "Rocketknight1",
    "resolution_time_hours": 276.4058333333333,
    "first_comments": [
      {
        "author": "Rocketknight1",
        "created_at": "2025-01-14T17:28:19+00:00",
        "body": "Yep, it's definitely O(n^2). Let me see if I can whip something up!"
      },
      {
        "author": "Rocketknight1",
        "created_at": "2025-01-23T21:36:09+00:00",
        "body": "Fixed by #35493"
      }
    ],
    "url": "https://github.com/huggingface/transformers/issues/35635"
  },
  {
    "repository": "huggingface/transformers",
    "issue_number": 35630,
    "title": "[i18n-<languageCode>] Translating docs to <languageName>",
    "body": "<!--\nNote: Please search to see if an issue already exists for the language you are trying to translate.\n-->\n\nHi!\n\nLet's bring the documentation to all the <languageName>-speaking community ðŸŒ (currently 0 out of 267 complete)\n\nWho would want to translate? Please follow the ðŸ¤— [TRANSLATING guide](https://github.com/huggingface/transformers/blob/main/docs/TRANSLATING.md). Here is a list of the files ready for translation. Let us know in this issue if you'd like to translate any, and we'll add your name to the list.\n\nSome notes:\n\n* Please translate using an informal tone (imagine you are talking with a friend about transformers ðŸ¤—).\n* Please translate in a gender-neutral way.\n* Add your translations to the folder called `<languageCode>` inside the [source folder](https://github.com/huggingface/transformers/tree/main/docs/source).\n* Register your translation in `<languageCode>/_toctree.yml`; please follow the order of the [English version](https://github.com/huggingface/transformers/blob/main/docs/source/en/_toctree.yml).\n* Once you're finished, open a pull request and tag this issue by including #issue-number in the description, where issue-number is the number of this issue. Please ping @stevhliu and @MKhalusova for review.\n* ðŸ™‹ If you'd like others to help you with the translation, you can also post in the ðŸ¤— [forums](https://discuss.huggingface.co/).\n\n## Get Started section\n\n- [ ] [index.md](https://github.com/huggingface/transformers/blob/main/docs/source/en/index.md) https://github.com/huggingface/transformers/pull/20180\n- [ ] [quicktour.md](https://github.com/huggingface/transformers/blob/main/docs/source/en/quicktour.md) (waiting for initial PR to go through)\n- [ ] [installation.md](https://github.com/huggingface/transformers/blob/main/docs/source/en/installation.md).\n\n## Tutorial section\n- [ ] [pipeline_tutorial.md](https://github.com/huggingface/transformers/blob/main/docs/source/en/pipeline_tutorial.md)\n- [ ]  [autoclass_tutorial.md](https://github.com/huggingface/transformers/blob/main/docs/source/en/autoclass_tutorial.md)\n- [ ]  [preprocessing.md](https://github.com/huggingface/transformers/blob/main/docs/source/en/preprocessing.md)\n- [ ]  [training.md](https://github.com/huggingface/transformers/blob/main/docs/source/en/training.md)\n- [ ]  [accelerate.md](https://github.com/huggingface/transformers/blob/main/docs/source/en/accelerate.md)\n- [ ]  [model_sharing.md](https://github.com/huggingface/transformers/blob/main/docs/source/en/model_sharing.md)\n- [ ]  [multilingual.md](https://github.com/huggingface/transformers/blob/main/docs/source/en/multilingual.md)\n\n<!--\nKeep on adding more as you go ðŸ”¥\n-->",
    "state": "closed",
    "created_at": "2025-01-11T21:16:42+00:00",
    "closed_at": "2025-01-13T17:20:20+00:00",
    "updated_at": "2025-01-13T17:20:22+00:00",
    "author": "lawchingman",
    "author_type": "User",
    "comments_count": 1,
    "reactions_count": 0,
    "labels": "WIP",
    "milestone": null,
    "closed_by": "stevhliu",
    "resolution_time_hours": 44.06055555555555,
    "first_comments": [
      {
        "author": "stevhliu",
        "created_at": "2025-01-13T17:20:20+00:00",
        "body": "Closing this issue since it's not clear what language you're interested in and because you've opened the [same issue](https://github.com/huggingface/transformers/issues?q=is:issue%20state:closed%20author:lawchingman) multiple times before."
      }
    ],
    "url": "https://github.com/huggingface/transformers/issues/35630"
  },
  {
    "repository": "huggingface/transformers",
    "issue_number": 35629,
    "title": "Dose `num_logits_to_keep` in `model.generate()` really work?",
    "body": "### System Info\r\n\r\n- `transformers` version: 4.47.1\r\n- Platform: Linux-6.9.3-76060903-generic-x86_64-with-glibc2.35\r\n- Python version: 3.10.14\r\n- Huggingface_hub version: 0.27.0\r\n- Safetensors version: 0.5.0\r\n- Accelerate version: 1.2.1\r\n- Accelerate config:    not found\r\n- PyTorch version (GPU?): 2.5.1+cu124 (True)\r\n- Tensorflow version (GPU?): not installed (NA)\r\n- Flax version (CPU?/GPU?/TPU?): not installed (NA)\r\n- Jax version: not installed\r\n- JaxLib version: not installed\r\n- Using distributed or parallel set-up in script?: <fill in>\r\n- Using GPU in script?: <fill in>\r\n- GPU type: NVIDIA GeForce RTX 4090\r\n\r\n### Who can help?\r\n\r\n@zucchini-nlp @gante @ArthurZucker \r\n\r\n### Information\r\n\r\n- [ ] The official example scripts\r\n- [X] My own modified scripts\r\n\r\n### Tasks\r\n\r\n- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\r\n- [X] My own task or dataset (give details below)\r\n\r\n### Reproduction\r\n\r\nI want to customize the `num_logits_to_keep` in `LlamaForCausalLM`'s `forward` function by setting it through generate, but I failed or don't know how to check the logits of tokens produced by `num_logits_to_keep`.\r\n\r\na code snippet\r\n```python\r\noutputs = model.generate( \r\n    ids['input_ids'],\r\n    max_new_tokens=50,\r\n    pad_token_id=tokenizer.eos_token_id,\r\n    output_scores=True,\r\n    return_dict_in_generate=True,\r\n    num_logits_to_keep=3,\r\n)\r\n\r\nor\r\n\r\nmodel.generation_config.num_logits_to_keep = 3\r\n```\r\n\r\n### Expected behavior\r\n\r\noutputs.logits != None ?",
    "state": "closed",
    "created_at": "2025-01-11T17:28:35+00:00",
    "closed_at": "2025-01-13T10:06:32+00:00",
    "updated_at": "2025-01-13T14:01:48+00:00",
    "author": "bwnjnOEI",
    "author_type": "User",
    "comments_count": 3,
    "reactions_count": 0,
    "labels": "bug",
    "milestone": null,
    "closed_by": "zucchini-nlp",
    "resolution_time_hours": 40.6325,
    "first_comments": [
      {
        "author": "zucchini-nlp",
        "created_at": "2025-01-13T10:06:32+00:00",
        "body": "Hey @bwnjnOEI !\r\n\r\nThe `num_logits_to_keep` is working. You have to access `out.scores` since you are requesting to `output_scores` in the code snippet. For logits try to `output_logits=True`\r\n\r\nClosing as complete as there is no bug :)"
      },
      {
        "author": "bwnjnOEI",
        "created_at": "2025-01-13T13:37:21+00:00",
        "body": "> Hey @bwnjnOEI !\r\n> \r\n> The `num_logits_to_keep` is working. You have to access `out.scores` since you are requesting to `output_scores` in the code snippet. For logits try to `output_logits=True`\r\n> \r\n> Closing as complete as there is no bug :)\r\n\r\nThank you @zucchini-nlp for your answer, but I still have doubts. When the `LlamaForCausalLM.forward`, I want to keep the logits of the last `num_logits_to_keep` tokens at each generating step. According to your hint, I have set `num_logits_to_keep` and `output_logits=True` in the `.generate`. However, the shape of logits printed directly from `LlamaForCausalLM.forward` at each step is inconsistent with the shape of logits returned by `.generate` in the dictionary; it seems to only have the logits of the last token. Have I missed something?\r\n\r\n### result \r\n```bash\r\ninput ids len:  5\r\nmax_new_tokens:  10\r\nnum_logits_to_keep:  3\r\n\r\nThe shape of logits in generating step in .forward():  torch.Size([1, 3, 128256])\r\nThe shape of logits in generating step in .forward():  torch.Size([1, 3, 128256])\r\nThe shape of logits in generating step in .forward():  torch.Size([1, 3, 128256])\r\nThe shape of logits in generating step in .forward():  torch.Size([1, 3, 128256])\r\nThe shape of logits in generating step in .forward():  torch.Size([1, 3, 128256])\r\nThe shape of logits in generating step in .forward():  torch.Size([1, 3, 128256])\r\nThe shape of logits in generating step in .forward():  torch.Size([1, 3, 128256])\r\nThe shape of logits in generating step in .forward():  torch.Size([1, 3, 128256])\r\nThe shape of logits in generating step in .forward():  torch.Size([1, 3, 128256])\r\nThe shape of logits in generating step in .forward():  torch.Size([1, 3, 128256])\r\n\r\nThe shape of logits at each index of .generate().logits torch.Size([1, 128256])\r\nThe shape of logits at each index of .generate().logits torch.Size([1, 128256])\r\nThe shape of logits at each index of .generate().logits torch.Size([1, 128256])\r\nThe shape of logits at each index of .generate().logits torch.Size([1, 128256])\r\nThe shape of logits at each index of .generate().logits torch.Size([1, 128256])\r\nThe shape of logits at each index of .generate().logits torch.Size([1, 128256])\r\nThe shape of logits at each index of .generate().logits torch.Size([1, 128256])\r\nThe shape of logits at each index of .generate().logits torch.Size([1, 128256])\r\nThe shape of logits at each index of .generate().logits torch.Size([1, 128256])\r\nThe shape of logits at each index of .generate().logits torch.Size([1, 128256])\r\n```\r\nI apologize for asking such a naive question, but there are really very few related documents or examples, and the `GenerationMixin` codebase is quite large. Thank you again here."
      },
      {
        "author": "zucchini-nlp",
        "created_at": "2025-01-13T14:00:57+00:00",
        "body": "@bwnjnOEI right, since `generate()` usually return not the whole logits as per model output, but only the current logit for the token being generated. So we end up with one logit for each new token that the model generated. The main idea of `num_logits_to_keep` here is to be more memory efficient and get rid of logits that are not contributing to next token prediction, thus by default it has a value of `1` when not specified :)\n\nIf you want full logits, I'd recommend to use a `forward()` pass"
      }
    ],
    "url": "https://github.com/huggingface/transformers/issues/35629"
  },
  {
    "repository": "huggingface/transformers",
    "issue_number": 35625,
    "title": "The Phi model does not have lm_head bias after upgraded to v4.48.0",
    "body": "### System Info\n\n- `transformers` version: 4.48.0\r\n- Platform: Linux-5.15.0-88-generic-x86_64-with-glibc2.39\r\n- Python version: 3.12.3\r\n- Huggingface_hub version: 0.27.1\r\n- Safetensors version: 0.5.2\r\n- Accelerate version: 1.2.1\r\n- Accelerate config:    not found\r\n- PyTorch version (GPU?): 2.6.0a0+ecf3bae40a (True)\r\n- Tensorflow version (GPU?): not installed (NA)\r\n- Flax version (CPU?/GPU?/TPU?): not installed (NA)\r\n- Jax version: not installed\r\n- JaxLib version: not installed\r\n- Using distributed or parallel set-up in script?: <fill in>\r\n- Using GPU in script?: <fill in>\r\n- GPU type: NVIDIA H200\n\n### Who can help?\n\n@ArthurZucker\n\n### Information\n\n- [X] The official example scripts\n- [ ] My own modified scripts\n\n### Tasks\n\n- [X] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\n- [ ] My own task or dataset (give details below)\n\n### Reproduction\n\nThe Phi model's lm_head bias disappear after upgraded to v4.48.0, which is incorrect, see https://github.com/huggingface/transformers/blob/e97d7a5be545d123862d09f6c32e8d90b98bbf0c/src/transformers/models/phi/modeling_phi.py#L727\n\n### Expected behavior\n\nAdd lm_head bias back to the Phi model.",
    "state": "closed",
    "created_at": "2025-01-11T11:47:05+00:00",
    "closed_at": "2025-01-13T12:15:08+00:00",
    "updated_at": "2025-01-13T12:15:09+00:00",
    "author": "yuxianq",
    "author_type": "User",
    "comments_count": 1,
    "reactions_count": 0,
    "labels": "bug",
    "milestone": null,
    "closed_by": "ArthurZucker",
    "resolution_time_hours": 48.4675,
    "first_comments": [
      {
        "author": "ArthurZucker",
        "created_at": "2025-01-13T08:40:44+00:00",
        "body": "Indeed, thanks for noticing! This will be in the patch!\r\n"
      }
    ],
    "url": "https://github.com/huggingface/transformers/issues/35625"
  },
  {
    "repository": "huggingface/transformers",
    "issue_number": 35621,
    "title": "The argument \"dim\" is gone from LlamaRotaryEmbedding initializer. Intentional?",
    "body": "### System Info\n\n4.48.0\n\n### Who can help?\n\n_No response_\n\n### Information\n\n- [ ] The official example scripts\n- [ ] My own modified scripts\n\n### Tasks\n\n- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\n- [ ] My own task or dataset (give details below)\n\n### Reproduction\n\nWe see the following error with the latest version 4.48.0 of transformers when initializating LlamaRotaryEmbedding:\r\n\r\n```\r\n2025-01-10 14:08:19,223::DEBUG: [stdout]         cache_ids = cache_ids.view(-1, 1)\r\n2025-01-10 14:08:19,223::DEBUG: [stdout] >       embed = LlamaRotaryEmbedding(dim=d_head, max_position_embeddings=2048, base=10000)\r\n2025-01-10 14:08:19,223::DEBUG: [stdout] E       TypeError: LlamaRotaryEmbedding.__init__() got an unexpected keyword argument 'dim'\r\n2025-01-10 14:08:19,223::DEBUG: [stdout] \r\n2025-01-10 14:08:19,223::DEBUG: [stdout] transformers_neuronx_test/unit/1_core/test_rotary.py:108: TypeError\r\n```\r\nTo test simply instantiate LlamaRotaryEmbedding with dim equal something:\r\n```\r\nfrom transformers.models.llama.modeling_llama import LlamaRotaryEmbedding\r\nembed = LlamaRotaryEmbedding(dim=96, max_position_embeddings=2048, base=10000)\r\n```\n\n### Expected behavior\n\nThe dim argument was there in previous version. Is the argument no longer needed?",
    "state": "closed",
    "created_at": "2025-01-11T04:50:19+00:00",
    "closed_at": "2025-01-17T05:09:48+00:00",
    "updated_at": "2025-01-17T05:09:49+00:00",
    "author": "jeffhataws",
    "author_type": "User",
    "comments_count": 3,
    "reactions_count": 0,
    "labels": "bug",
    "milestone": null,
    "closed_by": "jeffhataws",
    "resolution_time_hours": 144.32472222222222,
    "first_comments": [
      {
        "author": "pagezyhf",
        "created_at": "2025-01-13T10:12:52+00:00",
        "body": "cc @ArthurZucker "
      },
      {
        "author": "ArthurZucker",
        "created_at": "2025-01-13T10:14:07+00:00",
        "body": "Hey! We used a deprecation cycle with #32135. We had a warning saying that you should pass the config starting 4.45 (only removed support in 4.48)! ðŸ¤— "
      },
      {
        "author": "jeffhataws",
        "created_at": "2025-01-17T05:09:48+00:00",
        "body": "Thanks. Then it is working as expected. We will update."
      }
    ],
    "url": "https://github.com/huggingface/transformers/issues/35621"
  },
  {
    "repository": "huggingface/transformers",
    "issue_number": 35590,
    "title": "torch.compile + DataCollatorWithFlattening + flash_attention_2.7 causes crash when training",
    "body": "### System Info\n\n- `transformers` version: 4.47.1\r\n- Platform: Linux-6.6.20-aufs-1-x86_64-with-glibc2.36\r\n- Python version: 3.11.2\r\n- Huggingface_hub version: 0.26.2\r\n- Safetensors version: 0.4.5\r\n- Accelerate version: 1.2.1\r\n- Accelerate config:    not found\r\n- PyTorch version (GPU?): 2.5.1+cu124 (True)\r\n- Tensorflow version (GPU?): not installed (NA)\r\n- Flax version (CPU?/GPU?/TPU?): not installed (NA)\r\n- Jax version: not installed\r\n- JaxLib version: not installed\r\n- Using distributed or parallel set-up in script?: No\r\n- Using GPU in script?: Yes\r\n- GPU type: NVIDIA RTX A5000\n\n### Who can help?\n\n@ArthurZucker \n\n### Information\n\n- [ ] The official example scripts\n- [X] My own modified scripts\n\n### Tasks\n\n- [X] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\n- [ ] My own task or dataset (give details below)\n\n### Reproduction\n\nwhen using a model with fa2 on the latest version (2.7.2) with torch.compile and DataCollatorWithFlattening.\r\nI am getting the following stacktrace:\r\n```\r\nTraceback (most recent call last):\r\n  File \"/cs/labs/oabend/avishai.elma/slm_eval/slm_eval/train.py\", line 89, in main\r\n    trainer.train(resume_from_checkpoint=cfg.cont_training)\r\n  File \"/cs/labs/oabend/avishai.elma/slm_eval/.slm_env2/lib/python3.11/site-packages/transformers/trainer.py\", line 2164, in train\r\n    return inner_training_loop(\r\n           ^^^^^^^^^^^^^^^^^^^^\r\n  File \"/cs/labs/oabend/avishai.elma/slm_eval/.slm_env2/lib/python3.11/site-packages/transformers/trainer.py\", line 2524, in _inner_training_loop\r\n    tr_loss_step = self.training_step(model, inputs, num_items_in_batch)\r\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/cs/labs/oabend/avishai.elma/slm_eval/slm_eval/trainer/slam_trainer.py\", line 71, in training_step\r\n    return super().training_step(model, inputs, num_items_in_batch)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/cs/labs/oabend/avishai.elma/slm_eval/.slm_env2/lib/python3.11/site-packages/transformers/trainer.py\", line 3654, in training_step\r\n    loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/cs/labs/oabend/avishai.elma/slm_eval/.slm_env2/lib/python3.11/site-packages/transformers/trainer.py\", line 3708, in compute_loss\r\n    outputs = model(**inputs)\r\n              ^^^^^^^^^^^^^^^\r\n  File \"/cs/labs/oabend/avishai.elma/slm_eval/.slm_env2/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/cs/labs/oabend/avishai.elma/slm_eval/.slm_env2/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/cs/labs/oabend/avishai.elma/slm_eval/.slm_env2/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py\", line 465, in _fn\r\n    return fn(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^\r\n  File \"/cs/labs/oabend/avishai.elma/slm_eval/.slm_env2/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/cs/labs/oabend/avishai.elma/slm_eval/.slm_env2/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/cs/labs/oabend/avishai.elma/slm_eval/.slm_env2/lib/python3.11/site-packages/accelerate/utils/operations.py\", line 823, in forward\r\n    return model_forward(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/cs/labs/oabend/avishai.elma/slm_eval/.slm_env2/lib/python3.11/site-packages/accelerate/utils/operations.py\", line 811, in __call__\r\n    return convert_to_fp32(self.model_forward(*args, **kwargs))\r\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/cs/labs/oabend/avishai.elma/slm_eval/.slm_env2/lib/python3.11/site-packages/torch/amp/autocast_mode.py\", line 44, in decorate_autocast\r\n    return func(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/cs/labs/oabend/avishai.elma/slm_eval/slm_eval/model/unit_lm.py\", line 118, in forward\r\n    def forward(self,\r\n  File \"/cs/labs/oabend/avishai.elma/slm_eval/.slm_env2/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/cs/labs/oabend/avishai.elma/slm_eval/.slm_env2/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/cs/labs/oabend/avishai.elma/slm_eval/.slm_env2/lib/python3.11/site-packages/transformers/models/qwen2/modeling_qwen2.py\", line 1109, in forward\r\n    @add_start_docstrings_to_model_forward(QWEN2_INPUTS_DOCSTRING)\r\n  File \"/cs/labs/oabend/avishai.elma/slm_eval/.slm_env2/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/cs/labs/oabend/avishai.elma/slm_eval/.slm_env2/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/cs/labs/oabend/avishai.elma/slm_eval/.slm_env2/lib/python3.11/site-packages/transformers/models/qwen2/modeling_qwen2.py\", line 895, in forward\r\n    layer_outputs = decoder_layer(\r\n                    ^^^^^^^^^^^^^^\r\n  File \"/cs/labs/oabend/avishai.elma/slm_eval/.slm_env2/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/cs/labs/oabend/avishai.elma/slm_eval/.slm_env2/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/cs/labs/oabend/avishai.elma/slm_eval/.slm_env2/lib/python3.11/site-packages/transformers/models/qwen2/modeling_qwen2.py\", line 584, in forward\r\n    def forward(\r\n  File \"/cs/labs/oabend/avishai.elma/slm_eval/.slm_env2/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/cs/labs/oabend/avishai.elma/slm_eval/.slm_env2/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/cs/labs/oabend/avishai.elma/slm_eval/.slm_env2/lib/python3.11/site-packages/transformers/models/qwen2/modeling_qwen2.py\", line 364, in forward\r\n    def forward(\r\n  File \"/cs/labs/oabend/avishai.elma/slm_eval/.slm_env2/lib/python3.11/site-packages/transformers/models/qwen2/modeling_qwen2.py\", line 419, in torch_dynamo_resume_in_forward_at_419\r\n    logger.warning_once(\r\n  File \"/cs/labs/oabend/avishai.elma/slm_eval/.slm_env2/lib/python3.11/site-packages/transformers/modeling_flash_attention_utils.py\", line 231, in _flash_attention_forward\r\n    def _flash_attention_forward(\r\n  File \"/cs/labs/oabend/avishai.elma/slm_eval/.slm_env2/lib/python3.11/site-packages/transformers/modeling_flash_attention_utils.py\", line 329, in torch_dynamo_resume_in__flash_attention_forward_at_329\r\n    max_length_q is not None or (query_length != 1 and not (torch.diff(position_ids, dim=-1) >= 0).all())\r\n  File \"/cs/labs/oabend/avishai.elma/slm_eval/.slm_env2/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py\", line 1269, in __call__\r\n    return self._torchdynamo_orig_callable(\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/cs/labs/oabend/avishai.elma/slm_eval/.slm_env2/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py\", line 1064, in __call__\r\n    result = self._inner_convert(\r\n             ^^^^^^^^^^^^^^^^^^^^\r\n  File \"/cs/labs/oabend/avishai.elma/slm_eval/.slm_env2/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py\", line 526, in __call__\r\n    return _compile(\r\n           ^^^^^^^^^\r\n  File \"/cs/labs/oabend/avishai.elma/slm_eval/.slm_env2/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py\", line 924, in _compile\r\n    guarded_code = compile_inner(code, one_graph, hooks, transform)\r\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/cs/labs/oabend/avishai.elma/slm_eval/.slm_env2/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py\", line 666, in compile_inner\r\n    return _compile_inner(code, one_graph, hooks, transform)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/cs/labs/oabend/avishai.elma/slm_eval/.slm_env2/lib/python3.11/site-packages/torch/_utils_internal.py\", line 87, in wrapper_function\r\n    return function(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/cs/labs/oabend/avishai.elma/slm_eval/.slm_env2/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py\", line 699, in _compile_inner\r\n    out_code = transform_code_object(code, transform)\r\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/cs/labs/oabend/avishai.elma/slm_eval/.slm_env2/lib/python3.11/site-packages/torch/_dynamo/bytecode_transformation.py\", line 1322, in transform_code_object\r\n    transformations(instructions, code_options)\r\n  File \"/cs/labs/oabend/avishai.elma/slm_eval/.slm_env2/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py\", line 219, in _fn\r\n    return fn(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^\r\n  File \"/cs/labs/oabend/avishai.elma/slm_eval/.slm_env2/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py\", line 634, in transform\r\n    tracer.run()\r\n  File \"/cs/labs/oabend/avishai.elma/slm_eval/.slm_env2/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py\", line 2796, in run\r\n    super().run()\r\n  File \"/cs/labs/oabend/avishai.elma/slm_eval/.slm_env2/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py\", line 983, in run\r\n    while self.step():\r\n          ^^^^^^^^^^^\r\n  File \"/cs/labs/oabend/avishai.elma/slm_eval/.slm_env2/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py\", line 895, in step\r\n    self.dispatch_table[inst.opcode](self, inst)\r\n  File \"/cs/labs/oabend/avishai.elma/slm_eval/.slm_env2/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py\", line 582, in wrapper\r\n    return inner_fn(self, inst)\r\n           ^^^^^^^^^^^^^^^^^^^^\r\n  File \"/cs/labs/oabend/avishai.elma/slm_eval/.slm_env2/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py\", line 1680, in CALL_FUNCTION_EX\r\n    self.call_function(fn, argsvars.items, kwargsvars)\r\n  File \"/cs/labs/oabend/avishai.elma/slm_eval/.slm_env2/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py\", line 830, in call_function\r\n    self.push(fn.call_function(self, args, kwargs))  # type: ignore[arg-type]\r\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/cs/labs/oabend/avishai.elma/slm_eval/.slm_env2/lib/python3.11/site-packages/torch/_dynamo/variables/functions.py\", line 324, in call_function\r\n    return super().call_function(tx, args, kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/cs/labs/oabend/avishai.elma/slm_eval/.slm_env2/lib/python3.11/site-packages/torch/_dynamo/variables/functions.py\", line 111, in call_function\r\n    return tx.inline_user_function_return(self, [*self.self_args(), *args], kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/cs/labs/oabend/avishai.elma/slm_eval/.slm_env2/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py\", line 836, in inline_user_function_return\r\n    return InliningInstructionTranslator.inline_call(self, fn, args, kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/cs/labs/oabend/avishai.elma/slm_eval/.slm_env2/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py\", line 3011, in inline_call\r\n    return cls.inline_call_(parent, func, args, kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/cs/labs/oabend/avishai.elma/slm_eval/.slm_env2/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py\", line 3139, in inline_call_\r\n    tracer.run()\r\n  File \"/cs/labs/oabend/avishai.elma/slm_eval/.slm_env2/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py\", line 983, in run\r\n    while self.step():\r\n          ^^^^^^^^^^^\r\n  File \"/cs/labs/oabend/avishai.elma/slm_eval/.slm_env2/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py\", line 895, in step\r\n    self.dispatch_table[inst.opcode](self, inst)\r\n  File \"/cs/labs/oabend/avishai.elma/slm_eval/.slm_env2/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py\", line 582, in wrapper\r\n    return inner_fn(self, inst)\r\n           ^^^^^^^^^^^^^^^^^^^^\r\n  File \"/cs/labs/oabend/avishai.elma/slm_eval/.slm_env2/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py\", line 2279, in CALL\r\n    self._call(inst)\r\n  File \"/cs/labs/oabend/avishai.elma/slm_eval/.slm_env2/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py\", line 2273, in _call\r\n    self.call_function(fn, args, kwargs)\r\n  File \"/cs/labs/oabend/avishai.elma/slm_eval/.slm_env2/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py\", line 830, in call_function\r\n    self.push(fn.call_function(self, args, kwargs))  # type: ignore[arg-type]\r\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/cs/labs/oabend/avishai.elma/slm_eval/.slm_env2/lib/python3.11/site-packages/torch/_dynamo/variables/misc.py\", line 1024, in call_function\r\n    return self.obj.call_method(tx, self.name, args, kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/cs/labs/oabend/avishai.elma/slm_eval/.slm_env2/lib/python3.11/site-packages/torch/_dynamo/variables/misc.py\", line 774, in call_method\r\n    return self.call_apply(tx, args, kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/cs/labs/oabend/avishai.elma/slm_eval/.slm_env2/lib/python3.11/site-packages/torch/_dynamo/variables/misc.py\", line 699, in call_apply\r\n    ).call_function(tx, args, kwargs)\r\n      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/cs/labs/oabend/avishai.elma/slm_eval/.slm_env2/lib/python3.11/site-packages/torch/_dynamo/variables/higher_order_ops.py\", line 2015, in call_function\r\n    (fwd_out, _), fwd_graph, fwd_freevars = speculate_subgraph(\r\n                                            ^^^^^^^^^^^^^^^^^^^\r\n  File \"/cs/labs/oabend/avishai.elma/slm_eval/.slm_env2/lib/python3.11/site-packages/torch/_dynamo/variables/higher_order_ops.py\", line 462, in speculate_subgraph\r\n    output = f.call_function(tx, args, sub_kwargs)\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/cs/labs/oabend/avishai.elma/slm_eval/.slm_env2/lib/python3.11/site-packages/torch/_dynamo/variables/functions.py\", line 324, in call_function\r\n    return super().call_function(tx, args, kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/cs/labs/oabend/avishai.elma/slm_eval/.slm_env2/lib/python3.11/site-packages/torch/_dynamo/variables/functions.py\", line 111, in call_function\r\n    return tx.inline_user_function_return(self, [*self.self_args(), *args], kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/cs/labs/oabend/avishai.elma/slm_eval/.slm_env2/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py\", line 836, in inline_user_function_return\r\n    return InliningInstructionTranslator.inline_call(self, fn, args, kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/cs/labs/oabend/avishai.elma/slm_eval/.slm_env2/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py\", line 3011, in inline_call\r\n    return cls.inline_call_(parent, func, args, kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/cs/labs/oabend/avishai.elma/slm_eval/.slm_env2/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py\", line 3139, in inline_call_\r\n    tracer.run()\r\n  File \"/cs/labs/oabend/avishai.elma/slm_eval/.slm_env2/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py\", line 983, in run\r\n    while self.step():\r\n          ^^^^^^^^^^^\r\n  File \"/cs/labs/oabend/avishai.elma/slm_eval/.slm_env2/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py\", line 895, in step\r\n    self.dispatch_table[inst.opcode](self, inst)\r\n  File \"/cs/labs/oabend/avishai.elma/slm_eval/.slm_env2/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py\", line 582, in wrapper\r\n    return inner_fn(self, inst)\r\n           ^^^^^^^^^^^^^^^^^^^^\r\n  File \"/cs/labs/oabend/avishai.elma/slm_eval/.slm_env2/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py\", line 2279, in CALL\r\n    self._call(inst)\r\n  File \"/cs/labs/oabend/avishai.elma/slm_eval/.slm_env2/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py\", line 2273, in _call\r\n    self.call_function(fn, args, kwargs)\r\n  File \"/cs/labs/oabend/avishai.elma/slm_eval/.slm_env2/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py\", line 830, in call_function\r\n    self.push(fn.call_function(self, args, kwargs))  # type: ignore[arg-type]\r\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/cs/labs/oabend/avishai.elma/slm_eval/.slm_env2/lib/python3.11/site-packages/torch/_dynamo/variables/torch.py\", line 897, in call_function\r\n    tensor_variable = wrap_fx_proxy(\r\n                      ^^^^^^^^^^^^^^\r\n  File \"/cs/labs/oabend/avishai.elma/slm_eval/.slm_env2/lib/python3.11/site-packages/torch/_dynamo/variables/builder.py\", line 2037, in wrap_fx_proxy\r\n    return wrap_fx_proxy_cls(target_cls=TensorVariable, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/cs/labs/oabend/avishai.elma/slm_eval/.slm_env2/lib/python3.11/site-packages/torch/_dynamo/variables/builder.py\", line 2124, in wrap_fx_proxy_cls\r\n    example_value = get_fake_value(proxy.node, tx, allow_non_graph_fake=True)\r\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/cs/labs/oabend/avishai.elma/slm_eval/.slm_env2/lib/python3.11/site-packages/torch/_dynamo/utils.py\", line 2082, in get_fake_value\r\n    raise TorchRuntimeError(str(e)).with_traceback(e.__traceback__) from None\r\n  File \"/cs/labs/oabend/avishai.elma/slm_eval/.slm_env2/lib/python3.11/site-packages/torch/_dynamo/utils.py\", line 2017, in get_fake_value\r\n    ret_val = wrap_fake_exception(\r\n              ^^^^^^^^^^^^^^^^^^^^\r\n  File \"/cs/labs/oabend/avishai.elma/slm_eval/.slm_env2/lib/python3.11/site-packages/torch/_dynamo/utils.py\", line 1574, in wrap_fake_exception\r\n    return fn()\r\n           ^^^^\r\n  File \"/cs/labs/oabend/avishai.elma/slm_eval/.slm_env2/lib/python3.11/site-packages/torch/_dynamo/utils.py\", line 2018, in <lambda>\r\n    lambda: run_node(tx.output, node, args, kwargs, nnmodule)\r\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/cs/labs/oabend/avishai.elma/slm_eval/.slm_env2/lib/python3.11/site-packages/torch/_dynamo/utils.py\", line 2150, in run_node\r\n    raise RuntimeError(make_error_message(e)).with_traceback(\r\n  File \"/cs/labs/oabend/avishai.elma/slm_eval/.slm_env2/lib/python3.11/site-packages/torch/_dynamo/utils.py\", line 2132, in run_node\r\n    return node.target(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/cs/labs/oabend/avishai.elma/slm_eval/.slm_env2/lib/python3.11/site-packages/torch/_ops.py\", line 1116, in __call__\r\n    return self._op(*args, **(kwargs or {}))\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\ntorch._dynamo.exc.TorchRuntimeError: Failed running call_function flash_attn._flash_attn_varlen_forward(*(FakeTensor(..., device='cuda:0', size=(s3, s4, s5), dtype=torch.float16,\r\n           grad_fn=<AsStridedBackward0>), FakeTensor(..., device='cuda:0', size=(s6, s7, s8), dtype=torch.float16,\r\n           grad_fn=<Error>), FakeTensor(..., device='cuda:0', size=(s9, s10, s11), dtype=torch.float16,\r\n           grad_fn=<Error>), FakeTensor(..., device='cuda:0', size=(s13,), dtype=torch.int32), FakeTensor(..., device='cuda:0', size=(s13,), dtype=torch.int32), FakeTensor(..., device='cuda:0', size=(), dtype=torch.int64), FakeTensor(..., device='cuda:0', size=(), dtype=torch.int64), 0.0, FloatPow(ToFloat(s5), -0.5)), **{'causal': True, 'window_size_left': -1, 'window_size_right': -1, 'softcap': 0.0, 'alibi_slopes': None, 'return_softmax': False, 'block_table': None}):\r\nflash_attn::_flash_attn_varlen_forward() Expected a value of type 'int' for argument 'max_seqlen_q' but instead found type 'FakeTensor'.\r\nPosition: 5\r\nValue: FakeTensor(..., device='cuda:0', size=(), dtype=torch.int64)\r\nDeclaration: flash_attn::_flash_attn_varlen_forward(Tensor q, Tensor k, Tensor v, Tensor cu_seqlens_q, Tensor cu_seqlens_k, SymInt max_seqlen_q, SymInt max_seqlen_k, float dropout_p, float softmax_scale, bool causal, SymInt window_size_left=-1, SymInt window_size_right=-1, float softcap=0., Tensor? alibi_slopes=None, bool return_softmax=False, Tensor? block_table=None, Tensor? leftpad_k=None, Tensor? seqused_k=None) -> (Tensor, Tensor, Tensor, Tensor)\r\nCast error details: Unable to cast Python instance of type <class 'torch._subclasses.fake_tensor.FakeTensor'> to C++ type '?' (#define PYBIND11_DETAILED_ERROR_MESSAGES or compile in debug mode for details)\r\n\r\nfrom user code:\r\n   File \"/cs/labs/oabend/avishai.elma/slm_eval/.slm_env2/lib/python3.11/site-packages/transformers/modeling_flash_attention_utils.py\", line 346, in torch_dynamo_resume_in__flash_attention_forward_at_335\r\n    attn_output = flash_attn_varlen_func(\r\n  File \"/cs/labs/oabend/avishai.elma/slm_eval/.slm_env2/lib/python3.11/site-packages/flash_attn/flash_attn_interface.py\", line 1412, in flash_attn_varlen_func\r\n    return FlashAttnVarlenFunc.apply(\r\n  File \"/cs/labs/oabend/avishai.elma/slm_eval/.slm_env2/lib/python3.11/site-packages/flash_attn/flash_attn_interface.py\", line 901, in forward\r\n    out_padded, softmax_lse, S_dmask, rng_state = _wrapped_flash_attn_varlen_forward(\r\n\r\nSet TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\r\n\r\n\r\nYou can suppress this exception and fall back to eager by setting:\r\n    import torch._dynamo\r\n    torch._dynamo.config.suppress_errors = True\r\n```\r\n\r\nwhen not using complie it works.\r\nwhen using compile but not DataCollatorWithFlattening it doesn't crash but i am getting a graph break:\r\n```\r\nW0109 16:15:01.606000 4138800 torch/_dynamo/variables/tensor.py:776] [0/0] Graph break from `Tensor.item()`, consider setting:\r\nW0109 16:15:01.606000 4138800 torch/_dynamo/variables/tensor.py:776] [0/0]     torch._dynamo.config.capture_scalar_outputs = True\r\nW0109 16:15:01.606000 4138800 torch/_dynamo/variables/tensor.py:776] [0/0] or:\r\nW0109 16:15:01.606000 4138800 torch/_dynamo/variables/tensor.py:776] [0/0]     env TORCHDYNAMO_CAPTURE_SCALAR_OUTPUTS=1\r\nW0109 16:15:01.606000 4138800 torch/_dynamo/variables/tensor.py:776] [0/0] to include these operations in the captured graph.\r\nW0109 16:15:01.606000 4138800 torch/_dynamo/variables/tensor.py:776] [0/0] \r\nW0109 16:15:01.606000 4138800 torch/_dynamo/variables/tensor.py:776] [0/0] Graph break: from user code at:\r\nW0109 16:15:01.606000 4138800 torch/_dynamo/variables/tensor.py:776] [0/0]   File \"/cs/labs/oabend/avishai.elma/slm_eval/.slm_env2/lib/python3.11/site-packages/accelerate/utils/operations.py\", line 823, in forward\r\nW0109 16:15:01.606000 4138800 torch/_dynamo/variables/tensor.py:776] [0/0]     return model_forward(*args, **kwargs)\r\nW0109 16:15:01.606000 4138800 torch/_dynamo/variables/tensor.py:776] [0/0]   File \"/cs/labs/oabend/avishai.elma/slm_eval/.slm_env2/lib/python3.11/site-packages/accelerate/utils/operations.py\", line 811, in __call__\r\nW0109 16:15:01.606000 4138800 torch/_dynamo/variables/tensor.py:776] [0/0]     return convert_to_fp32(self.model_forward(*args, **kwargs))\r\nW0109 16:15:01.606000 4138800 torch/_dynamo/variables/tensor.py:776] [0/0]   File \"/cs/labs/oabend/avishai.elma/slm_eval/.slm_env2/lib/python3.11/site-packages/torch/amp/autocast_mode.py\", line 44, in decorate_autocast\r\nW0109 16:15:01.606000 4138800 torch/_dynamo/variables/tensor.py:776] [0/0]     return func(*args, **kwargs)\r\nW0109 16:15:01.606000 4138800 torch/_dynamo/variables/tensor.py:776] [0/0]   File \"/cs/labs/oabend/avishai.elma/slm_eval/slm_eval/model/unit_lm.py\", line 138, in forward\r\nW0109 16:15:01.606000 4138800 torch/_dynamo/variables/tensor.py:776] [0/0]     outputs = self.lm(\r\nW0109 16:15:01.606000 4138800 torch/_dynamo/variables/tensor.py:776] [0/0]   File \"/cs/labs/oabend/avishai.elma/slm_eval/.slm_env2/lib/python3.11/site-packages/transformers/models/qwen2/modeling_qwen2.py\", line 1165, in forward\r\nW0109 16:15:01.606000 4138800 torch/_dynamo/variables/tensor.py:776] [0/0]     outputs = self.model(\r\nW0109 16:15:01.606000 4138800 torch/_dynamo/variables/tensor.py:776] [0/0]   File \"/cs/labs/oabend/avishai.elma/slm_eval/.slm_env2/lib/python3.11/site-packages/transformers/models/qwen2/modeling_qwen2.py\", line 864, in forward\r\nW0109 16:15:01.606000 4138800 torch/_dynamo/variables/tensor.py:776] [0/0]     causal_mask = self._update_causal_mask(\r\nW0109 16:15:01.606000 4138800 torch/_dynamo/variables/tensor.py:776] [0/0]   File \"/cs/labs/oabend/avishai.elma/slm_eval/.slm_env2/lib/python3.11/site-packages/transformers/models/qwen2/modeling_qwen2.py\", line 943, in _update_causal_mask\r\nW0109 16:15:01.606000 4138800 torch/_dynamo/variables/tensor.py:776] [0/0]     if attention_mask is not None and 0.0 in attention_mask:\r\nW0109 16:15:01.606000 4138800 torch/_dynamo/variables/tensor.py:776] [0/0] \r\nW0109 16:15:01.606000 4138800 torch/_dynamo/variables/tensor.py:776] [0/0] \r\n```\n\n### Expected behavior\n\nTraining should work",
    "state": "closed",
    "created_at": "2025-01-09T14:22:05+00:00",
    "closed_at": "2025-01-09T14:23:11+00:00",
    "updated_at": "2025-01-23T08:52:15+00:00",
    "author": "avishaiElmakies",
    "author_type": "User",
    "comments_count": 3,
    "reactions_count": 0,
    "labels": "bug",
    "milestone": null,
    "closed_by": "avishaiElmakies",
    "resolution_time_hours": 0.018333333333333333,
    "first_comments": [
      {
        "author": "ArthurZucker",
        "created_at": "2025-01-09T14:28:08+00:00",
        "body": "Hey! Any reason why you closed this? "
      },
      {
        "author": "avishaiElmakies",
        "created_at": "2025-01-09T14:32:52+00:00",
        "body": "Hi @ArthurZucker. it seems that I opened the same issue twice because of something weird with github. When I submitted the first issue I got a 404. So I thought it didn't accept the original issue i submitted so i submitted another one.  after submitting this i noticed that the original issue was in fact submitted. I can't delete it so I closed it. sorry about that!\r\n\r\nThe original issue https://github.com/huggingface/transformers/issues/35588"
      },
      {
        "author": "ArthurZucker",
        "created_at": "2025-01-23T08:52:14+00:00",
        "body": "Okay thanks for informing me! ðŸ¤— "
      }
    ],
    "url": "https://github.com/huggingface/transformers/issues/35590"
  },
  {
    "repository": "huggingface/transformers",
    "issue_number": 35584,
    "title": "Malformed config when saving & loading locally custom models",
    "body": "### System Info\n\n- `transformers` version: 4.47.1\r\n- Platform: Linux-6.8.0-51-generic-x86_64-with-glibc2.39\r\n- Python version: 3.11.10\r\n- Huggingface_hub version: 0.27.0\r\n- Safetensors version: 0.4.5\r\n- Accelerate version: 1.2.1\r\n- Accelerate config:    not found\r\n- PyTorch version (GPU?): 2.5.1+cu124 (True)\r\n- Tensorflow version (GPU?): not installed (NA)\r\n- Flax version (CPU?/GPU?/TPU?): not installed (NA)\r\n- Jax version: not installed\r\n- JaxLib version: not installed\r\n- Using distributed or parallel set-up in script?:  NA\r\n- Using GPU in script?: Yes, but also NA\r\n- GPU type: NVIDIA GeForce RTX 4090\n\n### Who can help?\n\n@ArthurZucker\n\n### Information\n\n- [ ] The official example scripts\n- [X] My own modified scripts\n\n### Tasks\n\n- [X] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\n- [ ] My own task or dataset (give details below)\n\n### Reproduction\n\n```\r\nfrom transformers import AutoModel, AutoTokenizer, AutoConfig\r\n\r\n# Load a custom model from the Hub\r\nmodel = AutoModel.from_pretrained(\"jinaai/jina-embeddings-v2-base-de\", trust_remote_code=True)\r\ntokenizer = AutoTokenizer.from_pretrained(\"jinaai/jina-embeddings-v2-base-de\")\r\n\r\n# Save them locally\r\nmodel.save_pretrained(\"tmp\")\r\ntokenizer.save_pretrained(\"tmp\")\r\n\r\n# Now load that model again, via a Config\r\nconfig = AutoConfig.from_pretrained(\"tmp\", trust_remote_code=True)\r\nprint(config.__class__)\r\n# <class 'transformers_modules.jinaai.jina-bert-implementation.f3ec4cf7de7e561007f27c9efc7148b0bd713f81.configuration_bert.JinaBertConfig'>\r\nlocal_model = AutoModel.from_pretrained(\"tmp\", config=config, trust_remote_code=True)\r\n\r\n# And save it again\r\nlocal_model.save_pretrained(\"tmp_2\")\r\ntokenizer.save_pretrained(\"tmp_2\")\r\n\r\n# Now load that model again, via a Config\r\nconfig_2 = AutoConfig.from_pretrained(\"tmp_2\", trust_remote_code=True)\r\nprint(config_2.__class__)\r\n# <class 'transformers_modules.tmp_2.configuration_bert.JinaBertConfig'>\r\n# The second time around, the config is not the same as the first time\r\nlocal_model_2 = AutoModel.from_pretrained(\"tmp_2\", config=config_2, trust_remote_code=True)\r\n# ValueError: The model class you are passing has a `config_class` attribute that is not consistent\r\n# with the config class you passed (model has\r\n# <class 'transformers_modules.jinaai.jina-bert-implementation.f3ec4cf7de7e561007f27c9efc7148b0bd713f81.configuration_bert.JinaBertConfig'>\r\n# and you passed <class 'transformers_modules.tmp_2.configuration_bert.JinaBertConfig'>. Fix one of those so they match!\r\n```\n\n### Expected behavior\n\nModel should load without raising a `ValueError`",
    "state": "closed",
    "created_at": "2025-01-09T13:39:40+00:00",
    "closed_at": "2025-01-27T18:37:31+00:00",
    "updated_at": "2025-01-27T18:38:01+00:00",
    "author": "Alicimo",
    "author_type": "User",
    "comments_count": 2,
    "reactions_count": 0,
    "labels": "bug",
    "milestone": null,
    "closed_by": "Rocketknight1",
    "resolution_time_hours": 436.96416666666664,
    "first_comments": [
      {
        "author": "Rocketknight1",
        "created_at": "2025-01-09T14:29:38+00:00",
        "body": "This is related to my PR #29854, and it's definitely a bug, yes. Let me see if I can loosen up that test a little."
      },
      {
        "author": "Rocketknight1",
        "created_at": "2025-01-27T18:38:00+00:00",
        "body": "@Alicimo this should now be fixed, please let me know if you're still getting issues after installing the latest version from `main`!"
      }
    ],
    "url": "https://github.com/huggingface/transformers/issues/35584"
  },
  {
    "repository": "huggingface/transformers",
    "issue_number": 35577,
    "title": "<spam> ",
    "body": "bc1q298tpwlyca65qzegsg0ld9phd3056cg76ml7rq",
    "state": "closed",
    "created_at": "2025-01-09T09:46:52+00:00",
    "closed_at": "2025-01-09T14:18:43+00:00",
    "updated_at": "2025-01-09T14:53:22+00:00",
    "author": "MORTEZAMIRSALI",
    "author_type": "User",
    "comments_count": 1,
    "reactions_count": 0,
    "labels": "",
    "milestone": null,
    "closed_by": "Rocketknight1",
    "resolution_time_hours": 4.530833333333334,
    "first_comments": [
      {
        "author": "MORTEZAMIRSALI",
        "created_at": "2025-01-09T14:53:20+00:00",
        "body": "Spam"
      }
    ],
    "url": "https://github.com/huggingface/transformers/issues/35577"
  },
  {
    "repository": "huggingface/transformers",
    "issue_number": 35574,
    "title": "`Nan` logits when performing inference using ModernBERT",
    "body": "### System Info\r\n\r\ntransformers == 4.48.0.dev0\r\ntorch == 2.2.2\r\n\r\n### Description\r\nI have finetuned ModelBERT model for multi-label task and when performing batched inference I am getting `Nan` values for logits in a batch except for one sample. So, in each batch except one sample all the remaining logits are `Nan`\r\n### Who can help?\r\n\r\n@tomaarsen @ArthurZucker \r\n\r\n### Information\r\n\r\n- [ ] The official example scripts\r\n- [ ] My own modified scripts\r\n\r\n### Tasks\r\n\r\n- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\r\n- [ ] My own task or dataset (give details below)\r\n\r\n### Reproduction\r\n\r\nSimply passing model(**batch) is resulting in this - while with bs=1 no issue exists.\r\nCode: \r\n\r\n```\r\nmodel.eval()\r\nlabels, predictions, confidences = [], [], []\r\n\r\nwith torch.no_grad():\r\n    for batch in tqdm(val_loader):\r\n        labels.extend(batch['labels'].cpu().numpy())\r\n        batch.pop('labels')\r\n        batch = {k: v.to(device) for k, v in batch.items()}\r\n\r\n        outputs = model(**batch)\r\n\r\n        probs = torch.sigmoid(outputs.logits)\r\n        preds = (probs > 0.5).int()\r\n        confs = probs\r\n\r\n        predictions.extend(preds.cpu().numpy())\r\n        confidences.extend(confs.cpu().numpy())\r\n```\r\n\r\n<img width=\"1124\" alt=\"image\" src=\"https://github.com/user-attachments/assets/fe10bfdb-4921-4733-bae4-7f87d68581f4\" />\r\n\r\n\r\n### Expected behavior\r\n\r\nExpecting some values instead of `Nan`",
    "state": "closed",
    "created_at": "2025-01-09T07:20:10+00:00",
    "closed_at": "2025-01-10T10:31:51+00:00",
    "updated_at": "2025-01-10T10:33:46+00:00",
    "author": "yaswanthg15",
    "author_type": "User",
    "comments_count": 3,
    "reactions_count": 0,
    "labels": "bug",
    "milestone": null,
    "closed_by": "yaswanthg15",
    "resolution_time_hours": 27.19472222222222,
    "first_comments": [
      {
        "author": "tomaarsen",
        "created_at": "2025-01-09T07:21:31+00:00",
        "body": "Could you add a snippet to reproduce? I've done lots of inference and I've never seen `nan`s yet.\n\n- Tom Aarsen"
      },
      {
        "author": "yaswanthg15",
        "created_at": "2025-01-09T07:26:35+00:00",
        "body": "@tomaarsen While it is not possible for me to make the model public - but coming to the issue I am performing a simple inference using pytorch loop."
      },
      {
        "author": "ArthurZucker",
        "created_at": "2025-01-09T14:32:03+00:00",
        "body": "In order to help we would need to know: \r\n- the dtype\r\n- the attention implementation \r\nThis should help us as a lot of nans can come from, sdpa or casting"
      }
    ],
    "url": "https://github.com/huggingface/transformers/issues/35574"
  },
  {
    "repository": "huggingface/transformers",
    "issue_number": 35568,
    "title": "Any plans to integrate GTE model natively into transformers",
    "body": "### Model description\r\n\r\nAny plans to integrate `gte` model natively into transformers as right now we are using this model with `trust_remote_code=True` argument\r\n\r\n### Open source status\r\n\r\n- [X] The model implementation is available\r\n- [X] The model weights are available\r\n\r\n### Provide useful links for the implementation\r\nModel Implementation: https://huggingface.co/Alibaba-NLP/new-impl/blob/main/modeling.py\r\nModel Weights: https://huggingface.co/Alibaba-NLP/gte-base-en-v1.5",
    "state": "closed",
    "created_at": "2025-01-08T18:04:25+00:00",
    "closed_at": "2025-01-23T05:30:44+00:00",
    "updated_at": "2025-01-23T05:30:44+00:00",
    "author": "yaswanth19",
    "author_type": "User",
    "comments_count": 8,
    "reactions_count": 0,
    "labels": "New model",
    "milestone": null,
    "closed_by": "yaswanth19",
    "resolution_time_hours": 347.43861111111113,
    "first_comments": [
      {
        "author": "yaswanth19",
        "created_at": "2025-01-08T18:05:18+00:00",
        "body": "@ArthurZucker @Rocketknight1 If we do intend to integrate this model then I can work on creating a draft PR."
      },
      {
        "author": "mahimairaja",
        "created_at": "2025-01-08T22:06:13+00:00",
        "body": "Is there a place, I can help you to add the model @yaswanth19 ?"
      },
      {
        "author": "yaswanth19",
        "created_at": "2025-01-13T14:09:40+00:00",
        "body": "@ArthurZucker A gentle ping. \n@tomaarsen Can this also be integrated into `sentence-transformers` "
      },
      {
        "author": "Rocketknight1",
        "created_at": "2025-01-14T14:43:33+00:00",
        "body": "This seems popular enough to justify an integration, yes. WDYT @tomaarsen?"
      },
      {
        "author": "tomaarsen",
        "created_at": "2025-01-17T16:32:02+00:00",
        "body": "@Rocketknight1  \nI suspect there are 3 popular and promising models built on this architecture:\n* https://huggingface.co/Alibaba-NLP/gte-large-en-v1.5\n* https://huggingface.co/Alibaba-NLP/gte-base-en-v1.5\n* https://huggingface.co/Snowflake/snowflake-arctic-embed-m-v2.0\n\nBeyond that, the authors are now using another implementation on top of Qwen:\n* https://huggingface.co/Alibaba-NLP/gte-Qwen2-7B-instruct\n* https://huggingface.co/Alibaba-NLP/gte-Qwen2-1.5B-instruct\n\nSome of the mechanisms are similar to ModernBERT (I see unpadding), but some differ as well (xformers). It might require a good bit of effort to get everything to line up with `transformers`, and I think there's a chance that there will be no more big models based on this architecture.\n\n- Tom Aarsen"
      }
    ],
    "url": "https://github.com/huggingface/transformers/issues/35568"
  },
  {
    "repository": "huggingface/transformers",
    "issue_number": 35561,
    "title": "Any plans for a Test-Time Compute Scaling Package or Module?",
    "body": "### Feature request\n\nLast month, @lewtun , @edbeeching , and @srush posted this blog post about test-time compute scaling, which got me curious about whether there are any plans to create a Transformers module or a separate package for test-time compute scaling methods like self-consistency, best-of-n, beam search, etc.\n\n### Motivation\n\nScaling compute at test time can be combined with Agentic workflows to improve productivity, as well as paired with reinforcement fine-tuning methods for o1 like performance.\n\n### Your contribution\n\nI am very interested in the development of these modules; however, I am skeptical about their place in the HF ecosystem, whether as a module or a package, and how they will interact with the HF ecosystem. Therefore, I would like to seek your opinions and feedback on this matter.",
    "state": "closed",
    "created_at": "2025-01-08T10:34:21+00:00",
    "closed_at": "2025-01-08T13:39:37+00:00",
    "updated_at": "2025-01-08T13:39:37+00:00",
    "author": "August-murr",
    "author_type": "User",
    "comments_count": 1,
    "reactions_count": 0,
    "labels": "Feature request",
    "milestone": null,
    "closed_by": "edbeeching",
    "resolution_time_hours": 3.0877777777777777,
    "first_comments": [
      {
        "author": "edbeeching",
        "created_at": "2025-01-08T13:39:37+00:00",
        "body": "I think for the time being this feature is best kept in an external repo. I think TRL would be a more suitable place, but the field is moving so fast that it would be hard to maintain and the TRL team is stretched as it is. We welcome contributions, of course."
      }
    ],
    "url": "https://github.com/huggingface/transformers/issues/35561"
  },
  {
    "repository": "huggingface/transformers",
    "issue_number": 35559,
    "title": "iframe",
    "body": "### System Info\n\nTest\n\n### Who can help?\n\n_No response_\n\n### Information\n\n- [ ] The official example scripts\n- [ ] My own modified scripts\n\n### Tasks\n\n- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\n- [ ] My own task or dataset (give details below)\n\n### Reproduction\n\n<iframe\r\n    src=\"https://hkchengrex-mmaudio.hf.space\"\r\n    frameborder=\"0\"\r\n    width=\"850\"\r\n    height=\"450\"\r\n></iframe>\n\n### Expected behavior\n\nHello, what is the reason for the issue of the application not functioning in an iframe for some of the spaces? For example:\r\n\r\n\r\n<iframe\r\n    src=\"https://hkchengrex-mmaudio.hf.space\"\r\n    frameborder=\"0\"\r\n    width=\"850\"\r\n    height=\"450\"\r\n></iframe>\r\nWhen we use this in a webpage, the application does not work. Is there a solution to run it inside the iframe? Since I can't use Web components because the Gradio library is not available in my region. Is there a way to run that application using an iframe?",
    "state": "closed",
    "created_at": "2025-01-08T08:17:40+00:00",
    "closed_at": "2025-01-08T15:53:08+00:00",
    "updated_at": "2025-01-08T15:53:08+00:00",
    "author": "Hamed744",
    "author_type": "User",
    "comments_count": 1,
    "reactions_count": 0,
    "labels": "bug",
    "milestone": null,
    "closed_by": "Rocketknight1",
    "resolution_time_hours": 7.591111111111111,
    "first_comments": [
      {
        "author": "Rocketknight1",
        "created_at": "2025-01-08T15:53:08+00:00",
        "body": "This should probably be asked on the [Gradio repo](https://github.com/gradio-app/gradio) instead!"
      }
    ],
    "url": "https://github.com/huggingface/transformers/issues/35559"
  },
  {
    "repository": "huggingface/transformers",
    "issue_number": 35555,
    "title": "ModernBERT does not have `inputs_embeds` input",
    "body": "### Feature request\n\nImplement the `inputs_embeds` input field in ModernBERT and its variants.\n\n### Motivation\n\nAs with other models (BERT, LLaMA, etc.), Transformers provides an option to feed the input embeddings directly, which is important for research on different input representations and embedding methods. This is also important for parity with BERT.\n\n### Your contribution\n\nI wrote some preliminary code that adds the input field. I also found a potential duplication of `_unpad_modernbert_input`. It appears both in the internal model and in the Masked LM version, which I think could be cleaned (@warner-benjamin thoughts?): https://github.com/huggingface/transformers/blob/main/src/transformers/models/modernbert/modeling_modernbert.py#L1055\r\n\r\n\r\n",
    "state": "closed",
    "created_at": "2025-01-07T19:02:56+00:00",
    "closed_at": "2025-01-08T14:15:09+00:00",
    "updated_at": "2025-01-08T14:15:10+00:00",
    "author": "tbennun",
    "author_type": "User",
    "comments_count": 2,
    "reactions_count": 0,
    "labels": "Feature request",
    "milestone": null,
    "closed_by": "tbennun",
    "resolution_time_hours": 19.203611111111112,
    "first_comments": [
      {
        "author": "KoichiYasuoka",
        "created_at": "2025-01-08T08:50:37+00:00",
        "body": "Pull Request #35373 is ongoing."
      },
      {
        "author": "tbennun",
        "created_at": "2025-01-08T14:15:09+00:00",
        "body": "Sorry, for some reason GitHub search did not show that PR when looking up â€œModernBERTâ€. Thanks for the pointer!"
      }
    ],
    "url": "https://github.com/huggingface/transformers/issues/35555"
  },
  {
    "repository": "huggingface/transformers",
    "issue_number": 35547,
    "title": "FA2 broken for Cohere2 if Optional `Mask` is not passed in `forward`",
    "body": "### System Info\r\n\r\ntransformers==4.48.0.dev0 (from git+https://github.com/huggingface/transformers.git@5615a393691c81e00251e420c73e4d04c6fe22e5)\r\n\r\n### Who can help?\r\n\r\n@ArthurZucker @Cyrilvallez @SunMarc \r\n\r\n### Information\r\n\r\n- [ ] The official example scripts\r\n- [X] My own modified scripts\r\n\r\n### Tasks\r\n\r\n- [X] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\r\n- [ ] My own task or dataset (give details below)\r\n\r\n### Reproduction\r\n\r\nCheck our CI test failures:\r\n\r\n# Gemma\r\nhttps://github.com/ModelCloud/GPTQModel/actions/runs/12651906072/job/35253942521#step:12:1164\r\n\r\n# Cohere2\r\nhttps://github.com/ModelCloud/GPTQModel/actions/runs/12651906072/job/35253938235#step:12:922\r\n\r\nWe enabled FA2 by default on GPTQModel for inference of gptq quantized models and our CI tests are failing for multiple models. This looks like a regression in the `fa2` attention code where `seq_len` is never set if `mask is None`.  FA2 forward requires `seq_len`:\r\n\r\nhttps://github.com/huggingface/transformers/blob/fc74e3976d090b655a8278ca1b70f784ff8b7938/src/transformers/models/cohere2/modeling_cohere2.py#L235-L270\r\n\r\n@SunMarc  I don't think this is related to quantization and @ArthurZucker The FA2 code above is broken if `mask` is not passed or `None` as `seq_len` will never be set.  The `mask` param is explicitly declared as `Optional`.\r\n\r\n\r\n### Expected behavior\r\n\r\nWork and not crash.",
    "state": "closed",
    "created_at": "2025-01-07T14:30:25+00:00",
    "closed_at": "2025-01-09T16:54:59+00:00",
    "updated_at": "2025-01-09T16:54:59+00:00",
    "author": "Qubitium",
    "author_type": "User",
    "comments_count": 5,
    "reactions_count": 0,
    "labels": "bug",
    "milestone": null,
    "closed_by": "Cyrilvallez",
    "resolution_time_hours": 50.409444444444446,
    "first_comments": [
      {
        "author": "SunMarc",
        "created_at": "2025-01-07T15:03:59+00:00",
        "body": "cohere2 flash attention 2 code is the original one from the author as you can see [here](https://github.com/huggingface/transformers/pull/35224). cohere2 model is one of the few models that code its own  `flash_attention_forward` function. Maybe @alexrs-cohere can help you fix this issue. \r\n\r\nAlso, we are refactoring the attention https://github.com/huggingface/transformers/pull/35235, please let us know if you face any issues with other models ! "
      },
      {
        "author": "Cyrilvallez",
        "created_at": "2025-01-07T15:12:14+00:00",
        "body": "Not entirely sure why you chose this particular commit as a version, but this does not seem to be an issue on `main`"
      },
      {
        "author": "Qubitium",
        "created_at": "2025-01-07T16:36:26+00:00",
        "body": "> Not entirely sure why you chose this particular commit as a version, but this does not seem to be an issue on `main`\r\n\r\n@Cyrilvallez  My mistake. Our CI was force checking out a commit post 4.47.1 but not the latest main since Cohere2 code was merged right after 4.47.1 release. \r\n\r\nSo it looks like Cohere2 is the only model that still has the broken implementation code for Fa2.  \r\n\r\n@alexrs-cohere Please check. \r\n\r\nhttps://github.com/huggingface/transformers/blob/7f7677307cfc8722ccc4680b993811098379844c/src/transformers/models/cohere2/modeling_cohere2.py#L268\r\n"
      },
      {
        "author": "Cyrilvallez",
        "created_at": "2025-01-07T16:41:54+00:00",
        "body": "Ha indeed the issue persists for Cohere2! Thanks, I'll open a PR!"
      },
      {
        "author": "alexrs-cohere",
        "created_at": "2025-01-07T17:02:58+00:00",
        "body": "Thanks for reporting this @Qubitium!\r\n\r\n@Cyrilvallez let me know when the PR is ready and if you need any support from me!"
      }
    ],
    "url": "https://github.com/huggingface/transformers/issues/35547"
  },
  {
    "repository": "huggingface/transformers",
    "issue_number": 35545,
    "title": "ModernBERT export to onnx error",
    "body": "### System Info\n\n- `transformers` version: 4.48.0.dev0\r\n- Platform: Linux-5.15.0-84-generic-x86_64-with-glibc2.35\r\n- Python version: 3.11.11\r\n- Huggingface_hub version: 0.27.0\r\n- Safetensors version: 0.4.5\r\n- Accelerate version: 1.2.1\r\n- Accelerate config: \tnot found\r\n- PyTorch version (GPU?): 2.5.1 (True)\r\n- Tensorflow version (GPU?): not installed (NA)\r\n- Flax version (CPU?/GPU?/TPU?): not installed (NA)\r\n- Jax version: not installed\r\n- JaxLib version: not installed\r\n- Using distributed or parallel set-up in script?: <fill in>\r\n- Using GPU in script?: <fill in>\r\n- GPU type: NVIDIA GeForce RTX 4090\n\n### Who can help?\n\n@ArthurZucker\n\n### Information\n\n- [ ] The official example scripts\n- [X] My own modified scripts\n\n### Tasks\n\n- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\n- [X] My own task or dataset (give details below)\n\n### Reproduction\n\nWhen I trained a classification model based on ModernBERT tried to export to onnx with the following script.\r\n```\r\nimport torch\r\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\r\n\r\n\r\ndef export():\r\n\r\n    tokenizer = AutoTokenizer.from_pretrained(\"answerdotai/ModernBERT-base\", model_max_length=4096)\r\n    model = AutoModelForSequenceClassification.from_pretrained(\r\n            \"./checkpoints\",\r\n            num_labels=3,\r\n            # reference_compile=False,\r\n            )\r\n\r\n    model.eval()\r\n\r\n    samples = ['examples']\r\n\r\n    tokenized = tokenizer(samples,\r\n            return_tensors='pt',\r\n            max_length=tokenizer.model_max_length,\r\n            padding='max_length',\r\n            truncation=True)\r\n    input_ids = tokenized['input_ids'].to('cuda')\r\n    attention_mask = tokenized['attention_mask'].to('cuda')\r\n    model = model.to('cuda')\r\n\r\n    with torch.no_grad():\r\n        torch.onnx.export(\r\n                model,\r\n                (input_ids, attention_mask),\r\n                './model.onnx',\r\n                input_names=[\"input_ids\", \"attention_mask\"],\r\n                output_names=[\"logits\"],\r\n                )\r\n\r\nif __name__ == '__main__':\r\n    export()\r\n```\r\nGot errors. May Be related https://github.com/pytorch/pytorch/issues/104748\r\n```\r\nYou are attempting to use Flash Attention 2.0 without specifying a torch dtype. This might lead to unexpected behaviour\r\nYou are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\r\nFlash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in ModernBertForSequenceClassification is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained(\"openai/whisper-tiny\", attn_implementation=\"flash_attention_2\", torch_dtype=torch.float16)`\r\n/miniconda3/envs/bert/lib/python3.11/site-packages/transformers/models/modernbert/modeling_modernbert.py:711: TracerWarning: Converting a tensor to a Python number might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\r\n  max_seqlen_in_batch = int(seqlens_in_batch.max().item())\r\nTraceback (most recent call last):\r\n  File \"/modernBERT/export_onnx.py\", line 39, in <module>\r\n    export()\r\n  File \"/modernBERT/export_onnx.py\", line 28, in export\r\n    torch.onnx.export(\r\n  File \"/miniconda3/envs/bert/lib/python3.11/site-packages/torch/onnx/__init__.py\", line 375, in export\r\n    export(\r\n  File \"/miniconda3/envs/bert/lib/python3.11/site-packages/torch/onnx/utils.py\", line 502, in export\r\n    _export(\r\n  File \"/miniconda3/envs/bert/lib/python3.11/site-packages/torch/onnx/utils.py\", line 1564, in _export\r\n    graph, params_dict, torch_out = _model_to_graph(\r\n                                    ^^^^^^^^^^^^^^^^\r\n  File \"/miniconda3/envs/bert/lib/python3.11/site-packages/torch/onnx/utils.py\", line 1113, in _model_to_graph\r\n    graph, params, torch_out, module = _create_jit_graph(model, args)\r\n                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/miniconda3/envs/bert/lib/python3.11/site-packages/torch/onnx/utils.py\", line 997, in _create_jit_graph\r\n    graph, torch_out = _trace_and_get_graph_from_model(model, args)\r\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/miniconda3/envs/bert/lib/python3.11/site-packages/torch/onnx/utils.py\", line 904, in _trace_and_get_graph_from_model\r\n    trace_graph, torch_out, inputs_states = torch.jit._get_trace_graph(\r\n                                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/miniconda3/envs/bert/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py\", line 632, in _fn\r\n    return fn(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^\r\n  File \"/miniconda3/envs/bert/lib/python3.11/site-packages/torch/jit/_trace.py\", line 1500, in _get_trace_graph\r\n    outs = ONNXTracedModule(\r\n           ^^^^^^^^^^^^^^^^^\r\n  File \"/miniconda3/envs/bert/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/miniconda3/envs/bert/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/miniconda3/envs/bert/lib/python3.11/site-packages/torch/jit/_trace.py\", line 139, in forward\r\n    graph, out = torch._C._create_graph_by_tracing(\r\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/miniconda3/envs/bert/lib/python3.11/site-packages/torch/jit/_trace.py\", line 130, in wrapper\r\n    outs.append(self.inner(*trace_inputs))\r\n                ^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/miniconda3/envs/bert/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/miniconda3/envs/bert/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/miniconda3/envs/bert/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1726, in _slow_forward\r\n    result = self.forward(*input, **kwargs)\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/miniconda3/envs/bert/lib/python3.11/site-packages/transformers/models/modernbert/modeling_modernbert.py\", line 1160, in forward\r\n    outputs = self.model(\r\n              ^^^^^^^^^^^\r\n  File \"/miniconda3/envs/bert/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/miniconda3/envs/bert/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/miniconda3/envs/bert/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1726, in _slow_forward\r\n    result = self.forward(*input, **kwargs)\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/miniconda3/envs/bert/lib/python3.11/site-packages/transformers/models/modernbert/modeling_modernbert.py\", line 895, in forward\r\n    hidden_states = self.embeddings(input_ids)\r\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/miniconda3/envs/bert/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/miniconda3/envs/bert/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/miniconda3/envs/bert/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1726, in _slow_forward\r\n    result = self.forward(*input, **kwargs)\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/miniconda3/envs/bert/lib/python3.11/site-packages/transformers/models/modernbert/modeling_modernbert.py\", line 210, in forward\r\n    self.compiled_embeddings(input_ids)\r\n  File \"/miniconda3/envs/bert/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py\", line 444, in _fn\r\n    raise RuntimeError(\r\nRuntimeError: Detected that you are using FX to torch.jit.trace a dynamo-optimized function. This is not supported at the moment.\r\n```\r\n\r\nhttps://huggingface.co/answerdotai/ModernBERT-base/discussions/10\r\nWhen I read this post I modified part of the code as follows. \r\n```\r\n    model = AutoModelForSequenceClassification.from_pretrained(\r\n            \"./checkpoints\",\r\n            num_labels=3,\r\n            reference_compile=False,\r\n            )\r\n```\r\nI got another error.\r\n```\r\nYou are attempting to use Flash Attention 2.0 without specifying a torch dtype. This might lead to unexpected behaviour\r\nYou are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\r\nFlash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in ModernBertForSequenceClassification is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained(\"openai/whisper-tiny\", attn_implementation=\"flash_attention_2\", torch_dtype=torch.float16)`\r\n/miniconda3/envs/bert/lib/python3.11/site-packages/transformers/models/modernbert/modeling_modernbert.py:711: TracerWarning: Converting a tensor to a Python number might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\r\n  max_seqlen_in_batch = int(seqlens_in_batch.max().item())\r\n/miniconda3/envs/bert/lib/python3.11/site-packages/flash_attn/ops/triton/rotary.py:166: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\r\n  assert sin.shape == cos.shape\r\n/miniconda3/envs/bert/lib/python3.11/site-packages/flash_attn/ops/triton/rotary.py:168: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\r\n  assert rotary_dim <= headdim, \"rotary_dim must be <= headdim\"\r\n/miniconda3/envs/bert/lib/python3.11/site-packages/flash_attn/ops/triton/rotary.py:169: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\r\n  assert headdim <= 256, \"Only support headdim <= 256\"\r\n/miniconda3/envs/bert/lib/python3.11/site-packages/flash_attn/ops/triton/rotary.py:170: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\r\n  assert seqlen_ro >= seqlen, \"seqlen_ro must be >= seqlen\"\r\n/miniconda3/envs/bert/lib/python3.11/site-packages/flash_attn/ops/triton/rotary.py:185: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\r\n  assert seqlen_offsets + seqlen <= seqlen_ro\r\n/miniconda3/envs/bert/lib/python3.11/site-packages/flash_attn/ops/triton/rotary.py:188: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\r\n  if rotary_dim < headdim and not inplace:\r\n/miniconda3/envs/bert/lib/python3.11/site-packages/flash_attn/ops/triton/rotary.py:193: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\r\n  if rotary_dim <= 32\r\n/miniconda3/envs/bert/lib/python3.11/site-packages/flash_attn/ops/triton/rotary.py:194: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\r\n  else (64 if rotary_dim <= 64 else (128 if rotary_dim <= 128 else 256))\r\n/miniconda3/envs/bert/lib/python3.11/site-packages/flash_attn/ops/triton/rotary.py:197: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\r\n  BLOCK_M = 4 if interleaved else (8 if rotary_dim <= 128 else 4)\r\nTraceback (most recent call last):\r\n  File \"/modernBERT/export_onnx.py\", line 39, in <module>\r\n    export()\r\n  File \"/modernBERT/export_onnx.py\", line 28, in export\r\n    torch.onnx.export(\r\n  File \"/miniconda3/envs/bert/lib/python3.11/site-packages/torch/onnx/__init__.py\", line 375, in export\r\n    export(\r\n  File \"/miniconda3/envs/bert/lib/python3.11/site-packages/torch/onnx/utils.py\", line 502, in export\r\n    _export(\r\n  File \"/miniconda3/envs/bert/lib/python3.11/site-packages/torch/onnx/utils.py\", line 1564, in _export\r\n    graph, params_dict, torch_out = _model_to_graph(\r\n                                    ^^^^^^^^^^^^^^^^\r\n  File \"/miniconda3/envs/bert/lib/python3.11/site-packages/torch/onnx/utils.py\", line 1113, in _model_to_graph\r\n    graph, params, torch_out, module = _create_jit_graph(model, args)\r\n                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/miniconda3/envs/bert/lib/python3.11/site-packages/torch/onnx/utils.py\", line 997, in _create_jit_graph\r\n    graph, torch_out = _trace_and_get_graph_from_model(model, args)\r\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/miniconda3/envs/bert/lib/python3.11/site-packages/torch/onnx/utils.py\", line 904, in _trace_and_get_graph_from_model\r\n    trace_graph, torch_out, inputs_states = torch.jit._get_trace_graph(\r\n                                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/miniconda3/envs/bert/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py\", line 632, in _fn\r\n    return fn(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^\r\n  File \"/miniconda3/envs/bert/lib/python3.11/site-packages/torch/jit/_trace.py\", line 1500, in _get_trace_graph\r\n    outs = ONNXTracedModule(\r\n           ^^^^^^^^^^^^^^^^^\r\n  File \"/miniconda3/envs/bert/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/miniconda3/envs/bert/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/miniconda3/envs/bert/lib/python3.11/site-packages/torch/jit/_trace.py\", line 139, in forward\r\n    graph, out = torch._C._create_graph_by_tracing(\r\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/miniconda3/envs/bert/lib/python3.11/site-packages/torch/jit/_trace.py\", line 130, in wrapper\r\n    outs.append(self.inner(*trace_inputs))\r\n                ^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/miniconda3/envs/bert/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/miniconda3/envs/bert/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/miniconda3/envs/bert/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1726, in _slow_forward\r\n    result = self.forward(*input, **kwargs)\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/miniconda3/envs/bert/lib/python3.11/site-packages/transformers/models/modernbert/modeling_modernbert.py\", line 1160, in forward\r\n    outputs = self.model(\r\n              ^^^^^^^^^^^\r\n  File \"/miniconda3/envs/bert/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/miniconda3/envs/bert/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/miniconda3/envs/bert/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1726, in _slow_forward\r\n    result = self.forward(*input, **kwargs)\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/miniconda3/envs/bert/lib/python3.11/site-packages/transformers/models/modernbert/modeling_modernbert.py\", line 913, in forward\r\n    layer_outputs = encoder_layer(\r\n                    ^^^^^^^^^^^^^^\r\n  File \"/miniconda3/envs/bert/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/miniconda3/envs/bert/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/miniconda3/envs/bert/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1726, in _slow_forward\r\n    result = self.forward(*input, **kwargs)\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/miniconda3/envs/bert/lib/python3.11/site-packages/transformers/models/modernbert/modeling_modernbert.py\", line 529, in forward\r\n    attn_outputs = self.attn(\r\n                   ^^^^^^^^^^\r\n  File \"/miniconda3/envs/bert/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/miniconda3/envs/bert/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/miniconda3/envs/bert/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1726, in _slow_forward\r\n    result = self.forward(*input, **kwargs)\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/miniconda3/envs/bert/lib/python3.11/site-packages/transformers/models/modernbert/modeling_modernbert.py\", line 487, in forward\r\n    attn_outputs = MODERNBERT_ATTENTION_FUNCTION[self.config._attn_implementation](\r\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/miniconda3/envs/bert/lib/python3.11/site-packages/transformers/models/modernbert/modeling_modernbert.py\", line 349, in flash_attention_forward\r\n    qkv = rotary_emb(qkv, cu_seqlens=cu_seqlens, max_seqlen=max_seqlen)\r\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/miniconda3/envs/bert/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/miniconda3/envs/bert/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/miniconda3/envs/bert/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1726, in _slow_forward\r\n    result = self.forward(*input, **kwargs)\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/miniconda3/envs/bert/lib/python3.11/site-packages/transformers/models/modernbert/modeling_modernbert.py\", line 178, in forward\r\n    qkv = apply_rotary_unpadded(\r\n          ^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/miniconda3/envs/bert/lib/python3.11/site-packages/transformers/models/modernbert/modeling_modernbert.py\", line 136, in apply_rotary_unpadded\r\n    return ApplyRotaryEmbUnpad.apply(qkv, cos, sin, cu_seqlens, max_seqlen)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/miniconda3/envs/bert/lib/python3.11/site-packages/torch/autograd/function.py\", line 575, in apply\r\n    return super().apply(*args, **kwargs)  # type: ignore[misc]\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/miniconda3/envs/bert/lib/python3.11/site-packages/transformers/models/modernbert/modeling_modernbert.py\", line 75, in forward\r\n    apply_rotary(\r\n  File \"/miniconda3/envs/bert/lib/python3.11/site-packages/flash_attn/ops/triton/rotary.py\", line 202, in apply_rotary\r\n    rotary_kernel[grid](\r\n  File \"/miniconda3/envs/bert/lib/python3.11/site-packages/triton/runtime/jit.py\", line 345, in <lambda>\r\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\r\n                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/miniconda3/envs/bert/lib/python3.11/site-packages/triton/runtime/jit.py\", line 662, in run\r\n    kernel = self.compile(\r\n             ^^^^^^^^^^^^^\r\n  File \"/miniconda3/envs/bert/lib/python3.11/site-packages/triton/compiler/compiler.py\", line 276, in compile\r\n    module = src.make_ir(options, codegen_fns, context)\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/miniconda3/envs/bert/lib/python3.11/site-packages/triton/compiler/compiler.py\", line 113, in make_ir\r\n    return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\ntriton.compiler.errors.CompilationError: at 32:22:\r\n    # Meta-parameters\r\n    BLOCK_K: tl.constexpr,\r\n    IS_SEQLEN_OFFSETS_TENSOR: tl.constexpr,\r\n    IS_VARLEN: tl.constexpr,\r\n    INTERLEAVED: tl.constexpr,\r\n    CONJUGATE: tl.constexpr,\r\n    BLOCK_M: tl.constexpr,\r\n):\r\n    pid_m = tl.program_id(axis=0)\r\n    pid_batch = tl.program_id(axis=1)\r\n    pid_head = tl.program_id(axis=2)\r\n    rotary_dim_half = rotary_dim // 2\r\n                      ^\r\nIncompatibleTypeErrorImpl('invalid operands of type pointer<int64> and triton.language.int32')\r\n```\n\n### Expected behavior\n\nexport to model.onnx",
    "state": "closed",
    "created_at": "2025-01-07T10:26:26+00:00",
    "closed_at": "2025-01-14T10:22:09+00:00",
    "updated_at": "2025-01-14T10:22:11+00:00",
    "author": "wakaka6",
    "author_type": "User",
    "comments_count": 4,
    "reactions_count": 0,
    "labels": "bug,ONNX",
    "milestone": null,
    "closed_by": "wakaka6",
    "resolution_time_hours": 167.9286111111111,
    "first_comments": [
      {
        "author": "xenova",
        "created_at": "2025-01-07T11:49:02+00:00",
        "body": "Hi there ðŸ‘‹ Could you try using this branch of Optimum: https://github.com/huggingface/optimum/pull/2131?\r\n\r\nWe used that to create these ONNX exports: https://huggingface.co/answerdotai/ModernBERT-base/tree/main/onnx\r\n\r\nIf you'd prefer to use it in your own custom scripts, you can adapt this context manager and use it when loading the model:\r\n```py\r\nclass DisableCompileContextManager:\r\n    def __init__(self):\r\n        self._original_compile = torch.compile\r\n\r\n    def __enter__(self):\r\n        # Turn torch.compile into a no-op\r\n        torch.compile = lambda *args, **kwargs: lambda x: x\r\n\r\n    def __exit__(self, exc_type, exc_val, exc_tb):\r\n        torch.compile = self._original_compile\r\n```"
      },
      {
        "author": "wakaka6",
        "created_at": "2025-01-08T04:10:30+00:00",
        "body": "I using this branch of Optimum: https://github.com/huggingface/optimum/pull/2131\r\n```sh\r\noptimum-cli export onnx -m checkpoints/ --task text-classification classify_model\r\n```\r\ngot same error. @xenova \r\n```\r\ntriton.compiler.errors.CompilationError: at 32:22:\r\n    # Meta-parameters\r\n    BLOCK_K: tl.constexpr,\r\n    IS_SEQLEN_OFFSETS_TENSOR: tl.constexpr,\r\n    IS_VARLEN: tl.constexpr,\r\n    INTERLEAVED: tl.constexpr,\r\n    CONJUGATE: tl.constexpr,\r\n    BLOCK_M: tl.constexpr,\r\n):\r\n    pid_m = tl.program_id(axis=0)\r\n    pid_batch = tl.program_id(axis=1)\r\n    pid_head = tl.program_id(axis=2)\r\n    rotary_dim_half = rotary_dim // 2\r\n                      ^\r\nIncompatibleTypeErrorImpl('invalid operands of type pointer<int64> and triton.language.int32')\r\n```\r\n\r\nmy package\r\n```\r\naccelerate                1.2.1\r\naiohappyeyeballs          2.4.4\r\naiohttp                   3.11.11\r\naiosignal                 1.3.2\r\nanyio                     4.7.0\r\nargon2-cffi               23.1.0\r\nargon2-cffi-bindings      21.2.0\r\narrow                     1.3.0\r\nasttokens                 3.0.0\r\nasync-lru                 2.0.4\r\nattrs                     24.3.0\r\nbabel                     2.16.0\r\nbeautifulsoup4            4.12.3\r\nbleach                    6.2.0\r\nBrotli                    1.0.9\r\ncertifi                   2024.12.14\r\ncffi                      1.17.1\r\ncharset-normalizer        3.4.1\r\ncoloredlogs               15.0.1\r\ncomm                      0.2.2\r\ncontourpy                 1.3.1\r\ncycler                    0.12.1\r\ndatasets                  3.2.0\r\ndebugpy                   1.8.11\r\ndecorator                 5.1.1\r\ndefusedxml                0.7.1\r\ndill                      0.3.8\r\neinops                    0.8.0\r\nevaluate                  0.4.3\r\nexecuting                 2.1.0\r\nfastjsonschema            2.21.1\r\nfilelock                  3.16.1\r\nflash-attn                2.7.2.post1\r\nflatbuffers               24.12.23\r\nfonttools                 4.55.3\r\nfqdn                      1.5.1\r\nfrozenlist                1.5.0\r\nfsspec                    2024.9.0\r\ngmpy2                     2.1.2\r\nh11                       0.14.0\r\nhttpcore                  1.0.7\r\nhttpx                     0.28.1\r\nhuggingface-hub           0.27.1\r\nhumanfriendly             10.0\r\nidna                      3.10\r\nipykernel                 6.29.5\r\nipython                   8.31.0\r\nisoduration               20.11.0\r\njedi                      0.19.2\r\nJinja2                    3.1.5\r\njoblib                    1.4.2\r\njson5                     0.10.0\r\njsonpointer               3.0.0\r\njsonschema                4.23.0\r\njsonschema-specifications 2024.10.1\r\njupyter_client            8.6.3\r\njupyter_core              5.7.2\r\njupyter-events            0.11.0\r\njupyter-lsp               2.2.5\r\njupyter_server            2.15.0\r\njupyter_server_terminals  0.5.3\r\njupyterlab                4.3.4\r\njupyterlab_pygments       0.3.0\r\njupyterlab_server         2.27.3\r\nkiwisolver                1.4.8\r\nMarkupSafe                3.0.2\r\nmatplotlib                3.10.0\r\nmatplotlib-inline         0.1.7\r\nmistune                   3.1.0\r\nmkl_fft                   1.3.11\r\nmkl_random                1.2.8\r\nmkl-service               2.4.0\r\nmpmath                    1.3.0\r\nmultidict                 6.1.0\r\nmultiprocess              0.70.16\r\nnbclient                  0.10.2\r\nnbconvert                 7.16.4\r\nnbformat                  5.10.4\r\nnest-asyncio              1.6.0\r\nnetworkx                  3.4.2\r\nnotebook_shim             0.2.4\r\nnumpy                     2.2.1\r\nonnx                      1.17.0\r\nonnxruntime               1.20.1\r\noptimum                   1.24.0.dev0\r\noverrides                 7.7.0\r\npackaging                 24.2\r\npandas                    2.2.3\r\npandocfilters             1.5.1\r\nparso                     0.8.4\r\npexpect                   4.9.0\r\npillow                    11.0.0\r\npip                       24.2\r\nplatformdirs              4.3.6\r\nprometheus_client         0.21.1\r\nprompt_toolkit            3.0.48\r\npropcache                 0.2.1\r\nprotobuf                  5.29.2\r\npsutil                    6.1.1\r\nptyprocess                0.7.0\r\npure_eval                 0.2.3\r\npyarrow                   18.1.0\r\npycparser                 2.22\r\nPygments                  2.18.0\r\npyparsing                 3.2.1\r\nPySocks                   1.7.1\r\npython-dateutil           2.9.0.post0\r\npython-json-logger        3.2.1\r\npytz                      2024.2\r\nPyYAML                    6.0.2\r\npyzmq                     26.2.0\r\nreferencing               0.35.1\r\nregex                     2024.11.6\r\nrequests                  2.32.3\r\nrfc3339-validator         0.1.4\r\nrfc3986-validator         0.1.1\r\nrpds-py                   0.22.3\r\nsafetensors               0.5.1\r\nscikit-learn              1.6.0\r\nscipy                     1.15.0\r\nSend2Trash                1.8.3\r\nsetuptools                75.1.0\r\nsix                       1.17.0\r\nsniffio                   1.3.1\r\nsoupsieve                 2.6\r\nstack-data                0.6.3\r\nsympy                     1.13.1\r\nterminado                 0.18.1\r\nthreadpoolctl             3.5.0\r\ntinycss2                  1.4.0\r\ntokenizers                0.21.0\r\ntorch                     2.5.1\r\ntorchaudio                2.5.1\r\ntorchvision               0.20.1\r\ntornado                   6.4.2\r\ntqdm                      4.67.1\r\ntraitlets                 5.14.3\r\ntransformers              4.48.0.dev0\r\ntriton                    3.1.0\r\ntypes-python-dateutil     2.9.0.20241206\r\ntyping_extensions         4.12.2\r\ntzdata                    2024.2\r\nuri-template              1.3.0\r\nurllib3                   2.3.0\r\nwcwidth                   0.2.13\r\nwebcolors                 24.11.1\r\nwebencodings              0.5.1\r\nwebsocket-client          1.8.0\r\nwheel                     0.44.0\r\nxxhash                    3.5.0\r\nyarl                      1.18.3\r\n```"
      },
      {
        "author": "DeepakSinghRawat",
        "created_at": "2025-01-08T05:25:54+00:00",
        "body": "I am also facing same error `IncompatibleTypeErrorImpl('invalid operands of type pointer<int64> and triton.language.int32')` when using https://github.com/huggingface/optimum/pull/2131"
      },
      {
        "author": "wakaka6",
        "created_at": "2025-01-14T10:22:09+00:00",
        "body": "I found the problem and here is my final export script. Since Flash Attention 2.0's recalculated memory access patterns and partitioning policies caused onnx to report an error because it couldn't calculate the export mapping, it was able to export using standard flash attention! @DeepakSinghRawat @xenova \n```\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\n\nclass DisableCompileContextManager:\n    def __init__(self):\n        self._original_compile = torch.compile\n\n    def __enter__(self):\n        # Turn torch.compile into a no-op\n        torch.compile = lambda *args, **kwargs: lambda x: x\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        torch.compile = self._original_compile\n\ndef export():\n\n    with DisableCompileContextManager():\n        tokenizer = AutoTokenizer.from_pretrained(\"answerdotai/ModernBERT-base\", model_max_length=4096)\n        model = AutoModelForSequenceClassification.from_pretrained(\n                \"./checkpoints\",\n                num_labels=3,\n                # Flash Attention 2.0's recalculated memory access pattern and partitioning strategy causes\n                # onnx to report an error by not being able to compute the export map\n                attn_implementation=\"eager\"  # Use standard attention\n                # reference_compile=False, # disable triton compile\n                )\n\n        model.eval()\n\n        samples = ['example']\n\n        tokenized = tokenizer(samples,\n                return_tensors='pt',\n                max_length=tokenizer.model_max_length,\n                padding='max_length',\n                truncation=True)\n        input_ids = tokenized['input_ids'].to('cuda')\n        attention_mask = tokenized['attention_mask'].to('cuda')\n        model = model.to('cuda')\n\n        with torch.no_grad():\n            torch.onnx.export(\n                    model,\n                    (input_ids, attention_mask),\n                    './model.onnx',\n                    input_names=[\"input_ids\", \"attention_mask\"],\n                    output_names=[\"logits\"],\n                    )\n\n\n\nif __name__ == '__main__':\n    export()\n```"
      }
    ],
    "url": "https://github.com/huggingface/transformers/issues/35545"
  },
  {
    "repository": "huggingface/transformers",
    "issue_number": 35542,
    "title": "Enabling Access to Currently Training Model in Callback Handler",
    "body": "### Feature request\n\nUpdate `self.callback_handler.model` to reference the actual training model instance (`model`) instead of the pre-prepared model (`self.model`) in `Trainer._inner_training_loop`.\n\n### Motivation\n\n- When debugging model training, developers often need to monitor model weights and other parameters at each step, similar to `wandb.watch`\r\n- Currently, callbacks receive the pre-prepared model instance (`self.model`) rather than the actual training model (`model`)\r\n- This is evident from different memory IDs:\r\n  ```python\r\n  id(self.callback_handler.model): 140049287962320\r\n  id(model): 140048547892912  # Model memory id in training_step\r\n  ```\r\n- The code flow suggests this might be unintended:\r\n  ```python\r\n  model = self._wrap_model(self.model_wrapped)\r\n  self.callback_handler.model = self.model  # Current implementation\r\n  ```\r\n[trainer_code](https://github.com/huggingface/transformers/blob/main/src/transformers/trainer.py#L2303C9-L2410)\n\n### Your contribution\n\nThe proposed change is to update the model reference in the callback handler to point to the actual training model:\r\n```python\r\n# Current implementation\r\nself.callback_handler.model = self.model\r\n\r\n# Proposed change\r\nself.callback_handler.model = model\r\n```\r\n\r\nThis would allow callbacks to access and monitor the actual model being trained, enabling proper debugging and monitoring capabilities like examining weights during training steps.\r\n\r\nThis change would ensure that when users implement custom callbacks for debugging or monitoring purposes, they have access to the correct model instance that's being actively trained, rather than the pre-prepared version.",
    "state": "closed",
    "created_at": "2025-01-07T04:40:29+00:00",
    "closed_at": "2025-01-07T05:22:10+00:00",
    "updated_at": "2025-01-07T05:22:10+00:00",
    "author": "jp1924",
    "author_type": "User",
    "comments_count": 0,
    "reactions_count": 0,
    "labels": "Feature request",
    "milestone": null,
    "closed_by": "jp1924",
    "resolution_time_hours": 0.6947222222222222,
    "first_comments": [],
    "url": "https://github.com/huggingface/transformers/issues/35542"
  },
  {
    "repository": "huggingface/transformers",
    "issue_number": 35540,
    "title": "OverflowError: out of range integral type conversion attempted",
    "body": "### System Info\n\n- `transformers` version: 4.47.1\r\n- Platform: Windows-10-10.0.22631-SP0\r\n- Python version: 3.11.8\r\n- Huggingface_hub version: 0.27.0\r\n- Safetensors version: 0.4.5\r\n- Accelerate version: 1.2.1\r\n- Accelerate config:    not found\r\n- PyTorch version (GPU?): 2.3.1+cu118 (True)\r\n- Tensorflow version (GPU?): not installed (NA)\r\n- Flax version (CPU?/GPU?/TPU?): not installed (NA)\r\n- Jax version: not installed\r\n- JaxLib version: not installed\r\n- Using distributed or parallel set-up in script?: No\r\n- Using GPU in script?: Yes\r\n- GPU type: AMD Radeon RX 6600 [ZLUDA]\n\n### Who can help?\n\n@amyeroberts @qubvel \n\n### Information\n\n- [ ] The official example scripts\n- [ ] My own modified scripts\n\n### Tasks\n\n- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\n- [ ] My own task or dataset (give details below)\n\n### Reproduction\n\n![image](https://github.com/user-attachments/assets/9a8e53a2-bc7c-412b-80a4-9b716f3764ec)\r\n\r\n\r\nWhen trying to run this official example code from the GOT-OCR2_0 HuggingFace page:\r\n\r\n```python\r\nfrom transformers import AutoModel, AutoTokenizer\r\n\r\ntokenizer = AutoTokenizer.from_pretrained('ucaslcl/GOT-OCR2_0', trust_remote_code=True)\r\nmodel = AutoModel.from_pretrained('ucaslcl/GOT-OCR2_0', trust_remote_code=True, low_cpu_mem_usage=True, device_map='cuda', use_safetensors=True, pad_token_id=tokenizer.eos_token_id)\r\nmodel = model.eval().cuda()\r\n\r\n\r\n# input your test image\r\nimage_file = 'xxx.jpg'\r\n\r\n# plain texts OCR\r\nres = model.chat(tokenizer, image_file, ocr_type='ocr')\r\n\r\n# format texts OCR:\r\n# res = model.chat(tokenizer, image_file, ocr_type='format')\r\n\r\n# fine-grained OCR:\r\n# res = model.chat(tokenizer, image_file, ocr_type='ocr', ocr_box='')\r\n# res = model.chat(tokenizer, image_file, ocr_type='format', ocr_box='')\r\n# res = model.chat(tokenizer, image_file, ocr_type='ocr', ocr_color='')\r\n# res = model.chat(tokenizer, image_file, ocr_type='format', ocr_color='')\r\n\r\n# multi-crop OCR:\r\n# res = model.chat_crop(tokenizer, image_file, ocr_type='ocr')\r\n# res = model.chat_crop(tokenizer, image_file, ocr_type='format')\r\n\r\n# render the formatted OCR results:\r\n# res = model.chat(tokenizer, image_file, ocr_type='format', render=True, save_render_file = './demo.html')\r\n\r\nprint(res)\r\n```\n\n### Expected behavior\n\nShould print result.",
    "state": "closed",
    "created_at": "2025-01-07T01:34:49+00:00",
    "closed_at": "2025-01-10T22:12:32+00:00",
    "updated_at": "2025-01-10T22:12:32+00:00",
    "author": "test3211234",
    "author_type": "User",
    "comments_count": 1,
    "reactions_count": 0,
    "labels": "bug",
    "milestone": null,
    "closed_by": "test3211234",
    "resolution_time_hours": 92.62861111111111,
    "first_comments": [
      {
        "author": "qubvel",
        "created_at": "2025-01-07T09:59:54+00:00",
        "body": "Hi @test3211234! You are using 3rd party implementation, it's better to open discussion directly on the hub\r\nhttps://huggingface.co/stepfun-ai/GOT-OCR2_0/discussions\r\n\r\nWe also have `transformers` native implementation ongoing, you can try this out by installing it from the PR\r\nhttps://github.com/huggingface/transformers/pull/34721"
      }
    ],
    "url": "https://github.com/huggingface/transformers/issues/35540"
  },
  {
    "repository": "huggingface/transformers",
    "issue_number": 35538,
    "title": "is possible convert transforms tokenizers in sentence piece .model? ",
    "body": "\r\ni want use text-generators in mobile plataforms with onnxruntime in c++, but i can't convert tokenizer in a way for use it in c++\r\n\r\n![image](https://github.com/user-attachments/assets/857e3d08-ac49-4bb9-9ebd-9d6dd5f6677e)\r\n\r\nis possible convert these files in .model for sentence piece or another tokenizer that can load transforms tokenizers in c++?\r\n\r\n(note:these files are intalled from transform python api and save in the local machine)",
    "state": "closed",
    "created_at": "2025-01-06T21:09:46+00:00",
    "closed_at": "2025-01-20T04:27:02+00:00",
    "updated_at": "2025-01-20T04:27:02+00:00",
    "author": "Caio-lima-santos",
    "author_type": "User",
    "comments_count": 1,
    "reactions_count": 0,
    "labels": "",
    "milestone": null,
    "closed_by": "Caio-lima-santos",
    "resolution_time_hours": 319.28777777777776,
    "first_comments": [
      {
        "author": "Rocketknight1",
        "created_at": "2025-01-07T14:27:15+00:00",
        "body": "I don't believe we support this! This checkpoint is a \"Fast\" transformers tokenizer, with most of the tokenizer data stored in `tokenizer.json`. We have methods to convert original \"Slow\" tokenizers, which are often sentencepiece `.model` files, into fast `tokenizer.json` files, but we don't have any methods to do the inverse conversion."
      }
    ],
    "url": "https://github.com/huggingface/transformers/issues/35538"
  },
  {
    "repository": "huggingface/transformers",
    "issue_number": 35518,
    "title": "Batch size deprecation warning issued even when it is not used",
    "body": "### System Info\n\n- `transformers` version: 4.48.0.dev0\r\n- Platform: Linux-6.11.0-13-generic-x86_64-with-glibc2.40\r\n- Python version: 3.12.7\r\n- Huggingface_hub version: 0.27.0\r\n- Safetensors version: 0.5.0\r\n- Accelerate version: 1.2.1\r\n- Accelerate config:    not found\r\n- PyTorch version (GPU?): 2.4.0+cu121 (True)\r\n- Tensorflow version (GPU?): not installed (NA)\r\n- Flax version (CPU?/GPU?/TPU?): not installed (NA)\r\n- Jax version: not installed\r\n- JaxLib version: not installed\r\n\n\n### Who can help?\n\n@ArthurZucker @zucchini-nlp \n\n### Information\n\n- [X] The official example scripts\n- [ ] My own modified scripts\n\n### Tasks\n\n- [X] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\n- [ ] My own task or dataset (give details below)\n\n### Reproduction\n\n```\r\nfrom transformers import AutoModel\r\nfrom transformers.cache_utils import StaticCache\r\n\r\nmodel_id = \"deepset/roberta-base-squad2\"\r\nmodel = AutoModel.from_pretrained(model_id)\r\ncache = StaticCache(max_batch_size=10, config=model.config)\r\n\r\n```\r\n\r\n**Output**\r\n`The 'batch_size' attribute of StaticCache is deprecated and will be removed in v4.49. Use the more precisely named 'self.max_batch_size' attribute instead.\r\n`\n\n### Expected behavior\n\nNo warning message ",
    "state": "closed",
    "created_at": "2025-01-06T01:09:44+00:00",
    "closed_at": "2025-01-16T16:48:13+00:00",
    "updated_at": "2025-01-16T16:48:13+00:00",
    "author": "quintenroets",
    "author_type": "User",
    "comments_count": 0,
    "reactions_count": 0,
    "labels": "bug",
    "milestone": null,
    "closed_by": "ArthurZucker",
    "resolution_time_hours": 255.64138888888888,
    "first_comments": [],
    "url": "https://github.com/huggingface/transformers/issues/35518"
  },
  {
    "repository": "huggingface/transformers",
    "issue_number": 35510,
    "title": "Can't reproduce results from simple examples on huggingface with popular transformer models?",
    "body": "### System Info\n\nTransformers 4.46.2, Python 3.11.11\n\n### Who can help?\n\n@amyeroberts, @qubvel\n\n### Information\n\n- [X] The official example scripts\n- [ ] My own modified scripts\n\n### Tasks\n\n- [X] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\n- [ ] My own task or dataset (give details below)\n\n### Reproduction\n\nIf I use the Inference API widget on the model card tab of this page, for the following case, (i.e, download this image, then enter as text cat,not_cat), I get a score of 0.53,0.47 cat/not_cat:\r\n![image](https://github.com/user-attachments/assets/3872f990-9ca2-4668-8d35-d98b4eeb7d6f)\r\n\r\n\r\nIf I run what I would assume would be the identical code for computing the result, I get a wildly different score. What gives?\r\n\r\nfrom PIL import Image\r\nimport requests\r\n\r\nfrom transformers import CLIPProcessor, CLIPModel\r\n\r\nmodel = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\r\nprocessor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\r\n\r\nurl = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\r\nimage = Image.open(requests.get(url, stream=True).raw)\r\n\r\ninputs = processor(text=[\"cat\", \"not_cat\"], images=image, return_tensors=\"pt\", padding=True)\r\n\r\noutputs = model(**inputs)\r\nlogits_per_image = outputs.logits_per_image # this is the image-text similarity score\r\nlogits_per_image.softmax(dim=1) # we can take the softmax to get the label probabilities\r\nYields: tensor([[0.2322, 0.7678]], grad_fn=<SoftmaxBackward0>)\r\n\r\nThis is totally unworkable if this is the case? I have to be missing something, right? I can't be this off-base. This is on Transformers 4.46.2, but seems to happen with latest as well?\n\n### Expected behavior\n\nI would expect the results to be even somewhat close to what the inference API endpoint produces? What do I do to get them to be closer? What is wrong here?",
    "state": "closed",
    "created_at": "2025-01-04T17:57:56+00:00",
    "closed_at": "2025-01-05T13:27:08+00:00",
    "updated_at": "2025-01-05T13:27:08+00:00",
    "author": "DGaffney",
    "author_type": "User",
    "comments_count": 2,
    "reactions_count": 0,
    "labels": "bug",
    "milestone": null,
    "closed_by": "DGaffney",
    "resolution_time_hours": 19.486666666666668,
    "first_comments": [
      {
        "author": "NielsRogge",
        "created_at": "2025-01-04T18:41:04+00:00",
        "body": "Yes this is due to the fact that the inference API uses the pipeline API behind the scenes, which by default uses the prefix \"A photo of {class}\" as seen [here](https://github.com/huggingface/transformers/blob/e5fd865ebae062b7cf03a81b8c6affeb39f30bec/src/transformers/pipelines/zero_shot_image_classification.py#L136).\r\n\r\nThis was also noted in https://github.com/huggingface/transformers/issues/30951 for which a disclaimer was added in the docs of SigLIP + the task page, but we could add a similar disclaimer in the docs of the pipeline."
      },
      {
        "author": "DGaffney",
        "created_at": "2025-01-04T18:54:45+00:00",
        "body": "Awesome ok this it vital info thank you for this. Adding it upfront maybe helpful for other folks I imagine. Also, what would a â€œcanonicalâ€ negative class look like, if you had to advise on one style? â€œ*not* a photo of a catâ€?DevinOn Jan 4, 2025, at 10:41â€¯AM, NielsRogge ***@***.***> wrote:ï»¿\r\nYes this is due to the fact that the inference API uses the pipeline API behind the scenes, which by default uses the prefix \"A photo of {class}\" as seen here.\r\nThis was also noted in #30951 for which a disclaimer was added in the docs, but we could add a similar disclaimer in the docs of the pipeline.\r\n\r\nâ€”Reply to this email directly, view it on GitHub, or unsubscribe.You are receiving this because you authored the thread.Message ID: ***@***.***>"
      }
    ],
    "url": "https://github.com/huggingface/transformers/issues/35510"
  },
  {
    "repository": "huggingface/transformers",
    "issue_number": 35505,
    "title": "qwen2 rope device matching bug",
    "body": "### System Info\n\n- `transformers` version: 4.47.1\r\n- Platform: Linux-5.15.0-125-generic-x86_64-with-glibc2.35\r\n- Python version: 3.11.10\r\n- Huggingface_hub version: 0.27.0\r\n- Safetensors version: 0.4.5\r\n- Accelerate version: 1.2.1\r\n- Accelerate config:    not found\r\n- PyTorch version (GPU?): 2.4.1+cu121 (True)\r\n- Tensorflow version (GPU?): not installed (NA)\r\n- Flax version (CPU?/GPU?/TPU?): not installed (NA)\r\n- Jax version: not installed\r\n- JaxLib version: not installed\r\n- Using distributed or parallel set-up in script?: <fill in>\r\n- Using GPU in script?: <fill in>\r\n- GPU type: NVIDIA RTX A6000\n\n### Who can help?\n\nI faced this bug running VLM InternVL2.5-78B.\r\n\r\ntext models: @ArthurZucker\r\nvision models: @amyeroberts, @qubvel\n\n### Information\n\n- [ ] The official example scripts\n- [X] My own modified scripts\n\n### Tasks\n\n- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\n- [X] My own task or dataset (give details below)\n\n### Reproduction\n\n# Code sample\r\n\r\nI tried to run demo code of [`internVL2_5-78B`](https://huggingface.co/OpenGVLab/InternVL2_5-78B).\r\n\r\n```python\r\nimport os\r\nos.environ[\"HF_HOME\"] = \"/dataset/huggingface\"\r\n\r\nimport math\r\nimport numpy as np\r\nimport torch\r\nimport torchvision.transforms as T\r\nfrom decord import VideoReader, cpu\r\nfrom PIL import Image\r\nfrom torchvision.transforms.functional import InterpolationMode\r\nfrom transformers import AutoModel, AutoTokenizer\r\n\r\nIMAGENET_MEAN = (0.485, 0.456, 0.406)\r\nIMAGENET_STD = (0.229, 0.224, 0.225)\r\n\r\ndef build_transform(input_size):\r\n    MEAN, STD = IMAGENET_MEAN, IMAGENET_STD\r\n    transform = T.Compose([\r\n        T.Lambda(lambda img: img.convert('RGB') if img.mode != 'RGB' else img),\r\n        T.Resize((input_size, input_size), interpolation=InterpolationMode.BICUBIC),\r\n        T.ToTensor(),\r\n        T.Normalize(mean=MEAN, std=STD)\r\n    ])\r\n    return transform\r\n\r\ndef find_closest_aspect_ratio(aspect_ratio, target_ratios, width, height, image_size):\r\n    best_ratio_diff = float('inf')\r\n    best_ratio = (1, 1)\r\n    area = width * height\r\n    for ratio in target_ratios:\r\n        target_aspect_ratio = ratio[0] / ratio[1]\r\n        ratio_diff = abs(aspect_ratio - target_aspect_ratio)\r\n        if ratio_diff < best_ratio_diff:\r\n            best_ratio_diff = ratio_diff\r\n            best_ratio = ratio\r\n        elif ratio_diff == best_ratio_diff:\r\n            if area > 0.5 * image_size * image_size * ratio[0] * ratio[1]:\r\n                best_ratio = ratio\r\n    return best_ratio\r\n\r\ndef dynamic_preprocess(image, min_num=1, max_num=12, image_size=448, use_thumbnail=False):\r\n    orig_width, orig_height = image.size\r\n    aspect_ratio = orig_width / orig_height\r\n\r\n    # calculate the existing image aspect ratio\r\n    target_ratios = set(\r\n        (i, j) for n in range(min_num, max_num + 1) for i in range(1, n + 1) for j in range(1, n + 1) if\r\n        i * j <= max_num and i * j >= min_num)\r\n    target_ratios = sorted(target_ratios, key=lambda x: x[0] * x[1])\r\n\r\n    # find the closest aspect ratio to the target\r\n    target_aspect_ratio = find_closest_aspect_ratio(\r\n        aspect_ratio, target_ratios, orig_width, orig_height, image_size)\r\n\r\n    # calculate the target width and height\r\n    target_width = image_size * target_aspect_ratio[0]\r\n    target_height = image_size * target_aspect_ratio[1]\r\n    blocks = target_aspect_ratio[0] * target_aspect_ratio[1]\r\n\r\n    # resize the image\r\n    resized_img = image.resize((target_width, target_height))\r\n    processed_images = []\r\n    for i in range(blocks):\r\n        box = (\r\n            (i % (target_width // image_size)) * image_size,\r\n            (i // (target_width // image_size)) * image_size,\r\n            ((i % (target_width // image_size)) + 1) * image_size,\r\n            ((i // (target_width // image_size)) + 1) * image_size\r\n        )\r\n        # split the image\r\n        split_img = resized_img.crop(box)\r\n        processed_images.append(split_img)\r\n    assert len(processed_images) == blocks\r\n    if use_thumbnail and len(processed_images) != 1:\r\n        thumbnail_img = image.resize((image_size, image_size))\r\n        processed_images.append(thumbnail_img)\r\n    return processed_images\r\n\r\ndef load_image(image_file, input_size=448, max_num=12):\r\n    image = Image.open(image_file).convert('RGB')\r\n    transform = build_transform(input_size=input_size)\r\n    images = dynamic_preprocess(image, image_size=input_size, use_thumbnail=True, max_num=max_num)\r\n    pixel_values = [transform(image) for image in images]\r\n    pixel_values = torch.stack(pixel_values)\r\n    return pixel_values\r\n\r\ndef split_model(model_name):\r\n    device_map = {}\r\n    world_size = torch.cuda.device_count()\r\n    num_layers = {\r\n        'InternVL2_5-1B': 24, 'InternVL2_5-2B': 24, 'InternVL2_5-4B': 36, 'InternVL2_5-8B': 32,\r\n        'InternVL2_5-26B': 48, 'InternVL2_5-38B': 64, 'InternVL2_5-78B': 80}[model_name]\r\n    # Since the first GPU will be used for ViT, treat it as half a GPU.\r\n    num_layers_per_gpu = math.ceil(num_layers / (world_size - 0.5))\r\n    num_layers_per_gpu = [num_layers_per_gpu] * world_size\r\n    num_layers_per_gpu[0] = math.ceil(num_layers_per_gpu[0] * 0.5)\r\n    layer_cnt = 0\r\n    for i, num_layer in enumerate(num_layers_per_gpu):\r\n        for j in range(num_layer):\r\n            device_map[f'language_model.model.layers.{layer_cnt}'] = i\r\n            layer_cnt += 1\r\n    device_map['vision_model'] = 0\r\n    device_map['mlp1'] = 0\r\n    device_map['language_model.model.tok_embeddings'] = 0\r\n    device_map['language_model.model.embed_tokens'] = 0\r\n    device_map['language_model.output'] = 0\r\n    device_map['language_model.model.norm'] = 0\r\n    device_map['language_model.lm_head'] = 0\r\n    device_map[f'language_model.model.layers.{num_layers - 1}'] = 0\r\n\r\n    return device_map\r\n\r\npath = \"OpenGVLab/InternVL2_5-78B\"\r\ndevice_map = split_model('InternVL2_5-78B')\r\nmodel = AutoModel.from_pretrained(\r\n    path,\r\n    torch_dtype=torch.bfloat16,\r\n    low_cpu_mem_usage=True,\r\n    use_flash_attn=True,\r\n    trust_remote_code=True,\r\n    device_map=device_map).eval()\r\n\r\ntokenizer = AutoTokenizer.from_pretrained(path, trust_remote_code=True, use_fast=False)\r\ngeneration_config = dict(max_new_tokens=1024, do_sample=True)\r\n\r\n# single-image single-round conversation (å•å›¾å•è½®å¯¹è¯)\r\nquestion = '<image>\\nPlease describe the image shortly.'\r\n\r\n# set the max number of tiles in `max_num`\r\npixel_values = load_image('sample.jpg', max_num=12).to(torch.bfloat16).cuda()\r\n\r\nresponse = model.chat(tokenizer, pixel_values, question, generation_config)\r\nprint(f'User: {question}\\nAssistant: {response}')\r\n\r\n'''\r\nThis image was misclassified by the fire and smoke classifier. Please provide a detailed explanation of the reasons for this misclassification, considering the visual features, the object's position within the image, the overall context, and what the object actually represents.\r\n'''\r\n```\r\nError\r\n```\r\nTraceback (most recent call last):\r\n  File \"/workspace/internvl.py\", line 132, in <module>\r\n    response = model.chat(tokenizer, pixel_values, question, generation_config)\r\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/dataset/huggingface/modules/transformers_modules/OpenGVLab/InternVL2_5-78B/ea891f50e952a1bdf9dd44df66a932bc5a4f40ec/modeling_internvl_chat.py\", line 290, in chat\r\n    generation_output = self.generate(\r\n                        ^^^^^^^^^^^^^^\r\n  File \"/opt/conda/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\r\n    return func(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/dataset/huggingface/modules/transformers_modules/OpenGVLab/InternVL2_5-78B/ea891f50e952a1bdf9dd44df66a932bc5a4f40ec/modeling_internvl_chat.py\", line 339, in generate\r\n    outputs = self.language_model.generate(\r\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/opt/conda/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\r\n    return func(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/opt/conda/lib/python3.11/site-packages/transformers/generation/utils.py\", line 2252, in generate\r\n    result = self._sample(\r\n             ^^^^^^^^^^^^^\r\n  File \"/opt/conda/lib/python3.11/site-packages/transformers/generation/utils.py\", line 3251, in _sample\r\n    outputs = self(**model_inputs, return_dict=True)\r\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/opt/conda/lib/python3.11/site-packages/transformers/models/qwen2/modeling_qwen2.py\", line 1169, in forward\r\n    outputs = self.model(\r\n              ^^^^^^^^^^^\r\n  File \"/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/opt/conda/lib/python3.11/site-packages/transformers/models/qwen2/modeling_qwen2.py\", line 875, in forward\r\n    position_embeddings = self.rotary_emb(hidden_states, position_ids)\r\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/opt/conda/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\r\n    return func(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/opt/conda/lib/python3.11/site-packages/transformers/models/qwen2/modeling_qwen2.py\", line 167, in forward\r\n    freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\r\n             ~~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nRuntimeError: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat2 in method wrapper_CUDA_bmm)\r\n```\r\nI printed device of its tensor and I got these results.\r\n```python\r\ndevice_type = cuda\r\ninv_freq_expanded.device.type = cpu\r\nposition_ids_expanded.device.type = cuda\r\n```\r\n\r\n\n\n### Expected behavior\n\nNo errors and print image caption.",
    "state": "closed",
    "created_at": "2025-01-04T04:20:21+00:00",
    "closed_at": "2025-01-09T13:59:56+00:00",
    "updated_at": "2025-01-09T14:03:07+00:00",
    "author": "developer0hye",
    "author_type": "User",
    "comments_count": 9,
    "reactions_count": 0,
    "labels": "bug",
    "milestone": null,
    "closed_by": "Cyrilvallez",
    "resolution_time_hours": 129.65972222222223,
    "first_comments": [
      {
        "author": "developer0hye",
        "created_at": "2025-01-06T05:20:55+00:00",
        "body": "It is related to https://github.com/OpenGVLab/InternVL/issues/774"
      },
      {
        "author": "ArthurZucker",
        "created_at": "2025-01-09T09:12:00+00:00",
        "body": "We are working on it!"
      },
      {
        "author": "Cyrilvallez",
        "created_at": "2025-01-09T11:09:35+00:00",
        "body": "Hey @developer0hye, thanks for opening this issue! It appeared because we refactored the RoPE module to be inside the Model, instead of the Layers. Adding `device_map['language_model.model.rotary_emb'] = 0` to your device_map should fix it!\r\nWe are working to see if we can do it by default/raise a warning if the module was skipped, but this will fix it in the meantime!"
      },
      {
        "author": "ArthurZucker",
        "created_at": "2025-01-09T12:01:32+00:00",
        "body": "For BC we should probably just add that line no? "
      },
      {
        "author": "Cyrilvallez",
        "created_at": "2025-01-09T13:22:55+00:00",
        "body": "https://github.com/huggingface/transformers/pull/35583 will automatically take care of setting it if not provided\r\n\r\nEDIT: but it's best to explicitly set it, as custom device_map should be the responsibility of the user, this is only provided for BC"
      }
    ],
    "url": "https://github.com/huggingface/transformers/issues/35505"
  },
  {
    "repository": "huggingface/transformers",
    "issue_number": 35501,
    "title": "Fix typo is_soundfile_availble ",
    "body": "### Feature request\n\nPlease fix the typo in transformers.utils\r\n\r\nis_soundfile_availble should be is_soundfile_available\n\n### Motivation\n\nShould be self explanatory\n\n### Your contribution\n\nThere already is a PR but it hasn't been merged for multiple months even though this fix takes 1 minute. \r\n\r\nhttps://github.com/huggingface/transformers/pull/35030",
    "state": "closed",
    "created_at": "2025-01-03T09:23:09+00:00",
    "closed_at": "2025-01-03T13:37:59+00:00",
    "updated_at": "2025-01-03T13:37:59+00:00",
    "author": "j0yk1ll",
    "author_type": "User",
    "comments_count": 1,
    "reactions_count": 0,
    "labels": "Feature request",
    "milestone": null,
    "closed_by": "LysandreJik",
    "resolution_time_hours": 4.247222222222222,
    "first_comments": [
      {
        "author": "LysandreJik",
        "created_at": "2025-01-03T13:37:57+00:00",
        "body": "We just merged the other PR, thank you!"
      }
    ],
    "url": "https://github.com/huggingface/transformers/issues/35501"
  },
  {
    "repository": "huggingface/transformers",
    "issue_number": 35495,
    "title": "Wav2Vec2BertForSequenceClassification. return_attention_mask work wrong",
    "body": "### System Info\r\n\r\n- `transformers` version: 4.47.1\r\n- Platform: Linux-6.8.0-47-generic-x86_64-with-glibc2.39\r\n- Python version: 3.12.3\r\n- Huggingface_hub version: 0.27.0\r\n- Safetensors version: 0.4.5\r\n- Accelerate version: 0.26.0\r\n- Accelerate config:    not found\r\n- PyTorch version (GPU?): 2.5.1+cu124 (True)\r\n- Tensorflow version (GPU?): not installed (NA)\r\n- Flax version (CPU?/GPU?/TPU?): not installed (NA)\r\n- Jax version: not installed\r\n- JaxLib version: not installed\r\n- Using distributed or parallel set-up in script?: <fill in>\r\n- Using GPU in script?: <fill in>\r\n- GPU type: NVIDIA RTX A5000\r\n\r\n### Who can help?\r\n\r\n@ylacombe \r\n\r\n### Information\r\n\r\n- [X] The official example scripts\r\n- [ ] My own modified scripts\r\n\r\n### Tasks\r\n\r\n- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\r\n- [X] My own task or dataset (give details below)\r\n\r\n### Reproduction\r\n\r\nI'am using https://github.com/huggingface/transformers/blob/main/examples/pytorch/audio-classification/run_audio_classification.py\r\n\r\n```python\r\nfrom transformers import (\r\n    AutoConfig,\r\n    AutoFeatureExtractor,\r\n    AutoModelForAudioClassification\r\n)\r\n\r\n\r\ndataset = load_dataset(\"hf-internal-testing/librispeech_asr_demo\", \"clean\", split=\"validation\")\r\ndataset = dataset.sort(\"id\")\r\nsampling_rate = dataset.features[\"audio\"].sampling_rate\r\n\r\n\r\nlabels = raw_datasets[\"train\"].features[data_args.label_column_name].names\r\nlabel2id, id2label = {}, {}\r\nfor i, label in enumerate(labels):\r\n        label2id[label] = str(i)\r\n        id2label[str(i)] = label\r\n\r\n\r\nmodel_name_or_path = \"facebook/w2v-bert-2.0\"\r\n\r\nfeature_extractor = AutoFeatureExtractor.from_pretrained(model_name_or_path)\r\nconfig = AutoConfig.from_pretrained(\r\n        model_args.config_name or model_args.model_name_or_path,\r\n        num_labels=len(label_list),\r\n        label2id=label2id,\r\n        id2label=id2label,\r\n        finetuning_task=\"audio-classification\",\r\n)\r\nmodel = AutoModelForAudioClassification.from_pretrained(\r\n        model_name_or_path,\r\n        config=config,\r\n)\r\n\r\ndef train_transforms(batch):\r\n        \"\"\"Apply train_transforms across a batch.\"\"\"\r\n        subsampled_wavs = []\r\n        for audio in batch[data_args.audio_column_name]:\r\n            wav = random_subsample(\r\n                audio[\"array\"], max_length=data_args.max_length_seconds, sample_rate=feature_extractor.sampling_rate\r\n            )\r\n            subsampled_wavs.append(wav)\r\n        inputs = feature_extractor(subsampled_wavs, sampling_rate=feature_extractor.sampling_rate)\r\n        output_batch = {model_input_name: inputs.get(model_input_name)}\r\n        output_batch[\"labels\"] = list(batch[data_args.label_column_name])\r\n\r\n        return output_batch\r\n\r\ndef val_transforms(batch):\r\n        \"\"\"Apply val_transforms across a batch.\"\"\"\r\n        wavs = [audio[\"array\"] for audio in batch[data_args.audio_column_name]]\r\n        inputs = feature_extractor(wavs, sampling_rate=feature_extractor.sampling_rate)\r\n        output_batch = {model_input_name: inputs.get(model_input_name)}\r\n        output_batch[\"labels\"] = list(batch[data_args.label_column_name])\r\n\r\nraw_datasets[\"train\"].set_transform(train_transforms, output_all_columns=False)\r\nraw_datasets[\"eval\"].set_transform(val_transforms, output_all_columns=False)\r\n\r\n...\r\n## I don't know..\r\n```\r\n\r\n### Expected behavior\r\n\r\nWhen padding in batch, attention_mask will always [1,1,1,...,1] \r\nbut, I expect [0,0,0,...,1,1]",
    "state": "closed",
    "created_at": "2025-01-03T03:58:53+00:00",
    "closed_at": "2025-01-13T01:51:37+00:00",
    "updated_at": "2025-01-13T01:51:37+00:00",
    "author": "HERIUN",
    "author_type": "User",
    "comments_count": 3,
    "reactions_count": 0,
    "labels": "bug",
    "milestone": null,
    "closed_by": "HERIUN",
    "resolution_time_hours": 237.8788888888889,
    "first_comments": [
      {
        "author": "LysandreJik",
        "created_at": "2025-01-03T13:41:23+00:00",
        "body": "cc @eustlb "
      },
      {
        "author": "sambhavnoobcoder",
        "created_at": "2025-01-12T10:15:48+00:00",
        "body": "hey @HERIUN , could you verify if you strictly expected a left padded output , or if even a right padded output like [1,1,1..... 0,0,0] would be equally accurate for you ? I was giving this issue a look and used the following script for replication : \r\n```\r\nimport torch\r\nfrom datasets import load_dataset\r\nfrom transformers import AutoFeatureExtractor, AutoModelForAudioClassification, AutoConfig\r\n\r\n# Load a small test dataset\r\ndataset = load_dataset(\"hf-internal-testing/librispeech_asr_demo\", \"clean\", split=\"validation\")\r\ndataset = dataset.select(range(4))  # Just use 4 examples for testing\r\n\r\n# Setup labels\r\nlabel_list = [\"label1\", \"label2\"]  # Dummy labels for testing\r\nlabel2id = {label: str(i) for i, label in enumerate(label_list)}\r\nid2label = {str(i): label for i, label in enumerate(label_list)}\r\n\r\n# Load model and feature extractor\r\nmodel_name = \"facebook/w2v-bert-2.0\"\r\nfeature_extractor = AutoFeatureExtractor.from_pretrained(model_name)\r\nconfig = AutoConfig.from_pretrained(\r\n    model_name,\r\n    num_labels=len(label_list),\r\n    label2id=label2id,\r\n    id2label=id2label,\r\n    finetuning_task=\"audio-classification\",\r\n)\r\nmodel = AutoModelForAudioClassification.from_pretrained(model_name, config=config)\r\n\r\n# Process audio inputs\r\naudio_inputs = [x[\"audio\"][\"array\"] for x in dataset]\r\nsampling_rate = dataset.features[\"audio\"].sampling_rate\r\n\r\n# Create batch with different length inputs to force padding\r\ninputs = feature_extractor(\r\n    audio_inputs, \r\n    sampling_rate=sampling_rate,\r\n    padding=True,\r\n    return_tensors=\"pt\"\r\n)\r\n\r\nprint(\"Attention mask shape:\", inputs.attention_mask.shape)\r\nprint(\"Sample attention mask:\\n\", inputs.attention_mask[0])\r\n```\r\n\r\nand the output I got was \r\n```\r\nAttention mask shape: torch.Size([4, 624])\r\nSample attention mask:\r\n tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\r\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\r\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\r\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\r\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\r\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\r\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\r\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\r\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\r\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\r\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\r\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\r\n        1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\r\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\r\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\r\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\r\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\r\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\r\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\r\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\r\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\r\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\r\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\r\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\r\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\r\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\r\n       dtype=torch.int32)\r\n```\r\nwhich is different from your finding of all 1's . I was just checking to verify if this is in line with your expected outputs or not . if not , I will give a look into what is the cause if this issue . "
      },
      {
        "author": "HERIUN",
        "created_at": "2025-01-13T01:51:37+00:00",
        "body": "you're right. I was confused.."
      }
    ],
    "url": "https://github.com/huggingface/transformers/issues/35495"
  },
  {
    "repository": "huggingface/transformers",
    "issue_number": 35491,
    "title": "tokenizers v0.21 isn't supported",
    "body": "### Feature request\n\n[tokenizers v0.21.0](https://github.com/huggingface/tokenizers/releases/tag/v0.21.0) was released in November. There are no listed breaking changes.\n\n### Motivation\n\nOther packages conflict with `transformers` requirement of v0.20.x only.\n\n### Your contribution\n\nWhy is this a required field? If I'm going to open a PR, I'm going to do it *after* I file the issue so that the Git commit can mark it as fixed.",
    "state": "closed",
    "created_at": "2025-01-02T19:41:32+00:00",
    "closed_at": "2025-01-02T20:23:59+00:00",
    "updated_at": "2025-01-02T20:23:59+00:00",
    "author": "gremlation",
    "author_type": "User",
    "comments_count": 0,
    "reactions_count": 0,
    "labels": "Feature request",
    "milestone": null,
    "closed_by": "gremlation",
    "resolution_time_hours": 0.7075,
    "first_comments": [],
    "url": "https://github.com/huggingface/transformers/issues/35491"
  },
  {
    "repository": "huggingface/transformers",
    "issue_number": 35485,
    "title": " How to run the model on another machine and send the answer to another machine.",
    "body": "### System Info\n\ntransformers 4.31.0 , window os , python 3.10.12\n\n### Who can help?\n\nvision models: @amyeroberts, @qubvel\r\n\r\nI have tried using this model on my machine myself, and it works normally, but the processing is very slow because the GPU on my machine is not that powerful. However, I have a server with a strong GPU. If I install this model on the server and run the code on my machine, when it reaches the video processing stage, it sends the task to the server, and the server sends back the result. Then my machine will print the answer and display the result. Is this possible? If so, how can I do it?\r\n\n\n### Information\n\n- [ ] The official example scripts\n- [ ] My own modified scripts\n\n### Tasks\n\n- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\n- [ ] My own task or dataset (give details below)\n\n### Reproduction\n\n.\n\n### Expected behavior\n\nI expect it to work in a hybrid way between my computer and the server to achieve faster results.",
    "state": "closed",
    "created_at": "2025-01-02T10:03:42+00:00",
    "closed_at": "2025-01-07T10:20:02+00:00",
    "updated_at": "2025-01-07T10:20:46+00:00",
    "author": "ixn3rd3mxn",
    "author_type": "User",
    "comments_count": 2,
    "reactions_count": 0,
    "labels": "bug",
    "milestone": null,
    "closed_by": "qubvel",
    "resolution_time_hours": 120.27222222222223,
    "first_comments": [
      {
        "author": "david-waterworth",
        "created_at": "2025-01-05T21:47:55+00:00",
        "body": "You want a model/inference server. Perhaps start with [gradio](https://www.gradio.app/) as it's relatively simple. NVIDIA Triton is another example if you're looking for something \"industrial strength)"
      },
      {
        "author": "qubvel",
        "created_at": "2025-01-07T10:20:02+00:00",
        "body": "Hi @ixn3rd3mxn, we are trying to keep issues on github for bug reports, feature requests, and issues related to the library code. For your question, it's better to use the forum https://discuss.huggingface.co/. But in general, I agree with @david-waterworth, it's possible and gradio or any framework like Flask or FastAPI might be useful for your purpose."
      }
    ],
    "url": "https://github.com/huggingface/transformers/issues/35485"
  },
  {
    "repository": "huggingface/transformers",
    "issue_number": 35483,
    "title": "Wav2Vec2BertForSequenceClassification   freeze_feature_encoder() doesn't exist",
    "body": "### Feature request\n\n\r\n```\r\nmodel = AutoModelForAudioClassification.from_pretrained(\"facebook/w2v-bert-2.0\")\r\n\r\nmodel.freeze_feature_encoder() ## doesn't exist!!\r\n```\r\n\r\nIs it missed? \n\n### Motivation\n\nanother classification model has ```freeze_feature_encoder()```\n\n### Your contribution\n\nNone",
    "state": "closed",
    "created_at": "2025-01-02T09:16:12+00:00",
    "closed_at": "2025-01-03T05:57:25+00:00",
    "updated_at": "2025-01-03T05:57:25+00:00",
    "author": "HERIUN",
    "author_type": "User",
    "comments_count": 1,
    "reactions_count": 0,
    "labels": "Feature request",
    "milestone": null,
    "closed_by": "HERIUN",
    "resolution_time_hours": 20.686944444444446,
    "first_comments": [
      {
        "author": "HERIUN",
        "created_at": "2025-01-03T05:57:23+00:00",
        "body": "freeze_base_model() exists.."
      }
    ],
    "url": "https://github.com/huggingface/transformers/issues/35483"
  },
  {
    "repository": "huggingface/transformers",
    "issue_number": 35480,
    "title": "Links in release note are broken",
    "body": "This should be a github-config issue\r\n\r\n@ArthurZucker \r\n\r\n### Reproduction\r\n\r\n<img width=\"1034\" alt=\"image\" src=\"https://github.com/user-attachments/assets/0c27855f-bc4f-469d-b94c-62e0b84cbdc2\" />\r\n\r\n\r\n### Expected behavior\r\n\r\nlinks point to correct PR",
    "state": "closed",
    "created_at": "2025-01-02T04:10:11+00:00",
    "closed_at": "2025-01-21T15:44:56+00:00",
    "updated_at": "2025-01-26T03:37:16+00:00",
    "author": "oraluben",
    "author_type": "User",
    "comments_count": 2,
    "reactions_count": 0,
    "labels": "bug",
    "milestone": null,
    "closed_by": "ArthurZucker",
    "resolution_time_hours": 467.57916666666665,
    "first_comments": [
      {
        "author": "ArthurZucker",
        "created_at": "2025-01-21T15:44:56+00:00",
        "body": "Sorry just fixed it!"
      },
      {
        "author": "oraluben",
        "created_at": "2025-01-26T03:37:15+00:00",
        "body": "@ArthurZucker the links seems still wrong, it should be github/huggingface/transformers, not github/transformers, and there's also some left unchanged :)"
      }
    ],
    "url": "https://github.com/huggingface/transformers/issues/35480"
  },
  {
    "repository": "huggingface/transformers",
    "issue_number": 35463,
    "title": "Qwen2-VL used to work with `inputs_embeds` instead of `input_ids`, but no more",
    "body": "### System Info\r\n\r\n- `transformers` version: 4.47.1\r\n- Platform: Linux-4.18.0-513.18.1.el8_9.x86_64-x86_64-with-glibc2.35\r\n- Python version: 3.10.12\r\n- Huggingface_hub version: 0.27.0\r\n- Safetensors version: 0.4.5\r\n- Accelerate version: 1.2.1\r\n- Accelerate config:    not found\r\n- PyTorch version (GPU?): 2.5.1+cu121 (True)\r\n- Tensorflow version (GPU?): not installed (NA)\r\n- Flax version (CPU?/GPU?/TPU?): not installed (NA)\r\n- Jax version: not installed\r\n- JaxLib version: not installed\r\n- Using distributed or parallel set-up in script?: No\r\n- Using GPU in script?: Yes\r\n- GPU type: NVIDIA H100 80GB HBM3\r\n\r\n### Who can help?\r\n\r\n@zucchini-nlp \r\n\r\n### Information\r\n\r\n- [ ] The official example scripts\r\n- [X] My own modified scripts\r\n\r\n### Tasks\r\n\r\n- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\r\n- [X] My own task or dataset (give details below)\r\n\r\n### Reproduction\r\n\r\n### Preparation\r\n\r\n```python\r\nimport torch\r\nfrom transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor\r\nfrom qwen_vl_utils import process_vision_info\r\n\r\nmodel = Qwen2VLForConditionalGeneration.from_pretrained(\r\n \"Qwen/Qwen2-VL-7B-Instruct\",\r\n torch_dtype=torch.bfloat16,\r\n attn_implementation=\"eager\", # flash_attention_2 also produces the same error\r\n device_map=\"auto\",\r\n)\r\nprocessor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-7B-Instruct\")\r\n\r\nmessages = [\r\n    {\r\n        \"role\": \"user\",\r\n        \"content\": [\r\n            {\r\n                \"type\": \"image\",\r\n                \"image\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg\",\r\n            },\r\n            {\"type\": \"text\", \"text\": \"Describe this image.\"},\r\n        ],\r\n    }\r\n]\r\n\r\ntext = processor.apply_chat_template(\r\n    messages, tokenize=False, add_generation_prompt=True\r\n)\r\nimage_inputs, video_inputs = process_vision_info(messages)\r\ninputs = processor(\r\n    text=[text],\r\n    images=image_inputs,\r\n    videos=video_inputs,\r\n    padding=True,\r\n    return_tensors=\"pt\",\r\n)\r\ninputs = inputs.to(\"cuda\")\r\n```\r\n\r\n### Working example\r\n\r\n```python\r\ngenerated_ids = model.generate(**inputs, max_new_tokens=128)\r\n```\r\n\r\n### Used to work\r\nWorked in 9470d6532436e9db2951a196effd6f8841befb76 but not in v4.47.1 [[comparison]](https://github.com/huggingface/transformers/compare/9470d6532436e9db2951a196effd6f8841befb76...241c04d36867259cdf11dbb4e9d9a60f9cb65ebc)\r\n\r\n```python\r\ninput_ids = inputs[\"input_ids\"]\r\nattention_mask = inputs[\"attention_mask\"]\r\npixel_values = inputs[\"pixel_values\"]\r\nimage_grid_thw = inputs[\"image_grid_thw\"]\r\n\r\ninputs_embeds = model.model.embed_tokens(input_ids)\r\nif pixel_values is not None:\r\n    pixel_values = pixel_values.type(model.visual.get_dtype())\r\n    image_embeds = model.visual(pixel_values, grid_thw=image_grid_thw)\r\n    n_image_tokens = (input_ids == model.config.image_token_id).sum().item()\r\n    n_image_features = image_embeds.shape[0]\r\n    if n_image_tokens != n_image_features:\r\n        raise ValueError(\r\n            f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}\"\r\n        )\r\n    image_mask = (\r\n        (input_ids == model.config.image_token_id)\r\n        .unsqueeze(-1)\r\n        .expand_as(inputs_embeds)\r\n        .to(inputs_embeds.device)\r\n    )\r\n    image_embeds = image_embeds.to(inputs_embeds.device, inputs_embeds.dtype)\r\n    inputs_embeds = inputs_embeds.masked_scatter(image_mask, image_embeds)\r\n\r\nif attention_mask is not None:\r\n    attention_mask = attention_mask.to(inputs_embeds.device)\r\n\r\ngenerated_ids = model.generate(inputs_embeds=inputs_embeds, attention_mask=attention_mask, max_new_tokens=128)\r\n```\r\n\r\n### Expected behavior\r\n\r\nThe latter should work the same as the former.\r\n\r\nThe latter's error message example\r\n\r\n```\r\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/qwen2_vl/modeling_qwen2_vl.py\", line 578, in forward\r\n    attn_weights = attn_weights + causal_mask\r\nRuntimeError: The size of tensor a (2362) must match the size of tensor b (1182) at non-singleton dimension 3\r\n```",
    "state": "closed",
    "created_at": "2024-12-31T07:54:53+00:00",
    "closed_at": "2025-01-08T15:36:05+00:00",
    "updated_at": "2025-01-08T15:36:05+00:00",
    "author": "minostauros",
    "author_type": "User",
    "comments_count": 0,
    "reactions_count": 0,
    "labels": "bug,VLM",
    "milestone": null,
    "closed_by": "ArthurZucker",
    "resolution_time_hours": 199.68666666666667,
    "first_comments": [],
    "url": "https://github.com/huggingface/transformers/issues/35463"
  },
  {
    "repository": "huggingface/transformers",
    "issue_number": 35450,
    "title": "Assistant decoding w. Llava-Next does not work",
    "body": "### System Info\n\ntransformers==4.47.0\n\n### Who can help?\n\nHello @zucchini-nlp,\r\n\r\nI tried using Llava-Next 7b and 13b for assistant decoding, but I encountered some errors as below.\r\nCould you please provide advice on how to resolve these issues?\r\n\r\nDuring debugging, I noticed that the assistant model successfully completed the first round of drafting, but errors occurred during the second round of drafting.\r\n\r\nThank you in advance for your help!\r\n# Code\r\n```\r\nfrom transformers import LlavaNextProcessor, LlavaNextForConditionalGeneration\r\nfrom PIL import Image\r\nimport requests\r\n\r\nmain_model = LlavaNextForConditionalGeneration.from_pretrained(\r\n    \"llava-hf/llava-v1.6-vicuna-13b-hf\",\r\n    load_in_4bit=True,\r\n    low_cpu_mem_usage=True,\r\n).eval()\r\n\r\nassistant_model = LlavaNextForConditionalGeneration.from_pretrained(\r\n    \"llava-hf/llava-v1.6-vicuna-7b-hf\",\r\n    load_in_4bit=True,\r\n    low_cpu_mem_usage=True,\r\n).eval()\r\nprocessor = LlavaNextProcessor.from_pretrained(\"llava-hf/llava-v1.6-vicuna-13b-hf\")\r\n\r\nurl = \"https://raw.githubusercontent.com/haotian-liu/LLaVA/main/images/llava_v1_5_radar.jpg\"\r\nimage = Image.open(requests.get(url, stream=True).raw)\r\nconversation = [\r\n    {\r\n        \"role\": \"user\",\r\n        \"content\": [\r\n            {\"type\": \"text\", \"text\": \"What is shown in this image?\"},\r\n            {\"type\": \"image\"},\r\n        ],\r\n    }\r\n]\r\nprompt = processor.apply_chat_template(conversation, add_generation_prompt=True)\r\ninputs = processor(images=image, text=prompt, return_tensors=\"pt\").to(\"cuda:0\")\r\noutputs = main_model.generate(**inputs, assistant_model=assistant_model, max_new_tokens=100, num_assistant_tokens=5)\r\n```\r\n\r\n\r\n### Error\r\n```\r\n  File \"MY_COMPUTER_PATH/codes/multi-spec/test.py\", line 55, in <module>                                                                 [11/1861]\r\n    outputs = main_model.generate(**inputs, assistant_model=assistant_model, max_new_tokens=100, num_assistant_tokens=5)\r\n  File \"MY_COMPUTER_PATH/anaconda3/envs/spec_env/lib/python3.9/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"MY_COMPUTER_PATH/anaconda3/envs/spec_env/lib/python3.9/site-packages/transformers/generation/utils.py\", line 2199, in generate\r\n    result = self._assisted_decoding(\r\n  File \"MY_COMPUTER_PATH/anaconda3/envs/spec_env/lib/python3.9/site-packages/transformers/generation/utils.py\", line 4271, in _assisted_decoding\r\n    candidate_input_ids, candidate_logits = candidate_generator.get_candidates(input_ids)\r\n  File \"MY_COMPUTER_PATH/anaconda3/envs/spec_env/lib/python3.9/site-packages/transformers/generation/candidate_generator.py\", line 243, in get_can\r\ndidates\r\n    assistant_output = self.assistant_model.generate(**assistant_generation_kwargs, **self.assistant_kwargs)\r\n  File \"MY_COMPUTER_PATH/anaconda3/envs/spec_env/lib/python3.9/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"MY_COMPUTER_PATH/anaconda3/envs/spec_env/lib/python3.9/site-packages/transformers/generation/utils.py\", line 2256, in generate\r\n    result = self._sample(\r\n  File \"MY_COMPUTER_PATH/anaconda3/envs/spec_env/lib/python3.9/site-packages/transformers/generation/utils.py\", line 3255, in _sample\r\n    outputs = self(**model_inputs, return_dict=True)\r\n  File \"MY_COMPUTER_PATH/anaconda3/envs/spec_env/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"MY_COMPUTER_PATH/anaconda3/envs/spec_env/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"MY_COMPUTER_PATH/anaconda3/envs/spec_env/lib/python3.9/site-packages/accelerate/hooks.py\", line 165, in new_forward\r\n    output = module._old_forward(*args, **kwargs)\r\n  File \"MY_COMPUTER_PATH/anaconda3/envs/spec_env/lib/python3.9/site-packages/transformers/models/llava_next/modeling_llava_next.py\", line 873, in\r\nforward\r\n    inputs_embeds = inputs_embeds.to(image_features.dtype)\r\nAttributeError: 'NoneType' object has no attribute 'dtype'\r\n```\n\n### Information\n\n- [ ] The official example scripts\n- [ ] My own modified scripts\n\n### Tasks\n\n- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\n- [ ] My own task or dataset (give details below)\n\n### Reproduction\n\n```\r\nfrom transformers import LlavaNextProcessor, LlavaNextForConditionalGeneration\r\nfrom PIL import Image\r\nimport requests\r\n\r\nmain_model = LlavaNextForConditionalGeneration.from_pretrained(\r\n    \"llava-hf/llava-v1.6-vicuna-13b-hf\",\r\n    load_in_4bit=True,\r\n    low_cpu_mem_usage=True,\r\n).eval()\r\n\r\nassistant_model = LlavaNextForConditionalGeneration.from_pretrained(\r\n    \"llava-hf/llava-v1.6-vicuna-7b-hf\",\r\n    load_in_4bit=True,\r\n    low_cpu_mem_usage=True,\r\n).eval()\r\nprocessor = LlavaNextProcessor.from_pretrained(\"llava-hf/llava-v1.6-vicuna-13b-hf\")\r\n\r\nurl = \"https://raw.githubusercontent.com/haotian-liu/LLaVA/main/images/llava_v1_5_radar.jpg\"\r\nimage = Image.open(requests.get(url, stream=True).raw)\r\nconversation = [\r\n    {\r\n        \"role\": \"user\",\r\n        \"content\": [\r\n            {\"type\": \"text\", \"text\": \"What is shown in this image?\"},\r\n            {\"type\": \"image\"},\r\n        ],\r\n    }\r\n]\r\nprompt = processor.apply_chat_template(conversation, add_generation_prompt=True)\r\ninputs = processor(images=image, text=prompt, return_tensors=\"pt\").to(\"cuda:0\")\r\noutputs = main_model.generate(**inputs, assistant_model=assistant_model, max_new_tokens=100, num_assistant_tokens=5)\r\n```\n\n### Expected behavior\n\nAssistant decoding works well",
    "state": "closed",
    "created_at": "2024-12-29T11:47:23+00:00",
    "closed_at": "2025-01-08T09:35:25+00:00",
    "updated_at": "2025-01-08T09:35:25+00:00",
    "author": "ddehun",
    "author_type": "User",
    "comments_count": 3,
    "reactions_count": 0,
    "labels": "bug,VLM",
    "milestone": null,
    "closed_by": "zucchini-nlp",
    "resolution_time_hours": 237.80055555555555,
    "first_comments": [
      {
        "author": "zucchini-nlp",
        "created_at": "2025-01-06T09:57:20+00:00",
        "body": "I see, the error stems from the fact that LLaVA currently cannot generate from text-only inputs. That should be fixed by https://github.com/huggingface/transformers/pull/34502"
      },
      {
        "author": "ddehun",
        "created_at": "2025-01-06T10:07:14+00:00",
        "body": "Thank you for getting back to me. \r\n\r\nIt seems that in the current implementation of Llava, during the speculative decoding process, when a token generated by the verification model is introduced, two new input tokens are produced. This is incorrectly regarded as a legacy_processing, causing an error. Iâ€™m attaching the workaround I implemented. There might be a better implementation, but Iâ€™m sharing this in case others encounter a similar issue.\r\n\r\nOriginal Code\r\nhttps://github.com/huggingface/transformers/blob/v4.47.1/src/transformers/models/llava_next/modeling_llava_next.py#L838\r\n```\r\nlegacy_processing = False        \r\n        already_has_past = past_key_values is not None\r\n        if inputs_embeds is None:\r\n            inputs_embeds = self.get_input_embeddings()(input_ids)\r\n            if already_has_past and use_cache:                \r\n                legacy_processing = False\r\n            else:                \r\n                # if the number of image tokens is more than image embeddings seq length, then prob we expanded it in processing\r\n                # not very reliable, but we don't expect one to actually pass 500+ images for one prompt\r\n                # In case we're in decoding stage, legacy behavior is checked by presence of pixel values even if use_cache=True\r\n                legacy_processing = (\r\n                    (input_ids == self.config.image_token_index).sum(1).max() < self.config.image_seq_length\r\n                ) or (input_ids.shape[-1] == 1 and pixel_values is not None)\r\n   ```             "
      },
      {
        "author": "zucchini-nlp",
        "created_at": "2025-01-06T11:39:36+00:00",
        "body": "Thanks for sharing! The legacy path is to be removed in the next release hopefully which removes the need for further workarounds"
      }
    ],
    "url": "https://github.com/huggingface/transformers/issues/35450"
  },
  {
    "repository": "huggingface/transformers",
    "issue_number": 35442,
    "title": "Calling `to()` is not supported for `4-bit` quantized models with the installed version of bitsandbytes. The current device is `cuda:0`. If you intended to move the model, please install bitsandbytes >= 0.43.2.",
    "body": "### System Info\r\n\r\nTransformers : version 4.48.0.dev0\r\nOS : Arch Linux 6.12.6 Kernel \r\nPython version : 3.11.0\r\nRocm : 6.3.1 version \r\nGPU : AMD 6800 XT\r\n\r\n### Who can help?\r\n\r\n_No response_\r\n\r\n### Information\r\n\r\n- [X] The official example scripts\r\n- [ ] My own modified scripts\r\n\r\n### Tasks\r\n\r\n- [X ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\r\n- [ ] My own task or dataset (give details below)\r\n\r\n### Reproduction\r\n\r\nHi guys i got a big problem with this error. Few hours ago i was able to use CXH Joy Caption for make a detailled prompt from an image. Sadly it don't work anymore and i don't know at all how i made it work for the first time\r\nWhen i launch CXH Joy caption it give me this error even with the good bitsandbytes version.\r\ni saw @younesbelkada made a script `from transformers import AutoModelForCausalLM, AutoConfig, AutoTokenizer\r\nimport torch\r\n\r\nmodel_path=\"tiiuae/falcon-40b-instruct\"\r\n\r\nconfig = AutoConfig.from_pretrained(model_path, trust_remote_code=True)\r\nmodel = AutoModelForCausalLM.from_pretrained(model_path, trust_remote_code=True, load_in_4bit=True, device_map=\"auto\")\r\n\r\ntokenizer = AutoTokenizer.from_pretrained(\"tiiuae/falcon-40b-instruct\")\r\n\r\ninput_text = \"Describe the solar system.\"\r\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(\"cuda\")\r\n\r\noutputs = model.generate(input_ids, max_length=10)\r\nprint(tokenizer.decode(outputs[0]))` \r\n\r\nBut i don't know to use it and where to use it for solve this issue .\r\nHere you'll see my terminal log from my last session with ComfyUI. \r\nIf someone could help me it could be cool. \r\nI'm pretty in all this world. \r\n\r\nHere is the log. \r\n`(bit) [bryan@archlinux ComfyUI]$ python main.py \r\n[START] Security scan\r\nDEPRECATION: Loading egg at /home/bryan/.pyenv/versions/3.11.1/envs/bit/lib/python3.11/site-packages/flash_attn-2.7.2.post1-py3.11.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\r\n[DONE] Security scan\r\n## ComfyUI-Manager: installing dependencies done.\r\n** ComfyUI startup time: 2024-12-28 21:52:02.458268\r\n** Platform: Linux\r\n** Python version: 3.11.1 (main, Dec 26 2024, 18:00:13) [GCC 14.2.1 20240910]\r\n** Python executable: /home/bryan/.pyenv/versions/bit/bin/python\r\n** ComfyUI Path: /home/bryan/Documents/ComfyUI\r\n** Log path: /home/bryan/Documents/ComfyUI/comfyui.log\r\nDEPRECATION: Loading egg at /home/bryan/.pyenv/versions/3.11.1/envs/bit/lib/python3.11/site-packages/flash_attn-2.7.2.post1-py3.11.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\r\nDEPRECATION: Loading egg at /home/bryan/.pyenv/versions/3.11.1/envs/bit/lib/python3.11/site-packages/flash_attn-2.7.2.post1-py3.11.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\r\n\r\nPrestartup times for custom nodes:\r\n   0.9 seconds: /home/bryan/Documents/ComfyUI/custom_nodes/ComfyUI-Manager\r\n\r\nTotal VRAM 16368 MB, total RAM 31903 MB\r\npytorch version: 2.5.1+rocm6.2\r\nWARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:\r\n    PyTorch 2.5.1+cu121 with CUDA 1201 (you have 2.5.1+rocm6.2)\r\n    Python  3.11.11 (you have 3.11.1)\r\n  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)\r\n  Memory-efficient attention, SwiGLU, sparse and more won't be available.\r\n  Set XFORMERS_MORE_DETAILS=1 for more details\r\nxformers version: 0.0.29\r\nSet vram state to: NORMAL_VRAM\r\nDevice: cuda:0 AMD Radeon RX 6800 XT : native\r\nUsing sub quadratic optimization for attention, if you have memory or speed issues try using: --use-split-cross-attention\r\n[Prompt Server] web root: /home/bryan/Documents/ComfyUI/web\r\nTraceback (most recent call last):\r\n  File \"/home/bryan/Documents/ComfyUI/nodes.py\", line 2089, in load_custom_node\r\n    module_spec.loader.exec_module(module)\r\n  File \"<frozen importlib._bootstrap_external>\", line 940, in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\r\n  File \"/home/bryan/Documents/ComfyUI/custom_nodes/ComfyUI_LayerStyle/__init__.py\", line 35, in <module>\r\n    imported_module = importlib.import_module(\".py.{}\".format(name), __name__)\r\n                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/bryan/.pyenv/versions/3.11.1/lib/python3.11/importlib/__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"<frozen importlib._bootstrap>\", line 1206, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 1178, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 1149, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 690, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap_external>\", line 940, in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\r\n  File \"/home/bryan/Documents/ComfyUI/custom_nodes/ComfyUI_LayerStyle/py/inner_shadow_v2.py\", line 3, in <module>\r\n    from .imagefunc import log, tensor2pil, pil2tensor, image2mask, shift_image, expand_mask, chop_image_v2, chop_mode_v2\r\n  File \"/home/bryan/Documents/ComfyUI/custom_nodes/ComfyUI_LayerStyle/py/imagefunc.py\", line 30, in <module>\r\n    from skimage import img_as_float, img_as_ubyte\r\nModuleNotFoundError: No module named 'skimage'\r\n\r\nCannot import /home/bryan/Documents/ComfyUI/custom_nodes/ComfyUI_LayerStyle module for custom nodes: No module named 'skimage'\r\nTraceback (most recent call last):\r\n  File \"/home/bryan/Documents/ComfyUI/nodes.py\", line 2089, in load_custom_node\r\n    module_spec.loader.exec_module(module)\r\n  File \"<frozen importlib._bootstrap_external>\", line 936, in exec_module\r\n  File \"<frozen importlib._bootstrap_external>\", line 1073, in get_code\r\n  File \"<frozen importlib._bootstrap_external>\", line 1130, in get_data\r\nFileNotFoundError: [Errno 2] No such file or directory: '/home/bryan/Documents/ComfyUI/custom_nodes/joy-caption-batch/__init__.py'\r\n\r\nCannot import /home/bryan/Documents/ComfyUI/custom_nodes/joy-caption-batch module for custom nodes: [Errno 2] No such file or directory: '/home/bryan/Documents/ComfyUI/custom_nodes/joy-caption-batch/__init__.py'\r\n[VideoHelperSuite] - WARNING - Failed to import imageio_ffmpeg\r\nTraceback (most recent call last):\r\n  File \"/home/bryan/Documents/ComfyUI/nodes.py\", line 2089, in load_custom_node\r\n    module_spec.loader.exec_module(module)\r\n  File \"<frozen importlib._bootstrap_external>\", line 940, in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\r\n  File \"/home/bryan/Documents/ComfyUI/custom_nodes/ComfyUI_LayerStyle_Advance/__init__.py\", line 35, in <module>\r\n    imported_module = importlib.import_module(\".py.{}\".format(name), __name__)\r\n                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/bryan/.pyenv/versions/3.11.1/lib/python3.11/importlib/__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"<frozen importlib._bootstrap>\", line 1206, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 1178, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 1149, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 690, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap_external>\", line 940, in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\r\n  File \"/home/bryan/Documents/ComfyUI/custom_nodes/ComfyUI_LayerStyle_Advance/py/get_color_tone.py\", line 3, in <module>\r\n    from .imagefunc import log, tensor2pil, gaussian_blur, get_image_color_tone, get_image_color_average, RGB_to_HSV, Hex_to_RGB\r\n  File \"/home/bryan/Documents/ComfyUI/custom_nodes/ComfyUI_LayerStyle_Advance/py/imagefunc.py\", line 30, in <module>\r\n    from skimage import img_as_float, img_as_ubyte\r\nModuleNotFoundError: No module named 'skimage'\r\n\r\nCannot import /home/bryan/Documents/ComfyUI/custom_nodes/ComfyUI_LayerStyle_Advance module for custom nodes: No module named 'skimage'\r\nTotal VRAM 16368 MB, total RAM 31903 MB\r\npytorch version: 2.5.1+rocm6.2\r\nxformers version: 0.0.29\r\nSet vram state to: NORMAL_VRAM\r\nDevice: cuda:0 AMD Radeon RX 6800 XT : native\r\n### Loading: ComfyUI-Manager (V2.55.5)\r\n### ComfyUI Version: v0.3.10-4-g4b5bcd8 | Released on '2024-12-27'\r\n\r\nImport times for custom nodes:\r\n   0.0 seconds: /home/bryan/Documents/ComfyUI/custom_nodes/websocket_image_save.py\r\n   0.0 seconds: /home/bryan/Documents/ComfyUI/custom_nodes/ComfyUI-Unload-Model\r\n   0.0 seconds (IMPORT FAILED): /home/bryan/Documents/ComfyUI/custom_nodes/joy-caption-batch\r\n   0.0 seconds: /home/bryan/Documents/ComfyUI/custom_nodes/Comfyui_JC2\r\n   0.0 seconds: /home/bryan/Documents/ComfyUI/custom_nodes/Comfyui_joytag\r\n   0.0 seconds: /home/bryan/Documents/ComfyUI/custom_nodes/ComfyUI-Miaoshouai-Tagger\r\n   0.0 seconds: /home/bryan/Documents/ComfyUI/custom_nodes/ComfyUI_SLK_joy_caption_two\r\n   0.0 seconds: /home/bryan/Documents/ComfyUI/custom_nodes/ComfyUI-GGUF\r\n   0.0 seconds: /home/bryan/Documents/ComfyUI/custom_nodes/ComfyUI-Custom-Scripts\r\n   0.0 seconds (IMPORT FAILED): /home/bryan/Documents/ComfyUI/custom_nodes/ComfyUI_LayerStyle_Advance\r\n   0.0 seconds: /home/bryan/Documents/ComfyUI/custom_nodes/mikey_nodes\r\n   0.0 seconds: /home/bryan/Documents/ComfyUI/custom_nodes/ComfyUI-LTXTricks\r\n   0.0 seconds: /home/bryan/Documents/ComfyUI/custom_nodes/ComfyUI-VideoHelperSuite\r\n   0.0 seconds: /home/bryan/Documents/ComfyUI/custom_nodes/Comfyui_joy-caption-alpha-two\r\n   0.0 seconds (IMPORT FAILED): /home/bryan/Documents/ComfyUI/custom_nodes/ComfyUI_LayerStyle\r\n   0.0 seconds: /home/bryan/Documents/ComfyUI/custom_nodes/ComfyUI-Manager\r\n   0.3 seconds: /home/bryan/Documents/ComfyUI/custom_nodes/Comfyui_CXH_joy_caption\r\n   0.3 seconds: /home/bryan/Documents/ComfyUI/custom_nodes/ComfyUI_NYJY\r\n   0.3 seconds: /home/bryan/Documents/ComfyUI/custom_nodes/ComfyUI-LTXVideo\r\n\r\nStarting server\r\n\r\nTo see the GUI go to: http://127.0.0.1:8188\r\n[ComfyUI-Manager] default cache updated: https://raw.githubusercontent.com/ltdrdata/ComfyUI-Manager/main/model-list.json\r\n[ComfyUI-Manager] default cache updated: https://raw.githubusercontent.com/ltdrdata/ComfyUI-Manager/main/github-stats.json\r\n[ComfyUI-Manager] default cache updated: https://raw.githubusercontent.com/ltdrdata/ComfyUI-Manager/main/custom-node-list.json\r\n[ComfyUI-Manager] default cache updated: https://raw.githubusercontent.com/ltdrdata/ComfyUI-Manager/main/alter-list.json\r\n[ComfyUI-Manager] default cache updated: https://raw.githubusercontent.com/ltdrdata/ComfyUI-Manager/main/extension-node-map.json\r\n^Agot prompt\r\n/home/bryan/Documents/ComfyUI/models/clip/siglip-so400m-patch14-384\r\nLoading VLM's custom vision model\r\n/home/bryan/Documents/ComfyUI/custom_nodes/Comfyui_CXH_joy_caption/Joy_caption_alpha.py:237: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\r\n  checkpoint = torch.load(clip_model_path, map_location='cpu')\r\n/home/bryan/Documents/ComfyUI/models/LLM/Meta-Llama-3.1-8B-bnb-4bit\r\nSuccessfully modified 'base_model_name_or_path' value in '/home/bryan/Documents/ComfyUI/models/Joy_caption_alpha/text_model/adapter_config.json'.\r\nLoading tokenizer\r\nLoading LLM\r\nLoading VLM's custom text model\r\nUnused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\r\nWe will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\r\n!!! Exception during processing !!! Calling `to()` is not supported for `4-bit` quantized models with the installed version of bitsandbytes. The current device is `cuda:0`. If you intended to move the model, please install bitsandbytes >= 0.43.2.\r\nTraceback (most recent call last):\r\n  File \"/home/bryan/Documents/ComfyUI/execution.py\", line 328, in execute\r\n    output_data, output_ui, has_subgraph = get_output_data(obj, input_data_all, execution_block_cb=execution_block_cb, pre_execute_cb=pre_execute_cb)\r\n                                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/bryan/Documents/ComfyUI/execution.py\", line 203, in get_output_data\r\n    return_values = _map_node_over_list(obj, input_data_all, obj.FUNCTION, allow_interrupt=True, execution_block_cb=execution_block_cb, pre_execute_cb=pre_execute_cb)\r\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/bryan/Documents/ComfyUI/execution.py\", line 174, in _map_node_over_list\r\n    process_inputs(input_dict, i)\r\n  File \"/home/bryan/Documents/ComfyUI/execution.py\", line 163, in process_inputs\r\n    results.append(getattr(obj, func)(**inputs))\r\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/bryan/Documents/ComfyUI/custom_nodes/Comfyui_CXH_joy_caption/Joy_caption_alpha.py\", line 283, in gen\r\n    self.loadCheckPoint()\r\n  File \"/home/bryan/Documents/ComfyUI/custom_nodes/Comfyui_CXH_joy_caption/Joy_caption_alpha.py\", line 261, in loadCheckPoint\r\n    text_model = AutoModelForCausalLM.from_pretrained(CHECKPOINT_PATH, device_map=\"auto\",\r\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/bryan/.pyenv/versions/bit/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py\", line 564, in from_pretrained\r\n    return model_class.from_pretrained(\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/bryan/.pyenv/versions/bit/lib/python3.11/site-packages/transformers/modeling_utils.py\", line 4288, in from_pretrained\r\n    dispatch_model(model, **device_map_kwargs)\r\n  File \"/home/bryan/.pyenv/versions/bit/lib/python3.11/site-packages/accelerate/big_modeling.py\", line 498, in dispatch_model\r\n    model.to(device)\r\n  File \"/home/bryan/.pyenv/versions/bit/lib/python3.11/site-packages/transformers/modeling_utils.py\", line 3089, in to\r\n    raise ValueError(\r\nValueError: Calling `to()` is not supported for `4-bit` quantized models with the installed version of bitsandbytes. The current device is `cuda:0`. If you intended to move the model, please install bitsandbytes >= 0.43.2.\r\n\r\nPrompt executed in 3.92 seconds\r\n^C\r\nStopped server\r\n(bit) [bryan@archlinux ComfyUI]$ python -m bitsandbytes\r\n++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\r\n++++++++++++++++++ BUG REPORT INFORMATION ++++++++++++++++++\r\n++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\r\n++++++++++++++++++++++++++ OTHER +++++++++++++++++++++++++++\r\nCUDA specs: CUDASpecs(highest_compute_capability=(10, 3), cuda_version_string='62', cuda_version_tuple=(6, 2))\r\nPyTorch settings found: CUDA_VERSION=62, Highest Compute Capability: (10, 3).\r\nWARNING: CUDA versions lower than 11 are currently not supported for LLM.int8().\r\nYou will be only to use 8-bit optimizers and quantization routines!\r\nTo manually override the PyTorch CUDA version please see: https://github.com/TimDettmers/bitsandbytes/blob/main/docs/source/nonpytorchcuda.mdx\r\nThe directory listed in your path is found to be non-existent: local/archlinux\r\nThe directory listed in your path is found to be non-existent: @/tmp/.ICE-unix/935,unix/archlinux\r\nThe directory listed in your path is found to be non-existent: /usr/etc/pyenv.d\r\nThe directory listed in your path is found to be non-existent: /usr/local/etc/pyenv.d\r\nThe directory listed in your path is found to be non-existent: /etc/pyenv.d\r\nThe directory listed in your path is found to be non-existent: /usr/lib/pyenv/hooks\r\nThe directory listed in your path is found to be non-existent: /org/freedesktop/DisplayManager/Session1\r\nThe directory listed in your path is found to be non-existent: /etc/gtk/gtkrc\r\nThe directory listed in your path is found to be non-existent: /home/bryan/.gtkrc\r\nThe directory listed in your path is found to be non-existent: /etc/gtk-2.0/gtkrc\r\nThe directory listed in your path is found to be non-existent: /Sessions/1\r\nThe directory listed in your path is found to be non-existent: /org/freedesktop/DisplayManager/Seat0\r\nThe directory listed in your path is found to be non-existent: //debuginfod.archlinux.org \r\nThe directory listed in your path is found to be non-existent: /Windows/1\r\nCUDA SETUP: WARNING! CUDA runtime files not found in any environmental path.\r\n++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\r\n++++++++++++++++++++++ DEBUG INFO END ++++++++++++++++++++++\r\n++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\r\nChecking that the library is importable and CUDA is callable...\r\nSUCCESS!\r\nInstallation was successful!\r\n`\r\n\r\n### Expected behavior\r\n\r\nIt should return a prompt for an image but it don't",
    "state": "closed",
    "created_at": "2024-12-28T21:07:01+00:00",
    "closed_at": "2025-01-28T11:45:24+00:00",
    "updated_at": "2025-01-28T11:45:24+00:00",
    "author": "Kulbuntu",
    "author_type": "User",
    "comments_count": 3,
    "reactions_count": 0,
    "labels": "Quantization,bug",
    "milestone": null,
    "closed_by": "SunMarc",
    "resolution_time_hours": 734.6397222222222,
    "first_comments": [
      {
        "author": "LysandreJik",
        "created_at": "2024-12-29T13:57:56+00:00",
        "body": "WDYT @SunMarc "
      },
      {
        "author": "SunMarc",
        "created_at": "2024-12-30T15:12:07+00:00",
        "body": "Can you check your version of bitsandbytes @Kulbuntu , the error happens because your version is too low. "
      },
      {
        "author": "github-actions[bot]",
        "created_at": "2025-01-28T08:03:27+00:00",
        "body": "This issue has been automatically marked as stale because it has not had recent activity. If you think this still needs to be addressed please comment on this thread.\n\nPlease note that issues that do not follow the [contributing guidelines](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md) are likely to be ignored."
      }
    ],
    "url": "https://github.com/huggingface/transformers/issues/35442"
  },
  {
    "repository": "huggingface/transformers",
    "issue_number": 35433,
    "title": "tokenizers.apply_chat_template with `continue_final_message=True` with trailing spaces in input",
    "body": "### System Info\n\nAs title says, `tokenizers.apply_chat_template` fails with trailing spaces in input for Llama-3.1-Instruct.\r\n\r\nIf the last `assistant` message has a trailing space, such as\r\n{'role': 'assistant', 'content': 'some text '}\r\n\r\nand `continue_final_message` is True, it throws a \"ValueError: substring not found\"\r\n\r\nThis is because in the `apply_chat_template` function, there is a line\r\n`rendered_chat = rendered_chat[: rendered_chat.rindex(final_message) + len(final_message)].rstrip()`\r\n\r\nbut `rendered_chat` ends with `\"some text<|eot_id|>\"` while the `final_message` still has the trailing space: `\"some text \"`\r\n\r\n\r\n\r\n\n\n### Who can help?\n\n@ArthurZucker @itazap\n\n### Information\n\n- [X] The official example scripts\n- [ ] My own modified scripts\n\n### Tasks\n\n- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\n- [ ] My own task or dataset (give details below)\n\n### Reproduction\n\nSee above\n\n### Expected behavior\n\nI expect it to be able to continue after the trailing space",
    "state": "closed",
    "created_at": "2024-12-27T07:24:56+00:00",
    "closed_at": "2025-01-06T18:33:34+00:00",
    "updated_at": "2025-01-06T18:33:34+00:00",
    "author": "chuyishang",
    "author_type": "User",
    "comments_count": 2,
    "reactions_count": 0,
    "labels": "bug,Chat Template",
    "milestone": null,
    "closed_by": "Rocketknight1",
    "resolution_time_hours": 251.14388888888888,
    "first_comments": [
      {
        "author": "LysandreJik",
        "created_at": "2024-12-29T14:15:14+00:00",
        "body": "cc @Rocketknight1 "
      },
      {
        "author": "Rocketknight1",
        "created_at": "2025-01-06T18:33:34+00:00",
        "body": "Hi @chuyishang, this should be fixed now! Please try updating to the latest version, or installing from `main` if that doesn't work."
      }
    ],
    "url": "https://github.com/huggingface/transformers/issues/35433"
  },
  {
    "repository": "huggingface/transformers",
    "issue_number": 35432,
    "title": "apply class transformers.SequenceBiasLogitsProcessor on Qwen model",
    "body": "### Feature request\n\nI notice that your wrote:\r\nIn order to get the token ids of the sequences that you want to bias, make sure to set add_prefix_space=True when initializing the tokenizer, and use tokenizer(bad_words, add_special_tokens=False).input_ids. The add_prefix_space argument is only supported for some slow tokenizers, as fast tokenizersâ€™ prefixing behaviours come from pre tokenizers. Read more [here](https://huggingface.co/docs/tokenizers/api/pre-tokenizers).\r\n\r\n\r\n\r\nSo if I am using Qwen model, since its tokenizer is based on fast tokenizers, I can't use this bias logits feature?\r\n\r\nIs there anyway I can use the Qwen model and still use this feature?\n\n### Motivation\n\nthe old code is not working on the new model like qwen, want update.\n\n### Your contribution\n\nnone",
    "state": "closed",
    "created_at": "2024-12-27T07:14:51+00:00",
    "closed_at": "2025-01-20T11:00:17+00:00",
    "updated_at": "2025-01-20T11:00:17+00:00",
    "author": "buptspig",
    "author_type": "User",
    "comments_count": 8,
    "reactions_count": 0,
    "labels": "Feature request,Generation",
    "milestone": null,
    "closed_by": "gante",
    "resolution_time_hours": 579.7572222222223,
    "first_comments": [
      {
        "author": "LysandreJik",
        "created_at": "2024-12-29T14:14:57+00:00",
        "body": "cc @gante WDYT?"
      },
      {
        "author": "panguagency",
        "created_at": "2025-01-12T15:44:30+00:00",
        "body": "any idea on if this feature can get updated?"
      },
      {
        "author": "Rocketknight1",
        "created_at": "2025-01-13T16:14:56+00:00",
        "body": "gentle ping @gante "
      },
      {
        "author": "gante",
        "created_at": "2025-01-14T18:19:46+00:00",
        "body": "I wrote that a long ago :D I'm going to revisit it to see if we still have the same limitation with fast tokenizers."
      },
      {
        "author": "gante",
        "created_at": "2025-01-14T19:21:50+00:00",
        "body": "@buptspig @panguagency \n\nthe docstring was indeed outdated :) check #35699 for the updated docstring, which includes an example with a fast tokenizer (qwen)"
      }
    ],
    "url": "https://github.com/huggingface/transformers/issues/35432"
  },
  {
    "repository": "huggingface/transformers",
    "issue_number": 35429,
    "title": "`GPT2Attention()` class with `_attn()` method when `is_cross_attention=True`.",
    "body": "### Model description\n\nIt seems like `GPT2Attention()` class only allows`_attn()` method with `causal_mask` when `is_cross_attention=False`, but not with when `is_cross_attention=True`. \r\n\r\nIt would be more productive if `GPT2Attention()` supports `_attn()` method with `causal_mask` even with `is_cross_attention=True`.\n\n### Open source status\n\n- [ ] The model implementation is available\n- [ ] The model weights are available\n\n### Provide useful links for the implementation\n\n_No response_",
    "state": "closed",
    "created_at": "2024-12-27T04:49:48+00:00",
    "closed_at": "2024-12-27T04:50:25+00:00",
    "updated_at": "2024-12-27T04:50:25+00:00",
    "author": "CHLEE-Leo",
    "author_type": "User",
    "comments_count": 0,
    "reactions_count": 0,
    "labels": "New model",
    "milestone": null,
    "closed_by": "CHLEE-Leo",
    "resolution_time_hours": 0.010277777777777778,
    "first_comments": [],
    "url": "https://github.com/huggingface/transformers/issues/35429"
  },
  {
    "repository": "huggingface/transformers",
    "issue_number": 35427,
    "title": "Can't load model from state_dict + config when quantized",
    "body": "### System Info\n\nI used a colab notebook\r\n- `transformers` version: 4.47.1\r\n- Platform: Linux-6.1.85+-x86_64-with-glibc2.35\r\n- Python version: 3.10.12\r\n- Huggingface_hub version: 0.27.0\r\n- Safetensors version: 0.4.5\r\n- Accelerate version: 1.2.1\r\n- Accelerate config: \tnot found\r\n- PyTorch version (GPU?): 2.5.1+cu121 (True)\r\n- Tensorflow version (GPU?): 2.17.1 (True)\r\n- Flax version (CPU?/GPU?/TPU?): 0.8.5 (gpu)\r\n- Jax version: 0.4.33\r\n- JaxLib version: 0.4.33\r\n- Using distributed or parallel set-up in script?: NO\r\n- Using GPU in script?: YES\r\n- GPU type: Tesla T4\n\n### Who can help?\n\n@SunMarc\r\n@MekkCyber \r\n@SunMarc \n\n### Information\n\n- [ ] The official example scripts\n- [X] My own modified scripts\n\n### Tasks\n\n- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\n- [ ] My own task or dataset (give details below)\n\n### Reproduction\n\n1. Open Google Collab\r\n2. Then:\r\n```python\r\n!pip install bitsandbytes\r\nfrom transformers import AutoModelForCausalLM, BitsAndBytesConfig\r\nimport torch\r\n\r\nquantization_config = BitsAndBytesConfig(\r\n  load_in_4bit              = True,\r\n  bnb_4bit_use_double_quant = True,\r\n  bnb_4bit_quant_type       = \"nf4\",\r\n  bnb_4bit_compute_dtype    = torch.float32,\r\n)\r\nmodel_name = 'LGAI-EXAONE/EXAONE-3.5-2.4B-Instruct'\r\nexaone = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True, quantization_config=quantization_config)\r\nnew_exaone = AutoModelForCausalLM.from_pretrained(None, config=exaone.config, state_dict=exaone.state_dict(), quantization_config=quantization_config)\r\n```\r\n\r\nI receive the following error\r\n\r\n`low_cpu_mem_usage` was None, now default to True since model is quantized.\r\n\r\n---------------------------------------------------------------------------\r\n\r\nAttributeError                            Traceback (most recent call last)\r\n\r\n[<ipython-input-5-6db34245f396>](https://localhost:8080/#) in <cell line: 1>()\r\n----> 1 new_exaone = AutoModelForCausalLM.from_pretrained(None, config=exaone.config, state_dict=exaone.state_dict())\r\n\r\n3 frames\r\n\r\n[/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py](https://localhost:8080/#) in load_state_dict(checkpoint_file, is_quantized, map_location, weights_only)\r\n    500     Reads a PyTorch checkpoint file, returning properly formatted errors if they arise.\r\n    501     \"\"\"\r\n--> 502     if checkpoint_file.endswith(\".safetensors\") and is_safetensors_available():\r\n    503         # Check format of the archive\r\n    504         with safe_open(checkpoint_file, framework=\"pt\") as f:\r\n\r\nAttributeError: 'NoneType' object has no attribute 'endswith'\n\n### Expected behavior\n\nfor the model to load with the quantized weights.\r\n\r\nAlso, I am trying to [implement exaone into unsloth](https://github.com/unslothai/unsloth/issues/1406). And I want to ask your thoughts on the best way to load a model, that has it's own class defined in the huggingface repo, using another class. more specifically the [exaone model](https://huggingface.co/LGAI-EXAONE/EXAONE-3.5-2.4B-Instruct) has the same architecture as llama but it just has some config and layers renamed, which results in a different state_dict. My current approach, which seems to be working when the model isn't quantized, is to load the model in ExaoneForCausalLM using AutoModelForCausalLM and then create a LlamaConfig from it, we also create a rename the keys of the state_dict (we don't copy). And we provide both of them to the from_pretrained function. What are your thoughts on how to best solve this problem? Your help is greatly appreciated!",
    "state": "closed",
    "created_at": "2024-12-27T01:13:24+00:00",
    "closed_at": "2024-12-30T13:50:44+00:00",
    "updated_at": "2024-12-30T13:52:02+00:00",
    "author": "KareemMusleh",
    "author_type": "User",
    "comments_count": 5,
    "reactions_count": 0,
    "labels": "Core: Modeling,Quantization,bug",
    "milestone": null,
    "closed_by": "KareemMusleh",
    "resolution_time_hours": 84.62222222222222,
    "first_comments": [
      {
        "author": "MekkCyber",
        "created_at": "2024-12-27T01:45:07+00:00",
        "body": "Hey @KareemMusleh, I managed to reproduce the issue, to solve it you can install transformers from source : \r\n`pip install git+https://github.com/huggingface/transformers.git`. \r\nFor your question I'm not familiar with how things are implemented in unsloth, are you asking if we can load an exaone model as if it's a llama model ?"
      },
      {
        "author": "KareemMusleh",
        "created_at": "2024-12-27T02:11:07+00:00",
        "body": "Thanks for the quick response!\r\nYes, exaone has the same architecture as llama but they implemented it in [modeling_exaone](https://huggingface.co/LGAI-EXAONE/EXAONE-3.5-2.4B-Instruct/blob/main/modeling_exaone.py) with different names for the layers and different names for the config options. I am asking how would someone do this in transformers, unsloth uses transformers."
      },
      {
        "author": "KareemMusleh",
        "created_at": "2024-12-27T02:19:50+00:00",
        "body": "Also can you please point me to the commit that solves the issue, I couldn't find it. Just in case someone asks to make this backwards compatible"
      },
      {
        "author": "KareemMusleh",
        "created_at": "2024-12-27T02:26:37+00:00",
        "body": "just checked it, the solution works!"
      },
      {
        "author": "KareemMusleh",
        "created_at": "2024-12-27T15:40:10+00:00",
        "body": "@MekkCyber, I checked it again today and it doesn't work. Again I am using colab:\r\n```python\r\n!pip install -U bitsandbytes\r\n!pip install git+https://github.com/huggingface/transformers.git\r\nfrom transformers import __version__ as transformers_version\r\nfrom transformers import AutoModelForCausalLM, BitsAndBytesConfig\r\nfrom transformers.models.llama.modeling_llama import LlamaForCausalLM, LlamaConfig\r\nimport torch\r\n\r\ndef load_correct_model(model, **model_kwargs):\r\n    if model.config.model_type == 'exaone':\r\n        import re\r\n        new_model = AutoModelForCausalLM\r\n        # pretrained_model_name_or_path = model.config._name_or_path\r\n\r\n        # get the correct config\r\n        new_config_args =  {\r\n            'vocab_size': model.config.vocab_size,\r\n            'hidden_size': model.config.hidden_size,\r\n            'intermediate_size': model.config.intermediate_size,\r\n            'num_hidden_layers': model.config.num_hidden_layers,\r\n            'num_attention_heads': model.config.num_attention_heads,\r\n            'num_key_value_heads': model.config.num_key_value_heads,\r\n            'hidden_act': model.config.activation_function,\r\n            'max_position_embeddings': model.config.max_position_embeddings,\r\n            'initializer_range': model.config.initializer_range,\r\n            'rms_norm_eps': model.config.layer_norm_epsilon,\r\n            'use_cache': model.config.use_cache,\r\n            'pad_token_id': model.config.pad_token_id,\r\n            'bos_token_id': model.config.bos_token_id,\r\n            'eos_token_id': model.config.eos_token_id,\r\n            'tie_word_embeddings': model.config.tie_word_embeddings,\r\n            'rope_theta': model.config.rope_theta,\r\n            'rope_scaling': model.config.rope_scaling,\r\n            'attention_bias': False,\r\n            'attention_dropout': model.config.attention_dropout,\r\n            'mlp_bias': False,\r\n            'head_dim': model.config.head_dim,\r\n            'architectures': ['LlamaForCausalLM'],\r\n            'model_type': 'llama',\r\n            'torch_dtype': model.config.torch_dtype\r\n        }\r\n        new_config = LlamaConfig.from_dict(new_config_args)\r\n\r\n        mapping = {\r\n            re.compile(r\"^transformer\\.wte\\.weight$\"): \"model.embed_tokens.weight\",\r\n            re.compile(r\"^transformer\\.ln_f\\.weight$\"): \"model.norm.weight\",\r\n            re.compile(r\"^lm_head\\.weight$\"): \"lm_head.weight\",\r\n            re.compile(r\"^transformer\\.h\\.(\\d+)\\.ln_1\\.weight$\") : \"model.layers.{}.input_layernorm.weight\",\r\n            re.compile(r\"^transformer\\.h\\.(\\d+)\\.ln_2\\.weight$\") : \"model.layers.{}.post_attention_layernorm.weight\",\r\n            re.compile(r\"^transformer\\.h\\.(\\d+).mlp.c_fc_0.weight$\") : \"model.layers.{}.mlp.gate_proj.weight\",\r\n            re.compile(r\"^transformer\\.h\\.(\\d+).mlp.c_fc_0.weight\\.(absmax|quant_map|nested_absmax|nested_quant_map|quant_state\\.\\w+)$\") : \"model.layers.{}.mlp.gate_proj.weight.{}\",\r\n            re.compile(r\"^transformer\\.h\\.(\\d+).mlp.c_fc_1.weight$\") : \"model.layers.{}.mlp.up_proj.weight\",\r\n            re.compile(r\"^transformer\\.h\\.(\\d+).mlp.c_fc_1.weight\\.(absmax|quant_map|nested_absmax|nested_quant_map|quant_state\\.\\w+)$\") : \"model.layers.{}.mlp.up_proj.weight.{}\",\r\n            re.compile(r\"^transformer\\.h\\.(\\d+).mlp.c_proj.weight$\") : \"model.layers.{}.mlp.down_proj.weight\",\r\n            re.compile(r\"^transformer\\.h\\.(\\d+).mlp.c_proj.weight\\.(absmax|quant_map|nested_absmax|nested_quant_map|quant_state\\.\\w+)$\") : \"model.layers.{}.mlp.down_proj.weight.{}\",\r\n            re.compile(r\"^transformer\\.h\\.(\\d+)\\.attn\\.attention\\.(k_proj|v_proj|q_proj)\\.weight\\.(absmax|quant_map|nested_absmax|nested_quant_map|quant_state\\.\\w+)\") : \"model.layers.{}.self_attn.{}.weight.{}\",\r\n            re.compile(r\"^transformer\\.h\\.(\\d+)\\.attn\\.attention\\.(k_proj|v_proj|q_proj)\\.weight\") : \"model.layers.{}.self_attn.{}.weight\",\r\n            re.compile(r\"^transformer\\.h\\.(\\d+)\\.attn\\.attention\\.out_proj\\.weight\") : \"model.layers.{}.self_attn.o_proj.weight\",\r\n            re.compile(r\"^transformer\\.h\\.(\\d+)\\.attn\\.attention\\.out_proj\\.weight\\.(absmax|quant_map|nested_absmax|nested_quant_map|quant_state\\.\\w+)\") : \"model.layers.{}.self_attn.o_proj.weight.{}\"\r\n        }\r\n\r\n        old_state_dict = model.state_dict()\r\n        new_state_dict = {}\r\n\r\n        for key in old_state_dict:\r\n            for pattern in mapping:\r\n                match = pattern.match(key)\r\n                if match:\r\n                    new_key = mapping[pattern].format(*match.groups())\r\n                    new_state_dict[new_key] = old_state_dict[key]\r\n        assert len(old_state_dict) == len(new_state_dict), RuntimeError(f\"The mapping of {model.__class__} into {new_model.__class__} should have the same length\")\r\n        model = new_model.from_pretrained(None, config=new_config, state_dict=new_state_dict, **model_kwargs)\r\n    return model\r\n\r\n\r\nquantization_config = BitsAndBytesConfig(\r\n  load_in_4bit              = True,\r\n  bnb_4bit_use_double_quant = True,\r\n  bnb_4bit_quant_type       = \"nf4\",\r\n  bnb_4bit_compute_dtype    = torch.float32,\r\n)\r\nmodel_kwargs = {\r\n    \"quantization_config\": quantization_config,\r\n    \"device_map\": \"sequential\",\r\n    \"trust_remote_code\": True,\r\n}\r\nmodel_name = 'LGAI-EXAONE/EXAONE-3.5-2.4B-Instruct'\r\nexaone = AutoModelForCausalLM.from_pretrained(model_name, **model_kwargs)\r\nload_correct_model(exaone, **model_kwargs)\r\n```\r\n\r\nproviding quantization_config raises `ValueError: Blockwise quantization only supports 16/32-bit floats, but got torch.uint8`. And not providing quantization raises `ValueError: Trying to set a tensor of shape torch.Size([170]) in \"weight\" (which has shape torch.Size([640, 2560])), this looks incorrect.`\r\n\r\nI've checked the state_dict and compared it against a llama model, it seems to me to be correct. I think the problem might be with some param that the config is expected to have.\r\n\r\nEdit 1: Changing new_model to LlamaForCausalLM throws the same error\r\nEdit 2: Adding quantization_config to the config seems to solve the problem but there are still issues with inference. \r\n\r\n```python\r\n\r\ntokenizer = AutoTokenizer.from_pretrained(model_name)\r\n\r\n# Choose your prompt\r\nprompt = \"Explain how wonderful you are\"  # English example\r\n\r\nmessages = [\r\n    {\"role\": \"system\", \r\n     \"content\": \"You are EXAONE model from LG AI Research, a helpful assistant.\"},\r\n    {\"role\": \"user\", \"content\": prompt}\r\n]\r\ninput_ids = tokenizer.apply_chat_template(\r\n    messages,\r\n    tokenize=True,\r\n    add_generation_prompt=True,\r\n    return_tensors=\"pt\"\r\n)\r\n\r\noutput = model.generate(\r\n    input_ids.to(\"cuda\"),\r\n    eos_token_id=tokenizer.eos_token_id,\r\n    max_new_tokens=128,\r\n    do_sample=False,\r\n)\r\n```\r\n\r\nI get the following error: `RuntimeError: CUDA error: CUBLAS_STATUS_EXECUTION_FAILED when calling cublasSgemm( handle, opa, opb, m, n, k, &alpha, a, lda, b, ldb, &beta, c, ldc)`\r\n\r\nEdit3: I am so sorry there was a mistake in my regex (didn't have $ at the end for some of the patterns).\r\n\r\nThanks again for your help resolving the original issue.\r\n"
      }
    ],
    "url": "https://github.com/huggingface/transformers/issues/35427"
  },
  {
    "repository": "huggingface/transformers",
    "issue_number": 35424,
    "title": "LLaVa 1.5 and 1.6 not working with text-only inputs",
    "body": "### System Info\n\n- `transformers` version: 4.47.1\r\n- Platform: Linux-5.15.0-113-generic-x86_64-with-glibc2.35\r\n- Python version: 3.12.5\r\n- Huggingface_hub version: 0.26.1\r\n- Safetensors version: 0.4.5\r\n- Accelerate version: 0.34.2\r\n- Accelerate config:    - compute_environment: LOCAL_MACHINE\r\n        - distributed_type: MULTI_GPU\r\n        - mixed_precision: bf16\r\n        - use_cpu: False\r\n        - debug: False\r\n        - num_processes: 4\r\n        - machine_rank: 0\r\n        - num_machines: 1\r\n        - gpu_ids: 0,1,2,3\r\n        - rdzv_backend: static\r\n        - same_network: True\r\n        - main_training_function: main\r\n        - enable_cpu_affinity: False\r\n        - downcast_bf16: no\r\n        - tpu_use_cluster: False\r\n        - tpu_use_sudo: False\r\n        - tpu_env: []\r\n- PyTorch version (GPU?): 2.4.1 (True)\r\n- Tensorflow version (GPU?): not installed (NA)\r\n- Flax version (CPU?/GPU?/TPU?): not installed (NA)\r\n- Jax version: not installed\r\n- JaxLib version: not installed\r\n- Using distributed or parallel set-up in script?: NO\r\n- Using GPU in script?: YES\r\n- GPU type: NVIDIA A40\n\n### Who can help?\n\n@zucchini-nlp, @amyeroberts\n\n### Information\n\n- [ ] The official example scripts\n- [X] My own modified scripts\n\n### Tasks\n\n- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\n- [X] My own task or dataset (give details below)\n\n### Reproduction\n\nWhen using text-only inputs with the LLaVa 1.5 and 1.6 family we get an error. I think the issue has already been brought up here [bug](https://github.com/huggingface/transformers/issues/35421) but the error is different here. Also this is an interesting [discussion](https://huggingface.co/llava-hf/llava-1.5-7b-hf/discussions/38). The simple code to reproduce:\r\n\r\n```\r\nimport torch\r\n\r\nimport requests\r\nfrom PIL import Image\r\n\r\nfrom transformers import (\r\n    AutoModelForVision2Seq,\r\n    AutoProcessor\r\n)\r\n\r\nMODEL_ID = \"llava-hf/llava-v1.6-vicuna-7b-hf\" #\"llava-hf/llava-1.5-7b-hf\"\r\n\r\nmodel = AutoModelForVision2Seq.from_pretrained(MODEL_ID).to(0, torch.bfloat16)\r\nprocessor = AutoProcessor.from_pretrained(MODEL_ID)\r\n\r\nimage_file = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\r\nraw_image = Image.open(requests.get(image_file, stream=True).raw)\r\ninputs = processor(images=[raw_image, None], text=[\"<image> what do you see in the image?\", \"Do you think that 2+2 is equal to 4?\"], padding=True, return_tensors='pt').to(0, torch.bfloat16)\r\n\r\noutput = model.generate(**inputs, max_new_tokens=20, do_sample=False)\r\nprint(processor.decode(output[0][2:], skip_special_tokens=False))\r\n```\r\n\r\nThe error we get:\r\n\r\n```\r\nLoading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:02<00:00,  1.31it/s]\r\nTraceback (most recent call last):\r\n  File \"/mnt/llmdata/home/gbonetta/progetti/kimera/test_kimera_checkpoint.py\", line 25, in <module>\r\n    inputs = processor(images=[raw_image, None], text=[\"<image> what do you see in the image?\", \"Do you think that 2+2 is equal to 4?\"], padding=True, return_tensors='pt').to(0, torch.bfloat16)\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/mnt/llmdata/home/gbonetta/miniconda3/miniconda/envs/llava_env/lib/python3.12/site-packages/transformers/models/llava_next/processing_llava_next.py\", line 133, in __call__\r\n    images, text = _validate_images_text_input_order(images, text)\r\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/mnt/llmdata/home/gbonetta/miniconda3/miniconda/envs/llava_env/lib/python3.12/site-packages/transformers/processing_utils.py\", line 1205, in _validate_images_text_input_order\r\n    raise ValueError(\"Invalid input type. Check that `images` and/or `text` are valid inputs.\")\r\nValueError: Invalid input type. Check that `images` and/or `text` are valid inputs.\r\n```\r\n\n\n### Expected behavior\n\nThe model should run using the image features when provided. ",
    "state": "closed",
    "created_at": "2024-12-26T17:30:56+00:00",
    "closed_at": "2025-01-08T09:35:26+00:00",
    "updated_at": "2025-01-08T09:35:26+00:00",
    "author": "giobin",
    "author_type": "User",
    "comments_count": 1,
    "reactions_count": 0,
    "labels": "bug,VLM",
    "milestone": null,
    "closed_by": "zucchini-nlp",
    "resolution_time_hours": 304.075,
    "first_comments": [
      {
        "author": "zucchini-nlp",
        "created_at": "2025-01-06T08:57:18+00:00",
        "body": "Will be fixed by https://github.com/huggingface/transformers/pull/34502 :)"
      }
    ],
    "url": "https://github.com/huggingface/transformers/issues/35424"
  },
  {
    "repository": "huggingface/transformers",
    "issue_number": 35421,
    "title": "Text Only input using LlaVa Next",
    "body": "### System Info\r\n\r\n- `transformers` version: 4.47.1\r\n- Platform: Linux-5.4.0-113-generic-x86_64-with-glibc2.39\r\n- Python version: 3.11.11\r\n- Huggingface_hub version: 0.27.0\r\n- Safetensors version: 0.4.5\r\n- Accelerate version: 1.2.1\r\n- Accelerate config:    not found\r\n- PyTorch version (GPU?): 2.5.1 (True)\r\n- Tensorflow version (GPU?): not installed (NA)\r\n- Flax version (CPU?/GPU?/TPU?): not installed (NA)\r\n- Jax version: not installed\r\n- JaxLib version: not installed\r\n- Using distributed or parallel set-up in script?: No\r\n- Using GPU in script?: Yes\r\n- GPU type: NVIDIA A100-SXM4-80GB\r\n\r\n### Who can help?\r\n\r\n_No response_\r\n\r\n### Information\r\n\r\n- [X] The official example scripts\r\n- [ ] My own modified scripts\r\n\r\n### Tasks\r\n\r\n- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\r\n- [ ] My own task or dataset (give details below)\r\n\r\n### Reproduction\r\n\r\n1. Code Snippet\r\n```python\r\nfrom transformers import LlavaNextProcessor, LlavaNextForConditionalGeneration\r\nimport torch\r\n\r\nprocessor = LlavaNextProcessor.from_pretrained(\"llava-hf/llava-v1.6-mistral-7b-hf\")\r\n\r\nmodel = LlavaNextForConditionalGeneration.from_pretrained(\"llava-hf/llava-v1.6-mistral-7b-hf\", torch_dtype=torch.float16)\r\nmodel.to(\"cuda:0\")  # Use GPU if available\r\n\r\nconversation = [\r\n    {\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": \"What is the capital of France?\"}]},\r\n    {\"role\": \"assistant\", \"content\": [{\"type\": \"text\", \"text\": \"The capital of France is Paris.\"}]},\r\n    {\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": \"What is the capital of India?\"}]},\r\n]\r\n\r\ntext_prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)\r\n\r\nprompt = processor.apply_chat_template(conversation, add_generation_prompt=True)\r\ninputs = processor(text = prompt, images=None,  return_tensors=\"pt\").to(\"cuda:0\")\r\n\r\noutput = model.generate(**inputs, max_new_tokens=100)\r\n\r\ngenerated_text = processor.decode(output[0], skip_special_tokens=True)\r\nprint(generated_text)\r\n```\r\n\r\n2. Error Message\r\n```\r\nLoading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00,  6.30it/s]\r\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\r\nExpanding inputs for image tokens in LLaVa-NeXT should be done in processing. Please add `patch_size` and `vision_feature_select_strategy` to the model's processing config or set directly with `processor.patch_size = {{patch_size}}` and processor.vision_feature_select_strategy = {{vision_feature_select_strategy}}`. Using processors without these attributes in the config is deprecated and will throw an error in v4.50.\r\nTraceback (most recent call last):\r\n  File \"/home/gpuuser3/reddit_data_annotation/Llava_play.py\", line 20, in <module>\r\n    output = model.generate(**inputs, max_new_tokens=100)\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/gpuuser3/miniconda3/envs/mmsd_annotation/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\r\n    return func(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/gpuuser3/miniconda3/envs/mmsd_annotation/lib/python3.11/site-packages/transformers/generation/utils.py\", line 2252, in generate\r\n    result = self._sample(\r\n             ^^^^^^^^^^^^^\r\n  File \"/home/gpuuser3/miniconda3/envs/mmsd_annotation/lib/python3.11/site-packages/transformers/generation/utils.py\", line 3251, in _sample\r\n    outputs = self(**model_inputs, return_dict=True)\r\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/gpuuser3/miniconda3/envs/mmsd_annotation/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/gpuuser3/miniconda3/envs/mmsd_annotation/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/gpuuser3/miniconda3/envs/mmsd_annotation/lib/python3.11/site-packages/transformers/models/llava_next/modeling_llava_next.py\", line 874, in forward\r\n    inputs_embeds = inputs_embeds.to(image_features.dtype)\r\n                                     ^^^^^^^^^^^^^^^^^^^^\r\nAttributeError: 'NoneType' object has no attribute 'dtype'\r\n```\r\n\r\n### Expected behavior\r\n\r\nI was trying to generate a response using pure-text input. I was assuming that simply setting images=None in  processor() function call would achieve this. But it gives the mentioned error. Is there something I'm missing here?",
    "state": "closed",
    "created_at": "2024-12-26T14:57:33+00:00",
    "closed_at": "2025-01-20T08:23:55+00:00",
    "updated_at": "2025-01-20T08:23:56+00:00",
    "author": "sinngam-khaidem",
    "author_type": "User",
    "comments_count": 1,
    "reactions_count": 0,
    "labels": "bug,VLM",
    "milestone": null,
    "closed_by": "zucchini-nlp",
    "resolution_time_hours": 593.4394444444445,
    "first_comments": [
      {
        "author": "zucchini-nlp",
        "created_at": "2025-01-20T08:23:55+00:00",
        "body": "Will close as it was resolved in v4.48 by https://github.com/huggingface/transformers/pull/34502 ðŸ¤— "
      }
    ],
    "url": "https://github.com/huggingface/transformers/issues/35421"
  },
  {
    "repository": "huggingface/transformers",
    "issue_number": 35412,
    "title": "Qwen2VLProcessor cannot handle odd number of video frames",
    "body": "### System Info\r\n\r\n```\r\n- `transformers` version: 4.47.1\r\n- Platform: Linux-5.4.0-174-generic-x86_64-with-glibc2.31\r\n- Python version: 3.9.20\r\n- Huggingface_hub version: 0.26.2\r\n- Safetensors version: 0.4.5\r\n- Accelerate version: 1.0.1\r\n- Accelerate config:    not found\r\n- PyTorch version (GPU?): 2.5.1+cu124 (True)\r\n- Tensorflow version (GPU?): not installed (NA)\r\n- Flax version (CPU?/GPU?/TPU?): not installed (NA)\r\n- Jax version: not installed\r\n- JaxLib version: not installed\r\n- Using distributed or parallel set-up in script?: No\r\n- Using GPU in script?: Yes\r\n- GPU type: NVIDIA A10\r\n```\r\n\r\n### Who can help?\r\n\r\n@ArthurZucker @zucchini-nlp \r\n\r\n### Information\r\n\r\n- [ ] The official example scripts\r\n- [X] My own modified scripts\r\n\r\n### Tasks\r\n\r\n- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\r\n- [x] My own task or dataset (give details below)\r\n\r\n### Reproduction\r\n\r\nI found that the processor for Qwen2-VL cannot handle input videos with an odd number of frames (except for videos with a single frame). This occurs regardless of the channel format and image dimensions of each frame.\r\n```\r\nimport numpy as np\r\nfrom transformers import AutoProcessor\r\n\r\n# The processor fails when num_frames = 3, 5, 7, ...\r\nnum_frames = 3\r\nvideo = np.random.randint(0, 255, size=(num_frames, 256, 256, 3), dtype=np.uint8)\r\n\r\nprocessor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-2B-Instruct\")\r\nprocessor(text=\"<|vision_start|><|video_pad|><|vision_end|>\", videos=[video])\r\n```\r\n\r\nError when `num_frames = 3`\r\n```\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/cyrus/miniconda3/envs/vllm/lib/python3.9/site-packages/transformers/models/qwen2_vl/processing_qwen2_vl.py\", line 124, in __call__\r\n    videos_inputs = self.image_processor(images=None, videos=videos, **output_kwargs[\"videos_kwargs\"])\r\n  File \"/home/cyrus/miniconda3/envs/vllm/lib/python3.9/site-packages/transformers/image_processing_utils.py\", line 41, in __call__\r\n    return self.preprocess(images, **kwargs)\r\n  File \"/home/cyrus/miniconda3/envs/vllm/lib/python3.9/site-packages/transformers/models/qwen2_vl/image_processing_qwen2_vl.py\", line 439, in preprocess\r\n    patches, video_grid_thw = self._preprocess(\r\n  File \"/home/cyrus/miniconda3/envs/vllm/lib/python3.9/site-packages/transformers/models/qwen2_vl/image_processing_qwen2_vl.py\", line 299, in _preprocess\r\n    patches = patches.reshape(\r\nValueError: cannot reshape array of size 571536 into shape (1,2,3,9,2,14,9,2,14)\r\n```\r\n\r\n### Expected behavior\r\n\r\nThe processor should be able to handle videos with an odd number of frames.",
    "state": "closed",
    "created_at": "2024-12-25T06:58:11+00:00",
    "closed_at": "2025-01-08T12:49:01+00:00",
    "updated_at": "2025-01-08T12:49:01+00:00",
    "author": "DarkLight1337",
    "author_type": "User",
    "comments_count": 3,
    "reactions_count": 0,
    "labels": "bug,VLM",
    "milestone": null,
    "closed_by": "ArthurZucker",
    "resolution_time_hours": 341.84722222222223,
    "first_comments": [
      {
        "author": "zucchini-nlp",
        "created_at": "2025-01-06T09:15:56+00:00",
        "body": "Nice catch, haven't noticed this before. I think we either can repeat the last frame one more time until `temporal_patch_dim` is reached similar way to how images are repeated along time dimension, or raise a `ValueError` if the number of frames is not divisible\r\n\r\nLet me ask the authors to see if there are any possible issues with replicating the last frame, and I will submit a PR"
      },
      {
        "author": "DarkLight1337",
        "created_at": "2025-01-06T09:20:00+00:00",
        "body": "Thanks for looking into this! It looks like @jla524 has already opened a PR."
      },
      {
        "author": "zucchini-nlp",
        "created_at": "2025-01-06T09:51:34+00:00",
        "body": "Oh cool, will review that one then, thanks!"
      }
    ],
    "url": "https://github.com/huggingface/transformers/issues/35412"
  },
  {
    "repository": "huggingface/transformers",
    "issue_number": 35391,
    "title": "IdeficsImageProcessor raises unexpected ValueError",
    "body": "### System Info\r\n\r\n- `transformers` version: 4.47.1\r\n- Platform: Linux-5.15.0-126-generic-x86_64-with-glibc2.35\r\n- Python version: 3.13.1\r\n- Huggingface_hub version: 0.27.0\r\n- Safetensors version: 0.4.5\r\n- Accelerate version: not installed\r\n- Accelerate config: not found\r\n- PyTorch version (GPU?): 2.5.1+cu124 (True)\r\n- Tensorflow version (GPU?): not installed (NA)\r\n- Flax version (CPU?/GPU?/TPU?): not installed (NA)\r\n- Jax version: not installed\r\n- JaxLib version: not installed\r\n\r\n### Who can help?\r\n\r\n@amyeroberts @qubvel @zucchini-nlp \r\n\r\n### Information\r\n\r\n- [ ] The official example scripts\r\n- [X] My own modified scripts\r\n\r\n### Tasks\r\n\r\n- [X] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\r\n- [ ] My own task or dataset (give details below)\r\n\r\n### Reproduction\r\n\r\nWhen I was running the following code:\r\n```python\r\n# ignore imports...\r\n\r\ncheckpoint = \"HuggingFaceM4/idefics-9b-instruct\"\r\nmodel = IdeficsForVisionText2Text.from_pretrained(checkpoint, torch_dtype=torch.bfloat16).to(device)\r\nprocessor = AutoProcessor.from_pretrained(checkpoint)\r\nimages = [\r\n    PIL.Image.open(...),\r\n    PIL.Image.open(...),\r\n]\r\nbatched_text = [\"<image><image>\", \"<image><image>\"] \r\nbatched_image = [images, images]\r\nprocessor(batched_image, batched_text)\r\n```\r\nI got the following exception:\r\n```\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\nCell In[4], line 5\r\n      3 batched_text =  [\"<image><image>\", \"<image><image>\"] \r\n      4 batched_images = [images, images]\r\n----> 5 processor(batched_images, batched_text)\r\n\r\nFile ~/miniconda3/envs/icl/lib/python3.13/site-packages/transformers/utils/deprecation.py:165, in deprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func(*args, **kwargs)\r\n    161 elif minimum_action in (Action.NOTIFY, Action.NOTIFY_ALWAYS):\r\n    162     # DeprecationWarning is ignored by default, so we use FutureWarning instead\r\n    163     warnings.warn(message, FutureWarning, stacklevel=2)\r\n--> 165 return func(*args, **kwargs)\r\n\r\nFile ~/miniconda3/envs/icl/lib/python3.13/site-packages/transformers/models/idefics/processing_idefics.py:430, in IdeficsProcessor.__call__(self, images, text, audio, videos, **kwargs)\r\n    427 if add_eos_token:\r\n    428     full_text += self.tokenizer.eos_token\r\n--> 430 image_objects = self.image_processor(image_objects, **output_kwargs[\"images_kwargs\"])\r\n    432 all_prompts.append(full_text)\r\n    433 all_images.append(image_objects)\r\n\r\nFile ~/miniconda3/envs/icl/lib/python3.13/site-packages/transformers/image_processing_utils.py:41, in BaseImageProcessor.__call__(self, images, **kwargs)\r\n     39 def __call__(self, images, **kwargs) -> BatchFeature:\r\n     40     \"\"\"Preprocess an image or a batch of images.\"\"\"\r\n---> 41     return self.preprocess(images, **kwargs)\r\n\r\nFile ~/miniconda3/envs/icl/lib/python3.13/site-packages/transformers/models/idefics/image_processing_idefics.py:134, in IdeficsImageProcessor.preprocess(self, images, image_num_channels, image_size, image_mean, image_std, transform, return_tensors, **kwargs)\r\n    131 if isinstance(images, list) and len(images) == 0:\r\n    132     return []\r\n--> 134 images = make_list_of_images(images)\r\n    136 if not valid_images(images):\r\n    137     raise ValueError(\r\n    138         \"Invalid image type. Must be of type PIL.Image.Image, numpy.ndarray, \"\r\n    139         \"torch.Tensor, tf.Tensor or jax.ndarray.\"\r\n    140     )\r\n\r\nFile ~/miniconda3/envs/icl/lib/python3.13/site-packages/transformers/image_utils.py:206, in make_list_of_images(images, expected_ndims)\r\n    201         raise ValueError(\r\n    202             f\"Invalid image shape. Expected either {expected_ndims + 1} or {expected_ndims} dimensions, but got\"\r\n    203             f\" {images.ndim} dimensions.\"\r\n    204         )\r\n    205     return images\r\n--> 206 raise ValueError(\r\n    207     \"Invalid image type. Expected either PIL.Image.Image, numpy.ndarray, torch.Tensor, tf.Tensor or \"\r\n    208     f\"jax.ndarray, but got {type(images)}.\"\r\n    209 )\r\n\r\nValueError: Invalid image type. Expected either PIL.Image.Image, numpy.ndarray, torch.Tensor, tf.Tensor or jax.ndarray, but got <class 'list'>.\r\n```\r\nNote that this bug is not exist in v4.45.2.\r\n\r\n### Expected behavior\r\n\r\nEven I have followed instruction provided [here](https://github.com/huggingface/transformers/blob/8f38f58f3de5a35f9b8505e9b48985dce5470985/src/transformers/models/idefics/processing_idefics.py#L354-L358), I still got such a bug, why?",
    "state": "closed",
    "created_at": "2024-12-22T15:57:42+00:00",
    "closed_at": "2025-01-06T14:03:48+00:00",
    "updated_at": "2025-01-06T14:03:48+00:00",
    "author": "Kamichanw",
    "author_type": "User",
    "comments_count": 1,
    "reactions_count": 0,
    "labels": "bug,Vision",
    "milestone": null,
    "closed_by": "Kamichanw",
    "resolution_time_hours": 358.1016666666667,
    "first_comments": [
      {
        "author": "zucchini-nlp",
        "created_at": "2025-01-06T08:43:00+00:00",
        "body": "@Kamichanw Before v4.45.2 Idefics did not accept images/text as two separate kwargs and thus was working only with inputs formatted as chat by interleaving text and image within one list. See below as an example. We've added an option to pass images and text separately, but the nested input is not supported yet indeed\r\n\r\nhttps://github.com/huggingface/transformers/blob/8f38f58f3de5a35f9b8505e9b48985dce5470985/src/transformers/models/idefics/processing_idefics.py#L291-L298\r\n\r\nThere is a tracker PR here (https://github.com/huggingface/transformers/issues/34545) to track progress on making all VLM processors standard and thus accept different input types. @yonigozlan will be working on that and in the meanwhile you can still pass your inputs as interleaved prompts, same way you were doing before v4.45. Backward compatibility should have been preserved \r\n\r\n\r\n"
      }
    ],
    "url": "https://github.com/huggingface/transformers/issues/35391"
  },
  {
    "repository": "huggingface/transformers",
    "issue_number": 35387,
    "title": "Potentially incorrect calculation of `total_updates` on >=4.46.0 since #34198 affecting multi gpu training",
    "body": "### System Info\r\n\r\nOkay I have been pulling my hair out for a few hours and turns out this bug only happens when `average_tokens_across_devices` is True and epochs > 1\r\n\r\nSimplest case to reproduce\r\n\r\nDDP\r\n**world size 2**\r\ndataset length = 4\r\n**epochs = 2**\r\nmicro batch size  = 1 (aka per gpu batch size)\r\ngradient accumulation = 1\r\n**average_tokens_across_devices = True**\r\n\r\nSo every epoch, total 2 steps on both devices\r\n\r\nbut as first epoch finishes, we get\r\n\r\n```\r\n[rank1]:   File \"/home/jovyan/axolotl/src/axolotl/train.py\", line 191, in train\r\n[rank1]:     trainer.train(resume_from_checkpoint=resume_from_checkpoint)\r\n[rank1]:   File \"/home/jovyan/.conda/envs/jupyter-base/lib/python3.11/site-packages/transformers/trainer.py\", line 2164, in train\r\n[rank1]:     return inner_training_loop(\r\n[rank1]:            ^^^^^^^^^^^^^^^^^^^^\r\n[rank1]:   File \"/home/jovyan/.conda/envs/jupyter-base/lib/python3.11/site-packages/transformers/trainer.py\", line 2473, in _inner_training_loop\r\n[rank1]:     batch_samples, num_items_in_batch = self.get_batch_samples(epoch_iterator, num_batches)\r\n[rank1]:                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[rank1]:   File \"/home/jovyan/.conda/envs/jupyter-base/lib/python3.11/site-packages/transformers/trainer.py\", line 5142, in get_batch_samples\r\n[rank1]:     num_items_in_batch = self.accelerator.gather(num_items_in_batch).sum().item()\r\n[rank1]:                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[rank1]:   File \"/home/jovyan/.conda/envs/jupyter-base/lib/python3.11/site-packages/accelerate/accelerator.py\", line 2459, in gather\r\n[rank1]:     return gather(tensor)\r\n[rank1]:            ^^^^^^^^^^^^^^\r\n[rank1]:   File \"/home/jovyan/.conda/envs/jupyter-base/lib/python3.11/site-packages/accelerate/utils/operations.py\", line 376, in wrapper\r\n[rank1]:     return function(*args, **kwargs)\r\n[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[rank1]:   File \"/home/jovyan/.conda/envs/jupyter-base/lib/python3.11/site-packages/accelerate/utils/operations.py\", line 437, in gather\r\n[rank1]:     return _gpu_gather(tensor)\r\n[rank1]:            ^^^^^^^^^^^^^^^^^^^\r\n[rank1]:   File \"/home/jovyan/.conda/envs/jupyter-base/lib/python3.11/site-packages/accelerate/utils/operations.py\", line 356, in _gpu_gather\r\n[rank1]:     return recursively_apply(_gpu_gather_one, tensor, error_on_other_type=True)\r\n[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[rank1]:   File \"/home/jovyan/.conda/envs/jupyter-base/lib/python3.11/site-packages/accelerate/utils/operations.py\", line 129, in recursively_apply\r\n[rank1]:     raise TypeError(\r\n[rank1]: TypeError: Unsupported types (<class 'NoneType'>) passed to `_gpu_gather_one`. Only nested list/tuple/dicts of objects that are valid for `is_torch_tensor` should be passed.\r\n```\r\n\r\nThe main culprit here is \r\nhttps://github.com/huggingface/transformers/blob/8f38f58f3de5a35f9b8505e9b48985dce5470985/src/transformers/trainer.py#L2468-L2473\r\n\r\n`steps_in_epoch` per rank is correctly calculated as 2 but total updates is 3\r\nNormally that is harmless because dataloader would be exhausted and would result in empty batch and it won't enter the loop on 2473.\r\nHowever, when using the recently added option `average_tokens_across_devices`, it will try to gather number of total items in batches across all ranks and gather doesn't like broadcasting `None`\r\n\r\nhttps://github.com/huggingface/transformers/blob/8f38f58f3de5a35f9b8505e9b48985dce5470985/src/transformers/trainer.py#L5139-L5156\r\n\r\nThis problem does not surface with 1 gpu because `average_tokens_across_devices` is auto set to `False` and neither under epoch = 1 because `DefaultFlowCallback` stops the training process considering global step and expected max steps\r\n\r\n### Who can help?\r\n\r\n@muellerzr\r\n\r\n### Information\r\n\r\n- [ ] The official example scripts\r\n- [X] My own modified scripts\r\n\r\n### Tasks\r\n\r\n- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\r\n- [X] My own task or dataset (give details below)\r\n\r\n### Reproduction\r\n\r\nTrain any LM on more than one gpu, set at least average_tokens_across_devices = True and epochs  > 1\r\n\r\n### Expected behavior\r\n\r\nEither we fix `total_updates` count or we handle `None` for gather",
    "state": "closed",
    "created_at": "2024-12-21T20:36:54+00:00",
    "closed_at": "2025-01-07T13:23:47+00:00",
    "updated_at": "2025-01-07T13:23:47+00:00",
    "author": "chiragjn",
    "author_type": "User",
    "comments_count": 2,
    "reactions_count": 0,
    "labels": "trainer,bug",
    "milestone": null,
    "closed_by": "chiragjn",
    "resolution_time_hours": 400.7813888888889,
    "first_comments": [
      {
        "author": "SunMarc",
        "created_at": "2024-12-23T15:52:54+00:00",
        "body": "Thanks for the detailed report @chiragjn ! I think the simplest solution would be to set num_items_in_batch as a tensor of 0 when the value is None since it means that `batch_samples` doesn't have any element. Can you check if this solves the issue ? \r\n```python\r\nif self.args.average_tokens_across_devices:\r\n    if num_items_in_batch is None:\r\n        num_items_in_batch = torch.tensor(0)\r\n```"
      },
      {
        "author": "chiragjn",
        "created_at": "2025-01-07T13:23:47+00:00",
        "body": "Fixed by #35102"
      }
    ],
    "url": "https://github.com/huggingface/transformers/issues/35387"
  },
  {
    "repository": "huggingface/transformers",
    "issue_number": 35383,
    "title": "MultiModalityCausalLM does not support Flash Attention 2.0 yet",
    "body": "### System Info\r\n\r\ntransformers version 4.47.1\r\nGoogle colab\r\nPython 3.10.12\r\n\r\n-------------------\r\n\r\nI attempted to use Flash Attention with the [Janus-1.3B](https://github.com/deepseek-ai/Janus) model, but encountered the following error:\r\n\r\n`ValueError: MultiModalityCausalLM does not support Flash Attention 2.0 yet.`\r\n\r\nThis error was raised by the transformers/modeling_utils.py file:\r\n```\r\nif not cls._supports_flash_attn_2:\r\n    raise ValueError(\r\n        f\"{cls.__name__} does not support Flash Attention 2.0 yet. Please request to add support where\"\r\n        f\" the model is hosted, on its model hub page: https://huggingface.co/{config._name_or_path}/discussions/new\"\r\n        \" or in the Transformers GitHub repo: https://github.com/huggingface/transformers/issues/new\"\r\n    )\r\n```\r\n\r\nInstalled FlashAttention-2 using the command:\r\n`pip install flash-attn --no-build-isolation`\r\n\r\n Here is the code I used:\r\n```\r\nimport torch\r\nfrom transformers import AutoModelForCausalLM\r\nfrom janus.models import MultiModalityCausalLM, VLChatProcessor\r\nfrom janus.utils.io import load_pil_images\r\n\r\n# specify the path to the model\r\nmodel_path = \"deepseek-ai/Janus-1.3B\"\r\nvl_chat_processor: VLChatProcessor = VLChatProcessor.from_pretrained(model_path)\r\ntokenizer = vl_chat_processor.tokenizer\r\n\r\nvl_gpt: MultiModalityCausalLM = AutoModelForCausalLM.from_pretrained(\r\n    model_path, trust_remote_code=True,  attn_implementation=\"flash_attention_2\"\r\n)\r\nvl_gpt = vl_gpt.to(torch.bfloat16).cuda().eval()\r\n\r\nconversation = [\r\n    {\r\n        \"role\": \"User\",\r\n        \"content\": \"<image_placeholder>\\nConvert the formula into latex code.\",\r\n        \"images\": [\"images/equation.png\"],\r\n    },\r\n    {\"role\": \"Assistant\", \"content\": \"\"},\r\n]\r\n\r\n# load images and prepare for inputs\r\npil_images = load_pil_images(conversation)\r\nprepare_inputs = vl_chat_processor(\r\n    conversations=conversation, images=pil_images, force_batchify=True\r\n).to(vl_gpt.device)\r\n\r\n# # run image encoder to get the image embeddings\r\ninputs_embeds = vl_gpt.prepare_inputs_embeds(**prepare_inputs)\r\n\r\n# # run the model to get the response\r\noutputs = vl_gpt.language_model.generate(\r\n    inputs_embeds=inputs_embeds,\r\n    attention_mask=prepare_inputs.attention_mask,\r\n    pad_token_id=tokenizer.eos_token_id,\r\n    bos_token_id=tokenizer.bos_token_id,\r\n    eos_token_id=tokenizer.eos_token_id,\r\n    max_new_tokens=512,\r\n    do_sample=False,\r\n    use_cache=True,\r\n)\r\n\r\nanswer = tokenizer.decode(outputs[0].cpu().tolist(), skip_special_tokens=True)\r\nprint(f\"{prepare_inputs['sft_format'][0]}\", answer)\r\n```\r\n\r\n\r\n\r\n",
    "state": "closed",
    "created_at": "2024-12-21T04:52:01+00:00",
    "closed_at": "2025-01-28T08:03:42+00:00",
    "updated_at": "2025-01-28T08:03:42+00:00",
    "author": "AlanPonnachan",
    "author_type": "User",
    "comments_count": 1,
    "reactions_count": 0,
    "labels": "",
    "milestone": null,
    "closed_by": "github-actions[bot]",
    "resolution_time_hours": 915.1947222222223,
    "first_comments": [
      {
        "author": "github-actions[bot]",
        "created_at": "2025-01-20T08:03:02+00:00",
        "body": "This issue has been automatically marked as stale because it has not had recent activity. If you think this still needs to be addressed please comment on this thread.\n\nPlease note that issues that do not follow the [contributing guidelines](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md) are likely to be ignored."
      }
    ],
    "url": "https://github.com/huggingface/transformers/issues/35383"
  },
  {
    "repository": "huggingface/transformers",
    "issue_number": 35380,
    "title": "is_causal arg appears twice in FAttention call from GPT2Attention.forward()",
    "body": "### System Info\r\n\r\n4.48.dev ubuntu18, py3.11\r\n\r\n### Who can help?\r\n\r\n@ArthurZucker based on #35235\r\n@Cyrilvallez  based on  #35342\r\n\r\n### Information\r\n\r\n- [ ] The official example scripts\r\n- [X] My own modified scripts\r\n\r\n### Tasks\r\n\r\n- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\r\n- [X] My own task or dataset (give details below)\r\n\r\n### Reproduction\r\n\r\nuse GPT2 with flash attn.\r\n\r\nin https://github.com/huggingface/transformers/blob/608e163b527eaee41e650ffb9eb4c422d2679902/src/transformers/integrations/flash_attention.py#L47\r\n\r\nit inserts `is_causal` argument twice, from kwargs and explicitely. causes `TypeError: transformers.modeling_flash_attention_utils._flash_attention_forward() got multiple values for keyword argument 'is_causal'`\r\n\r\nthe kwargs get `is_causal` from GPT2Attention.forward()\r\n\r\n\r\n### Expected behavior\r\n\r\nuse `is_causal` just once\r\n\r\nplease add GPT2 to your release tests suite",
    "state": "closed",
    "created_at": "2024-12-21T03:08:05+00:00",
    "closed_at": "2025-01-09T10:56:27+00:00",
    "updated_at": "2025-01-09T10:56:27+00:00",
    "author": "poedator",
    "author_type": "User",
    "comments_count": 1,
    "reactions_count": 0,
    "labels": "bug",
    "milestone": null,
    "closed_by": "Cyrilvallez",
    "resolution_time_hours": 463.80611111111114,
    "first_comments": [
      {
        "author": "Cyrilvallez",
        "created_at": "2024-12-22T13:27:50+00:00",
        "body": "Hey @poedator, indeed, thanks for reporting this issue! I opened a PR [here](https://github.com/huggingface/transformers/pull/35390) to fix it!"
      }
    ],
    "url": "https://github.com/huggingface/transformers/issues/35380"
  },
  {
    "repository": "huggingface/transformers",
    "issue_number": 35371,
    "title": "Model loaded with `PretrainedModel.from_pretrained` and `with torch.device(\"cuda\"):` decorator leads to unexpected errors compared to `.to(\"cuda\")`",
    "body": "### System Info\r\n\r\n```\r\n- `transformers` version: 4.48.0.dev0\r\n- Platform: Linux-6.8.0-49-generic-x86_64-with-glibc2.39\r\n- Python version: 3.10.14\r\n- Huggingface_hub version: 0.26.3\r\n- Safetensors version: 0.4.5\r\n- Accelerate version: 0.34.2\r\n- Accelerate config:    not found\r\n- PyTorch version (GPU?): 2.5.1+rocm6.2 (True)\r\n- Tensorflow version (GPU?): not installed (NA)\r\n- Flax version (CPU?/GPU?/TPU?): not installed (NA)\r\n- Jax version: not installed\r\n- JaxLib version: not installed\r\n- Using distributed or parallel set-up in script?: <fill in>\r\n- Using GPU in script?: <fill in>\r\n- GPU type: AMD Instinct MI250X/MI250\r\n```\r\ntransformers commit 4567ee80572f51859f1454db687cacdf2ec12b13\r\n\r\n### Who can help?\r\n\r\n@mht-sharma maybe you know\r\n\r\n### Information\r\n\r\n- [X] The official example scripts\r\n- [ ] My own modified scripts\r\n\r\n### Tasks\r\n\r\n- [X] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\r\n- [ ] My own task or dataset (give details below)\r\n\r\n### Reproduction\r\n\r\n```python\r\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig, AutoConfig\r\nimport torch\r\n\r\nmodel_id = \"Qwen/Qwen1.5-MoE-A2.7B-Chat\"\r\n\r\ncfg = AutoConfig.from_pretrained(model_id)\r\n# cfg.num_hidden_layers = 4\r\n\r\nwith torch.device(\"cuda\"):\r\n   model = AutoModelForCausalLM.from_config(cfg, torch_dtype=torch.bfloat16)\r\n\r\n# with torch.device(\"cuda\"):\r\n#    model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16)\r\n\r\nparam_size = 0\r\nfor name, param in model.named_parameters():\r\n    param_size += param.nelement() * param.element_size()\r\n    print(name, param.dtype)\r\nbuffer_size = 0\r\nfor name, buffer in model.named_buffers():\r\n    buffer_size += buffer.nelement() * buffer.element_size()\r\n    print(name, buffer.dtype)\r\n\r\nsize_all_gb = (param_size + buffer_size) * 1e-9\r\nprint('model size: {:.3f} GB'.format(size_all_gb))\r\n\r\ntokenizer = AutoTokenizer.from_pretrained(model_id)\r\n\r\ninp = tokenizer(\"Hello my friends, how are you?\", return_tensors=\"pt\").to(\"cuda\")\r\n\r\ngen_config = GenerationConfig(\r\n    max_new_tokens=100,\r\n    min_new_tokens=100,\r\n    use_cache=True,\r\n    num_beams=1,\r\n    do_sample=False,\r\n)\r\n\r\nprint(\"generating\")\r\nres = model.generate(**inp, generation_config=gen_config)\r\n\r\nprint(tokenizer.batch_decode(res))\r\n```\r\n\r\nWhen using `with torch.device(\"cuda\")`, to load a model on device, I am getting various unexpected errors as `HIPBLAS_STATUS_INTERNAL_ERROR when calling hipblasLtMatmul` or `RuntimeError: HIP error: no kernel image is available for execution on the device`.\r\n\r\nHowever, when loading a (dummy) model with\r\n```\r\nwith torch.device(\"cuda\"):\r\n   model = AutoModelForCausalLM.from_config(cfg, torch_dtype=torch.bfloat16)\r\n```\r\neverything is fine at runtime, no error.\r\n\r\nSimilarly, when loading with\r\n```\r\nmodel = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16).to(\"cuda\")\r\n``` \r\nthere is no error at runtime.\r\n\r\nI do not have access to an Nvidia GPU at the time so could not reproduce there to see if this issue exists also on Nvidia distribution pytorch. So I am not sure if this is a ROCm, PyTorch or Transformers bug, this might need some investigations.\r\n\r\nI could reproduce the issue on a few previous Transformers versions (4.45, 4.46, 4.47).\r\n\r\nFilling for awareness, this might need some more investigation and/or extended testing in Transformers CI.\r\n\r\nInterestingly I could not reproduce this issue with `peft-internal-testing/tiny-random-qwen-1.5-MoE`, but only with `Qwen/Qwen1.5-MoE-A2.7B-Chat`.\r\n\r\n### Expected behavior\r\n\r\nNo error",
    "state": "closed",
    "created_at": "2024-12-20T16:00:15+00:00",
    "closed_at": "2025-01-28T08:03:47+00:00",
    "updated_at": "2025-01-28T08:03:47+00:00",
    "author": "fxmarty-amd",
    "author_type": "User",
    "comments_count": 3,
    "reactions_count": 0,
    "labels": "bug",
    "milestone": null,
    "closed_by": "github-actions[bot]",
    "resolution_time_hours": 928.0588888888889,
    "first_comments": [
      {
        "author": "fxmarty-amd",
        "created_at": "2024-12-20T16:23:08+00:00",
        "body": "The issue can not be reproduced with smaller models (e.g. this qwen moe with only 20 layers instead of 24).\r\n\r\nUsing the original failing `Qwen/Qwen1.5-MoE-A2.7B-Chat` with 24 layers and inspecting `torch.cuda.memory_reserved`/`torch.cuda.memory_allocated`, it appears that when using the decorator `with torch.device(\"cuda\"):`, PyTorch/Transformers reserves much more memory than the model size (number after loading):\r\n```\r\nmodel size: 28.632 GB\r\nmemory reserved GB: 68.176314368\r\nmemory allocated GB: 28.651504128000003\r\n```\r\nThe reserved memory is very close to the 64 GB of one MI250 GCD memory (~64 GiB), and hence some AMD libs/pytorch implem for rocm might act fuzzily, although I would rather expect the classic `HIP out of memory. Tried to allocate xxxx etc` message.\r\n\r\nCompare to using `model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16).to(\"cuda\")`:\r\n```\r\nmodel size: 28.632 GB\r\nmemory reserved GB: 34.120663040000004\r\nmemory allocated GB: 28.63262976\r\n```\r\n\r\nAllocated memory is similar, though."
      },
      {
        "author": "Rocketknight1",
        "created_at": "2024-12-20T18:36:56+00:00",
        "body": "Hmmn - there is no direct memory allocation in `transformers` that I'm aware of that doesn't go through Torch. Is the reserved memory because of a spike in usage during loading? If so, we might be able to mitigate it, but if it's just Torch reserving/fragmenting memory I'm not sure what we can do!"
      },
      {
        "author": "github-actions[bot]",
        "created_at": "2025-01-20T08:03:07+00:00",
        "body": "This issue has been automatically marked as stale because it has not had recent activity. If you think this still needs to be addressed please comment on this thread.\n\nPlease note that issues that do not follow the [contributing guidelines](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md) are likely to be ignored."
      }
    ],
    "url": "https://github.com/huggingface/transformers/issues/35371"
  },
  {
    "repository": "huggingface/transformers",
    "issue_number": 35369,
    "title": "Issue with Idefics3 sample code ",
    "body": "This sample code is given in  huggingface idefics3 documentation: [here](https://huggingface.co/docs/transformers/v4.47.1/en/model_doc/idefics3#transformers.Idefics3ForConditionalGeneration.forward.example)\r\n```\r\n\r\n`import requests\r\nimport torch\r\nfrom PIL import Image\r\nfrom io import BytesIO\r\n\r\nfrom transformers import AutoProcessor, AutoModelForVision2Seq\r\nfrom transformers.image_utils import load_image\r\n\r\n## Note that passing the image urls (instead of the actual pil images) to the processor is also possible\r\nimage1 = load_image(\"https://cdn.britannica.com/61/93061-050-99147DCE/Statue-of-Liberty-Island-New-York-Bay.jpg\")\r\nimage2 = load_image(\"https://cdn.britannica.com/59/94459-050-DBA42467/Skyline-Chicago.jpg\")\r\nimage3 = load_image(\"https://cdn.britannica.com/68/170868-050-8DDE8263/Golden-Gate-Bridge-San-Francisco.jpg\")\r\n\r\nprocessor = AutoProcessor.from_pretrained(\"HuggingFaceM4/Idefics3-8B-Llama3\")\r\nmodel = AutoModelForVision2Seq.from_pretrained(\"HuggingFaceM4/Idefics3-8B-Llama3\", torch_dtype=torch.bfloat16, device_map=\"auto\")\r\n\r\n## Create inputs\r\nmessages = [\r\n    {\r\n        \"role\": \"user\",\r\n        \"content\": [\r\n            {\"type\": \"image\"},\r\n            {\"type\": \"text\", \"text\": \"In this image, we can see the city of New York, and more specifically the Statue of Liberty.\"},\r\n            {\"type\": \"image\"},\r\n            {\"type\": \"text\", \"text\": \"What can we see in this image?\"},\r\n        ]\r\n    },\r\n    {\r\n        \"role\": \"user\",\r\n        \"content\": [\r\n            {\"type\": \"image\"},\r\n            {\"type\": \"text\", \"text\": \"In which city is that bridge located?\"},\r\n        ]\r\n    }\r\n]\r\n\r\nprompts = [processor.apply_chat_template([message], add_generation_prompt=True) for message in messages]\r\nimages = [[image1, image2], [image3]]\r\ninputs = processor(text=prompts, images=images, padding=True, return_tensors=\"pt\").to(model.device)\r\n\r\n## Generate\r\ngenerated_ids = model.generate(**inputs, max_new_tokens=256)\r\ngenerated_texts = processor.batch_decode(generated_ids, skip_special_tokens=True)\r\n\r\nprint(generated_texts[0])\r\n\r\nprint(generated_texts[1])`\r\n```\r\n \r\nWhen i use the above code, i get an output like this: \r\nUser:<image>In this image, we can see the city of New York, and more specifically the Statue of Liberty.<image>What can we see in this image?\r\nAssistant:orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_orte_\r\nUser:<image>In which city is that bridge located?\r\nAssistant:                                                                                                                                                                                                   ?       ?     ?   ?   ?   ? ?   ? ? ? ? ? ?  ?  ?  ?  ?  ?  ?  ?  ?  ?  ?  ?  ?  ?  ?  ?  ?  ?  ?  ?  ?  ?  ?  ?  ?  ?  ?  ?  ?  ?   \r\n\r\n\r\nCan anybody please tell me what the issue is? also, for this single inference, it takes several minutes to generate the output.",
    "state": "closed",
    "created_at": "2024-12-20T14:58:12+00:00",
    "closed_at": "2025-01-21T08:15:21+00:00",
    "updated_at": "2025-01-21T08:15:21+00:00",
    "author": "Debolena7",
    "author_type": "User",
    "comments_count": 5,
    "reactions_count": 0,
    "labels": "",
    "milestone": null,
    "closed_by": "zucchini-nlp",
    "resolution_time_hours": 761.2858333333334,
    "first_comments": [
      {
        "author": "github-actions[bot]",
        "created_at": "2025-01-20T08:03:08+00:00",
        "body": "This issue has been automatically marked as stale because it has not had recent activity. If you think this still needs to be addressed please comment on this thread.\n\nPlease note that issues that do not follow the [contributing guidelines](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md) are likely to be ignored."
      },
      {
        "author": "Rocketknight1",
        "created_at": "2025-01-20T15:10:00+00:00",
        "body": "cc @zucchini-nlp maybe?"
      },
      {
        "author": "zucchini-nlp",
        "created_at": "2025-01-20T15:47:28+00:00",
        "body": "@Debolena7 hey! Which version are you using? It works for me in the latest v4.48 so I guess there was some bug in earlier versions. Can you try to update with `pip install -U transformers`?"
      },
      {
        "author": "Debolena7",
        "created_at": "2025-01-20T19:35:39+00:00",
        "body": "@zucchini-nlp and @Rocketknight1  yes, it worked for transformers==4.48.0. Thank you!"
      },
      {
        "author": "zucchini-nlp",
        "created_at": "2025-01-21T08:15:21+00:00",
        "body": "Cool, closing as resolved then  :)"
      }
    ],
    "url": "https://github.com/huggingface/transformers/issues/35369"
  },
  {
    "repository": "huggingface/transformers",
    "issue_number": 35367,
    "title": "DOC: Enhance Documentation Of Audio Classification",
    "body": "The documentation for the Audio Classification requires improvements in grammar and phrasing to enhance clarity and readability. Some Suggested updates include:\r\n- \"contain\" to \"contains\"\r\n- \"called to load and resample the audio file\" to \"used to load and resample the audio file\"\r\n- \"Take a look at an example now:\" to \"Here's an example:\", and more.",
    "state": "closed",
    "created_at": "2024-12-20T14:51:19+00:00",
    "closed_at": "2024-12-20T17:17:30+00:00",
    "updated_at": "2024-12-20T17:17:30+00:00",
    "author": "Uvi-12",
    "author_type": "User",
    "comments_count": 0,
    "reactions_count": 0,
    "labels": "",
    "milestone": null,
    "closed_by": "stevhliu",
    "resolution_time_hours": 2.4363888888888887,
    "first_comments": [],
    "url": "https://github.com/huggingface/transformers/issues/35367"
  },
  {
    "repository": "huggingface/transformers",
    "issue_number": 35362,
    "title": "Transformers cannot load ModernBERT for sequence classification",
    "body": "### System Info\n\nI am trying to test the new ModernBER, following this notebook from the official documentation: https://github.com/AnswerDotAI/ModernBERT/blob/main/examples/finetune_modernbert_on_glue.ipynb model for sequence classification but I am getting the following error: \r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/ubuntu/req_datalab/lib/python3.12/site-packages/transformers/models/auto/configuration_auto.py\", line 1038, in from_pretrained\r\n    config_class = CONFIG_MAPPING[config_dict[\"model_type\"]]\r\n                   ~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/ubuntu/req_datalab/lib/python3.12/site-packages/transformers/models/auto/configuration_auto.py\", line 740, in __getitem__\r\n    raise KeyError(key)\r\nKeyError: 'modernbert'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/ubuntu/caf_requirements_training/caf_requirements_training/train_full_fine_tuning.py\", line 75, in <module>\r\n    train_model_ft(tmp_folder_dataset.name, args)\r\n  File \"/home/ubuntu/caf_requirements_training/caf_requirements_training/train_full_fine_tuning.py\", line 39, in train_model_ft\r\n    orchestrate_training_with_epoch_artifacts(dataset=dataset, args=args)\r\n  File \"/home/ubuntu/caf_requirements_training/caf_requirements_training/utils/training/training_utils.py\", line 153, in orchestrate_training_with_epoch_artifacts\r\n    tokenizer, model = get_model_tokenizer(args)\r\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/ubuntu/caf_requirements_training/caf_requirements_training/utils/training/training_utils.py\", line 46, in get_model_tokenizer\r\n    model = AutoModelForSequenceClassification.from_pretrained(training_model_name,\r\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/ubuntu/req_datalab/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py\", line 526, in from_pretrained\r\n    config, kwargs = AutoConfig.from_pretrained(\r\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/ubuntu/req_datalab/lib/python3.12/site-packages/transformers/models/auto/configuration_auto.py\", line 1040, in from_pretrained\r\n    raise ValueError(\r\nValueError: The checkpoint you are trying to load has model type `modernbert` but Transformers does not recognize this architecture. This could be because of an issue with the checkpoint, or because your version of Transformers is out of date.\r\n```\r\nI am using:\r\n- Python version: 3.12.7\r\n- Tranformers version: 4.47.1\r\n- Tranformers information:\r\n\r\n```\r\n- `transformers` version: 4.47.1\r\n- Platform: Linux-6.8.0-1018-aws-x86_64-with-glibc2.35\r\n- Python version: 3.12.7\r\n- Huggingface_hub version: 0.26.3\r\n- Safetensors version: 0.4.5\r\n- Accelerate version: 1.2.1\r\n- Accelerate config:    not found\r\n- PyTorch version (GPU?): 2.5.1+cu124 (True)\r\n- Tensorflow version (GPU?): not installed (NA)\r\n- Flax version (CPU?/GPU?/TPU?): not installed (NA)\r\n- Jax version: not installed\r\n- JaxLib version: not installed\r\n- Using distributed or parallel set-up in script?:  no\r\n- Using GPU in script?: yes\r\n- GPU type: NVIDIA A10G\r\n```\r\n\r\nThe code snippet used is this:\r\n\r\n```\r\nmodel = AutoModelForSequenceClassification.from_pretrained(\"answerdotai/ModernBERT-base\", cache_dir=model_saving_path,\r\n                                                               num_labels=12, compile=False)\r\n```\r\n\r\nThank you very much!\n\n### Who can help?\n\n@ArthurZucker\r\n@stevhliu\n\n### Information\n\n- [X] The official example scripts\n- [ ] My own modified scripts\n\n### Tasks\n\n- [X] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\n- [ ] My own task or dataset (give details below)\n\n### Reproduction\n\nJust executing \r\n```\r\nmodel = AutoModelForSequenceClassification.from_pretrained(\"answerdotai/ModernBERT-base\", cache_dir=model_saving_path,\r\n                                                         num_labels=12, compile=False)\r\n```\r\nthe problem will arise \n\n### Expected behavior\n\nTo load the model normally",
    "state": "closed",
    "created_at": "2024-12-20T13:01:04+00:00",
    "closed_at": "2025-01-08T14:05:08+00:00",
    "updated_at": "2025-01-08T14:05:08+00:00",
    "author": "eneko-caf",
    "author_type": "User",
    "comments_count": 4,
    "reactions_count": 0,
    "labels": "Usage,bug",
    "milestone": null,
    "closed_by": "ArthurZucker",
    "resolution_time_hours": 457.0677777777778,
    "first_comments": [
      {
        "author": "seanfarr788",
        "created_at": "2024-12-20T13:27:00+00:00",
        "body": "Currently need to compile the latest transformers  \r\n\r\n`pip install git+https://github.com/huggingface/transformers`\r\n\r\nsource: https://huggingface.co/answerdotai/ModernBERT-base/discussions/3"
      },
      {
        "author": "dzimmerman-nci",
        "created_at": "2024-12-29T19:59:05+00:00",
        "body": "Any idea why I am getting this error during a `train()` of ModernBert on the latest transformers dev branch?\r\n`PyTorch version: 2.4.1+cu118\r\nTransformers version: 4.48.0.dev0`\r\n\r\n`BackendCompilerFailed: backend='inductor' raised:\r\nAssertionError: Please convert all Tensors to FakeTensors first or instantiate FakeTensorMode with 'allow_non_fake_inputs'. Found in aten.clone.default(tensor([...], size=(16,), dtype=torch.uint8), memory_format=torch.contiguous_format)`"
      },
      {
        "author": "dzimmerman-nci",
        "created_at": "2024-12-29T20:18:36+00:00",
        "body": "upgrading to  `PyTorch version: 2.5.1` seemed to fix the issue"
      },
      {
        "author": "roei-shlezinger",
        "created_at": "2025-01-06T19:28:19+00:00",
        "body": "I am getting the following error when trying to import `transformers.trainer` on macOS:\r\n```\r\nRuntimeError: Failed to import transformers.trainer because of the following error (look up to see its traceback):\r\nFailed to import transformers.integrations.integration_utils because of the following error (look up to see its traceback):\r\nFailed to import transformers.modeling_utils because of the following error (look up to see its traceback):\r\ncannot import name 'CompileConfig' from 'transformers.generation'\r\n```\r\nI am using Python 3.12.8, PyTorch 2.5.1, and also have `bitsandbytes`, `accelerate`, and `protobuf` installed in my environment."
      }
    ],
    "url": "https://github.com/huggingface/transformers/issues/35362"
  },
  {
    "repository": "huggingface/transformers",
    "issue_number": 35343,
    "title": "Llama model, torch.compile output for custom device does not match with eager/cpu when generation_config.use_cache set to True",
    "body": "### System Info\n\n- `transformers` version: 4.43.2\r\n- Platform: Linux-5.15.0-126-generic-x86_64-with-glibc2.35\r\n- Python version: 3.10.12\r\n- Huggingface_hub version: 0.26.3\r\n- Safetensors version: 0.4.5\r\n- Accelerate version: 1.0.1\r\n- Accelerate config:    not found\r\n- PyTorch version (GPU?): 2.4.0a0+gitee1b680 (False)\r\n- Tensorflow version (GPU?): not installed (NA)\r\n- Flax version (CPU?/GPU?/TPU?): not installed (NA)\r\n- Jax version: not installed\r\n- JaxLib version: not installed\r\n- Using distributed or parallel set-up in script?: <fill in>\n\n### Who can help?\n\n@ArthurZucker @gone\n\n### Information\n\n- [ ] The official example scripts\n- [ ] My own modified scripts\n\n### Tasks\n\n- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\n- [ ] My own task or dataset (give details below)\n\n### Reproduction\n\nFor a custom device I am working on adding `torch.compile()` with `CPP`  inductor backend.\r\nI am trying run `\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"` and it has output difference when using KV cache in generation.\r\nif I use following config output of compiled mode matches with eager mode.\r\n`compiled_model.generation_config.use_cache = False`\r\n\r\nAnd for large content length generation I see output similar to https://github.com/huggingface/transformers/issues/30347\n\n### Expected behavior\n\nPlease help me debugging this further so that my backend generates correct output with compile mode even with KV cache.",
    "state": "closed",
    "created_at": "2024-12-19T13:24:04+00:00",
    "closed_at": "2024-12-24T08:06:49+00:00",
    "updated_at": "2024-12-24T08:06:49+00:00",
    "author": "vpandya-quic",
    "author_type": "User",
    "comments_count": 2,
    "reactions_count": 0,
    "labels": "bug",
    "milestone": null,
    "closed_by": "vpandya-quic",
    "resolution_time_hours": 114.7125,
    "first_comments": [
      {
        "author": "ArthurZucker",
        "created_at": "2024-12-23T13:03:29+00:00",
        "body": "Hey! could you upgrade to the latest version of `transformers`? ðŸ¤— "
      },
      {
        "author": "vpandya-quic",
        "created_at": "2024-12-24T08:06:49+00:00",
        "body": "Thanks @ArthurZucker with latest version 4.47.1 things works as per expectations!"
      }
    ],
    "url": "https://github.com/huggingface/transformers/issues/35343"
  },
  {
    "repository": "huggingface/transformers",
    "issue_number": 35337,
    "title": "Option to Disable Model Caching When Using \"pipeline\"",
    "body": "### Feature request\n\nAdd a new parameter `disable_caching` to the `pipeline` function to control model caching. This feature would be useful for users who need to load models temporarily without storing them in cache, reducing unnecessary disk usage and improving performance for one-time use cases.\r\n\r\n```python\r\nfrom transformers import pipeline\r\n# Example of disabling caching\r\npipe = pipeline(task=\"zero-shot-classification\", model=\"typeform/distilbert-base-uncased-mnli\", disable_caching=True)\r\n```\n\n### Motivation\n\nWhen testing different models temporarily, it's inefficient to cache each model, especially when they will not be reused. This can lead to wasted disk space and potential performance issues and errors for users who frequently experiment with various models.\n\n### Your contribution\n\nI am not very experienced with the library but I am willing to contribute to the initial implementation and testing of this feature if no one wants to work on it.",
    "state": "closed",
    "created_at": "2024-12-19T08:36:54+00:00",
    "closed_at": "2025-01-13T08:34:40+00:00",
    "updated_at": "2025-01-13T08:34:40+00:00",
    "author": "FadiAmon",
    "author_type": "User",
    "comments_count": 4,
    "reactions_count": 0,
    "labels": "Feature request",
    "milestone": null,
    "closed_by": "FadiAmon",
    "resolution_time_hours": 599.9627777777778,
    "first_comments": [
      {
        "author": "Rocketknight1",
        "created_at": "2024-12-19T14:12:42+00:00",
        "body": "Hi @FadiAmon, I'm not sure we can add this! It's not really possible to load a model without downloading it first, so the files will have to be downloaded somewhere. The only way to reduce cache usage would be to delete the model files **after** the model has finished loading.\r\n\r\nIf this is a problem for you, you can use the existing `cache_dir` argument to control where models are downloaded. You can then empty this directory as part of your script, or even make it a `TemporaryDirectory()` so it automatically gets cleaned up after the model has been loaded."
      },
      {
        "author": "FadiAmon",
        "created_at": "2024-12-23T09:30:59+00:00",
        "body": "@Rocketknight1 Thanks you for the response!\r\nDo you know what is the reason we can't load the model straight from the Hugging Face servers?"
      },
      {
        "author": "Rocketknight1",
        "created_at": "2025-01-06T18:10:16+00:00",
        "body": "Hi @FadiAmon, the way loading works requires the index and all of the safetensors files to be available at once, because tensors can be loaded in a different order than the order of the safetensors shards. We'd have to refactor loading entirely to make it possible to stream weights, and I think it would add a lot of complexity. Also, most users generally want to cache the models rather than download them every single time, so it would require a lot of code for a change that probably wouldn't be used that much."
      },
      {
        "author": "FadiAmon",
        "created_at": "2025-01-13T08:34:40+00:00",
        "body": "Alright this makes sense, thanks you! @Rocketknight1 "
      }
    ],
    "url": "https://github.com/huggingface/transformers/issues/35337"
  },
  {
    "repository": "huggingface/transformers",
    "issue_number": 35335,
    "title": "Default arguments in `DebertaConfig` disable relative attention, contrary to the docs and `deberta-base`",
    "body": "### System Info\n\ntransformers 4.47.0\n\n### Who can help?\n\n@ArthurZucker \n\n### Information\n\n- [ ] The official example scripts\n- [ ] My own modified scripts\n\n### Tasks\n\n- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\n- [ ] My own task or dataset (give details below)\n\n### Reproduction\n\nThe documentation for `DebertaConfig` says that\r\n\r\n> Instantiating a configuration with the defaults will yield a similar configuration to that of the DeBERTa [microsoft/deberta-base](https://huggingface.co/microsoft/deberta-base) architecture.\r\n\r\nYet, the **most important part** of DeBERTa, namely the relative attention, is disabled by default in the model and in the config:\r\n\r\nhttps://github.com/huggingface/transformers/blob/9613933b022ddbf085e2c593ed4ceea4c734179a/src/transformers/models/deberta/modeling_deberta.py#L191\r\n\r\nhttps://github.com/huggingface/transformers/blob/9613933b022ddbf085e2c593ed4ceea4c734179a/src/transformers/models/deberta/configuration_deberta.py#L71-L75\r\nhttps://github.com/huggingface/transformers/blob/9613933b022ddbf085e2c593ed4ceea4c734179a/src/transformers/models/deberta/configuration_deberta.py#L120\r\n\r\nEven when users request a given amount of `max_relative_positions`, relative attention stays disabled as long as that option is set to False.\r\n\r\nhttps://github.com/huggingface/transformers/blob/9613933b022ddbf085e2c593ed4ceea4c734179a/src/transformers/models/deberta/modeling_deberta.py#L201-L210\r\n\r\nAnd indeed:\r\n```python\r\nfrom transformers import DebertaConfig\r\n\r\nconfig = DebertaConfig()\r\nprint(config.relative_attention)\r\n```\r\nThis prints False, and when you instantiate a new DeBERTa model, e.g. like\r\n\r\n```python\r\nfrom transformers import DebertaConfig, DebertaForMaskedLM\r\n\r\nprint(DebertaForMaskedLM._from_config(DebertaConfig()))\r\nprint(DebertaForMaskedLM._from_config(DebertaConfig(max_relative_positions=512)))\r\n```\r\n\r\n...there are **no relative positional embeddings** in the model, only absolute positional embeddings. This model will also not do any disentangled attention.\n\n### Expected behavior\n\nConform to the documentation by setting `relative_attention=True` in the `DebertaConfig` by default. \r\n\r\nI would also add a warning when relative attention is False, so that users know very clearly that *despite* using a DeBERTa model, they are not getting the core feature offered by DeBERTa, namely the relative attention.",
    "state": "closed",
    "created_at": "2024-12-19T04:13:40+00:00",
    "closed_at": "2025-01-26T08:03:10+00:00",
    "updated_at": "2025-01-26T08:03:10+00:00",
    "author": "bauwenst",
    "author_type": "User",
    "comments_count": 7,
    "reactions_count": 0,
    "labels": "bug",
    "milestone": null,
    "closed_by": "github-actions[bot]",
    "resolution_time_hours": 915.825,
    "first_comments": [
      {
        "author": "Rocketknight1",
        "created_at": "2024-12-19T14:17:57+00:00",
        "body": "Hi @bauwenst, I would prefer **not** to update the default here, because it might cause an unexpected change in behaviour for users. Instead, I think updating the documentation to explain that relative attention is not enabled by default in the config class is probably the right course of action."
      },
      {
        "author": "bauwenst",
        "created_at": "2024-12-19T16:02:36+00:00",
        "body": "I understand your point @Rocketknight1, but consider this:\r\n- Configs are only instantiated from scratch for new training runs, rather than recurrently in the life cycle of a model (unlike e.g. the default arguments in `AutoModel.from_pretrained`). The people whose implementations would be affected, would be those people who are *now* getting ready to train DeBERTa AND don't want relative attention AND don't explicitly disable it by using `DebertaConfig(relative_attention=False)` AND will update to the newer version of `transformers` before running their code. That is an exceedingly tiny number of cases.\r\n- Just because technical debt exists, does not justify that it keeps existing for \"backwards compatibility\" reasons, which again, isn't even really needed here.\r\n\r\nI know at least two academic researchers who have had to throw out all their DeBERTa experiments, and I accidentally avoided this because I initialised `DebertaConfig.from_pretrained()` rather than `DebertaConfig()`, but could have easily had to trash weeks of GPU hours. As you put it, the *current* HuggingFace defaults themselves *\"cause an unexpected change in behaviour for users\".* Most users expect **De**BERTa to have disentangled attention, because that's what the paper is about and what the name implies. The current defaults *are* the unexpected change in behaviour. Keeping the current defaults means you are assuming that less harm is done by tricking users who *want* disentangled attention than tricking the tiny number of users above. I don't see how this trade-off makes sense."
      },
      {
        "author": "Rocketknight1",
        "created_at": "2024-12-19T18:22:00+00:00",
        "body": "Hi @bauwenst, you're right that this is a key feature of DeBERTa, but I think we prefer to keep the default as-is both for backward compatibility reasons, but also because this matches [the code in the original DeBERTa repo](https://github.com/microsoft/DeBERTa/blob/4d7fe0bd4fb3c7d4f4005a7cafabde9800372098/DeBERTa/deberta/config.py#L65).\r\n\r\nHowever, you were totally right to point out that setting this to `False` does not in fact yield \"a similar configuration to that of the DeBERTa `microsoft/deberta-base` architecture\", as the docs suggest. If you want to open a PR to correct the docs, we'd be happy to accept it!"
      },
      {
        "author": "bauwenst",
        "created_at": "2024-12-20T06:17:15+00:00",
        "body": "> both for backward compatibility reasons\r\n\r\nTo reiterate my previous comment: *Keeping the current defaults means you are assuming that less harm is done by tricking users who want disentangled attention than tricking the tiny number of users above. I don't see how this trade-off makes sense.*\r\n\r\n> but also because this matches [the code in the original DeBERTa repo](https://github.com/microsoft/DeBERTa/blob/4d7fe0bd4fb3c7d4f4005a7cafabde9800372098/DeBERTa/deberta/config.py#L65)\r\n\r\nI fail to see how this is a case of backwards compatibility.\r\n1. People who use**d** the original repo are definitely not the same people who use `transformers`. They don't live in the same ecosystem.\r\n2. In `transformers` there is a dedicated config class for DeBERTa, namely the `DebertaConfig`. You may notice that the original repo hesitates about whether the `relative_attention` field even *exists* ([with `getattr(config, \"relative_attention\", False)`](https://github.com/microsoft/DeBERTa/blob/4d7fe0bd4fb3c7d4f4005a7cafabde9800372098/DeBERTa/deberta/bert.py#L136)) rather than just using `config.relative_attention`. Have you considered why this is? Your point seems to be that this is because they intended the default DeBERTa to have `relative_attention` be False. This is not right. The reason is that they did **not** have a dedicated config class for DeBERTa. They wrote **one config class** whose fields where those of BERT (which is why `relative_attention` only appears in the docstring of the file you linked to, but is not a field that appears in the constructor). Extra fields for DeBERTa were added directly in the config JSONs. If you go look at [the configs they provide](https://github.com/search?q=repo%3Amicrosoft%2FDeBERTa%20relative_attention&type=code), you'll see that for all configs that aren't the BERT-base config, *all instances of `relative_attention` are True*. \r\n\r\nIn other words: by saying you keep it False for backwards compatibility with the DeBERTa repo, you are saying that you are keeping it False so that DeBERTa's default config is a BERT config. This makes no sense, because `DebertaConfig` is not intended to replace `BertConfig`. What that docstring you linked to is saying is *not* that the default value in a config for DeBERTa should be False. It is saying that *because DeBERTa is a [modified BERT](https://github.com/microsoft/DeBERTa/blob/4d7fe0bd4fb3c7d4f4005a7cafabde9800372098/DeBERTa/deberta/bert.py#L129-L145) and BERT configs don't contain the `relative_attention` field, the modified BERT constructor will assume that you want to construct BERT if you use an old config that has no `relative_attention` field.*\r\n\r\nIn even other words: the default value of False, which is now apparently sacred technical debt in `transformers`, was itself grandfathered in from technical debt by Microsoft. Zero use for keeping it this way."
      },
      {
        "author": "Rocketknight1",
        "created_at": "2024-12-20T13:35:51+00:00",
        "body": "Regarding backward compatibility, we're just keeping it `False` because people who have are running code that instantiates the class will find their model suddenly changing when they upgrade `transformers`, which we would prefer not to do without a strong reason. `Config` classes are not intended to have perfectly optimal default settings - we assume that users advanced enough to define their own configs and train models from scratch can figure out how to read a docstring and set a few kwargs. \r\n\r\nAs a result, I really do think the problem is just that one misleading line in the documentation, as I mentioned! You're welcome to submit a PR to fix it, but I don't want to keep going back and forth over the default value"
      }
    ],
    "url": "https://github.com/huggingface/transformers/issues/35335"
  },
  {
    "repository": "huggingface/transformers",
    "issue_number": 35331,
    "title": "MPI environment variables are not set.",
    "body": "### System Info\n\nHPC ubuntu 22.04 2nodesx8H100\r\n\r\nLSF as scheduler\r\n\r\n[tool.poetry.dependencies]\r\npython = \"^3.10\"\r\n\r\nimportlib-metadata = { version = \"~=1.0\", python = \"<3.8\" }\r\ntensorboard = \"^2.16.2\"\r\nsge-data-package = {version = \"*\", source = \"sgedata\"}\r\ntorch = \"2.2.1\"\r\ntorchvision = \"0.17.1\"\r\ntorchaudio = \"2.2.1\"\r\ntransformers = \"4.42.0\"\r\ndatasets = \"2.18.*\"\r\naccelerate = \"0.28.0\"\r\ndeepspeed = \"0.13.4\"\r\nsafetensors = \"0.4.2\"\r\nmpi4py = \"^4.0.0\"\n\n### Who can help?\n\n_No response_\n\n### Information\n\n- [ ] The official example scripts\n- [X] My own modified scripts\n\n### Tasks\n\n- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\n- [X] My own task or dataset (give details below)\n\n### Reproduction\n\nmodule load cuda-12.1.1\r\nmodule load ISG/experimental/fg12/openmpi/5.0.4-cuda12.1-lsf\r\nexport LD_LIBRARY_PATH=$CUDA_HOME/lib64:$LD_LIBRARY_PATH\r\n\r\n```\r\nmodule load cuda-12.1.1\r\nmodule load ISG/experimental/fg12/openmpi/5.0.4-cuda12.1-lsf\r\nexport LD_LIBRARY_PATH=$CUDA_HOME/lib64:$LD_LIBRARY_PATH\r\n\r\ndeepspeed \\\r\n    --hostfile=${HOSTFILE_PATH} \\\r\n    --launcher=OPENMPI \\\r\n    --launcher_args=\"-bind-to none -map-by slot --mca pml ob1 --oversubscribe --display-allocation --display-map\" \\\r\n    --master_addr=${MASTER_ADDR} \\\r\n    --master_port=${_M_PORT} \\\r\n    --no_ssh_check \\\r\n    src/dna_mlm/runner.py\r\n```\n\n### Expected behavior\n\n```\r\ndef setup_env_ranks() -> tp.Tuple[int, int, int]:\r\n\r\n    # Map MPI environment variables to those expected by DeepSpeed/PyTorch\r\n    if 'OMPI_COMM_WORLD_LOCAL_RANK' in os.environ:\r\n        os.environ['LOCAL_RANK'] = os.environ['OMPI_COMM_WORLD_LOCAL_RANK']\r\n        os.environ['RANK'] = os.environ['OMPI_COMM_WORLD_RANK']\r\n        os.environ['WORLD_SIZE'] = os.environ['OMPI_COMM_WORLD_SIZE']\r\n    else:\r\n        raise EnvironmentError(\r\n            \"MPI environment variables are not set. \"\r\n            \"Ensure you are running the script with an MPI-compatible launcher.\"\r\n        )\r\n \r\n setup_env_ranks()\r\n```\r\n\r\nthe function should set the env vars but instaed it raises the error",
    "state": "closed",
    "created_at": "2024-12-18T23:14:49+00:00",
    "closed_at": "2025-01-26T08:03:12+00:00",
    "updated_at": "2025-01-26T08:03:12+00:00",
    "author": "fabiogeraci",
    "author_type": "User",
    "comments_count": 3,
    "reactions_count": 0,
    "labels": "bug",
    "milestone": null,
    "closed_by": "github-actions[bot]",
    "resolution_time_hours": 920.8063888888889,
    "first_comments": [
      {
        "author": "fabiogeraci",
        "created_at": "2024-12-19T09:57:02+00:00",
        "body": "apparently the openmpi arg needs to passed as a string\r\n\r\n```\r\ndeepspeed \\\r\n    --hostfile=${HOSTFILE_PATH} \\\r\n    --launcher=\"OPENMPI\" \\\r\n    --launcher_args=\"-bind-to none -map-by slot --mca pml ob1 --oversubscribe --display-allocation --display-map\" \\\r\n    --master_addr=${MASTER_ADDR} \\\r\n    --master_port=${_M_PORT} \\\r\n    --no_ssh_check \\\r\n    src/dna_mlm/runner.py\r\n```\r\n\r\nthe real question is why I need to setup\r\n\r\n```\r\n    if 'OMPI_COMM_WORLD_LOCAL_RANK' in os.environ:\r\n        os.environ['LOCAL_RANK'] = os.environ['OMPI_COMM_WORLD_LOCAL_RANK']\r\n        os.environ['RANK'] = os.environ['OMPI_COMM_WORLD_RANK']\r\n        os.environ['WORLD_SIZE'] = os.environ['OMPI_COMM_WORLD_SIZE']\r\n    else:\r\n        raise EnvironmentError(\r\n            \"MPI environment variables are not set. \"\r\n            \"Ensure you are running the script with an MPI-compatible launcher.\"\r\n        )\r\n```\r\n"
      },
      {
        "author": "Rocketknight1",
        "created_at": "2024-12-19T14:08:29+00:00",
        "body": "cc @muellerzr "
      },
      {
        "author": "github-actions[bot]",
        "created_at": "2025-01-18T08:02:45+00:00",
        "body": "This issue has been automatically marked as stale because it has not had recent activity. If you think this still needs to be addressed please comment on this thread.\n\nPlease note that issues that do not follow the [contributing guidelines](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md) are likely to be ignored."
      }
    ],
    "url": "https://github.com/huggingface/transformers/issues/35331"
  },
  {
    "repository": "huggingface/transformers",
    "issue_number": 35326,
    "title": "unable to convert llama 3.3 weights to hf.py",
    "body": "### System Info\n\npython == 3.10.12\r\ntransformers == 4.45.1\r\n\n\n### Who can help?\n\n_No response_\n\n### Information\n\n- [X] The official example scripts\n- [ ] My own modified scripts\n\n### Tasks\n\n- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\n- [ ] My own task or dataset (give details below)\n\n### Reproduction\n\nI ran the `convert_llama_weightsto_hf.py` script on the llama-3.3-70B-Instruct checkpoint weights \r\n`!python convert_llama_weights_to_hf.py --input_dir \"llama_3_3_model/Llama-3.3-70B-Instruct/original\" --model_size 70B --output_dir \"llama_3_3_model/Llama-3.3-70B-Instruct/hf-converted\"`, \r\nand got the \r\n`ValueError: Failed to instantiate tokenizer.Pleae, make sure you have sentencepiece and protobuf.`\r\nI tried upgrading both sentencepiece and protobuf but same error is occurring again\r\n\n\n### Expected behavior\n\ndownloaded from HuggingFace Files.",
    "state": "closed",
    "created_at": "2024-12-18T18:47:30+00:00",
    "closed_at": "2025-01-26T08:03:16+00:00",
    "updated_at": "2025-01-27T02:16:06+00:00",
    "author": "AshishMulupuri",
    "author_type": "User",
    "comments_count": 11,
    "reactions_count": 0,
    "labels": "bug",
    "milestone": null,
    "closed_by": "github-actions[bot]",
    "resolution_time_hours": 925.2627777777777,
    "first_comments": [
      {
        "author": "Ulton321",
        "created_at": "2024-12-20T16:51:48+00:00",
        "body": "# I think the issue here is with your dependencies \r\n\r\n## 1. Upgrade Dependencies\r\nEnsure that the necessary dependencies are installed and up-to-date. Run the following command:\r\n\r\n```bash\r\npip install --upgrade sentencepiece protobuf\r\n```\r\n\r\n```bash\r\n\r\npip show sentencepiece protobuf\r\n\r\n```\r\nI think it should run like normal\r\n\r\n\r\n\r\n\r\n"
      },
      {
        "author": "AshishMulupuri",
        "created_at": "2024-12-20T18:46:05+00:00",
        "body": "I tried it, upgraded versions `protobuf-5.29.2 and sentencepiece-0.2.0`\r\nbut still got the same error."
      },
      {
        "author": "hvaara",
        "created_at": "2024-12-21T22:31:53+00:00",
        "body": "Were you using a notebook? Did you remember to restart the notebook session after installing the dependencies? I was able to convert the llama weights by using a dev install of transformers."
      },
      {
        "author": "Ulton321",
        "created_at": "2024-12-23T16:43:28+00:00",
        "body": "Maybe environmental problems? \r\n\r\n```\r\n .\\<your_env>\\Scripts\\activate \r\n \r\n ````\r\n \r\n \r\n another option could be downgrade your library but I'm not sure if it's going to work  "
      },
      {
        "author": "AshishMulupuri",
        "created_at": "2024-12-23T17:17:16+00:00",
        "body": "> Were you using a notebook? Did you remember to restart the notebook session after installing the dependencies? I was able to convert the llama weights by using a dev install of transformers.\r\n\r\nYes, I'm using the notebook. I restarted it after upgrading the `sentencepiece and protobuf` before running the `convert_llama_weights_to_hf.py`, but I still ended up with the same error."
      }
    ],
    "url": "https://github.com/huggingface/transformers/issues/35326"
  },
  {
    "repository": "huggingface/transformers",
    "issue_number": 35316,
    "title": "How to use a custom Image Processor?",
    "body": "I want to use the processor in the form of `auto_map` but when using `AutoProcessor.from_pretrained`, I am unable to load the custom `ImageProcessor`.\r\n\r\nThe root cause lies in the use of the `transformers_module` to initialize the class in `ProcessorMixin`. \r\n\r\nhttps://github.com/huggingface/transformers/blob/c7e48053aab09ad11efa2ad12513e9ab56f29563/src/transformers/processing_utils.py#L1018\r\n\r\nEven though I have overridden the _get_arguments_from_pretrained method, this issue still exists in the `__init__`. \r\n\r\nhttps://github.com/huggingface/transformers/blob/c7e48053aab09ad11efa2ad12513e9ab56f29563/src/transformers/processing_utils.py#L383\r\n\r\nPerhaps I could avoid inheriting from ProcessorMixin, but I would like to know if there is a more elegant way to achieve this functionality?",
    "state": "closed",
    "created_at": "2024-12-18T12:04:33+00:00",
    "closed_at": "2024-12-19T02:53:43+00:00",
    "updated_at": "2024-12-19T02:53:43+00:00",
    "author": "glamourzc",
    "author_type": "User",
    "comments_count": 1,
    "reactions_count": 0,
    "labels": "",
    "milestone": null,
    "closed_by": "glamourzc",
    "resolution_time_hours": 14.819444444444445,
    "first_comments": [
      {
        "author": "zucchini-nlp",
        "created_at": "2024-12-18T12:27:12+00:00",
        "body": "Slightly may be related to https://github.com/huggingface/transformers/issues/34307#issuecomment-2547746188. There is a way to do that by adding `transformers.MyImageProcessor = MyImageProcessor` in the `processor_mymodel.py` file, but apparently it causes errors in certain import order"
      }
    ],
    "url": "https://github.com/huggingface/transformers/issues/35316"
  },
  {
    "repository": "huggingface/transformers",
    "issue_number": 35315,
    "title": "`train_new_from_iterator()` does not work when pre_tokenizer is null",
    "body": "### System Info\n\ntransformers version 4.47.1\r\nUbuntu 20.04.6 LTS\r\nPython 3.10\n\n### Who can help?\n\n@ArthurZucker, @itazap\n\n### Information\n\n- [X] The official example scripts\n- [ ] My own modified scripts\n\n### Tasks\n\n- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\n- [ ] My own task or dataset (give details below)\n\n### Reproduction\n\nFollow the steps listed in https://huggingface.co/learn/nlp-course/chapter6/2, however use `microsoft/Phi-3.5-mini-instruct` as the model instead of `gpt2`.\r\n\r\n```py\r\nfrom datasets import load_dataset\r\nfrom transformers import AutoTokenizer\r\n\r\nraw_datasets = load_dataset(\"code_search_net\", \"python\", trust_remote_code=True)\r\n\r\ndef get_training_corpus():\r\n    dataset = raw_datasets[\"train\"]\r\n    for start_idx in range(0, len(dataset), 1000):\r\n        samples = dataset[start_idx : start_idx + 1000]\r\n        yield samples[\"whole_func_string\"]\r\n\r\ntraining_corpus = get_training_corpus()\r\n\r\nold_tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3.5-mini-instruct\")\r\n\r\ntokenizer = old_tokenizer.train_new_from_iterator(training_corpus, 52000)\r\n```\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/azureuser/tokenizer.py\", line 16, in <module>\r\n    tokenizer = old_tokenizer.train_new_from_iterator(training_corpus, 52000)\r\n  File \"/anaconda/envs/myenv/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py\", line 819, in train_new_from_iterator\r\n    or tokenizer_json[\"pre_tokenizer\"][\"type\"] == \"Sequence\"\r\nTypeError: 'NoneType' object is not subscriptable\r\n```\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\n\n### Expected behavior\n\nThe tokenizer should be trained.",
    "state": "closed",
    "created_at": "2024-12-18T10:59:32+00:00",
    "closed_at": "2025-01-09T14:34:53+00:00",
    "updated_at": "2025-01-09T14:34:53+00:00",
    "author": "cecheta",
    "author_type": "User",
    "comments_count": 3,
    "reactions_count": 0,
    "labels": "bug",
    "milestone": null,
    "closed_by": "ArthurZucker",
    "resolution_time_hours": 531.5891666666666,
    "first_comments": [
      {
        "author": "cecheta",
        "created_at": "2024-12-18T11:00:13+00:00",
        "body": "Appears to have regressed in #33556"
      },
      {
        "author": "Rocketknight1",
        "created_at": "2024-12-18T18:27:30+00:00",
        "body": "pinging @umarbutler @itazap from PR #33556"
      },
      {
        "author": "ArthurZucker",
        "created_at": "2025-01-09T14:34:52+00:00",
        "body": "Closing as PR is merged!"
      }
    ],
    "url": "https://github.com/huggingface/transformers/issues/35315"
  },
  {
    "repository": "huggingface/transformers",
    "issue_number": 35311,
    "title": "Unclear what happens when using torchrun, multi-gpu and trainer arguments.",
    "body": "### System Info\r\n\r\n- `transformers` version: 4.47.0\r\n- Platform: Linux-6.8.0-51-generic-x86_64-with-glibc2.35\r\n- Python version: 3.11.6\r\n- Huggingface_hub version: 0.26.3\r\n- Safetensors version: 0.4.5\r\n- Accelerate version: 1.1.1\r\n- Accelerate config:    not found\r\n- PyTorch version (GPU?): 2.0.1+cu117 (True)\r\n- Tensorflow version (GPU?): 2.15.1 (True)\r\n- Flax version (CPU?/GPU?/TPU?): not installed (NA)\r\n- Jax version: not installed\r\n- JaxLib version: not installed\r\n- Using distributed or parallel set-up in script?:  CUDA_VISIBLE_DEVICES=0,1,2,3 torchrun --nproc-per-node 4 stages/train_model.py\r\n- Using GPU in script?: Yes\r\n- GPU type: NVIDIA RTX A4000\r\n\r\n### Who can help?\r\n\r\n@muellerzr \r\n@SunMarc \r\n\r\nI have a piece of python code, that loads a training dataset from a file,  then sets up a Trainer with the training dataset. I am training a distilBERT model (I dont think that detail matters).\r\n\r\nIt works great. And then I can do torchrun --nproc_per_node = 4, and it seems to run great too, it seems to spawn 4 versions and trains them on the 4 GPUs.\r\n\r\nHowever, I got suspicious - given that Iâ€™m passing the same dataset in everyt ime, is the Trainer basically training on just the first 25% of the training set? \r\n\r\nIâ€™d assumed initially it was smarter than this, and some how only used one of the train dataset instances (presumably the first), but now I realize I cannot be certain of this. So, can anyone tell me if I need to â€œpre-partitionâ€ the datasets into 4 shards, and then load them keyed by LOCAL RANK?\r\n\r\nIf it's not doing the \"right\" thing - i.e. training on all the data, then a warning should be given when this is detected.\r\n\r\nScript looks like this:\r\ntrain_test_dataset = Dataset.load_from_disk('train_test_dataset_file')\r\ntrainer = Trainer(..., train_dataset=train_test_dataset['train'], validation_dataset=train_test_dataset['test'],...)\r\n\r\n\r\nShould script looks like this:\r\ntrain_test_dataset = Dataset.load_from_disk(f\"one_quarter_train_test_dataset_file_{LOCAL_RANK)\")\r\ntrainer = Trainer(..., train_dataset=train_test_dataset['train'], validation_dataset=train_test_dataset['test'],...)\r\n\r\n\r\n\r\n### Information\r\n\r\n- [ ] The official example scripts\r\n- [X] My own modified scripts\r\n\r\n### Tasks\r\n\r\n- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\r\n- [X] My own task or dataset (give details below)\r\n\r\n### Reproduction\r\n\r\nclarity is needed on what behaviour is happening with the training data. Is the training dataset being used by all the nodes with an equal division of labor over each quarter of the dataset (or interleaved every 4 minibatches), or is it sending the same minibatch from the training set, and never getting to the final three quarters of the dataset.\r\n\r\n### Expected behavior\r\n\r\nA better understanding of what data is being trained on when invoking Trainer through torchrun.",
    "state": "closed",
    "created_at": "2024-12-17T20:59:41+00:00",
    "closed_at": "2025-01-13T16:43:15+00:00",
    "updated_at": "2025-01-13T16:43:15+00:00",
    "author": "davies-w",
    "author_type": "User",
    "comments_count": 5,
    "reactions_count": 0,
    "labels": "bug",
    "milestone": null,
    "closed_by": "davies-w",
    "resolution_time_hours": 643.7261111111111,
    "first_comments": [
      {
        "author": "davies-w",
        "created_at": "2025-01-07T19:08:49+00:00",
        "body": "Gentle ping?"
      },
      {
        "author": "SunMarc",
        "created_at": "2025-01-08T11:34:54+00:00",
        "body": "Trainer will use accelerate to prepare the dataloader. See the following video to help you better understand how it works ! https://www.youtube.com/watch?v=9Vfauv4ErwA"
      },
      {
        "author": "davies-w",
        "created_at": "2025-01-09T18:01:53+00:00",
        "body": "Thanks SunMarc, it still leave me puzzled. \r\n\r\nI have a single script, which as I mentioned just has:\r\n\r\ntrain_test_dataset = Dataset.load_from_disk('train_test_dataset_file')\r\ntrainer = Trainer(..., train_dataset=train_test_dataset['train'], validation_dataset=train_test_dataset['test'],...)\r\n\r\nWhen I run it with say 4 GPUs via torchrun, I see it execute this code 4 times. Does one of the executions become the master node? EG maybe always the local_rank 0 one? \r\n\r\nIn which case, if I interpret the video correctly, I just need to set global_batch to 4 x the batch size?  And the other three executions of the script ignore the train_test_dataset? \r\n\r\nOr am I doing it completely wrong?  \r\n\r\n\r\n\r\n"
      },
      {
        "author": "muellerzr",
        "created_at": "2025-01-09T18:07:38+00:00",
        "body": "When you pass in a `per_device_batch_size`, it's as it is stated in the variable. The GBS will be pdbs * world_size. \r\n\r\nOne of those, on rank 0, will become the master (usually whichever one started the script). Each GPU needs its own copy of the data, hence why its ran 4 times (but usually for tokenizing its pulled in from a cache). \r\n\r\nA better way to think about training on single GPU vs multi-GPU and what's happening under the hood would be:\r\n\r\nSay I have 400 items of training data, and training on 1 GPU or 4 GPUs. \r\n\r\nOn 1 GPU, I train with a BS of 80, leading to 5 iterations to get through the data.\r\n\r\nWith how accelerate does multi-GPU dataloaders, on 4 GPUs you would need a *per device batch size* of 20 to do the equivalent, which will as a result be a degree faster since we're processing **the same amount of data faster**.\r\n\r\nAnother way to think about it is if we kept our same BS of 80 *per GPU*, our real bs would be 320, and it'd be 1 and some change to get through all our data.\r\n\r\nDoes that make sense?"
      },
      {
        "author": "davies-w",
        "created_at": "2025-01-13T16:43:09+00:00",
        "body": "PERFECT! Thank you SO much. That's a huge relief I don't need to re-engineer and re-run this 1000 hour model! "
      }
    ],
    "url": "https://github.com/huggingface/transformers/issues/35311"
  },
  {
    "repository": "huggingface/transformers",
    "issue_number": 35309,
    "title": "Torch is installed but still getting this error \"RuntimeError: At least one of TensorFlow 2.0 or PyTorch should be installed. \"",
    "body": "### System Info\n\n- `transformers` version: 4.47.0\r\n- Platform: Linux-5.11.0-1021-azure-x86_64-with-glibc2.31\r\n- Python version: 3.10.16\r\n- Huggingface_hub version: 0.27.0\r\n- Safetensors version: 0.4.5\r\n- Accelerate version: not installed\r\n- Accelerate config: not found\r\n- PyTorch version (GPU?): 2.5.1+cu124 (False)\r\n- Tensorflow version (GPU?): 2.18.0 (False)\r\n- Flax version (CPU?/GPU?/TPU?): not installed (NA)\r\n- Jax version: not installed\r\n- JaxLib version: not installed\r\n- Using distributed or parallel set-up in script?: <fill in>\n\n### Who can help?\n\n_No response_\n\n### Information\n\n- [X] The official example scripts\n- [ ] My own modified scripts\n\n### Tasks\n\n- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\n- [ ] My own task or dataset (give details below)\n\n### Reproduction\n\n1. I created a new environment in conda & activated it with python=3.10\r\n2. Installed jupyter, transformers,tensorflow,torch\r\n3. Tried running this code in jupyter\r\n```\r\nfrom transformers import pipeline\r\nsentiment_pipeline = pipeline(\"sentiment-analysis\")\r\n```\r\n4. Got the error \"RuntimeError: At least one of TensorFlow 2.0 or PyTorch should be installed.\"\n\n### Expected behavior\n\nHi @sanchit-gandhi @Rocketknight1 & HF Team,\r\nI am not able to run transformer pipelines due to the error mentioned in the reproduction steps.\r\nPlease advise, thank you!",
    "state": "closed",
    "created_at": "2024-12-17T11:30:42+00:00",
    "closed_at": "2024-12-18T06:39:08+00:00",
    "updated_at": "2024-12-18T06:39:08+00:00",
    "author": "kartikihx",
    "author_type": "User",
    "comments_count": 3,
    "reactions_count": 0,
    "labels": "bug",
    "milestone": null,
    "closed_by": "kartikihx",
    "resolution_time_hours": 19.140555555555554,
    "first_comments": [
      {
        "author": "Rocketknight1",
        "created_at": "2024-12-17T15:27:56+00:00",
        "body": "This is unusual - I haven't seen any reports of this failing for anyone else, which suggests it might be an environment issue. Can you try a fresh environment with conda or something like that and check if the issue recurs?"
      },
      {
        "author": "bhuvanmdev",
        "created_at": "2024-12-18T06:25:34+00:00",
        "body": "You most probably have installed the packages in a different environment than the current one your using to execute the code. Check it once."
      },
      {
        "author": "kartikihx",
        "created_at": "2024-12-18T06:39:08+00:00",
        "body": "I re-ran and got a new error asking me to install tf-keras. After installing that, I am now able to import transformers successfully"
      }
    ],
    "url": "https://github.com/huggingface/transformers/issues/35309"
  },
  {
    "repository": "huggingface/transformers",
    "issue_number": 35304,
    "title": "DOC: Typo In Audio Classification Documentation",
    "body": "The term \"finetune\" is used throughout the documentation, but the correct form should be \"fine-tune\".",
    "state": "closed",
    "created_at": "2024-12-17T08:34:48+00:00",
    "closed_at": "2024-12-17T15:07:50+00:00",
    "updated_at": "2024-12-17T15:07:50+00:00",
    "author": "Uvi-12",
    "author_type": "User",
    "comments_count": 1,
    "reactions_count": 0,
    "labels": "",
    "milestone": null,
    "closed_by": "Rocketknight1",
    "resolution_time_hours": 6.5505555555555555,
    "first_comments": [
      {
        "author": "Rocketknight1",
        "created_at": "2024-12-17T15:07:50+00:00",
        "body": "I think both are acceptable! Words that are constantly used in the same hyphenated pairing in English often get merged into a single word over time (e.g. \"e-mail\" was the only correct form 20 years ago but now \"email\" is completely fine). See the [relevant Wikipedia section](https://en.wikipedia.org/wiki/Hyphen#Prefixes_and_suffixes) for more.\r\n\r\n"
      }
    ],
    "url": "https://github.com/huggingface/transformers/issues/35304"
  },
  {
    "repository": "huggingface/transformers",
    "issue_number": 35298,
    "title": "[Question] Why doesn't `trainer.state.epoch` fall round after training?",
    "body": "```python\r\n# run.py\r\nfrom datasets import Dataset\r\nfrom transformers import TrainingArguments, Trainer, AutoModelForCausalLM\r\n\r\n\r\ndef main():\r\n    model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen2-0.5B\")\r\n\r\n    dataset = Dataset.from_dict(\r\n        {\r\n            \"input_ids\": [[1, 2, 3] for _ in range(260)],\r\n            \"labels\": [[4, 5, 6] for _ in range(260)],\r\n        }\r\n    )\r\n\r\n    trainer = Trainer(\r\n        model=model,\r\n        args=TrainingArguments(\r\n            output_dir=\"my_output_dir\",\r\n            per_device_train_batch_size=16,\r\n            gradient_accumulation_steps=2,\r\n            num_train_epochs=1,\r\n            report_to=\"none\",\r\n        ),\r\n        train_dataset=dataset,\r\n    )\r\n\r\n    trainer.train()\r\n    print(trainer.state.epoch)  # 0.9411\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n```\r\n\r\n\r\n```\r\npython run.py\r\n```\r\n\r\nIn this case, I would expect `trainer.state.epoch` to be 1 after the training, but I end up with 0.9411 (=16/17). How to explain this?\r\n\r\n@muellerzr \r\n\r\n## System info\r\n\r\n- `transformers` version: 4.47.0.dev0 22834eeba1c2bf8d632e22fca238ab7c15d1b904\r\n- Platform: Linux-5.15.0-1048-aws-x86_64-with-glibc2.31\r\n- Python version: 3.11.10\r\n- Huggingface_hub version: 0.26.2\r\n- Safetensors version: 0.4.5\r\n- Accelerate version: 1.2.0.dev0\r\n- Accelerate config:    not found\r\n- PyTorch version (GPU?): 2.5.0+cu124 (True)\r\n- Tensorflow version (GPU?): not installed (NA)\r\n- Flax version (CPU?/GPU?/TPU?): not installed (NA)\r\n- Jax version: not installed\r\n- JaxLib version: not installed\r\n- Using distributed or parallel set-up in script?: <fill in>\r\n- Using GPU in script?: <fill in>\r\n- GPU type: NVIDIA H100 80GB HBM3",
    "state": "closed",
    "created_at": "2024-12-16T17:10:13+00:00",
    "closed_at": "2025-01-24T08:03:29+00:00",
    "updated_at": "2025-01-24T08:03:29+00:00",
    "author": "qgallouedec",
    "author_type": "User",
    "comments_count": 2,
    "reactions_count": 0,
    "labels": "trainer",
    "milestone": null,
    "closed_by": "github-actions[bot]",
    "resolution_time_hours": 926.8877777777777,
    "first_comments": [
      {
        "author": "Ulton321",
        "created_at": "2024-12-20T16:55:25+00:00",
        "body": "Try adjusting your data set to ensure that the epoch reach 1\r\n\r\n```\r\ndataset = Dataset.from_dict(\r\n    {\r\n        \"input_ids\": [[1, 2, 3] for _ in range(288)],  # changes from 260 to 288 \r\n        \"labels\": [[4, 5, 6] for _ in range(288)],\r\n    }\r\n)\r\n```\r\n\r\n\r\nThis won't give any leftover, give it a shot \r\n"
      },
      {
        "author": "github-actions[bot]",
        "created_at": "2025-01-16T08:03:03+00:00",
        "body": "This issue has been automatically marked as stale because it has not had recent activity. If you think this still needs to be addressed please comment on this thread.\n\nPlease note that issues that do not follow the [contributing guidelines](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md) are likely to be ignored."
      }
    ],
    "url": "https://github.com/huggingface/transformers/issues/35298"
  },
  {
    "repository": "huggingface/transformers",
    "issue_number": 35286,
    "title": "version 4.47.0 provides different generation results when using quantized awq model",
    "body": "### System Info\r\n\r\n- `transformers` version: 4.47.0\r\n- Platform: Linux-5.4.0-169-generic-x86_64-with-glibc2.31\r\n- Python version: 3.9.19\r\n- Huggingface_hub version: 0.26.5\r\n- Safetensors version: 0.4.2\r\n- Accelerate version: 0.27.2\r\n- Accelerate config:    not found\r\n- PyTorch version (GPU?): 2.3.0+cu121 (True)\r\n- Tensorflow version (GPU?): not installed (NA)\r\n- Flax version (CPU?/GPU?/TPU?): not installed (NA)\r\n- Jax version: not installed\r\n- JaxLib version: not installed\r\n- Using distributed or parallel set-up in script?: <fill in>\r\n- Using GPU in script?: <fill in>\r\n- GPU type: NVIDIA A100-SXM4-80GB\r\n\r\n\r\n### Who can help?\r\n\r\n@gante @SunMarc @MekkCyber \r\n\r\n### Information\r\n\r\n- [ ] The official example scripts\r\n- [X] My own modified scripts\r\n\r\n### Tasks\r\n\r\n- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\r\n- [X] My own task or dataset (give details below)\r\n\r\n### Reproduction\r\n\r\n```python\r\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\r\n\r\nautoawq_model = \"casperhansen/opt-125m-awq\"\r\nprompt = \"One day, the little girl\"\r\nuser_model = AutoModelForCausalLM.from_pretrained(autoawq_model).to('cuda:0')\r\ntokenizer = AutoTokenizer.from_pretrained(autoawq_model)\r\ninput_ids = tokenizer(prompt, return_tensors=\"pt\")[\"input_ids\"].to('cuda:0')\r\ngenerate_kwargs = dict(do_sample=False, temperature=0.9, num_beams=4)\r\ngen_ids = user_model.generate(input_ids, **generate_kwargs)\r\ngen_text = tokenizer.batch_decode(gen_ids, skip_special_tokens=True)\r\ntarget_text = [\"One day, the little girl in the back of my mind will ask me if I'm a\"]\r\nassert gen_text == target_text, f\"Expect: {target_text}\\n but get: {gen_text}.\"\r\n```\r\n\r\n### Expected behavior\r\nWhen version < 4.47.0, it works well. Version 4.47.0 provides different result\r\n```log\r\nTraceback (most recent call last):\r\n  File \"/data6/xinhe/fx_test/test.py\", line 13, in <module>\r\n    assert gen_text == target_text, f\"Expect: {target_text}\\n but get: {gen_text}.\"\r\nAssertionError: Expect: [\"One day, the little girl in the back of my mind will ask me if I'm a\"]\r\n but get: ['One day, the little girl in the back of my mind will say, ??I??m so glad you??'].\r\n```",
    "state": "closed",
    "created_at": "2024-12-16T02:41:27+00:00",
    "closed_at": "2025-01-20T14:12:47+00:00",
    "updated_at": "2025-01-20T14:12:47+00:00",
    "author": "xin3he",
    "author_type": "User",
    "comments_count": 2,
    "reactions_count": 0,
    "labels": "bug",
    "milestone": null,
    "closed_by": "SunMarc",
    "resolution_time_hours": 851.5222222222222,
    "first_comments": [
      {
        "author": "SunMarc",
        "created_at": "2024-12-24T16:23:44+00:00",
        "body": "Thanks for the report @xin3he, can you check which commit in transformers introduced this issue ? Maybe due to attention refactor "
      },
      {
        "author": "github-actions[bot]",
        "created_at": "2025-01-18T08:03:00+00:00",
        "body": "This issue has been automatically marked as stale because it has not had recent activity. If you think this still needs to be addressed please comment on this thread.\n\nPlease note that issues that do not follow the [contributing guidelines](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md) are likely to be ignored."
      }
    ],
    "url": "https://github.com/huggingface/transformers/issues/35286"
  },
  {
    "repository": "huggingface/transformers",
    "issue_number": 35277,
    "title": "Provide support and guidance for training vision-LLM with a customized vision encoder, projector, and LLM.",
    "body": "### Feature request\n\nProvide support and guidance for training vision-LLM with a customized vision encoder, projector, and LLM.\n\n### Motivation\n\nProvide support and guidance for training vision-LLM with a customized vision encoder, projector, and LLM.\n\n### Your contribution\n\nProvide support and guidance for training vision-LLM with a customized vision encoder, projector, and LLM.",
    "state": "closed",
    "created_at": "2024-12-14T16:20:06+00:00",
    "closed_at": "2024-12-16T14:17:03+00:00",
    "updated_at": "2024-12-16T14:17:03+00:00",
    "author": "lucasjinreal",
    "author_type": "User",
    "comments_count": 3,
    "reactions_count": 0,
    "labels": "Feature request",
    "milestone": null,
    "closed_by": "lucasjinreal",
    "resolution_time_hours": 45.94916666666666,
    "first_comments": [
      {
        "author": "elxandre",
        "created_at": "2024-12-14T20:00:10+00:00",
        "body": "hello @lucasjinreal \r\n\r\nTo train a vision-LLM with a customized vision encoder, projector, and language model (LLM), you need to:\r\n\r\n-Define and integrate a custom vision encoder.\r\n-Implement a projector to map vision features to the text embedding space.\r\n-Use a pre-existing language model (LLM) or customize it for the task.\r\n-Ensure compatibility between the components (e.g., embeddings dimensions).\r\n-Provide guidance for loading datasets, fine-tuning, and training.\r\n\r\nHereâ€™s how you can proceed:\r\n\r\n\r\nStep 1: Custom Vision Encoder\r\nImplement or load a custom vision encoder using transformers or torchvision:\r\nIf youâ€™re using a pretrained vision model from transformers, you can modify the existing AutoModel for vision models (e.g., ViT, Swin).\r\nAlternatively, you can use models from torchvision.models (e.g., ResNet, EfficientNet).\r\nEnsure the vision encoder outputs feature embeddings suitable for the projector.\r\n\r\nExample:\r\n\r\n```\r\nfrom transformers import AutoModel\r\nfrom torchvision.models import resnet50\r\n\r\n# Using a pretrained vision model from transformers\r\nvision_encoder = AutoModel.from_pretrained(\"google/vit-base-patch16-224\")\r\n\r\n# Or using a model from torchvision\r\nvision_encoder = resnet50(pretrained=True)\r\n```\r\nStep 2: Custom Projector\r\n\r\nCreate a projector module to map the vision encoderâ€™s output to the LLMâ€™s input embedding space.\r\nUse a simple feedforward network with optional activation (e.g., ReLU) to perform this mapping.\r\n\r\nExample:\r\n\r\n```\r\nimport torch.nn as nn\r\n\r\nclass VisionToTextProjector(nn.Module):\r\n    def __init__(self, vision_dim, text_dim):\r\n        super().__init__()\r\n        self.projector = nn.Sequential(\r\n            nn.Linear(vision_dim, text_dim),\r\n            nn.ReLU(),\r\n            nn.Linear(text_dim, text_dim)\r\n        )\r\n\r\n    def forward(self, vision_features):\r\n        return self.projector(vision_features)\r\n```\r\n        \r\nStep 3: Language Model (LLM)\r\nLoad a language model (e.g., LLaMA, GPT, BERT) using the transformers library.\r\nEnsure the LLM can accept embeddings from the projector.\r\n\r\nExample:\r\n\r\n```\r\nfrom transformers import AutoModelForCausalLM\r\n\r\n# Load a pretrained language model\r\nllm = AutoModelForCausalLM.from_pretrained(\"llava-hf/llava-1.5-7b-hf\")\r\n```\r\n\r\nStep 4: Combine the Modules\r\nCreate a wrapper class to integrate the vision encoder, projector, and LLM.\r\nEnsure the forward pass aligns vision and text inputs.\r\n\r\nExample:\r\n\r\n```\r\nclass VisionLLM(nn.Module):\r\n    def __init__(self, vision_encoder, projector, llm):\r\n        super().__init__()\r\n        self.vision_encoder = vision_encoder\r\n        self.projector = projector\r\n        self.llm = llm\r\n\r\n    def forward(self, vision_inputs, text_inputs):\r\n        # Extract vision features\r\n        vision_features = self.vision_encoder(vision_inputs).last_hidden_state\r\n        # Project vision features to text embedding space\r\n        projected_features = self.projector(vision_features)\r\n        # Pass text and vision embeddings to the language model\r\n        outputs = self.llm(inputs_embeds=projected_features, labels=text_inputs)\r\n        return outputs.loss, outputs.logits\r\n```\r\n        \r\nStep 5: Dataset Preparation\r\nUse a multimodal dataset containing both images and text.\r\nTokenize the text using the LLMâ€™s tokenizer and preprocess the images using the vision encoderâ€™s processor.\r\n\r\nExample:\r\n\r\n```\r\nfrom transformers import AutoTokenizer, AutoImageProcessor\r\n\r\n# Tokenizer for the LLM\r\ntokenizer = AutoTokenizer.from_pretrained(\"llava-hf/llava-1.5-7b-hf\")\r\n# Image processor for the vision encoder\r\nimage_processor = AutoImageProcessor.from_pretrained(\"google/vit-base-patch16-224\")\r\n\r\n# Example preprocessing\r\nimage_inputs = image_processor(images, return_tensors=\"pt\")\r\ntext_inputs = tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True)\r\n```\r\n\r\nStep 6: Training\r\nDefine a loss function and optimizer.\r\nUse the Trainer API from transformers or write a custom PyTorch training loop.\r\n\r\nExample with Trainer:\r\n\r\n```\r\nfrom transformers import Trainer, TrainingArguments\r\n\r\n# Define training arguments\r\ntraining_args = TrainingArguments(\r\n    output_dir=\"./vision-llm\",\r\n    per_device_train_batch_size=8,\r\n    learning_rate=5e-5,\r\n    num_train_epochs=3,\r\n    save_strategy=\"epoch\",\r\n    evaluation_strategy=\"epoch\",\r\n    logging_dir=\"./logs\",\r\n    logging_steps=10\r\n)\r\n\r\n# Define the model\r\nvision_llm = VisionLLM(vision_encoder, projector, llm)\r\n\r\n# Initialize the Trainer\r\ntrainer = Trainer(\r\n    model=vision_llm,\r\n    args=training_args,\r\n    train_dataset=train_dataset,\r\n    eval_dataset=eval_dataset,\r\n    tokenizer=tokenizer\r\n)\r\n\r\n# Train the model\r\ntrainer.train()\r\n```\r\n"
      },
      {
        "author": "lucasjinreal",
        "created_at": "2024-12-15T01:45:02+00:00",
        "body": "@elxandre Hi, thank u so much for the detailed answer.\r\n\r\nHowever, I already made a training script similar to your suggestions, but I found the training didn't goes well. Still can't found a clue. So I am just looking for something inside transformers would do this job.\r\n\r\nThis is current my not working code:\r\n\r\n```python\r\n\r\nclass MyForConditionalGeneration(MyPretrainedModel, Qwen2ForCausalLM):\r\n    def __init__(self, config: MonoConfig):\r\n        # super().__init__(config)\r\n        MyPretrainedModel.__init__(self, config)\r\n        super(Qwen2ForCausalLM, self).__init__(config)\r\n\r\n        self.vision_tower = ViTLarge(config=config.vision_config)\r\n        self._attn_implementation = config._attn_implementation\r\n\r\n        self._build_image_projection_layers(config)\r\n\r\n        self.model = Qwen2Model(config)\r\n        self.vocab_size = config.vocab_size\r\n        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\r\n\r\n        self.pad_token_id = config.pad_token_id\r\n        print(f\"==> pad_token_id: {self.pad_token_id}\")\r\n        self.post_init()\r\n\r\ndef _build_image_projection_layers(self, config):\r\n        image_dim_out = config.vision_config.hidden_size\r\n        dim_projection = config.hidden_size\r\n        self.mm_projector = nn.Linear(image_dim_out, dim_projection)\r\n        print(f\"==> build mm_projector: {image_dim_out} -> {dim_projection}\")\r\n\r\n    def get_vision_tower(self):\r\n        return self.vision_tower\r\n\r\n    def get_input_embeddings(self):\r\n        return self.model.get_input_embeddings()\r\n\r\n    def _encode_image(self, pixel_values):\r\n        # print(f\"pixel_values: {pixel_values}\")\r\n        batch_size, C, H, W = pixel_values.shape\r\n        x = self.vision_tower(pixel_values, output_hidden_states=True)\r\n        x = x[-2]\r\n        x = self.mm_projector(x)\r\n        # print(f\"image features: {x}\")\r\n        return x\r\n\r\n    def _get_input_embeds_with_image(self, input_ids, image_features):\r\n        # 1. replace image token with features; 2. replace -100 in input_ids into zeroes\r\n        # 3. handling right attention_mask\r\n        batch_size = input_ids.size(0)\r\n        processed_embeds = []\r\n        processed_masks = []\r\n\r\n        max_seq_len = 0\r\n        for idx in range(batch_size):\r\n            seq = input_ids[idx]\r\n            im_pos = (seq == -200).nonzero(as_tuple=True)[0]\r\n\r\n            if im_pos.numel() > 0:\r\n                im_pos = im_pos.item()\r\n                before = seq[:im_pos]\r\n                after = seq[im_pos + 1 :]\r\n                # Exclude -100 tokens (maybe, input_ids padding with -100 intentionly)\r\n                before = before[before != -100]\r\n                after = after[after != -100]\r\n                # Get embeddings for before and after\r\n                before_embed = self.get_input_embeddings()(before)\r\n                after_embed = self.get_input_embeddings()(after)\r\n                # Concatenate before, image features, and after\r\n                seq_embed = torch.cat(\r\n                    [before_embed, image_features[idx], after_embed], dim=0\r\n                )\r\n                new_seq_len = seq_embed.size(0)\r\n            else:\r\n                # Exclude -100 tokens\r\n                valid_tokens = seq[seq != -100]\r\n                seq_embed = self.get_input_embeddings()(valid_tokens)\r\n                new_seq_len = seq_embed.size(0)\r\n\r\n            # Update the maximum sequence length\r\n            if new_seq_len > max_seq_len:\r\n                max_seq_len = new_seq_len\r\n\r\n            processed_embeds.append(seq_embed)\r\n            attn_mask = torch.ones(new_seq_len, dtype=torch.bool, device=seq.device)\r\n            processed_masks.append(attn_mask)\r\n\r\n        # rest embedding is 0, rest mask is False, just padding it\r\n        inputs_embeds = torch.nn.utils.rnn.pad_sequence(\r\n            processed_embeds, batch_first=True, padding_value=0.0\r\n        )\r\n        attn_masks = torch.nn.utils.rnn.pad_sequence(\r\n            processed_masks, batch_first=True, padding_value=0\r\n        )\r\n        return inputs_embeds, attn_masks\r\n\r\n    def forward(\r\n        self,\r\n        input_ids: Optional[torch.LongTensor] = None,\r\n        pixel_values: torch.FloatTensor = None,\r\n        attention_mask: Optional[torch.FloatTensor] = None,\r\n        position_ids: Optional[torch.LongTensor] = None,\r\n        past_key_values: Optional[Tuple[Tuple[torch.Tensor]]] = None,\r\n        inputs_embeds: Optional[torch.FloatTensor] = None,\r\n        labels: Optional[torch.LongTensor] = None,\r\n        use_cache: Optional[bool] = None,\r\n        output_attentions: Optional[bool] = None,\r\n        output_hidden_states: Optional[bool] = None,\r\n        return_dict: Optional[bool] = None,\r\n        cache_position=None,\r\n    ) -> Union[Tuple, CausalLMOutputWithPast]:\r\n        output_attentions = (\r\n            output_attentions\r\n            if output_attentions is not None\r\n            else self.config.output_attentions\r\n        )\r\n        output_hidden_states = (\r\n            output_hidden_states\r\n            if output_hidden_states is not None\r\n            else self.config.output_hidden_states\r\n        )\r\n        return_dict = (\r\n            return_dict if return_dict is not None else self.config.use_return_dict\r\n        )\r\n\r\n        image_features = None\r\n        if inputs_embeds is None:\r\n            if pixel_values is not None:\r\n                # (batch_size, num_image_tokens, hidden_size)\r\n                image_features = self._encode_image(pixel_values)\r\n\r\n            if input_ids is not None:\r\n                inputs_embeds, attention_mask = self._get_input_embeds_with_image(\r\n                    input_ids, image_features\r\n                )\r\n\r\n        # padding all to normal sequence length only train\r\n        if labels is not None:\r\n            input_length = inputs_embeds.shape[1]\r\n            label_length = labels.shape[1]\r\n\r\n            if labels is not None:\r\n                labels = F.pad(labels, (input_length, 0), value=-100)\r\n\r\n            if inputs_embeds is not None:\r\n                # append embeds and attn_mask to labels length\r\n                padding = torch.zeros(\r\n                    inputs_embeds.shape[0],\r\n                    label_length,\r\n                    inputs_embeds.shape[2],\r\n                    dtype=inputs_embeds.dtype,\r\n                    device=inputs_embeds.device,\r\n                )\r\n                inputs_embeds = torch.cat([inputs_embeds, padding], dim=1)\r\n                attention_mask = attention_mask.to(inputs_embeds.dtype)\r\n                attention_mask = F.pad(attention_mask, (0, label_length), value=0)\r\n\r\n        return super().forward(\r\n            input_ids=None,\r\n            attention_mask=attention_mask,\r\n            position_ids=position_ids,\r\n            past_key_values=past_key_values,\r\n            inputs_embeds=inputs_embeds,\r\n            labels=labels,\r\n            use_cache=use_cache,\r\n            output_attentions=output_attentions,\r\n            output_hidden_states=output_hidden_states,\r\n            return_dict=return_dict,\r\n        )\r\n```\r\nThis code is responsible for handling input_ids. In this context, a -200 value for the image position placeholder indicates the replacement of image feature tokens.\r\n\r\nThe remaining task is simply to input inputs_embeds into the Language Learning Model (LLM).\r\n\r\nNevertheless, the training loss remains unchanging and does not decrease at all.\r\n\r\nAs a result, the following inquiries arise:\r\n\r\n1. Is there any specific aspect where my implementation might have gone wrong in detail?\r\n2. If my code is not functioning as expected, is there any functionality within the transformers framework that could assist me in accomplishing my task? (It is actually similar to llava, but I do not require such complexity. I simply desire a ViT (Vision Transformer) + LLM combination and aim to train a simple caption).\r\n"
      },
      {
        "author": "zucchini-nlp",
        "created_at": "2024-12-16T11:42:47+00:00",
        "body": "@lucasjinreal hey! This question is better places on the forum (https://discuss.huggingface.co/latest), as we try to reserve GH for bugs and feature requests.\r\n\r\nSince your model is similar to llava, feel free to use th `LlavaForConditionalGeneration` class providing your won vision and text configs to change the LM and vision backbones. For further question please ask in the forum :)"
      }
    ],
    "url": "https://github.com/huggingface/transformers/issues/35277"
  },
  {
    "repository": "huggingface/transformers",
    "issue_number": 35276,
    "title": "inconsistent generation",
    "body": "### System Info\n\n- `transformers` version: 4.45.2\r\n- Python version: 3.8.18\r\n- Huggingface_hub version: 0.26.3\r\n- Safetensors version: 0.4.1\r\n- Accelerate version: 0.32.1\r\n- PyTorch version (GPU?): 2.1.0+cu121 (True)\r\n- GPU type: NVIDIA A10\n\n### Who can help?\n\n_No response_\n\n### Information\n\n- [ ] The official example scripts\n- [x] My own modified scripts\n\n### Tasks\n\n- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\n- [x] My own task or dataset (give details below)\n\n### Reproduction\n\n\r\nI used the same input, but changed the code logic slightly, resulting in different results\r\n\r\nhere is the context of code(mainly load model)\r\n```\r\nimport torch\r\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig, DynamicCache\r\n\r\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\r\nmodel_path = \"meta-llama/Meta-Llama-3-8B-Instruct\"\r\nmodel = AutoModelForCausalLM.from_pretrained(model_path, attn_implementation=\"flash_attention_2\", device_map=device).eval()\r\ntokenizer = AutoTokenizer.from_pretrained(model_path)\r\n\r\nencoded_input = tokenizer(\"what is your name\", return_tensors='pt').to(device)\r\nwindow_size = 1\r\nfront_input = {key: value[:, :-window_size] for key, value in encoded_input.items()}\r\nrear_input = {key: value[:, -window_size:] for key, value in encoded_input.items()}\r\n```\r\n\r\nand here is the first generation code\r\n```\r\npast_key_values = DynamicCache()\r\ngeneration = model.generate(**encoded_input, past_key_values=past_key_values, max_new_tokens=32, do_sample=False)\r\ngeneration = tokenizer.batch_decode(generation)[0]\r\nprint(generation)\r\n```\r\nthe generation is as below:\r\n```\r\nwhat is your name?\" and \"what is your occupation?\" are not necessary. The form is designed to be as simple and easy to fill out as possible, while still gathering the\r\n```\r\n\r\nand the seconde generation code is:\r\n```\r\npast_key_values = DynamicCache()\r\nwith torch.no_grad():\r\n  _ = model(**front_input, past_key_values=past_key_values, use_cache=True)\r\ngeneration = model.generate(**encoded_input, past_key_values=past_key_values, max_new_tokens=32, do_sample=False)\r\ngeneration = tokenizer.batch_decode(generation)[0]\r\n```\r\nthe generation is as below:\r\n```\r\nwhat is your name?\" and \"what is your occupation?\" are not necessary. The form is designed to be as simple and easy to fill out as possible, so that you can\r\n```\n\n### Expected behavior\n\nwell, it's weird, I think these two generation process is the same since I do not use sampling, but why the results are different. Is there anything wrong with my operation?",
    "state": "closed",
    "created_at": "2024-12-14T15:50:00+00:00",
    "closed_at": "2024-12-15T05:34:54+00:00",
    "updated_at": "2024-12-15T05:34:54+00:00",
    "author": "slatter666",
    "author_type": "User",
    "comments_count": 3,
    "reactions_count": 0,
    "labels": "bug",
    "milestone": null,
    "closed_by": "slatter666",
    "resolution_time_hours": 13.748333333333333,
    "first_comments": [
      {
        "author": "slatter666",
        "created_at": "2024-12-14T15:53:01+00:00",
        "body": "but when I change to use A100, the result is the same, OMG why is that"
      },
      {
        "author": "zucchini-nlp",
        "created_at": "2024-12-14T18:16:42+00:00",
        "body": "Hey @slatter666 ,\r\n\r\nSince you are generating with precomputed cache of size `window length` in one of the examples, while in another you generate from the whole input, it might lead to tiny numerical precision errors.\r\n\r\nSee https://github.com/huggingface/transformers/issues/25420#issuecomment-1775317535 for more on why caching can accumulate numerical precision errors"
      },
      {
        "author": "slatter666",
        "created_at": "2024-12-15T05:34:54+00:00",
        "body": "Thank you so much, that solves my issue.\r\n"
      }
    ],
    "url": "https://github.com/huggingface/transformers/issues/35276"
  },
  {
    "repository": "huggingface/transformers",
    "issue_number": 35274,
    "title": "microsoft/Phi-3.5-mini-instruct not working with FA2 due to position_ids",
    "body": "### System Info\n\n- `transformers` version: 4.44.0\r\n- Platform: Linux-5.14.0-427.20.1.el9_4.x86_64-x86_64-with-glibc2.34\r\n- Python version: 3.10.13\r\n- Huggingface_hub version: 0.26.3\r\n- Safetensors version: 0.4.5\r\n- Accelerate version: 0.33.0\r\n- Accelerate config:    - compute_environment: LOCAL_MACHINE\r\n        - distributed_type: MULTI_GPU\r\n        - mixed_precision: bf16\r\n        - use_cpu: False\r\n        - debug: False\r\n        - num_processes: 2\r\n        - machine_rank: 0\r\n        - num_machines: 1\r\n        - gpu_ids: all\r\n        - rdzv_backend: static\r\n        - same_network: True\r\n        - main_training_function: main\r\n        - enable_cpu_affinity: False\r\n        - downcast_bf16: no\r\n        - tpu_use_cluster: False\r\n        - tpu_use_sudo: False\r\n        - tpu_env: []\r\n- PyTorch version (GPU?): 2.4.0+cu121 (True)\r\n- Tensorflow version (GPU?): not installed (NA)\r\n- Flax version (CPU?/GPU?/TPU?): not installed (NA)\r\n- Jax version: not installed\r\n- JaxLib version: not installed\r\n- Using distributed or parallel set-up in script?: <fill in>\r\n- Using GPU in script?: <fill in>\r\n- GPU type: NVIDIA GeForce RTX 3090\r\n\n\n### Who can help?\n\n@ArthurZucker @sunmarc\n\n### Information\n\n- [ ] The official example scripts\n- [ ] My own modified scripts\n\n### Tasks\n\n- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\n- [ ] My own task or dataset (give details below)\n\n### Reproduction\n\n```python\r\nimport torch\r\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\r\n\r\ntokenizer = AutoTokenizer.from_pretrained('microsoft/Phi-3.5-mini-instruct')\r\nmodel = AutoModelForCausalLM.from_pretrained(\r\n    'microsoft/Phi-3.5-mini-instruct',\r\n    device_map={\"\": 0},\r\n    torch_dtype=torch.bfloat16,\r\n    attn_implementation=\"flash_attention_2\"  # Commenting this line will remove the issue\r\n)\r\nmodel.eval()\r\n\r\nmodel = torch.compile(model)\r\n# print(model.config.eos_token_id)\r\nprint(tokenizer.eos_token_id)\r\n\r\ngen_kwargs = {\r\n    \"max_new_tokens\": 1,\r\n    \"pad_token_id\": tokenizer.eos_token_id,\r\n    \"eos_token_id\": tokenizer.eos_token_id,\r\n    \"do_sample\": False,\r\n    \"top_p\": None,\r\n    \"top_k\": None,\r\n    \"temperature\": None,\r\n}\r\nctx_len = 128\r\ndevice = model.device\r\nwith torch.inference_mode():\r\n    inputs = {\r\n        \"input_ids\": torch.LongTensor([[42] * ctx_len]).to(device),\r\n        \"attention_mask\": torch.ones(1, ctx_len).to(device),\r\n        \"position_ids\": torch.arange(ctx_len).unsqueeze(0).to(device),\r\n    }\r\n\r\n    model.generate(**inputs, **gen_kwargs)\r\n\r\n```\r\n\r\nError:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/pricie/vanroy/.config/JetBrains/PyCharm2024.1/scratches/scratch_68.py\", line 35, in <module>\r\n    model.generate(**inputs, **gen_kwargs)\r\n  File \"/home/local/vanroy/clin34-benchmarks/.venv/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/home/local/vanroy/clin34-benchmarks/.venv/lib/python3.10/site-packages/transformers/generation/utils.py\", line 2024, in generate\r\n    result = self._sample(\r\n  File \"/home/local/vanroy/clin34-benchmarks/.venv/lib/python3.10/site-packages/transformers/generation/utils.py\", line 2982, in _sample\r\n    outputs = self(**model_inputs, return_dict=True)\r\n  File \"/home/local/vanroy/clin34-benchmarks/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/home/local/vanroy/clin34-benchmarks/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/home/local/vanroy/clin34-benchmarks/.venv/lib/python3.10/site-packages/transformers/models/phi3/modeling_phi3.py\", line 1247, in forward\r\n    outputs = self.model(\r\n  File \"/home/local/vanroy/clin34-benchmarks/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/home/local/vanroy/clin34-benchmarks/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/home/local/vanroy/clin34-benchmarks/.venv/lib/python3.10/site-packages/transformers/models/phi3/modeling_phi3.py\", line 1051, in forward\r\n    layer_outputs = decoder_layer(\r\n  File \"/home/local/vanroy/clin34-benchmarks/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/home/local/vanroy/clin34-benchmarks/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/home/local/vanroy/clin34-benchmarks/.venv/lib/python3.10/site-packages/transformers/models/phi3/modeling_phi3.py\", line 788, in forward\r\n    attn_outputs, self_attn_weights, present_key_value = self.self_attn(\r\n  File \"/home/local/vanroy/clin34-benchmarks/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/home/local/vanroy/clin34-benchmarks/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/home/local/vanroy/clin34-benchmarks/.venv/lib/python3.10/site-packages/transformers/models/phi3/modeling_phi3.py\", line 543, in forward\r\n    cos, sin = self.rotary_emb(value_states, seq_len=rotary_seq_len)\r\n  File \"/home/local/vanroy/clin34-benchmarks/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/home/local/vanroy/clin34-benchmarks/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/home/local/vanroy/clin34-benchmarks/.venv/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\r\n    return func(*args, **kwargs)\r\nTypeError: Phi3LongRoPEScaledRotaryEmbedding.forward() missing 1 required positional argument: 'position_ids'\r\n```\n\n### Expected behavior\n\nNo error",
    "state": "closed",
    "created_at": "2024-12-14T14:49:24+00:00",
    "closed_at": "2025-01-22T08:03:28+00:00",
    "updated_at": "2025-01-22T08:03:28+00:00",
    "author": "BramVanroy",
    "author_type": "User",
    "comments_count": 2,
    "reactions_count": 0,
    "labels": "bug",
    "milestone": null,
    "closed_by": "github-actions[bot]",
    "resolution_time_hours": 929.2344444444444,
    "first_comments": [
      {
        "author": "zucchini-nlp",
        "created_at": "2024-12-14T18:22:06+00:00",
        "body": "Hey @BramVanroy , this is already fixed and working in v4.45. Can you update with `pip install -U transformers`"
      },
      {
        "author": "github-actions[bot]",
        "created_at": "2025-01-14T08:03:01+00:00",
        "body": "This issue has been automatically marked as stale because it has not had recent activity. If you think this still needs to be addressed please comment on this thread.\n\nPlease note that issues that do not follow the [contributing guidelines](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md) are likely to be ignored."
      }
    ],
    "url": "https://github.com/huggingface/transformers/issues/35274"
  },
  {
    "repository": "huggingface/transformers",
    "issue_number": 35267,
    "title": "Enhance documentation of Automatic speech recognition",
    "body": "The ASR documentation requires improvements in grammar, phrasing, and consistency to ensure clarity and readability. Suggested updates include:\r\n\r\n- Replacing informal language (e.g., \"this'll\" to \"this will\").\r\n- Correcting grammatical errors (e.g., \"everyday\" to \"every day\").\r\n- Refining phrasing to match standard technical documentation style (e.g., \"focuses on\" instead of \"you'll focus on\").",
    "state": "closed",
    "created_at": "2024-12-13T19:54:48+00:00",
    "closed_at": "2024-12-16T17:50:12+00:00",
    "updated_at": "2024-12-16T17:50:12+00:00",
    "author": "Uvi-12",
    "author_type": "User",
    "comments_count": 1,
    "reactions_count": 0,
    "labels": "",
    "milestone": null,
    "closed_by": "stevhliu",
    "resolution_time_hours": 69.92333333333333,
    "first_comments": [
      {
        "author": "Rocketknight1",
        "created_at": "2024-12-16T13:22:03+00:00",
        "body": "cc @stevhliu "
      }
    ],
    "url": "https://github.com/huggingface/transformers/issues/35267"
  },
  {
    "repository": "huggingface/transformers",
    "issue_number": 35265,
    "title": "inconsistent execution time",
    "body": "### System Info\r\n\r\n```\r\n- `transformers` version: 4.28.1\r\n- Platform: Windows-10-10.0.22631-SP0\r\n- Python version: 3.10.6\r\n- Huggingface_hub version: 0.13.4\r\n- Safetensors version: not installed\r\n- PyTorch version (GPU?): 2.0.0+cu118 (True)\r\n- Tensorflow version (GPU?): 2.12.0 (False)\r\n- Flax version (CPU?/GPU?/TPU?): not installed (NA)\r\n- Jax version: not installed\r\n- JaxLib version: not installed\r\n- Using GPU in script?: <fill in>\r\n- Using distributed or parallel set-up in script?: <fill in>\r\n```\r\n\r\n### Who can help?\r\n\r\n@Arther\r\n\r\n### Information\r\n\r\n- [ ] The official example scripts\r\n- [X] My own modified scripts\r\n\r\n### Tasks\r\n\r\n- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\r\n- [X] My own task or dataset (give details below)\r\n\r\n### Reproduction\r\n\r\nBasically, I have 100 rows dataframe, total 10 files. I like to send it each one by one to my 4 GPUs. I use Llama 3. I'm testing the execution or completion time. Now, either use the data-parallel or model-parallel for inference, I got the around same execution time. Let's say\r\n\r\n```\r\n# using 1 GPU\r\none dataframe - > prompt template \r\nresponse = model.generate(prompt)\r\n\r\nexecution time: 20 second.\r\n```\r\n\r\n```\r\n# using 3 GPU \r\n# with data-parallel\r\none dataframe - > prompt template \r\nresponse = model.generate(prompt)\r\n\r\nexecution time: 20 second (GPU:0)\r\nexecution time: 21 second (GPU:0)\r\nexecution time: 19 second (GPU:0)\r\n```\r\n\r\nModel definition\r\n\r\n```python\r\nmodel_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\r\n\r\ntorch_dtype = torch.float16\r\nattn_implementation = \"eager\"\r\n\r\nbnb_config = BitsAndBytesConfig(\r\n    load_in_4bit=True,\r\n    bnb_4bit_quant_type=\"nf4\",\r\n    bnb_4bit_compute_dtype=torch_dtype,\r\n    bnb_4bit_use_double_quant=True,\r\n)\r\nmodel = AutoModelForCausalLM.from_pretrained(\r\n    model_id,\r\n    quantization_config=bnb_config,\r\n    device_map={\"\": torch.cuda.current_device()},\r\n    attn_implementation=attn_implementation\r\n)\r\n\r\npeft_config = LoraConfig(\r\n    r=16,\r\n    lora_alpha=32,\r\n    lora_dropout=0.05,\r\n    bias=\"none\",\r\n    task_type=\"CAUSAL_LM\",\r\n    target_modules=[\r\n        'up_proj', 'down_proj', 'gate_proj', \r\n        'k_proj', 'q_proj', 'v_proj', 'o_proj'\r\n    ]\r\n)\r\nmodel = get_peft_model(model, peft_config)\r\n\r\nfor df_file in tqdm(xcel_list):\r\n    df = pd.read_excel(df_file)\r\n    messages = prepare_prompt_from_excel(df)\r\n\r\n    prompt = tokenizer.apply_chat_template(\r\n        messages, \r\n        tokenize=False, \r\n        add_generation_prompt=True\r\n    )\r\n\r\n    inputs = tokenizer(\r\n        prompt, \r\n        return_tensors='pt', \r\n        padding=True, \r\n        truncation=True\r\n    ).to(\"cuda\")\r\n\r\n    start = time.time()\r\n    outputs = model.generate(\r\n        **inputs, \r\n        max_length=2048, \r\n        num_return_sequences=1\r\n    )\r\n    exe = time.time() - start\r\n\r\n...\r\n```\r\n\r\n### Expected behavior\r\n\r\nIf I use data-parallel on multiple GPU, a replicate of model will be placed to all GPUs and data will be splitted across GPUs.  Or, if I use model-parallel (`device_map=auto'), layers of that model will be distributed across GPUs. Apart from this, if no data or model parallel, just single GPU inference, I was expecting to get longer time for inference, and less time or faster inference if I use multi-gpu, either data or model parallel. But single GPU inference time and multi-gpu inference times are almost comparable. My another concern is that, while using data-parallel, as I am sending one data-frame / prompt template to all GPUs - does this single prompt template gets splitted into many chunks? I doubt that. Each GPUs still see the full data in data-parallel setup here, and that's the cause to get similar execution time, no!",
    "state": "closed",
    "created_at": "2024-12-13T17:41:02+00:00",
    "closed_at": "2025-01-20T14:12:35+00:00",
    "updated_at": "2025-01-20T14:12:35+00:00",
    "author": "pure-rgb",
    "author_type": "User",
    "comments_count": 3,
    "reactions_count": 0,
    "labels": "bug",
    "milestone": null,
    "closed_by": "SunMarc",
    "resolution_time_hours": 908.5258333333334,
    "first_comments": [
      {
        "author": "Rocketknight1",
        "created_at": "2024-12-16T13:19:38+00:00",
        "body": "I think this is really an `accelerate` question, so pinging @muellerzr @SunMarc "
      },
      {
        "author": "SunMarc",
        "created_at": "2024-12-24T15:51:20+00:00",
        "body": "> My another concern is that, while using data-parallel, as I am sending one data-frame / prompt template to all GPUs - does this single prompt template gets splitted into many chunks? I doubt that. Each GPUs still see the full data in data-parallel setup here, and that's the cause to get similar execution time, no!\r\n\r\nYou need to send chunks of a batch automatically to each loaded model. Please read the following documentation : https://huggingface.co/docs/accelerate/en/usage_guides/distributed_inference#distributed-inference"
      },
      {
        "author": "github-actions[bot]",
        "created_at": "2025-01-18T08:03:05+00:00",
        "body": "This issue has been automatically marked as stale because it has not had recent activity. If you think this still needs to be addressed please comment on this thread.\n\nPlease note that issues that do not follow the [contributing guidelines](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md) are likely to be ignored."
      }
    ],
    "url": "https://github.com/huggingface/transformers/issues/35265"
  },
  {
    "repository": "huggingface/transformers",
    "issue_number": 35261,
    "title": "Fix Typos in Audio Classification Documentation",
    "body": "Typos to be fixed:\r\n- \"The MInDS-14 dataset has a sampling rate of 8000khz (you can find this information in it's dataset card)\"- Correct \"it's\" to \"its\" (possessive form) and fix \"khz\" to \"kHz\" for proper capitalization.\r\n- \"The only required parameter is output_dir which specifies where to save your model.\" (Add a comma for grammatical correctness)\r\n",
    "state": "closed",
    "created_at": "2024-12-13T16:16:30+00:00",
    "closed_at": "2024-12-13T17:43:46+00:00",
    "updated_at": "2024-12-13T17:43:46+00:00",
    "author": "Uvi-12",
    "author_type": "User",
    "comments_count": 0,
    "reactions_count": 0,
    "labels": "",
    "milestone": null,
    "closed_by": "stevhliu",
    "resolution_time_hours": 1.4544444444444444,
    "first_comments": [],
    "url": "https://github.com/huggingface/transformers/issues/35261"
  },
  {
    "repository": "huggingface/transformers",
    "issue_number": 35260,
    "title": "Adding CohereForSequenceClassification to Cohere",
    "body": "### Feature request\n\nIâ€™ve noticed that the Cohere models do not include a ForSequenceClassification implementation. Is it due to some issues that prevent this from being done, or was there simply not enough time to implement this requirement?\n\n### Motivation\n\nRecently, I've been learning how to use large models for human dialogue preference classification. I noticed that Cohere has a model called aya-expanse-8b. I wanted to use this model but found that there is no corresponding ForSequenceClassification implementation available for it. However, I noticed that such an implementation is available for Gemma2.\n\n### Your contribution\n\nI have attempted to implement it by following the approach used for Gemma2, but I have not yet validated it.\r\nIf itâ€™s merely a time issue preventing the implementation of ForSequenceClassification in transformers, Iâ€™d be more than willing to contribute by adding this feature for Cohere.",
    "state": "closed",
    "created_at": "2024-12-13T15:16:58+00:00",
    "closed_at": "2024-12-17T09:12:05+00:00",
    "updated_at": "2024-12-17T09:12:05+00:00",
    "author": "liuhaoran124578",
    "author_type": "User",
    "comments_count": 2,
    "reactions_count": 0,
    "labels": "Feature request",
    "milestone": null,
    "closed_by": "liuhaoran124578",
    "resolution_time_hours": 89.9186111111111,
    "first_comments": [
      {
        "author": "Rocketknight1",
        "created_at": "2024-12-16T13:16:21+00:00",
        "body": "I think we'd be happy to accept this PR, yes!"
      },
      {
        "author": "liuhaoran124578",
        "created_at": "2024-12-17T08:58:46+00:00",
        "body": "> I think we'd be happy to accept this PR, yes!\r\n\r\nthxðŸ˜Š\r\n"
      }
    ],
    "url": "https://github.com/huggingface/transformers/issues/35260"
  },
  {
    "repository": "huggingface/transformers",
    "issue_number": 35256,
    "title": " RuntimeError: \"rshift_cuda\" not implemented for 'Half'",
    "body": "### System Info\r\n\r\nHardware details\r\nCPU:Intel(R) Xeon(R) Silver 4410Y\r\nGPU:NVIDIA RTX A6000\r\nSoftware version\r\nVersion of relevant software such as operation system, cuda toolkit, python, auto-gptq, pytorch, transformers, accelerate, etc.\r\noperation system:\r\nDistributor ID: Ubuntu\r\nDescription: Ubuntu 24.04.1 LTS\r\nRelease: 24.04\r\nCodename: noble\r\n\r\ncuda toolkit:\r\nnvcc: NVIDIA (R) Cuda compiler driver\r\nCopyright (c) 2005-2024 NVIDIA Corporation\r\nBuilt on Tue_Oct_29_23:50:19_PDT_2024\r\nCuda compilation tools, release 12.6, V12.6.85\r\nBuild cuda_12.6.r12.6/compiler.35059454_0\r\n\r\nPython 3.10.15\r\n\r\nName: auto_gptq\r\nVersion: 0.7.1\r\n\r\npytorch:2.2.1+cu121\r\ntransformer:4.35.0\r\naccelerate:0.26.0\r\n\r\n### Who can help?\r\n\r\nhttps://huggingface.co/jp288881/slim-llm-llama-7b\r\n\r\n@ArthurZucker @itazap @mueller\r\n\r\n### Information\r\n\r\n- [ ] The official example scripts\r\n- [X] My own modified scripts\r\n\r\n### Tasks\r\n\r\n- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\r\n- [X] My own task or dataset (give details below)\r\n\r\n### Reproduction\r\n\r\nhttps://huggingface.co/jp288881/slim-llm-llama-7b\r\n\r\n### Expected behavior\r\n\r\n  I used the quantization method from Aaronhuang-778/SliM-LLM (https://github.com/Aaronhuang-778/SliM-LLM?tab=readme-ov-file) and wanted to load and use the model , but encountered the issue mentioned in the title.I would like to ask if any experts know how to resolve this issue. I have also uploaded my model to Hugging Face for reference.\r\n\r\n\"Traceback (most recent call last):\r\n  File \"/mnt/8tb_raid/david_model/SliM-LLM/AutoGPTQ/test.py\", line 31, in <module>\r\n    generated_ids = model.generate(**input_ids)\r\n  File \"/mnt/8tb_raid/david_model/SliM-LLM/AutoGPTQ/auto_gptq/modeling/_base.py\", line 480, in generate\r\n    return self.model.generate(**kwargs)\r\n  File \"/home/david/miniconda3/envs/slimllm6/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/home/david/miniconda3/envs/slimllm6/lib/python3.10/site-packages/transformers/generation/utils.py\", line 2252, in generate\r\n    result = self._sample(\r\n  File \"/home/david/miniconda3/envs/slimllm6/lib/python3.10/site-packages/transformers/generation/utils.py\", line 3251, in _sample\r\n    outputs = self(**model_inputs, return_dict=True)\r\n  File \"/home/david/miniconda3/envs/slimllm6/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/home/david/miniconda3/envs/slimllm6/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/home/david/miniconda3/envs/slimllm6/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 1163, in forward\r\n    outputs = self.model(\r\n  File \"/home/david/miniconda3/envs/slimllm6/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/home/david/miniconda3/envs/slimllm6/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/home/david/miniconda3/envs/slimllm6/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 913, in forward\r\n    layer_outputs = decoder_layer(\r\n  File \"/home/david/miniconda3/envs/slimllm6/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/home/david/miniconda3/envs/slimllm6/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/home/david/miniconda3/envs/slimllm6/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 640, in forward\r\n    hidden_states, self_attn_weights, present_key_value = self.self_attn(\r\n  File \"/home/david/miniconda3/envs/slimllm6/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/home/david/miniconda3/envs/slimllm6/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/home/david/miniconda3/envs/slimllm6/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 522, in forward\r\n    query_states = self.q_proj(hidden_states)\r\n  File \"/home/david/miniconda3/envs/slimllm6/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/home/david/miniconda3/envs/slimllm6/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/mnt/8tb_raid/david_model/SliM-LLM/AutoGPTQ/auto_gptq/nn_modules/qlinear/qlinear_cuda.py\", line 319, in forward\r\n    zeros = torch.bitwise_right_shift(\r\nRuntimeError: \"rshift_cuda\" not implemented for 'Half'\r\n\"",
    "state": "closed",
    "created_at": "2024-12-13T10:52:39+00:00",
    "closed_at": "2025-01-21T08:03:26+00:00",
    "updated_at": "2025-01-21T08:03:26+00:00",
    "author": "davidray222",
    "author_type": "User",
    "comments_count": 2,
    "reactions_count": 0,
    "labels": "bug",
    "milestone": null,
    "closed_by": "github-actions[bot]",
    "resolution_time_hours": 933.1797222222223,
    "first_comments": [
      {
        "author": "Rocketknight1",
        "created_at": "2024-12-13T14:17:50+00:00",
        "body": "This isn't really a Transformers issue - it seems like the problem originates in the external quantization library you're using!"
      },
      {
        "author": "github-actions[bot]",
        "created_at": "2025-01-13T08:02:58+00:00",
        "body": "This issue has been automatically marked as stale because it has not had recent activity. If you think this still needs to be addressed please comment on this thread.\n\nPlease note that issues that do not follow the [contributing guidelines](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md) are likely to be ignored."
      }
    ],
    "url": "https://github.com/huggingface/transformers/issues/35256"
  },
  {
    "repository": "huggingface/transformers",
    "issue_number": 35254,
    "title": "Mismatch Between txt img_token and Image Count in Multimodal Processor Causes Debugging",
    "body": "### Feature request\r\n\r\nAs with Qwen2-VL, if the number of img_tokens input in the multimodal processor does not match the number of images, \r\na warning or error should be displayed.\r\n\r\n\r\n### Motivation\r\n\r\n## reproduction code\r\n```python\r\nimport requests\r\nfrom PIL import Image\r\n\r\nfrom transformers import Qwen2VLForConditionalGeneration, Qwen2VLProcessor\r\n\r\n\r\nmodel = Qwen2VLForConditionalGeneration.from_pretrained(\"Qwen/Qwen2-VL-7B-Instruct\")\r\nprocessor = Qwen2VLProcessor.from_pretrained(\"Qwen/Qwen2-VL-7B-Instruct\")\r\n\r\nprompts = [\r\n    f\"USER: {processor.image_token}{processor.image_token}\\nWhat are the things I should be cautious about when I visit this place? What should I bring with me? ASSISTANT:\",\r\n]\r\nimage1 = Image.open(requests.get(\"https://llava-vl.github.io/static/images/view.jpg\", stream=True).raw)\r\n\r\ninputs = processor(images=[image1], text=prompts, return_tensors=\"pt\", padding=True)\r\n```\r\n## env\r\n```\r\n- `transformers` version: 4.47.0.dev0\r\n- Platform: Linux-5.15.0-124-generic-x86_64-with-glibc2.35\r\n- Python version: 3.10.12\r\n- Huggingface_hub version: 0.26.2\r\n- Safetensors version: 0.4.5\r\n- Accelerate version: 1.1.1\r\n```\r\n```\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/qwen2_vl/processing_qwen2_vl.py\", line 139, in __call__\r\n    self.image_token, \"<|placeholder|>\" * (image_grid_thw[index].prod() // merge_length), 1\r\nIndexError: index 1 is out of bounds for dimension 0 with size 1\r\n```\r\n\r\nWhen running the code, an error like the one above occurs. \r\nThe cause is that the number of img_tokens does not match the number of images.\r\n\r\nHowever, the error is not very intuitive, so it took some time to find the cause. \r\nTherefore, I think it would be good to explicitly display a warning or error \r\nwhen the number of img_tokens and images do not match.\r\n\r\n### Your contribution\r\n\r\nIt seems possible to add a statement that explicitly displays an error or warning \r\nwhen the number of img_tokens and images do not match in the multimodal processor.",
    "state": "closed",
    "created_at": "2024-12-13T07:12:08+00:00",
    "closed_at": "2025-01-22T02:46:21+00:00",
    "updated_at": "2025-01-22T02:46:21+00:00",
    "author": "jp1924",
    "author_type": "User",
    "comments_count": 3,
    "reactions_count": 0,
    "labels": "Feature request,Multimodal",
    "milestone": null,
    "closed_by": "jp1924",
    "resolution_time_hours": 955.5702777777777,
    "first_comments": [
      {
        "author": "zucchini-nlp",
        "created_at": "2024-12-13T09:12:23+00:00",
        "body": "@jp1924 I think we already do similar checks for number of images and image tokens in some processors (e.g. Mllama), so it shouldn't hurt adding that in Qwen2-VL. Would like to submit a PR? "
      },
      {
        "author": "jp1924",
        "created_at": "2024-12-13T09:23:50+00:00",
        "body": "@zucchini-nlp \r\nOh, great. Then, how about adding this to the Llava series processor as well, since it is not applied there either?"
      },
      {
        "author": "zucchini-nlp",
        "created_at": "2024-12-16T11:48:39+00:00",
        "body": "@jp1924 feel free to add to other VLMs, though I would say that users must check their inputs before processing as we are adding too many checks at this moment imo"
      }
    ],
    "url": "https://github.com/huggingface/transformers/issues/35254"
  },
  {
    "repository": "huggingface/transformers",
    "issue_number": 35252,
    "title": "xpu: parallelize() not supported for PyTorch XPU backend",
    "body": "With https://github.com/huggingface/transformers/releases/tag/v4.47.0.\r\n\r\nTransforms gpt2, mt5, t5 and umt5 models don't support model parallelism when running on PyTorch XPU backend (on few gpu devices) as can be observed by running Transformers tests - see logs below.\r\n\r\nCan model parallelism be supported for XPU backend?\r\n- [ ] For GPT2 model, https://github.com/huggingface/transformers/pull/35253\r\n- [ ] For MT5 model\r\n- [ ] For T5 model\r\n- [ ] For UMT5 model\r\n\r\n```\r\n$ cat spec.py\r\nimport torch\r\nDEVICE_NAME = 'xpu'\r\nMANUAL_SEED_FN = torch.xpu.manual_seed\r\nEMPTY_CACHE_FN = torch.xpu.empty_cache\r\nDEVICE_COUNT_FN = torch.xpu.device_count\r\n\r\n$ TRANSFORMERS_TEST_DEVICE_SPEC=spec.py python3 -m pytest -rsf tests/models/ -k \"test_model_parallelization or test_model_parallel_equal_results\"\r\n<...>\r\nFAILED tests/models/gpt2/test_modeling_gpt2.py::GPT2ModelTest::test_model_parallel_equal_results - ZeroDivisionError: division by zero\r\nFAILED tests/models/gpt2/test_modeling_gpt2.py::GPT2ModelTest::test_model_parallelization - AssertionError: Torch not compiled with CUDA enabled\r\nFAILED tests/models/mt5/test_modeling_mt5.py::MT5ModelTest::test_model_parallel_equal_results - ZeroDivisionError: division by zero\r\nFAILED tests/models/mt5/test_modeling_mt5.py::MT5ModelTest::test_model_parallelization - AssertionError: Torch not compiled with CUDA enabled\r\nFAILED tests/models/mt5/test_modeling_mt5.py::MT5EncoderOnlyModelTest::test_model_parallel_equal_results - ZeroDivisionError: division by zero\r\nFAILED tests/models/mt5/test_modeling_mt5.py::MT5EncoderOnlyModelTest::test_model_parallelization - AssertionError: Torch not compiled with CUDA enabled\r\nFAILED tests/models/t5/test_modeling_t5.py::T5ModelTest::test_model_parallel_equal_results - ZeroDivisionError: division by zero\r\nFAILED tests/models/t5/test_modeling_t5.py::T5ModelTest::test_model_parallelization - AssertionError: Torch not compiled with CUDA enabled\r\nFAILED tests/models/t5/test_modeling_t5.py::T5EncoderOnlyModelTest::test_model_parallel_equal_results - ZeroDivisionError: division by zero\r\nFAILED tests/models/t5/test_modeling_t5.py::T5EncoderOnlyModelTest::test_model_parallelization - AssertionError: Torch not compiled with CUDA enabled\r\nFAILED tests/models/umt5/test_modeling_umt5.py::UMT5EncoderOnlyModelTest::test_model_parallel_equal_results - AttributeError: 'UMT5EncoderModel' object has no attribute 'parallelize'\r\nFAILED tests/models/umt5/test_modeling_umt5.py::UMT5EncoderOnlyModelTest::test_model_parallelization - AssertionError: Torch not compiled with CUDA enabled\r\n=============================== 12 failed, 682 skipped, 76163 deselected, 5 warnings in 24.79s ================================\r\n```\r\n\r\nCC: @ArthurZucker @SunMarc",
    "state": "closed",
    "created_at": "2024-12-13T00:29:28+00:00",
    "closed_at": "2025-01-07T09:16:36+00:00",
    "updated_at": "2025-01-07T09:16:36+00:00",
    "author": "dvrogozh",
    "author_type": "User",
    "comments_count": 2,
    "reactions_count": 0,
    "labels": "",
    "milestone": null,
    "closed_by": "ydshieh",
    "resolution_time_hours": 608.7855555555556,
    "first_comments": [
      {
        "author": "dvrogozh",
        "created_at": "2024-12-13T20:41:03+00:00",
        "body": "As discussed in https://github.com/huggingface/transformers/pull/35253#pullrequestreview-2502872803, `parallelize()` API is deprecated. As such it's not reasonable to add changes with new features (adding XPU backend support). Instead we've agreed to mark such tests as CUDA specific (https://github.com/huggingface/transformers/pull/35253#issuecomment-2542039642). See the following PR for that:\r\n* https://github.com/huggingface/transformers/pull/35269"
      },
      {
        "author": "Rocketknight1",
        "created_at": "2024-12-16T13:22:29+00:00",
        "body": "cc @SunMarc "
      }
    ],
    "url": "https://github.com/huggingface/transformers/issues/35252"
  },
  {
    "repository": "huggingface/transformers",
    "issue_number": 35248,
    "title": "Hi",
    "body": "Hi \r\n   I have installed the TGI based on Nivida RTX4090, if hope to use text-generation-launcher to trigger the op which is written by triton(such as server/text_generation_server/layers/attention/flash_attn_triton.py), pls tell me the whole command, thanks .\r\n\r\n_Originally posted by @alanguo1234 in https://github.com/huggingface/text-generation-inference/discussions/2759_",
    "state": "closed",
    "created_at": "2024-12-12T18:20:08+00:00",
    "closed_at": "2024-12-19T14:23:44+00:00",
    "updated_at": "2024-12-19T14:23:45+00:00",
    "author": "nosu3380",
    "author_type": "User",
    "comments_count": 0,
    "reactions_count": 0,
    "labels": "",
    "milestone": null,
    "closed_by": "Rocketknight1",
    "resolution_time_hours": 164.06,
    "first_comments": [],
    "url": "https://github.com/huggingface/transformers/issues/35248"
  },
  {
    "repository": "huggingface/transformers",
    "issue_number": 35244,
    "title": "StopStringCriteria relies on `len(tokenizer)==model.config.vocab_size`, leading to index errors",
    "body": "### System Info\n\nPython: 3.12.0\r\nTransformers: 4.46.3\r\n\n\n### Who can help?\n\n@gante\r\n@ArthurZucker\n\n### Information\n\n- [ ] The official example scripts\n- [X] My own modified scripts\n\n### Tasks\n\n- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\n- [X] My own task or dataset (give details below)\n\n### Reproduction\n\nAfter fine-tuning EleutherAI/pythia-14m using transformer's Trainer, I run inference like this:\r\n\r\n```python\r\ncheckpoint = \"models/checkpoint-166000\"\r\ndevice = \"cuda\"\r\n\r\nmodel = AutoModelForCausalLM.from_pretrained(checkpoint)\r\nmodel.to(device)\r\n\r\ntokenizer = AutoTokenizer.from_pretrained(checkpoint, padding_side=\"left\")\r\ntokenizer.pad_token_id = 1\r\ntokenizer.pad_token = \"<|padding|>\"\r\n\r\nprompts = [\r\n    \"prompt1\",\r\n    \"prompt2\",\r\n]\r\ninputs = tokenizer(\r\n    prompts, return_tensors=\"pt\", padding=True, truncation=True, max_length=512,\r\n)\r\n\r\ngen_config = copy.deepcopy(model.generation_config)\r\ngen_config.update(\r\n    max_new_tokens=max_length,\r\n    do_sample=True,\r\n    top_k=0,\r\n    pad_token_id=tokenizer.pad_token_id,\r\n    stop_strings=\"end\",\r\n)\r\ngen_config.validate()\r\n\r\noutputs = model.generate(\r\n    input_ids=inputs[\"input_ids\"].to(device),\r\n    attention_mask=inputs[\"attention_mask\"].to(device),\r\n    num_return_sequences=32,\r\n    generation_config=gen_config,\r\n    output_scores=True,\r\n    return_dict_in_generate=True,\r\n    tokenizer=tokenizer,\r\n)\r\n```\r\n\r\nNote that `tokenizer.pad_token_id` has to be set explicitly because it is not present in Pythia's `special_tokens_map.json`. This code leads to the following error (run with `CUDA_LAUNCH_BLOCKING=1`):\r\n\r\n```\r\n../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [1,0,0], thread: [100,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [1,0,0], thread: [101,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [1,0,0], thread: [102,0,0] Assertion `srcIndex < srcSelectDimSize` failed.      \r\n../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [1,0,0], thread: [103,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\nTraceback (most recent call last):                   \r\n  File \"home/m/src/playground.py\", line 43, in <module>                 \r\n    outputs = model.generate(\r\n              ^^^^^^^^^^^^^^^\r\n  File \"/home/m/venv/lib/python3.12/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context                            \r\n    return func(*args, **kwargs)             \r\n           ^^^^^^^^^^^^^^^^^^^^^                        \r\n  File \"/home/m/venv/lib/python3.12/site-packages/transformers/generation/utils.py\", line 2215, in generate                             \r\n    result = self._sample(\r\n             ^^^^^^^^^^^^^\r\n  File \"/home/m/venv/lib/python3.12/site-packages/transformers/generation/utils.py\", line 3262, in _sample                              \r\n    unfinished_sequences = unfinished_sequences & ~stopping_criteria(input_ids, scores)\r\n                                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/m/venv/lib/python3.12/site-packages/transformers/generation/stopping_criteria.py\", line 496, in __call__                  \r\n    is_done = is_done | criteria(input_ids, scores, **kwargs)\r\n                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/m/venv/lib/python3.12/site-packages/transformers/generation/stopping_criteria.py\", line 402, in __call__                  \r\n    embedded = F.embedding(flipped_ids, self.embedding_vec)\r\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   \r\n  File \"/home/m/venv/lib/python3.12/site-packages/torch/nn/functional.py\", line 2551, in embedding                                      \r\n    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)  \r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nRuntimeError: CUDA error: device-side assert triggered\r\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n```\r\n\r\nThis is due to mismatch between `len(tokenizer)` (50277) and `model.config.vocab_size` (50304 or 50432). This decision to round up the size of the embedding matrix to the next multiple of 128 or 256 was presumably made due to efficiency reasons. However, during sampling, tokens above `len(tokenizer)` can sometimes be generated. This is silently ignored by the tokenizer, converting such tokens to empty string. However, `StopStringCriteria` is implemented by indexing into an embedding with size determined by `len(tokenizer)` and therefore fails when it encounters a higher token.\r\n\r\nA temporary fix is to explicitly suppress the unknown tokens from being generated:\r\n```python\r\nif len(tokenizer) < model.config.vocab_size:\r\n    model.generation_config.suppress_tokens = list(range(len(tokenizer), model.config.vocab_size))\r\n```\r\n\r\nI propose that a more principled solution would to be modify `StopStringCriteria` to ignore tokens above `len(tokenizer)`.\r\n\n\n### Expected behavior\n\nExpected behavior of the `generate` method is to not fail.",
    "state": "closed",
    "created_at": "2024-12-12T16:25:56+00:00",
    "closed_at": "2025-01-20T08:03:39+00:00",
    "updated_at": "2025-01-20T08:03:39+00:00",
    "author": "Kripner",
    "author_type": "User",
    "comments_count": 6,
    "reactions_count": 0,
    "labels": "bug",
    "milestone": null,
    "closed_by": "github-actions[bot]",
    "resolution_time_hours": 927.6286111111111,
    "first_comments": [
      {
        "author": "Rocketknight1",
        "created_at": "2024-12-12T16:38:52+00:00",
        "body": "Hi @kripner - yes, this does look like a bug, although it should only be triggered rarely. I think the simplest solution would be to extend the stop string embedding matrix to `model.config.vocab_size`, but keep everything else the same. The extra rows will basically be 'null', and so the extra tokens cannot contribute to a stop string match, but at least it won't crash the whole library. What do you think?"
      },
      {
        "author": "Kripner",
        "created_at": "2024-12-13T12:00:05+00:00",
        "body": "> Hi @Kripner - yes, this does look like a bug, although it should only be triggered rarely. I think the simplest solution would be to extend the stop string embedding matrix to `model.config.vocab_size`, but keep everything else the same. The extra rows will basically be 'null', and so the extra tokens cannot contribute to a stop string match, but at least it won't crash the whole library. What do you think?\r\n\r\nHi @Rocketknight1, this might not be possible because `StoppingCriteria` does not have access to the model, only to the tokenizer. I think the fix should be in the `__call__` method of `StopStringCriteria` by cropping `flipped_ids` from the first occurrence of an unknown token."
      },
      {
        "author": "Rocketknight1",
        "created_at": "2024-12-13T14:28:03+00:00",
        "body": "@Kripner Sounds good, but to keep static shapes for better compilation, how about:\r\n\r\n- Make a mask array for `flipped_ids >= len(tokenizer)`\r\n- Clip the values in `flipped_ids` to `len(tokenizer) - 1` so there are no more embedding lookup errors\r\n- Apply the mask after the embedding lookup, and set all the masked positions to -1 so that token matches end at that point\r\n\r\nI'm more familiar with XLA, so I don't know how much the torch compiler depends on static shapes, but if there's a static shape solution I think we should use it regardless!"
      },
      {
        "author": "Kripner",
        "created_at": "2024-12-13T16:27:12+00:00",
        "body": "> @Kripner Sounds good, but to keep static shapes for better compilation, how about:\r\n> \r\n> * Make a mask array for `flipped_ids >= len(tokenizer)`\r\n> * Clip the values in `flipped_ids` to `len(tokenizer) - 1` so there are no more embedding lookup errors\r\n> * Apply the mask after the embedding lookup, and set all the masked positions to -1 so that token matches end at that point\r\n> \r\n> I'm more familiar with XLA, so I don't know how much the torch compiler depends on static shapes, but if there's a static shape solution I think we should use it regardless!\r\n\r\n@Rocketknight1 This looks great to me!"
      },
      {
        "author": "Rocketknight1",
        "created_at": "2024-12-13T16:56:59+00:00",
        "body": "@Kripner Cool! Would you be willing to make that PR?"
      }
    ],
    "url": "https://github.com/huggingface/transformers/issues/35244"
  },
  {
    "repository": "huggingface/transformers",
    "issue_number": 35233,
    "title": "Shape mismatch in RoPE embeddings gpt_neox model when rotary_ndims is odd",
    "body": "### System Info\r\n\r\n- `transformers` version: 4.48.0.dev0\r\n- Platform: macOS-15.1.1-arm64-arm-64bit\r\n- Python version: 3.12.7\r\n- Huggingface_hub version: 0.26.5\r\n- Safetensors version: 0.4.5\r\n- Accelerate version: 1.2.0\r\n- Accelerate config: \tnot found\r\n- PyTorch version (GPU?): 2.5.1 (False)\r\n- Tensorflow version (GPU?): not installed (NA)\r\n- Flax version (CPU?/GPU?/TPU?): not installed (NA)\r\n- Jax version: not installed\r\n- JaxLib version: not installed\r\n- Using distributed or parallel set-up in script?: No\r\n\r\n### Who can help?\r\n\r\n_No response_\r\n\r\n### Information\r\n\r\n- [ ] The official example scripts\r\n- [x] My own modified scripts\r\n\r\n### Tasks\r\n\r\n- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\r\n- [x] My own task or dataset (give details below)\r\n\r\n### Reproduction\r\n\r\nI just appended the following to https://github.com/huggingface/transformers/blob/main/src/transformers/models/gpt_neox/modeling_gpt_neox.py:\r\n\r\n```python\r\ndef reproduce_bug():\r\n    # Then:\r\n    # head_size = hidden_size // num_attention_heads = 4\r\n    # rotary_ndims = int(head_size * rotary_pct) = 3\r\n    config = GPTNeoXConfig(\r\n        vocab_size=96,\r\n        max_position_embeddings=32,\r\n        hidden_size=32,\r\n        num_hidden_layers=2,\r\n        num_attention_heads=8,\r\n        intermediate_size=3 * 32,\r\n        rotary_pct=0.75,\r\n        use_parallel_residual=False,\r\n    )\r\n    model = GPTNeoXModel(config)\r\n    input_ids = torch.randint(0, config.vocab_size, (1, config.max_position_embeddings))\r\n    logits = model(input_ids)\r\n    print(f\"logits.shape = {logits.shape}\")\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    reproduce_bug()\r\n```\r\n Then, I ran\r\n```bash\r\npython -m src.transformers.models.gpt_neox.modeling_gpt_neox src/transformers/models/gpt_neox/modeling_gpt_neox.py\r\n```\r\n\r\nThis gives me the following error output:\r\n```text\r\nThe `GPTNeoXSdpaAttention` class is deprecated in favor of simply modifying the `config._attn_implementation`attribute of the `GPTNeoXAttention` class! It will be removed in v4.48\r\nTraceback (most recent call last):\r\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\r\n  File \"<frozen runpy>\", line 88, in _run_code\r\n  File \"/Users/seeger/git/transformers/src/transformers/models/gpt_neox/modeling_gpt_neox.py\", line 1510, in <module>\r\n    reproduce_bug()\r\n  File \"/Users/seeger/git/transformers/src/transformers/models/gpt_neox/modeling_gpt_neox.py\", line 1505, in reproduce_bug\r\n    logits = model(input_ids)\r\n             ^^^^^^^^^^^^^^^^\r\n  File \"/Users/seeger/venvs/transformers_venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/seeger/venvs/transformers_venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/seeger/git/transformers/src/transformers/models/gpt_neox/modeling_gpt_neox.py\", line 910, in forward\r\n    outputs = layer(\r\n              ^^^^^^\r\n  File \"/Users/seeger/venvs/transformers_venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/seeger/venvs/transformers_venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/seeger/git/transformers/src/transformers/models/gpt_neox/modeling_gpt_neox.py\", line 657, in forward\r\n    attention_layer_outputs = self.attention(\r\n                              ^^^^^^^^^^^^^^^\r\n  File \"/Users/seeger/venvs/transformers_venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/seeger/venvs/transformers_venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/seeger/git/transformers/src/transformers/models/gpt_neox/modeling_gpt_neox.py\", line 319, in forward\r\n    query, key, value, present = self._attn_projections_and_rope(\r\n                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/seeger/git/transformers/src/transformers/models/gpt_neox/modeling_gpt_neox.py\", line 431, in _attn_projections_and_rope\r\n    query, key = apply_rotary_pos_emb(query_rot, key_rot, cos, sin)\r\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/seeger/git/transformers/src/transformers/models/gpt_neox/modeling_gpt_neox.py\", line 607, in apply_rotary_pos_emb\r\n    q_embed = (q * cos) + (rotate_half(q) * sin)\r\n               ~~^~~~~\r\nRuntimeError: The size of tensor a (3) must match the size of tensor b (4) at non-singleton dimension 3\r\n```\r\n\r\nThis is what I expected. Your code does not work if `rotary_ndims` is odd. Here, it is 3. The way that `cos`, `sin` are computed gives them a final dim size `2 * ceil(rotary_ndims / 2) == rotary_ndims + 1`, this is 1 too large.\r\n\r\nNote that your code actually \"works\" if `rotary_ndims = 1`. Then, `cos`, `sin` have final dim size 2 and the code above works due to broadcasting (1 broadcast to 2), both `q`, `k` have final dim 1 too large, but that still works. But once `rotary_ndims` is odd and larger than 1, it fails.\r\n\r\n### Expected behavior\r\n\r\nWithout this bug, `cos` and `sin` should have size `rotary_ndims` in the final dimension, no matter whether this is even or odd. My suggestions:\r\n- Restrict `rotary_ndim` to be even, or\r\n- Subselect `cos`, `sin` so their final dim size is `rotary_ndims`\r\n\r\nMy feeling is this does not only affect this single model, but many others as well. But I did not check.",
    "state": "closed",
    "created_at": "2024-12-12T13:01:28+00:00",
    "closed_at": "2025-01-22T08:03:35+00:00",
    "updated_at": "2025-01-22T08:03:35+00:00",
    "author": "mseeger",
    "author_type": "User",
    "comments_count": 12,
    "reactions_count": 0,
    "labels": "bug",
    "milestone": null,
    "closed_by": "github-actions[bot]",
    "resolution_time_hours": 979.0352777777778,
    "first_comments": [
      {
        "author": "Rocketknight1",
        "created_at": "2024-12-12T13:58:41+00:00",
        "body": "Hi @mseeger, thanks for the bug report!\r\n\r\nFirst question: Does this affect any of the major gpt-neox checkpoints on the Hub, or do they all have `rotary_pct` values that don't trigger the issue?\r\n\r\nSecondly: Would you be willing to make a PR to fix this? I think either of the two solutions you suggested are viable, as long as they don't change the output for existing models that aren't broken."
      },
      {
        "author": "mseeger",
        "created_at": "2024-12-12T15:15:52+00:00",
        "body": "Sure, I can do this. I could also try to find out whether any of the other models have the same issue.\r\n\r\nI'd be surprised if any models on the hub surface this issue, since otherwise their creators would have noted, no? But I can do a quick check.\r\n\r\nI'd prefer the second option. Unless I am missing something, since `cos`, `sin` are derived tensors, they'd never be part of a checkpoint. Also, even if `q`, `k` shapes are changing, this only happens in the dimension where their inner products are over, so this would not affect any other sizes."
      },
      {
        "author": "mseeger",
        "created_at": "2024-12-12T15:33:28+00:00",
        "body": "@Rocketknight1 . I found a model where the issue would likely arise:\r\n\r\nhttps://huggingface.co/Isotonic/gpt_neox_225M/blob/main/config.json\r\n\r\nPretty odd one, `hidden_size=1024, num_attention_heads=12, rotary_pct=0.25: head_size=85, rotary_ndims=21`.\r\nNote that 12 * 85 = 1020, does this even work?"
      },
      {
        "author": "mseeger",
        "created_at": "2024-12-12T15:47:31+00:00",
        "body": "https://huggingface.co/mkshing/gpt-neox-285m-init/blob/main/config.json\r\n\r\nAnother one of the same size.\r\nSomebody who knows better than me should write a script to run over all models on the hub."
      },
      {
        "author": "Rocketknight1",
        "created_at": "2024-12-12T16:34:06+00:00",
        "body": "Hmm, yeah - their `num_attention_heads` doesn't seem to evenly divide their `hidden_size`, so I think there are other issues in those checkpoints."
      }
    ],
    "url": "https://github.com/huggingface/transformers/issues/35233"
  },
  {
    "repository": "huggingface/transformers",
    "issue_number": 35230,
    "title": "[i18n-<languageCode>] Translating perf_train_cpu.md to Chinese",
    "body": "I noticed that no one has translated docs/source/en/perf_train_cpu.md before. Could I translate this to Chinese?",
    "state": "closed",
    "created_at": "2024-12-12T11:49:54+00:00",
    "closed_at": "2024-12-14T07:34:37+00:00",
    "updated_at": "2024-12-14T07:34:37+00:00",
    "author": "asdkfjsd",
    "author_type": "User",
    "comments_count": 1,
    "reactions_count": 0,
    "labels": "WIP",
    "milestone": null,
    "closed_by": "asdkfjsd",
    "resolution_time_hours": 43.74527777777778,
    "first_comments": [
      {
        "author": "Rocketknight1",
        "created_at": "2024-12-12T13:50:16+00:00",
        "body": "cc @stevhliu "
      }
    ],
    "url": "https://github.com/huggingface/transformers/issues/35230"
  },
  {
    "repository": "huggingface/transformers",
    "issue_number": 35228,
    "title": "Breaking change due to `multiprocessing.Process` when loading `pytorch_model.bin`-based model ",
    "body": "## Bug Overview\r\n* Loading any `transformers` models fails if:\r\n  * the model only has a `pytorch_model.bin`, and\r\n  * you're not inside of `__name__ == \"__main__\"`\r\n\r\n(Taken from https://github.com/huggingface/transformers/pull/34966#issuecomment-2538598145)\r\n\r\n## Details\r\nIn short: `multiprocessing.Process` never works when not inside of `__name__ == \"__main__\"`. I recognize that most programs should be using that line, but I'd rather not force it on my users.\r\n\r\nIf one of my users loads any model that only has a `pytorch_model.bin`, then it'll fail, e.g.:\r\n```python\r\nfrom sentence_transformers import SentenceTransformer\r\n\r\nmodel = SentenceTransformer(\"embaas/sentence-transformers-gte-base\")\r\n```\r\nor\r\n```python\r\nfrom sentence_transformers import CrossEncoder\r\n\r\nmodel = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L-6-v2\")\r\n```\r\nwhich internally call\r\n```python\r\nfrom transformers import AutoModel\r\n\r\nmodel = AutoModel.from_pretrained(\"embaas/sentence-transformers-gte-base\")\r\n```\r\nor\r\n```python\r\nfrom transformers import AutoModelForSequenceClassification\r\n\r\nmodel = AutoModelForSequenceClassification.from_pretrained(\"cross-encoder/ms-marco-MiniLM-L-6-v2\")\r\n```\r\n\r\nAll of these get:\r\n```\r\nRuntimeError:\r\n        An attempt has been made to start a new process before the\r\n        current process has finished its bootstrapping phase.\r\n\r\n        This probably means that you are not using fork to start your\r\n        child processes and you have forgotten to use the proper idiom\r\n        in the main module:\r\n\r\n            if __name__ == '__main__':\r\n                freeze_support()\r\n                ...\r\n\r\n        The \"freeze_support()\" line can be omitted if the program\r\n        is not going to be frozen to produce an executable.\r\n\r\n        To fix this issue, refer to the \"Safe importing of main module\"\r\n        section in https://docs.python.org/3/library/multiprocessing.html\r\n```\r\n\r\nEdit: To prevent people experiencing errors, I've updated all [`cross-encoder` models](https://huggingface.co/cross-encoder) to safetensors. So you can't reproduce those anymore without specifying the `revision`.\r\n\r\n- Tom Aarsen",
    "state": "closed",
    "created_at": "2024-12-12T11:19:15+00:00",
    "closed_at": "2024-12-12T15:05:05+00:00",
    "updated_at": "2024-12-12T15:05:05+00:00",
    "author": "tomaarsen",
    "author_type": "User",
    "comments_count": 1,
    "reactions_count": 0,
    "labels": "",
    "milestone": null,
    "closed_by": "ydshieh",
    "resolution_time_hours": 3.763888888888889,
    "first_comments": [
      {
        "author": "ydshieh",
        "created_at": "2024-12-12T11:51:28+00:00",
        "body": "On it :-) Thanks for reporting"
      }
    ],
    "url": "https://github.com/huggingface/transformers/issues/35228"
  },
  {
    "repository": "huggingface/transformers",
    "issue_number": 35227,
    "title": "if split_special_tokens==Trueï¼Œfast_tokenizer is slower than slow_tokenizer",
    "body": "### System Info\r\n\r\nif split_special_tokens==Trueï¼Œfast_tokenizer is slower than slow_tokenizer\r\n\r\n### Who can help?\r\n\r\n_No response_\r\n\r\n### Information\r\n\r\n- [ ] The official example scripts\r\n- [ ] My own modified scripts\r\n\r\n### Tasks\r\n\r\n- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\r\n- [ ] My own task or dataset (give details below)\r\n\r\n### Reproduction\r\n\r\n```python\r\nfrom transformers import LlamaTokenizer, LlamaTokenizerFast\r\nimport time\r\ntokenizer1 = LlamaTokenizer.from_pretrained(\"./Llama-2-7b-chat-hf\", split_special_tokens=True) # LlamaTokenizer\r\ntokenizer2 = LlamaTokenizerFast.from_pretrained(\"./Llama-2-7b-chat-hf\", split_special_tokens=True) # LlamaTokenizer\r\nprint(tokenizer1, tokenizer2)\r\n\r\ns_time = time.time()\r\nfor i in range(1000):\r\n    tokenizer1.tokenize(\"ä½ å¥½ï¼Œwhere are you?\"*100)\r\nprint(f\"slow: {time.time() - s_time}\")\r\n\r\ns_time = time.time()\r\nfor i in range(1000):\r\n    tokenizer2.tokenize(\"ä½ å¥½ï¼Œwhere are you?\"*100)\r\nprint(f\"fast: {time.time() - s_time}\")\r\n```\r\noutput: \r\n        slow: 0.6021890640258789\r\n        fast: 0.7353882789611816\r\n\r\n### Expected behavior\r\n\r\n-",
    "state": "closed",
    "created_at": "2024-12-12T10:05:30+00:00",
    "closed_at": "2025-01-25T08:04:07+00:00",
    "updated_at": "2025-01-25T12:10:42+00:00",
    "author": "gongel",
    "author_type": "User",
    "comments_count": 4,
    "reactions_count": 0,
    "labels": "bug",
    "milestone": null,
    "closed_by": "github-actions[bot]",
    "resolution_time_hours": 1053.9769444444444,
    "first_comments": [
      {
        "author": "Rocketknight1",
        "created_at": "2024-12-12T13:47:43+00:00",
        "body": "cc @arthurzucker @itazap "
      },
      {
        "author": "ArthurZucker",
        "created_at": "2024-12-23T10:54:53+00:00",
        "body": "Hey! This is not related to `split_special_tokens`! \r\nSee #25873 ðŸ¤— "
      },
      {
        "author": "github-actions[bot]",
        "created_at": "2025-01-17T08:03:18+00:00",
        "body": "This issue has been automatically marked as stale because it has not had recent activity. If you think this still needs to be addressed please comment on this thread.\n\nPlease note that issues that do not follow the [contributing guidelines](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md) are likely to be ignored."
      },
      {
        "author": "gongel",
        "created_at": "2025-01-25T12:10:40+00:00",
        "body": "thx~"
      }
    ],
    "url": "https://github.com/huggingface/transformers/issues/35227"
  },
  {
    "repository": "huggingface/transformers",
    "issue_number": 35225,
    "title": "Typo in audio_utils.py: \"indentifier\" should be \"identifier\"",
    "body": "There is a typo in audio_utils.py where the word \"indentifier\" is used instead of \"identifier\".",
    "state": "closed",
    "created_at": "2024-12-12T09:41:45+00:00",
    "closed_at": "2024-12-12T13:45:06+00:00",
    "updated_at": "2024-12-12T13:45:06+00:00",
    "author": "Uvi-12",
    "author_type": "User",
    "comments_count": 0,
    "reactions_count": 0,
    "labels": "",
    "milestone": null,
    "closed_by": "Rocketknight1",
    "resolution_time_hours": 4.055833333333333,
    "first_comments": [],
    "url": "https://github.com/huggingface/transformers/issues/35225"
  },
  {
    "repository": "huggingface/transformers",
    "issue_number": 35223,
    "title": "Deleting quantization_config broken",
    "body": "### System Info\r\n\r\nVersion 4.47.0\r\n\r\n### Reproduction\r\n\r\n```\r\nfrom transformers import AutoModelForCausalLM\r\nmodel = AutoModelForCausalLM.from_pretrained(\"h2oai/h2o-danube3-500m-chat\", load_in_4bit=True)\r\ndel model.config.quantization_config\r\nmodel.config\r\n```\r\n\r\n`TypeError: Object of type dtype is not JSON serializable`\r\n\r\n### Expected behavior\r\n\r\nI am unable to delete the `quantization_config` from an existing model. Whenever I do it, it just completely breaks the whole config.\r\n\r\nI also tried setting `is_quantized=False` but it does not change anything.\r\n\r\nIs there another way of achieving this?\r\n\r\nI am aware that there is a `.dequantize` function, but in this case Im changing dtypes on my own and want to exclude that `quantization_config` particularly when saving the model.",
    "state": "closed",
    "created_at": "2024-12-12T07:33:25+00:00",
    "closed_at": "2025-01-17T10:34:33+00:00",
    "updated_at": "2025-01-17T10:34:33+00:00",
    "author": "psinger",
    "author_type": "User",
    "comments_count": 6,
    "reactions_count": 0,
    "labels": "bug",
    "milestone": null,
    "closed_by": "SunMarc",
    "resolution_time_hours": 867.0188888888889,
    "first_comments": [
      {
        "author": "Rocketknight1",
        "created_at": "2024-12-12T13:36:09+00:00",
        "body": "I think this might be a question for the forums/Discord, but pinging @sunmarc @MekkCyber just in case"
      },
      {
        "author": "MekkCyber",
        "created_at": "2024-12-18T12:50:37+00:00",
        "body": "Hey @psinger, what's the reason you want to delete the quantization_config after the model is loaded ?"
      },
      {
        "author": "psinger",
        "created_at": "2024-12-18T12:55:12+00:00",
        "body": "Because I am manually dequantizing and am not relying on the HF functionality for it. And then before pushing to hub, I want to remove the quantization_config from the config."
      },
      {
        "author": "MekkCyber",
        "created_at": "2024-12-18T15:49:17+00:00",
        "body": "Thanks for the clarification @psinger, I think you can make it work as follows : \r\n```\r\nfrom transformers import AutoModelForCausalLM\r\n\r\nmodel = AutoModelForCausalLM.from_pretrained(\r\n    \"h2oai/h2o-danube3-500m-chat\", load_in_4bit=True\r\n)\r\n\r\nconfig_dict = model.config.to_dict()\r\nif \"quantization_config\" in config_dict:\r\n    del config_dict[\"quantization_config\"]\r\n\r\nfrom transformers import PretrainedConfig\r\n\r\nmodel.config = PretrainedConfig.from_dict(config_dict)\r\nprint(model.config)\r\n```"
      },
      {
        "author": "SunMarc",
        "created_at": "2024-12-23T17:24:06+00:00",
        "body": "Does it fix the issue @psinger ? "
      }
    ],
    "url": "https://github.com/huggingface/transformers/issues/35223"
  }
]