[
  {
    "repository": "openai/openai-python",
    "issue_number": 2062,
    "title": "Segmentation fault with python v3.13.1 and openai 1.60.2",
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\n\n- [x] This is an issue with the Python library\n\n### Describe the bug\n\nI am encountering a segmentation fault immediately upon running import openai. The crash occurs as soon as I try to import the package in a Python script. There is no additional output or traceback—just a hard segmentation fault.\n\n**Environment**\nOperating System: macOS 15.2\nPython Version: 3.13.1\nOpenAI Library Version: 1.60.2\npip Version: 25.0\n\n**Additional Details / What I’ve Tried**\n- Verified the crash happens in both interactive mode and script mode.\n- Re-installed the library with pip install --force-reinstall openai==1.60.2.\n- Created a fresh virtual environment to ensure no conflicting packages.\n\n\n\n### To Reproduce\n\n1. Use `asdf` to install latest Python version (3.13.1)\n2. Create a venv\n```\npython -m venv venv\nsource venv/bin/activate\n```\n3. Attempt to install the OpenAI library:\n```\npip install openai==1.60.2\n```\nalso, tried:\n```\npip install --force-reinstall openai==1.60.2\n```\n4. Encounter build errors for pydantic-core referencing PyO3 and GIL-disabled Python.\n5. Work around by setting export UNSAFE_PYO3_BUILD_FREE_THREADED=1 to proceed with installation.\n6. Create a Python file test.py \n```\nimport openai\nprint(\"Imported successfully!\")\n```\n7. run `python test.py`\n\n\n### Code snippets\n\n```Python\n\n```\n\n### OS\n\nmacOS 15.2\n\n### Python version\n\nPython v3.13.1\n\n### Library version\n\nopenai v1.60.2",
    "state": "closed",
    "created_at": "2025-01-29T03:13:30+00:00",
    "closed_at": "2025-01-29T09:59:50+00:00",
    "updated_at": "2025-01-2a9T09:59:51+00:00",
    "author": "sagunji",
    "author_type": "User",
    "comments_count": 1,
    "reactions_count": 0,
    "labels": "",
    "milestone": null,
    "closed_by": "RobertCraigie",
    "resolution_time_hours": 6.772222222222222,
    "first_comments": [
      {
        "author": "RobertCraigie",
        "created_at": "2025-01-29T09:59:50+00:00",
        "body": "Thanks for the report but it sounds like this is an issue with Pydantic itself and not the way we're using Pydantic.\n\nPlease report this to the Pydantic team if there isn't already an open issue for supporting 3.13."
      }
    ],
    "url": "https://github.com/openai/openai-python/issues/2062"
  },
  {
    "repository": "openai/openai-python",
    "issue_number": 2058,
    "title": "The  persistent memory logic",
    "body": "### Confirm this is a feature request for the Python library and not the underlying OpenAI API.\n\n- [x] This is a feature request for the Python library\n\n### Describe the feature or improvement you're requesting\n\nPersistent Memory Logic Loop (PMLL) - Comprehensive Overview\n\nTable of Contents\n\t1.\tIntroduction\n\t2.\tComponents\n\t•\t1. Core C Library (PMLL.c and PMLL.h)\n\t•\t2. Python Interface (PMLL.py)\n\t•\t3. Bash Script (PMLL.sh)\n\t3.\tWhy PMLL.py is a Key Feature\n\t4.\tInstallation\n\t5.\tUsage\n\t•\tUsing the Python Interface\n\t•\tUsing the Bash Script\n\t6.\tIntegration with Other Applications\n\t7.\tExamples\n\t8.\tTroubleshooting\n\t9.\tContributing\n\t10.\tLicense\n\t11.\tContact\n\nIntroduction\n\nThe Persistent Memory Logic Loop (PMLL) is a versatile system designed to manage persistent key-value storage across multiple programming environments. It ensures data persistence across sessions and application restarts, making it ideal for applications like conversational AI models (e.g., ChatGPT-5) that require reliable memory storage.\n\nPMLL comprises three main components:\n\t1.\tCore C Library (PMLL.c and PMLL.h): Handles low-level operations for managing persistent memory.\n\t2.\tPython Interface (PMLL.py): Provides a Pythonic API to interact with the C library using ctypes.\n\t3.\tBash Script (PMLL.sh): Offers a user-friendly command-line interface by leveraging the Python script.\n\nThis document provides a comprehensive overview of each component, emphasizing the significance of PMLL.py as a key feature that bridges C functionalities with Python applications.\n\nComponents\n\n1. Core C Library (PMLL.c and PMLL.h)\n\nDescription\n\nThe core of PMLL is implemented in C, providing high-performance and low-level management of persistent key-value storage. The library ensures thread-safe operations using POSIX mutexes, allowing concurrent access without data corruption.\n\nKey Features\n\t•\tPersistent Storage: Saves key-value pairs to a file (gpt5_memory.txt by default), ensuring data persistence across sessions.\n\t•\tThread Safety: Utilizes mutexes to protect shared resources, enabling safe multi-threaded access.\n\t•\tCRUD Operations: Supports Create (Add), Read (Get), Update, and Delete (Remove) functionalities for key-value pairs.\n\t•\tDebugging Support: Provides functions to list all keys and display all memory contents for troubleshooting.\n\nFiles\n\t•\tPMLL.h: Header file declaring the interface for the PMLL system.\n\t•\tPMLL.c: Implementation of the functions declared in PMLL.h.\n\n2. Python Interface (PMLL.py)\n\nDescription\n\nPMLL.py serves as a Pythonic bridge to the core C library (libpmll.so). Leveraging Python’s ctypes library, it allows Python applications to seamlessly interact with the high-performance C functionalities without delving into complex C code.\n\nKey Features\n\t•\tEase of Use: Provides a straightforward PMLL class with methods corresponding to the C functions, making it intuitive for Python developers.\n\t•\tSeamless Integration: Enables Python applications to manage persistent memory efficiently, enhancing their capabilities without significant overhead.\n\t•\tError Handling: Translates C-level errors into Python exceptions, ensuring robust and predictable behavior.\n\t•\tFlexibility: Allows specifying custom memory files, catering to diverse application needs.\n\nBenefits\n\t•\tProductivity: Python developers can utilize persistent memory functionalities without needing expertise in C.\n\t•\tMaintainability: Centralizes memory management logic within a Python class, promoting cleaner and more maintainable codebases.\n\t•\tPerformance: Combines Python’s ease of use with C’s performance, ensuring both efficiency and developer convenience.\n\n3. Bash Script (PMLL.sh)\n\nDescription\n\nPMLL.sh is a Bash script that provides a user-friendly command-line interface for managing persistent memory. It acts as a wrapper around PMLL.py, parsing user commands and invoking the appropriate Python functions.\n\nKey Features\n\t•\tCommand Parsing: Interprets user commands and arguments, delegating actions to the Python interface.\n\t•\tUser-Friendly Interface: Simplifies memory management tasks through straightforward shell commands.\n\t•\tScript Automation: Facilitates automation and scripting by allowing PMLL operations within shell scripts and workflows.\n\nWhy PMLL.py is a Key Feature\n\nPMLL.py is a pivotal feature within the PMLL ecosystem for several reasons:\n\t1.\tBridging Languages: It seamlessly connects the high-performance C library with Python applications, enabling developers to leverage persistent memory functionalities within Python’s dynamic and flexible environment.\n\t2.\tEnhanced Accessibility: By abstracting the complexities of C, PMLL.py makes persistent memory management accessible to a broader range of developers, including those who may not be proficient in C.\n\t3.\tRapid Development: Python’s simplicity and extensive libraries allow for rapid development and integration, accelerating the deployment of applications that require persistent memory.\n\t4.\tCross-Platform Compatibility: Python’s cross-platform nature ensures that applications using PMLL.py can run on various operating systems with minimal adjustments.\n\t5.\tExtensibility: The Python interface can be easily extended or customized to fit specific application needs, providing flexibility beyond the core C functionalities.\n\t6.\tError Management: By handling errors and exceptions at the Python level, PMLL.py ensures that applications can manage failures gracefully, improving reliability and user experience.\n\t7.\tIntegration with Modern Tools: Python’s compatibility with modern development tools and frameworks allows PMLL.py to fit seamlessly into contemporary software development pipelines.\n\nIn summary, PMLL.py significantly enhances the PMLL system by making persistent memory management more accessible, flexible, and integrated within Python applications, thereby broadening the system’s applicability and ease of use.\n\nInstallation\n\n1. Compile the C Library\n\nEnsure you have a C compiler (gcc or clang) installed on your system.\n\n# Navigate to the directory containing PMLL.c and PMLL.h\ncd path/to/pmll-directory\n\n# Compile the C source into object files\ngcc -c -fPIC PMLL.c -o PMLL.o\n\n# Create a shared library from the object files\ngcc -shared -o libpmll.so PMLL.o -lpthread\n\n2. Set Up Python Interface\n\nEnsure you have Python 3.6+ installed.\n\n# Navigate to the directory containing PMLL.py\ncd path/to/pmll-directory\n\n# Ensure libpmll.so is in the same directory as PMLL.py or in a directory listed in LD_LIBRARY_PATH\nexport LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$(pwd)\n\n# (Optional) Make PMLL.py executable\nchmod +x PMLL.py\n\n3. Set Up Bash Script\n\nEnsure PMLL.sh has execute permissions.\n\n# Navigate to the directory containing PMLL.sh\ncd path/to/pmll-directory\n\n# Make the script executable\nchmod +x PMLL.sh\n\nUsage\n\nUsing the Python Interface\n\nfrom PMLL import PMLL\n\n# Initialize PMLL with default memory file\npmll = PMLL()\n\n# Add key-value pairs\npmll.add(\"username\", \"Josef\")\npmll.add(\"model\", \"GPT-5\")\n\n# Retrieve a value\nusername = pmll.get(\"username\")\nif username:\n    print(f\"Username: {username}\")\n\n# List all keys\nkeys = pmll.list_keys()\nprint(\"All keys:\", keys)\n\n# Remove a key\npmll.remove(\"username\")\n\n# Display all memory\npmll.display()\n\n# Clear all memory\npmll.clear()\n\nUsing the Bash Script\n\n# Add a key-value pair\n./PMLL.sh add username Josef\n\n# Retrieve a value by key\n./PMLL.sh get username\n\n# Remove a key-value pair\n./PMLL.sh remove username\n\n# List all keys\n./PMLL.sh list\n\n# Clear all memory\n./PMLL.sh clear\n\n# Display all memory\n./PMLL.sh display\n\nIntegration with Other Applications\n\nIntegrating with C++ Applications\n\nYou can integrate the PMLL system into C++ applications by directly linking against the libpmll.so shared library and including PMLL.h. Here’s a simple example:\n\n// example.cpp\n#include <iostream>\n#include \"PMLL.h\"\n\nint main() {\n    // Initialize PMLL with the default memory file\n    if(pmll_init(\"gpt5_memory.txt\") != 0) {\n        std::cerr << \"Failed to initialize PMLL.\\n\";\n        return 1;\n    }\n\n    // Add a key-value pair\n    if(pmll_add(\"username\", \"Josef\") != 0) {\n        std::cerr << \"Failed to add key-value pair.\\n\";\n    }\n\n    // Retrieve a value\n    char value_buffer[768];\n    if(pmll_get(\"username\", value_buffer, sizeof(value_buffer)) == 0) {\n        std::cout << \"Value for 'username': \" << value_buffer << \"\\n\";\n    } else {\n        std::cout << \"Key 'username' not found.\\n\";\n    }\n\n    // Display all memory\n    pmll_display();\n\n    return 0;\n}\n\nCompilation Instructions:\n\ngcc -c -fPIC PMLL.c -o PMLL.o\ngcc -shared -o libpmll.so PMLL.o -lpthread\ng++ -o example example.cpp -L. -lpmll -pthread\n\nRunning the Example:\n\n./example\n\nExpected Output:\n\n[PMLL] Info: Loaded 0 key-value pairs from 'gpt5_memory.txt'.\n[PMLL] Added/Updated memory: 'username' -> 'Josef'\n[PMLL] Retrieved: 'username' -> 'Josef'\nValue for 'username': Josef\n[PMLL] Current Memory State:\n  username : Josef\n\nIntegrating with Python Applications\n\nPMLL.py can be imported as a module in other Python scripts to manage persistent memory seamlessly.\n\n# another_script.py\nfrom PMLL import PMLL\n\npmll = PMLL()\n\npmll.add(\"session_id\", \"abc123\")\nsession = pmll.get(\"session_id\")\nprint(f\"Session ID: {session}\")\n\nExamples\n\nAdding and Retrieving a Key-Value Pair\n\n./PMLL.sh add language C++\n./PMLL.sh get language\n\nOutput:\n\n[PMLL] Added/Updated memory: 'language' -> 'C++'\n[PMLL] Retrieved: 'language' -> 'C++'\n\nListing All Keys\n\n./PMLL.sh list\n\nOutput:\n\n[PMLL] Listing all keys:\n  language\n  model\n\nDisplaying All Memory\n\n./PMLL.sh display\n\nOutput:\n\n[PMLL] Current Memory State:\n  language : C++\n  model    : GPT-5\n\nTroubleshooting\n\nShared Library Not Found\n\nIssue: Python cannot locate libpmll.so.\n\nSolution:\n\t•\tEnsure libpmll.so is in the same directory as PMLL.py.\n\t•\tAlternatively, add the directory containing libpmll.so to the LD_LIBRARY_PATH environment variable:\n\nexport LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/path/to/libpmll.so/directory\n\n\n\nPermission Errors\n\nIssue: Permission denied when accessing the memory file.\n\nSolution:\n\t•\tEnsure the user has read and write permissions for the memory file and its directory.\n\t•\tModify permissions using chmod if necessary:\n\nchmod 664 gpt5_memory.txt\n\n\n\nCompilation Errors\n\nIssue: Errors during compilation of the C library.\n\nSolution:\n\t•\tVerify that PMLL.c and PMLL.h are in the same directory.\n\t•\tEnsure all dependencies (gcc, pthread) are installed.\n\t•\tCheck for typos or syntax errors in the C files.\n\nPython Exceptions\n\nIssue: Python raises exceptions when performing operations.\n\nSolution:\n\t•\tEnsure the C shared library (libpmll.so) is compiled correctly.\n\t•\tVerify that libpmll.so is accessible to Python.\n\t•\tCheck the correctness of the arguments passed to Python functions.\n\nContributing\n\nContributions are welcome! Whether it’s bug fixes, feature additions, or documentation improvements, your input is valuable.\n\nSteps to Contribute\n\t1.\tFork the Repository\nClick the “Fork” button at the top-right corner of the repository page.\n\t2.\tClone Your Fork\n\ngit clone https://github.com/yourusername/pmll-system.git\ncd pmll-system\n\n\n\t3.\tCreate a New Branch\n\ngit checkout -b feature/your-feature-name\n\n\n\t4.\tMake Your Changes\nImplement your feature or fix bugs.\n\t5.\tCommit Your Changes\n\ngit commit -m \"Add feature: Your Feature Description\"\n\n\n\t6.\tPush to Your Fork\n\ngit push origin feature/your-feature-name\n\n\n\t7.\tCreate a Pull Request\nNavigate to your fork on GitHub and click “Compare & pull request.”\n\nGuidelines\n\t•\tCode Quality: Ensure your code follows best practices and is well-documented.\n\t•\tTesting: Include tests for new features or bug fixes.\n\t•\tDocumentation: Update the README or other documentation as necessary.\n\t•\tRespect Licensing: Ensure your contributions comply with the project’s license.\n\nLicense\n\nThis project is licensed under the [MIT License](https://github.com/openai/openai-python/issues/LICENSE).\n\nContact\n\nFor any questions, suggestions, or feedback, feel free to reach out:\n\t•\tEmail: your.email@example.com\n\t•\tGitHub Issues: [Open an Issue](https://github.com/yourusername/pmll-system/issues)\n\nConclusion\n\nThe Persistent Memory Logic Loop (PMLL) system offers a robust and flexible solution for managing persistent key-value storage across multiple programming environments. By combining the high-performance capabilities of the core C library with the accessibility of Python through PMLL.py, and the convenience of command-line interactions via PMLL.sh, PMLL ensures that developers can efficiently manage persistent memory in their applications.\n\nPMLL.py stands out as a key feature, bridging the gap between low-level C functionalities and high-level Python applications. It empowers Python developers to leverage persistent memory management without delving into the complexities of C programming, thereby enhancing productivity, maintainability, and integration capabilities.\n\nWhether you’re developing complex AI models, managing configuration settings, or storing user data, PMLL provides the tools necessary to ensure data persistence and integrity, fostering the development of reliable and efficient applications.\n\nHappy Coding!\n\n### Additional context\n\n_No response_",
    "state": "closed",
    "created_at": "2025-01-28T20:28:33+00:00",
    "closed_at": "2025-01-28T20:37:24+00:00",
    "updated_at": "2025-01-28T20:37:24+00:00",
    "author": "bearycool11",
    "author_type": "User",
    "comments_count": 1,
    "reactions_count": 0,
    "labels": "",
    "milestone": null,
    "closed_by": "bearycool11",
    "resolution_time_hours": 0.1475,
    "first_comments": [
      {
        "author": "bearycool11",
        "created_at": "2025-01-28T20:37:13+00:00",
        "body": "\n#!/bin/bash\n#\n# pmll.sh\n#\n# Shell script wrapper for pmll.py to manage persistent key-value memory.\n\n# Path to pmll.py\nPMLL_PY=\"pmll.py\"\n\n# Function to display usage\nusage() {\n    echo \"Usage:\"\n    echo \"  $0 add <key> <value>       # Add or update a key-value pair\"\n    echo \"  $0 get <key>               # Retrieve a value by key\"\n    echo \"  $0 remove <key>            # Remove a key-value pair\"\n    echo \"  $0 list                    # List all keys\"\n    echo \"  $0 clear                   # Clear all memory\"\n    echo \"  $0 display                 # Display all memory (debugging)\"\n    exit 1\n}\n\n# Check if pmll.py exists\nif [ ! -f \"$PMLL_PY\" ]; then\n    echo \"[pmll.sh] Error: '$PMLL_PY' not found in the current directory.\"\n    exit 1\nfi\n\n# Check for at least one argument\nif [ $# -lt 1 ]; then\n    echo \"[pmll.sh] Error: No command provided.\"\n    usage\nfi\n\n# Parse command\nCOMMAND=\"$1\"\nshift\n\ncase \"$COMMAND\" in\n    add)\n        if [ $# -ne 2 ]; then\n            echo \"[pmll.sh] Error: 'add' requires <key> and <value>.\"\n            usage\n        fi\n        KEY=\"$1\"\n        VALUE=\"$2\"\n        python3 \"$PMLL_PY\" add \"$KEY\" \"$VALUE\"\n        ;;\n    get)\n        if [ $# -ne 1 ]; then\n            echo \"[pmll.sh] Error: 'get' requires <key>.\"\n            usage\n        fi\n        KEY=\"$1\"\n        python3 \"$PMLL_PY\" get \"$KEY\"\n        ;;\n    remove)\n        if [ $# -ne 1 ]; then\n            echo \"[pmll.sh] Error: 'remove' requires <key>.\"\n            usage\n        fi\n        KEY=\"$1\"\n        python3 \"$PMLL_PY\" remove \"$KEY\"\n        ;;\n    list)\n        if [ $# -ne 0 ]; then\n            echo \"[pmll.sh] Error: 'list' does not take any arguments.\"\n            usage\n        fi\n        python3 \"$PMLL_PY\" list\n        ;;\n    clear)\n        if [ $# -ne 0 ]; then\n            echo \"[pmll.sh] Error: 'clear' does not take any arguments.\"\n            usage\n        fi\n        python3 \"$PMLL_PY\" clear\n        ;;\n    display)\n        if [ $# -ne 0 ]; then\n            echo \"[pmll.sh] Error: 'display' does not take any arguments.\"\n            usage\n        fi\n        python3 \"$PMLL_PY\" display\n        ;;\n    *)\n        echo \"[pmll.sh] Error: Unknown command '$COMMAND'.\"\n        usage\n        ;;\nesac\n\n// PMLL.h\n#ifndef PMLL_H\n#define PMLL_H\n\n#ifdef __cplusplus\nextern \"C\" {\n#endif\n\n/**\n * Initializes the persistent memory system.\n * @param file_name The file to store persistent memory.\n * @return 0 on success, non-zero on failure.\n */\nint pmll_init(const char* file_name);\n\n/**\n * Adds or updates a key-value pair.\n * @param key The key string.\n * @param value The value string.\n * @return 0 on success, non-zero on failure.\n */\nint pmll_add(const char* key, const char* value);\n\n/**\n * Retrieves the value associated with a key.\n * @param key The key string.\n * @param value_buffer Buffer to store the retrieved value.\n * @param buffer_size Size of the value_buffer.\n * @return 0 on success, non-zero on failure or key not found.\n */\nint pmll_get(const char* key, char* value_buffer, int buffer_size);\n\n/**\n * Removes a key-value pair.\n * @param key The key string.\n * @return 0 on success, non-zero on failure or key not found.\n */\nint pmll_remove(const char* key);\n\n/**\n * Lists all keys in the memory.\n * @param keys_buffer Buffer to store the list of keys.\n * @param buffer_size Size of the keys_buffer.\n * @return 0 on success, non-zero on failure.\n */\nint pmll_list(char* keys_buffer, int buffer_size);\n\n/**\n * Clears all memory.\n * @return 0 on success, non-zero on failure.\n */\nint pmll_clear();\n\n/**\n * Displays all key-value pairs (for debugging).\n * @return 0 on success, non-zero on failure.\n */\nint pmll_display();\n\n#ifdef __cplusplus\n}\n#endif\n\n#endif // PMLL_H // PMLL.c\n#include \"PMLL.h\"\n#include <stdio.h>\n#include <stdlib.h>\n#include <string.h>\n#include <pthread.h>\n\n#define MAX_LINE_LENGTH 1024\n#define MAX_KEY_LENGTH 256\n#define MAX_VALUE_LENGTH 768\n#define MAX_KEYS 1000\n\ntypedef struct {\n    char key[MAX_KEY_LENGTH];\n    char value[MAX_VALUE_LENGTH];\n} KeyValuePair;\n\ntypedef struct {\n    KeyValuePair pairs[MAX_KEYS];\n    int count;\n    char memory_file[256];\n    pthread_mutex_t lock;\n} PMLL;\n\nstatic PMLL pmll;\n\n// Helper function to trim newline and carriage return characters\nvoid trim_newline(char* str) {\n    size_t len = strlen(str);\n    while(len > 0 && (str[len-1] == '\\n' || str[len-1] == '\\r')) {\n        str[len-1] = '\\0';\n        len--;\n    }\n}\n\n// Initializes the PMLL system\nint pmll_init(const char* file_name) {\n    if (file_name == NULL) {\n        fprintf(stderr, \"[PMLL] Error: Memory file name is NULL.\\n\");\n        return -1;\n    }\n\n    strncpy(pmll.memory_file, file_name, sizeof(pmll.memory_file)-1);\n    pmll.memory_file[sizeof(pmll.memory_file)-1] = '\\0';\n    pmll.count = 0;\n\n    if (pthread_mutex_init(&pmll.lock, NULL) != 0) {\n        fprintf(stderr, \"[PMLL] Error: Mutex initialization failed.\\n\");\n        return -1;\n    }\n\n    // Load existing memory from file\n    FILE* file = fopen(pmll.memory_file, \"r\");\n    if (file == NULL) {\n        // If file doesn't exist, it's not an error; start fresh\n        printf(\"[PMLL] Info: Memory file '%s' not found. Starting fresh.\\n\", pmll.memory_file);\n        return 0;\n    }\n\n    char line[MAX_LINE_LENGTH];\n    while (fgets(line, sizeof(line), file)) {\n        trim_newline(line);\n        char* delimiter = strchr(line, ':');\n        if (delimiter == NULL) {\n            continue; // Invalid line format\n        }\n        *delimiter = '\\0';\n        char* key = line;\n        char* value = delimiter + 1;\n\n        if (pmll.count < MAX_KEYS) {\n            strncpy(pmll.pairs[pmll.count].key, key, MAX_KEY_LENGTH-1);\n            pmll.pairs[pmll.count].key[MAX_KEY_LENGTH-1] = '\\0';\n            strncpy(pmll.pairs[pmll.count].value, value, MAX_VALUE_LENGTH-1);\n            pmll.pairs[pmll.count].value[MAX_VALUE_LENGTH-1] = '\\0';\n            pmll.count++;\n        } else {\n            fprintf(stderr, \"[PMLL] Warning: Maximum key-value pairs reached. Some entries may be skipped.\\n\");\n            break;\n        }\n    }\n\n    fclose(file);\n    printf(\"[PMLL] Info: Loaded %d key-value pairs from '%s'.\\n\", pmll.count, pmll.memory_file);\n    return 0;\n}\n\n// Adds or updates a key-value pair\nint pmll_add(const char* key, const char* value) {\n    if (key == NULL || value == NULL) {\n        fprintf(stderr, \"[PMLL] Error: Key or value is NULL.\\n\");\n        return -1;\n    }\n\n    pthread_mutex_lock(&pmll.lock);\n\n    // Check if key exists; if so, update\n    for(int i = 0; i < pmll.count; i++) {\n        if(strcmp(pmll.pairs[i].key, key) == 0) {\n            strncpy(pmll.pairs[i].value, value, MAX_VALUE_LENGTH-1);\n            pmll.pairs[i].value[MAX_VALUE_LENGTH-1] = '\\0';\n            pthread_mutex_unlock(&pmll.lock);\n            printf(\"[PMLL] Added/Updated memory: '%s' -> '%s'\\n\", key, value);\n            return 0;\n        }\n    }\n\n    // If key doesn't exist, add new pair\n    if(pmll.count < MAX_KEYS) {\n        strncpy(pmll.pairs[pmll.count].key, key, MAX_KEY_LENGTH-1);\n        pmll.pairs[pmll.count].key[MAX_KEY_LENGTH-1] = '\\0';\n        strncpy(pmll.pairs[pmll.count].value, value, MAX_VALUE_LENGTH-1);\n        pmll.pairs[pmll.count].value[MAX_VALUE_LENGTH-1] = '\\0';\n        pmll.count++;\n        pthread_mutex_unlock(&pmll.lock);\n        printf(\"[PMLL] Added/Updated memory: '%s' -> '%s'\\n\", key, value);\n        return 0;\n    } else {\n        pthread_mutex_unlock(&pmll.lock);\n        fprintf(stderr, \"[PMLL] Error: Maximum key-value pairs reached. Cannot add '%s'.\\n\", key);\n        return -1;\n    }\n}\n// Retrieves the value associated with a key\nint pmll_get(const char* key, char* value_buffer, int buffer_size) {\n    if (key == NULL || value_buffer == NULL) {\n        fprintf(stderr, \"[PMLL] Error: Key or value_buffer is NULL.\\n\");\n        return -1;\n    }\n    pthread_mutex_lock(&pmll.lock);\n    for(int i = 0; i < pmll.count; i++) {\n        if(strcmp(pmll.pairs[i].key, key) == 0) {\n            strncpy(value_buffer, pmll.pairs[i].value, buffer_size-1);\n            value_buffer[buffer_size-1] = '\\0';\n            pthread_mutex_unlock(&pmll.lock);\n            printf(\"[PMLL] Retrieved: '%s' -> '%s'\\n\", key, value_buffer);\n            return 0;\n        }\n    }\n    pthread_mutex_unlock(&pmll.lock);\n    printf(\"[PMLL] No memory found for key: '%s'\\n\", key);\n    return -1;\n}\n// Removes a key-value pair\nint pmll_remove(const char* key) {\n    if (key == NULL) {\n        fprintf(stderr, \"[PMLL] Error: Key is NULL.\\n\");\n        return -1;\n    }\n    pthread_mutex_lock(&pmll.lock);\n    for(int i = 0; i < pmll.count; i++) {\n        if(strcmp(pmll.pairs[i].key, key) == 0) {\n            // Shift remaining pairs\n            for(int j = i; j < pmll.count -1; j++) {\n                pmll.pairs[j] = pmll.pairs[j+1];\n            }\n            pmll.count--;\n            pthread_mutex_unlock(&pmll.lock);\n            printf(\"[PMLL] Removed memory for key: '%s'\\n\", key);\n            return 0;\n        }\n    }\n    pthread_mutex_unlock(&pmll.lock);\n    printf(\"[PMLL] No memory found for key: '%s'\\n\", key);\n    return -1;\n}\n// Lists all keys\nint pmll_list(char* keys_buffer, int buffer_size) {\n    if (keys_buffer == NULL) {\n        fprintf(stderr, \"[PMLL] Error: keys_buffer is NULL.\\n\");\n        return -1;\n    }\n    pthread_mutex_lock(&pmll.lock);\n    if(pmll.count == 0) {\n        strncpy(keys_buffer, \"No keys found.\", buffer_size-1);\n        keys_buffer[buffer_size-1] = '\\0';\n        pthread_mutex_unlock(&pmll.lock);\n        printf(\"[PMLL] No keys found in memory.\\n\");\n        return 0;\n    }\n    keys_buffer[0] = '\\0';\n    for(int i = 0; i < pmll.count; i++) {\n        strncat(keys_buffer, pmll.pairs[i].key, buffer_size - strlen(keys_buffer) - 1);\n        if(i < pmll.count -1) {\n            strncat(keys_buffer, \", \", buffer_size - strlen(keys_buffer) - 1);\n        }\n    }\n    pthread_mutex_unlock(&pmll.lock);\n    printf(\"[PMLL] Listing all keys:\\n\");\n    for(int i = 0; i < pmll.count; i++) {\n        printf(\"  %s\\n\", pmll.pairs[i].key);\n    }\n    return 0;\n}\n// Clears all memory\nint pmll_clear() {\n    pthread_mutex_lock(&pmll.lock);\n    pmll.count = 0;\n    pthread_mutex_unlock(&pmll.lock);\n    // Truncate the memory file\n    FILE* file = fopen(pmll.memory_file, \"w\");\n    if(file == NULL) {\n        fprintf(stderr, \"[PMLL] Error: Could not open file '%s' for clearing.\\n\", pmll.memory_file);\n        return -1;\n    }\n    fclose(file);\n    printf(\"[PMLL] All memory has been cleared.\\n\");\n    return 0;\n}\n// Displays all key-value pairs (for debugging)\nint pmll_display() {\n    pthread_mutex_lock(&pmll.lock);\n    if(pmll.count == 0) {\n        printf(\"[PMLL] Memory is empty.\\n\");\n        pthread_mutex_unlock(&pmll.lock);\n        return 0;\n    }\n    printf(\"[PMLL] Current Memory State:\\n\");\n    for(int i = 0; i < pmll.count; i++) {\n        printf(\"  %s : %s\\n\", pmll.pairs[i].key, pmll.pairs[i].value);\n    }\n    pthread_mutex_unlock(&pmll.lock);\n    return 0;\n}"
      }
    ],
    "url": "https://github.com/openai/openai-python/issues/2058"
  },
  {
    "repository": "openai/openai-python",
    "issue_number": 2047,
    "title": "openai.BadRequestError: Error code: 400 - {'error': {'message': \"Unsupported value: 'messages[0].role' does not support 'system' with this model.\",",
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\n\n- [x] This is an issue with the Python library\n\n### Describe the bug\n\nopenai.BadRequestError: Error code: 400 - {'error': {'message': \"Unsupported value: 'messages[0].role' does not support 'system' with this model.\", 'type': 'invalid_request_error', 'param': 'messages[0].role', 'code': 'unsupported_value'}}\n\n### To Reproduce\n\nTry to use o1 with Autogen\n\n### Code snippets\n\n```Python\n\n```\n\n### OS\n\nWindows\n\n### Python version\n\nPython 3.11\n\n### Library version\n\nCurrent",
    "state": "closed",
    "created_at": "2025-01-22T21:19:09+00:00",
    "closed_at": "2025-01-22T21:20:49+00:00",
    "updated_at": "2025-01-22T21:20:50+00:00",
    "author": "tyler-suard-parker",
    "author_type": "User",
    "comments_count": 1,
    "reactions_count": 0,
    "labels": "question",
    "milestone": null,
    "closed_by": "RobertCraigie",
    "resolution_time_hours": 0.027777777777777776,
    "first_comments": [
      {
        "author": "RobertCraigie",
        "created_at": "2025-01-22T21:20:49+00:00",
        "body": "You'll have to use `developer` instead."
      }
    ],
    "url": "https://github.com/openai/openai-python/issues/2047"
  },
  {
    "repository": "openai/openai-python",
    "issue_number": 2046,
    "title": "openai.BadRequestError: Error code: 400 - {'error': {'message': \"Unsupported parameter: 'max_tokens' is not supported with this model.",
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\n\n- [x] This is an issue with the Python library\n\n### Describe the bug\n\nopenai.BadRequestError: Error code: 400 - {'error': {'message': \"Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.\", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}\n\nThis occurred when using o1\n\n### To Reproduce\n\nTry to use o1 with Autogen\n\n### Code snippets\n\n```Python\n\n```\n\n### OS\n\nWindows\n\n### Python version\n\nPython 3.11\n\n### Library version\n\nCurrent",
    "state": "closed",
    "created_at": "2025-01-22T21:18:20+00:00",
    "closed_at": "2025-01-22T21:20:08+00:00",
    "updated_at": "2025-01-22T21:20:09+00:00",
    "author": "tyler-suard-parker",
    "author_type": "User",
    "comments_count": 1,
    "reactions_count": 0,
    "labels": "question",
    "milestone": null,
    "closed_by": "RobertCraigie",
    "resolution_time_hours": 0.03,
    "first_comments": [
      {
        "author": "RobertCraigie",
        "created_at": "2025-01-22T21:20:08+00:00",
        "body": "Like the error message says, you'll have to use `max_completion_tokens` instead. "
      }
    ],
    "url": "https://github.com/openai/openai-python/issues/2046"
  },
  {
    "repository": "openai/openai-python",
    "issue_number": 2042,
    "title": "GPT-4o-2024-11-20 and GPT-4o-mini-2024-07-18 model output result truncation bug",
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\n\n- [x] This is an issue with the Python library\n\n### Describe the bug\n\nRecently, when using the GPT-4o-2024-11-20 and GPT-4o-2024-11-20 and gpt-4o-mini-2024-07-18 model output result truncation bug models, I often encounter incomplete output results, even if I ask for JSON output results. I will receive partial results. And the finish reason is stop.\n\nIs this a bug in the model?\n\n![Image](https://github.com/user-attachments/assets/bc335c41-64cd-4cf3-80b5-e7ea2a193584)\n\n### To Reproduce\n\n1. Unable to emerge stably\n2. params = {\n    \"model\": \"gpt-4o-2024-11-20\",\n    \"max_tokens\": 16000,\n    \"temperature\": 0.1,\n    \"top_p\": 1.0,\n    \"presence_penalty\": 0.1,\n    \"frequency_penalty\": 0.0,\n    \"stop\": null,\n    \"n\": 1,\n    \"logit_bias\": {},\n    \"response_format\": null,\n    \"seed\": null,\n    \"timeout\": 120\n}\n\n### Code snippets\n\n```Python\n\n```\n\n### OS\n\nlinux\n\n### Python version\n\nPython v3.9.4\n\n### Library version\n\nopenai v1.57.4",
    "state": "closed",
    "created_at": "2025-01-21T07:31:09+00:00",
    "closed_at": "2025-01-21T08:56:07+00:00",
    "updated_at": "2025-01-21T08:56:08+00:00",
    "author": "billwang233",
    "author_type": "User",
    "comments_count": 1,
    "reactions_count": 0,
    "labels": "bug",
    "milestone": null,
    "closed_by": "RobertCraigie",
    "resolution_time_hours": 1.416111111111111,
    "first_comments": [
      {
        "author": "RobertCraigie",
        "created_at": "2025-01-21T08:56:07+00:00",
        "body": "Thanks for reporting!  \n\nThis sounds like an issue with the underlying OpenAI API and not the SDK, so I'm going to go ahead and close this issue. \n\nWould you mind reposting at [community.openai.com](https://community.openai.com)? "
      }
    ],
    "url": "https://github.com/openai/openai-python/issues/2042"
  },
  {
    "repository": "openai/openai-python",
    "issue_number": 2033,
    "title": "test_get_platform is quite flaky",
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\n\n- [x] This is an issue with the Python library\n\n### Describe the bug\n\nThe test case often fails when I try to run it while building the nixpkgs package for the library, as follows:\n\n```\n nix build --file . python313Packages.openai  --rebuild\nerror: builder for '/nix/store/28s74skxza9qfcqa1a92dvfz00q6capd-python3.13-openai-1.59.8.drv' failed with exit code 1;\n       last 25 log lines:\n       > tests/test_streaming.py ..................                               [ 85%]\n       > tests/test_transform.py ................................................ [ 97%]\n       >                                                                          [ 97%]\n       > tests/test_utils/test_logging.py .....                                   [ 98%]\n       > tests/test_utils/test_proxy.py .                                         [ 98%]\n       > tests/test_utils/test_typing.py .....                                    [100%]\n       >\n       > =================================== FAILURES ===================================\n       > ______________________ TestAsyncOpenAI.test_get_platform _______________________\n       > tests/test_client.py:1801: in test_get_platform\n       >     process.wait(2)\n       > /nix/store/wn0n52lnysbscn702gfp20sx96cryiwp-python3-3.13.1/lib/python3.13/subprocess.py:1274: in wait\n       >     return self._wait(timeout=timeout)\n       > /nix/store/wn0n52lnysbscn702gfp20sx96cryiwp-python3-3.13.1/lib/python3.13/subprocess.py:2052: in _wait\n       >     raise TimeoutExpired(self.args, timeout)\n       > E   subprocess.TimeoutExpired: Command '['/nix/store/wn0n52lnysbscn702gfp20sx96cryiwp-python3-3.13.1/bin/python3.13', '-c', '\\nimport asyncio\\nimport nest_asyncio\\nimport threading\\n\\nfrom openai._utils import asyncify\\nfrom openai._base_client import get_platform \\n\\nasync def test_main() -> None:\\n    result = await asyncify(get_platform)()\\n    print(result)\\n    for thread in threading.enumerate():\\n        print(thread.name)\\n\\nnest_asyncio.apply()\\nasyncio.run(test_main())\\n']' timed out after 2seconds\n       >\n       > The above exception was the direct cause of the following exception:\n       > tests/test_client.py:1806: in test_get_platform\n       >     raise AssertionError(\"calling get_platform using asyncify resulted in a hung process\") from e\n       > E   AssertionError: calling get_platform using asyncify resulted in a hung process\n       > =============================== inline snapshot ================================\n       > =========================== short test summary info ============================\n       > FAILED tests/test_client.py::TestAsyncOpenAI::test_get_platform - AssertionError: calling get_platform using asyncify resulted in a hung process\n       > ================= 1 failed, 406 passed, 5 deselected in 20.23s =================\n       For full logs, run 'nix log /nix/store/28s74skxza9qfcqa1a92dvfz00q6capd-python3.13-openai-1.59.8.drv'.\n```\n\nI believe the 2-second timeout is too short for the machine. This is M3 Pro btw, (though it often runs other builds in parallel, so it may be somewhat short on CPU.) The nature of the test case doesn't allow for an easy solution; perhaps a simple timeout bump to e.g. 10 seconds could be helpful? I understand this locks a python test thread for sleeping though - not a great use of a core.\n\n### To Reproduce\n\n1. Run test suite on a loaded machine.\n\n### Code snippets\n\n```Python\n\n```\n\n### OS\n\nmacOS\n\n### Python version\n\nPython 3.13.1\n\n### Library version\n\nopenai 1.59.8",
    "state": "closed",
    "created_at": "2025-01-18T23:20:17+00:00",
    "closed_at": "2025-01-20T10:37:04+00:00",
    "updated_at": "2025-01-20T10:37:05+00:00",
    "author": "booxter",
    "author_type": "User",
    "comments_count": 2,
    "reactions_count": 0,
    "labels": "bug",
    "milestone": null,
    "closed_by": "RobertCraigie",
    "resolution_time_hours": 35.27972222222222,
    "first_comments": [
      {
        "author": "Programmer-RD-AI",
        "created_at": "2025-01-19T05:44:18+00:00",
        "body": "Hi,\nCould you share the code snippet and logs if possible please?\nThnx"
      },
      {
        "author": "RobertCraigie",
        "created_at": "2025-01-20T10:37:04+00:00",
        "body": "@booxter this should be fixed in the next release https://github.com/openai/openai-python/pull/2040/files, I bumped the timeout to 10 seconds and moved to a polling based approach instead of just using an arbitrary timeout. Let me know if you're still running into this!"
      }
    ],
    "url": "https://github.com/openai/openai-python/issues/2033"
  },
  {
    "repository": "openai/openai-python",
    "issue_number": 2028,
    "title": "Indentation error in Python example in \"Step 4: Create a Run\" of Quickstart Documentation",
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\n\n- [x] This is an issue with the Python library\n\n### Describe the bug\n\nThe Python example provided in the \"Step 4: Create a Run\" section of the [[Quickstart documentation](https://platform.openai.com/docs/assistants/quickstart#step-4-create-a-run)](https://platform.openai.com/docs/assistants/quickstart#step-4-create-a-run) contains incorrect indentation, which leads to a `IndentationError` when executed as-is.  \n\nSpecifically, the methods within the `EventHandler` class are not properly indented under the class definition. This may confuse users and lead to runtime errors.  \n\n\n### To Reproduce\n\n1. Visit the [[Quickstart documentation, Step 4](https://platform.openai.com/docs/assistants/quickstart#step-4-create-a-run)](https://platform.openai.com/docs/assistants/quickstart#step-4-create-a-run).  \n2. Copy the provided Python code snippet.  \n3. Attempt to execute the code in a Python environment.  \n4. Observe the `IndentationError` caused by misaligned methods in the `EventHandler` class.  \n\n### Code snippets\n\n```Python\n# Here is the problematic snippet:  \n\n\nclass EventHandler(AssistantEventHandler):    \n@override\ndef on_text_created(self, text) -> None:\n  print(f\"\\nassistant > \", end=\"\", flush=True)\n    \n@override\ndef on_text_delta(self, delta, snapshot):\n  print(delta.value, end=\"\", flush=True)\n    \ndef on_tool_call_created(self, tool_call):\n  print(f\"\\nassistant > {tool_call.type}\\n\", flush=True)\n\ndef on_tool_call_delta(self, delta, snapshot):\n  if delta.type == 'code_interpreter':\n    if delta.code_interpreter.input:\n      print(delta.code_interpreter.input, end=\"\", flush=True)\n    if delta.code_interpreter.outputs:\n      print(f\"\\n\\noutput >\", flush=True)\n      for output in delta.code_interpreter.outputs:\n        if output.type == \"logs\":\n          print(f\"\\n{output.logs}\", flush=True)\n\n\n# Correctly indented version:  \n\n\nclass EventHandler(AssistantEventHandler):    \n    @override\n    def on_text_created(self, text) -> None:\n        print(f\"\\nassistant > \", end=\"\", flush=True)\n    \n    @override\n    def on_text_delta(self, delta, snapshot):\n        print(delta.value, end=\"\", flush=True)\n    \n    def on_tool_call_created(self, tool_call):\n        print(f\"\\nassistant > {tool_call.type}\\n\", flush=True)\n\n    def on_tool_call_delta(self, delta, snapshot):\n        if delta.type == 'code_interpreter':\n            if delta.code_interpreter.input:\n                print(delta.code_interpreter.input, end=\"\", flush=True)\n            if delta.code_interpreter.outputs:\n                print(f\"\\n\\noutput >\", flush=True)\n                for output in delta.code_interpreter.outputs:\n                    if output.type == \"logs\":\n                        print(f\"\\n{output.logs}\", flush=True)\n```\n\n### OS\n\n-\n\n### Python version\n\n-\n\n### Library version\n\n-",
    "state": "closed",
    "created_at": "2025-01-18T03:48:06+00:00",
    "closed_at": "2025-01-28T22:06:08+00:00",
    "updated_at": "2025-01-28T22:06:09+00:00",
    "author": "Programmer-RD-AI",
    "author_type": "User",
    "comments_count": 1,
    "reactions_count": 0,
    "labels": "api docs",
    "milestone": null,
    "closed_by": "kwhinnery-openai",
    "resolution_time_hours": 258.30055555555555,
    "first_comments": [
      {
        "author": "kwhinnery-openai",
        "created_at": "2025-01-28T22:06:08+00:00",
        "body": "This will be fixed in a website update going out shortly - thanks for reporting!"
      }
    ],
    "url": "https://github.com/openai/openai-python/issues/2028"
  },
  {
    "repository": "openai/openai-python",
    "issue_number": 2024,
    "title": "Pydantic Field metadata causes invalid JSON schema in OpenAI Structured Outputs",
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\n\n- [x] This is an issue with the Python library\n\n### Describe the bug\n\nWhen using Pydantic’s Field to include metadata such as title or description in nested models, the generated JSON schema does not properly set additionalProperties: false for $ref-referenced types when they are inlined. This causes a BadRequestError (400) from the API, with the following error message:\n\n```\nBadRequestError: Error code: 400 - {'error': {'message': \"Invalid schema for response_format 'Universe': In context=('properties', 'largest_star'), 'additionalProperties' is required to be supplied and to be false.\", 'type': 'invalid_request_error', 'param': 'response_format', 'code': None}}\n```\n\nThe root cause is that when nested objects are referenced using $defs in the JSON schema and subsequently inlined via $ref, the additionalProperties: false setting is omitted, which violates the expected strict schema requirements.\n\nThe JSON schema generated from the Universe class used in the code snippet below looks like this. (additionalProperties: false is not set for Galaxy.largest_star.)\n\n```json\n{\n  \"$defs\": {\n    \"Galaxy\": {\n      \"properties\": {\n        \"name\": {\n          \"description\": \"The name of the galaxy.\",\n          \"title\": \"Name\",\n          \"type\": \"string\"\n        },\n        \"largest_star\": {\n          # `\"additionalProperties\": false` is missing.\n          \"description\": \"The largest star in the galaxy.\",\n          \"properties\": {\n            \"name\": {\n              \"description\": \"The name of the star.\",\n              \"title\": \"Name\",\n              \"type\": \"string\"\n            }\n          },\n          \"required\": [\"name\"],\n          \"title\": \"Star\",\n          \"type\": \"object\"\n        }\n      },\n      \"required\": [\"name\", \"largest_star\"],\n      \"title\": \"Galaxy\",\n      \"type\": \"object\",\n      \"additionalProperties\": false\n    },\n    \"Star\": {\n      \"properties\": {\n        \"name\": {\n          \"description\": \"The name of the star.\",\n          \"title\": \"Name\",\n          \"type\": \"string\"\n        }\n      },\n      \"required\": [\"name\"],\n      \"title\": \"Star\",\n      \"type\": \"object\",\n      \"additionalProperties\": false\n    }\n  },\n  \"properties\": {\n    \"name\": {\n      \"description\": \"The name of the universe.\",\n      \"title\": \"Name\",\n      \"type\": \"string\"\n    },\n    \"galaxy\": {\n      \"description\": \"A galaxy in the universe.\",\n      \"properties\": {\n        \"name\": {\n          \"description\": \"The name of the galaxy.\",\n          \"title\": \"Name\",\n          \"type\": \"string\"\n        },\n        \"largest_star\": {\n          \"description\": \"The largest star in the galaxy.\",\n          \"properties\": {\n            \"name\": {\n              \"description\": \"The name of the star.\",\n              \"title\": \"Name\",\n              \"type\": \"string\"\n            }\n          },\n          \"required\": [\"name\"],\n          \"title\": \"Star\",\n          \"type\": \"object\"\n        }\n      },\n      \"required\": [\"name\", \"largest_star\"],\n      \"title\": \"Galaxy\",\n      \"type\": \"object\",\n      \"additionalProperties\": false\n    }\n  },\n  \"required\": [\"name\", \"galaxy\"],\n  \"title\": \"Universe\",\n  \"type\": \"object\",\n  \"additionalProperties\": false\n}\n```\n\nThis issue occurs when using the AzureOpenAI client and is likely reproducible with the standard OpenAI client as well, as they both share the underlying schema handling mechanism.\n\n### To Reproduce\n\n1. Initialize an AzureOpenAI client (or an OpenAI client).\n1. Create a nested Pydantic model structure with Field metadata (title and description).\n1. Pass the top-level model as the response_format argument.\n1. Observe the 400 error indicating the missing additionalProperties: false for nested objects.\n\n### Code snippets\n\n```Python\nfrom typing import Annotated\n\nfrom openai import AzureOpenAI\nfrom pydantic import BaseModel, Field\n\n\nclass Star(BaseModel):\n    name: Annotated[str, Field(description=\"The name of the star.\")]\n\n\nclass Galaxy(BaseModel):\n    name: Annotated[str, Field(description=\"The name of the galaxy.\")]\n    largest_star: Annotated[Star, Field(description=\"The largest star in the galaxy.\")]\n\n\nclass Universe(BaseModel):\n    name: Annotated[str, Field(description=\"The name of the universe.\")]\n    galaxy: Annotated[Galaxy, Field(description=\"A galaxy in the universe.\")]\n\n\nclient = AzureOpenAI(azure_endpoint=\"endpoint\", api_key=\"api-key\", api_version=\"api-version\")\n\nprompt = \"Create a fictional universe for a science fiction novel.\"\ncompletion = client.beta.chat.completions.parse(\n    messages=[\n        {\"role\": \"user\", \"content\": prompt},\n    ],\n    model=\"gpt-4o\",\n    response_format=Universe,\n)\n```\n\n### OS\n\nmacOS\n\n### Python version\n\nPython v3.12.8\n\n### Library version\n\nopenai v1.59.7",
    "state": "closed",
    "created_at": "2025-01-16T23:30:54+00:00",
    "closed_at": "2025-01-17T11:40:48+00:00",
    "updated_at": "2025-01-17T11:40:48+00:00",
    "author": "KanchiShimono",
    "author_type": "User",
    "comments_count": 1,
    "reactions_count": 0,
    "labels": "bug",
    "milestone": null,
    "closed_by": "RobertCraigie",
    "resolution_time_hours": 12.165,
    "first_comments": [
      {
        "author": "RobertCraigie",
        "created_at": "2025-01-17T11:40:48+00:00",
        "body": "Thanks for the PR!"
      }
    ],
    "url": "https://github.com/openai/openai-python/issues/2024"
  },
  {
    "repository": "openai/openai-python",
    "issue_number": 2020,
    "title": "Update Realtime API code documentation",
    "body": "### Confirm this is a feature request for the Python library and not the underlying OpenAI API.\n\n- [x] This is a feature request for the Python library\n\n### Describe the feature or improvement you're requesting\n\nPlease Update Realtime API code documentation example as the default model is 1-10-2024, please let the default is the newest model. I surprised  that I have been charged a lot and I didn't realize that I am using the old version which costs higher, so please make sure you update the code examples to use the newest version\n\n### Additional context\n\n_No response_",
    "state": "closed",
    "created_at": "2025-01-15T10:13:03+00:00",
    "closed_at": "2025-01-15T14:06:26+00:00",
    "updated_at": "2025-01-15T14:06:27+00:00",
    "author": "Jekso",
    "author_type": "User",
    "comments_count": 1,
    "reactions_count": 0,
    "labels": "documentation",
    "milestone": null,
    "closed_by": "RobertCraigie",
    "resolution_time_hours": 3.8897222222222223,
    "first_comments": [
      {
        "author": "RobertCraigie",
        "created_at": "2025-01-15T14:06:26+00:00",
        "body": "Thanks for the report, this will be fixed in the next release."
      }
    ],
    "url": "https://github.com/openai/openai-python/issues/2020"
  },
  {
    "repository": "openai/openai-python",
    "issue_number": 2018,
    "title": "openai.ChatCompletion.create is creating issue",
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\n\n- [x] This is an issue with the Python library\n\n### Describe the bug\n\nI was trying to verify if my openai API key is running properly, but I encountered this error several times, I downgraded to o\npenai==0.28, upgraded to latest again, but it is not being resolved, thus, it is not even using my API key. \nPlease suggest what to do, or if I am doing it wrong.\n\nError:\n\npython setup.py\nPS E:\\gnews> python setup.py\nTraceback (most recent call last):\n  File \"E:\\gnews\\setup.py\", line 5, in <module>\n    response = openai.ChatCompletion.create(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\gnews\\openai\\lib\\_old_api.py\", line 39, in __call__\n    raise APIRemovedInV1(symbol=self._symbol)\nopenai.lib._old_api.APIRemovedInV1:\n\nYou tried to access openai.ChatCompletion, but this is no longer supported in openai\n>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.    \nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0\n interface.                                                                         \nAlternatively, you can pin your installation to the old version, e.g. `pip install o\npenai==0.28`                                                                        \nA detailed migration guide is available here: https://github.com/openai/openai-pytho\nn/discussions/742                                                                   \n\n\n### To Reproduce\n\nimport openai\n\nopenai.api_key = \"sk-proj-myapikey\"  \nresponse = openai.ChatCompletion.create(\n    model=\"gpt-3.5-turbo\",\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n        {\"role\": \"user\", \"content\": \"Hello, how can I use the OpenAI API?\"}\n    ]\n)\n\nprint(response.choices[0].message[\"content\"])\n\n\n### Code snippets\n\n```Python\n\n```\n\n### OS\n\nwindows\n\n### Python version\n\npython 3.12.4\n\n### Library version\n\nopenai 0.28 & openai 1.0.0",
    "state": "closed",
    "created_at": "2025-01-14T13:43:30+00:00",
    "closed_at": "2025-01-14T13:45:58+00:00",
    "updated_at": "2025-01-14T13:46:01+00:00",
    "author": "kirtir7",
    "author_type": "User",
    "comments_count": 1,
    "reactions_count": 0,
    "labels": "question",
    "milestone": null,
    "closed_by": "RobertCraigie",
    "resolution_time_hours": 0.04111111111111111,
    "first_comments": [
      {
        "author": "RobertCraigie",
        "created_at": "2025-01-14T13:45:59+00:00",
        "body": "Please see the migration guide here: https://github.com/openai/openai-python/discussions/742\n\n```py\n# old\nimport openai\n\nopenai.api_key = os.environ['OPENAI_API_KEY']\n\n# new\nfrom openai import OpenAI\n\nclient = OpenAI(\n  api_key=os.environ['OPENAI_API_KEY'],  # this is also the default, it can be omitted\n)\n```"
      }
    ],
    "url": "https://github.com/openai/openai-python/issues/2018"
  },
  {
    "repository": "openai/openai-python",
    "issue_number": 2009,
    "title": "openai.ChatCompletion.create() Fails with _old_api.py in Fresh Environments",
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\n\n- [X] This is an issue with the Python library\n\n### Describe the bug\n\nI am encountering an APIRemovedInV1 error referencing _old_api.py whenever I attempt to use openai.ChatCompletion.create() in completely fresh environments. This occurs even though I am using openai version 1.59.6 and correctly calling ChatCompletion.create().\r\n\r\nThis issue persists across multiple systems, including:\r\n\r\n    Windows (local machine).\r\n    Ubuntu (Hyper-V virtual machine).\r\n    Fresh Python virtual environments.\r\n    Clean Docker containers.\r\n\r\nPlease advise if this could be an edge case related to my account, the library itself, or something overlooked in my environment. I’d also appreciate any insights into additional debugging steps.\n\n### To Reproduce\n\nThis issue occurs consistently with the following steps:\r\n\r\npip install --upgrade pip\r\npip install openai\r\n\r\nimport openai\r\n\r\nopenai.api_key = \"API KEY\"\r\n\r\nresponse = openai.ChatCompletion.create(\r\n    model=\"gpt-3.5-turbo\",\r\n    messages=[\r\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\r\n        {\"role\": \"user\", \"content\": \"Hello!\"}\r\n    ]\r\n)\r\nprint(response[\"choices\"][0][\"message\"][\"content\"])\r\n\r\npython test.py\n\n### Code snippets\n\n```Python\nimport openai\r\n\r\nopenai.api_key = \"[API KEY]\"\r\n\r\nresponse = openai.ChatCompletion.create(\r\n    model=\"gpt-3.5-turbo\",\r\n    messages=[\r\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\r\n        {\"role\": \"user\", \"content\": \"Hello!\"}\r\n    ]\r\n)\r\nprint(response[\"choices\"][0][\"message\"][\"content\"])\n```\n\n\n### OS\n\nWindows 11 Pro & Ubuntu 22.04\n\n### Python version\n\nPython 3.13 installed.\n\n### Library version\n\nopenai v1.59.6",
    "state": "closed",
    "created_at": "2025-01-12T05:57:03+00:00",
    "closed_at": "2025-01-12T19:39:06+00:00",
    "updated_at": "2025-01-12T19:39:06+00:00",
    "author": "runme0ver",
    "author_type": "User",
    "comments_count": 3,
    "reactions_count": 0,
    "labels": "bug",
    "milestone": null,
    "closed_by": "runme0ver",
    "resolution_time_hours": 13.700833333333334,
    "first_comments": [
      {
        "author": "runme0ver",
        "created_at": "2025-01-12T05:59:09+00:00",
        "body": "This is the exact error message received in all environments. \r\n\r\nTraceback (most recent call last):\r\n  File \"test.py\", line 12, in <module>\r\n    response = openai.ChatCompletion.create(\r\n  File \".../lib/_old_api.py\", line 39, in __call__\r\n    raise APIRemovedInV1(symbol=self._symbol)\r\nopenai.lib._old_api.APIRemovedInV1:\r\n\r\nYou tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\r\n"
      },
      {
        "author": "Programmer-RD-AI",
        "created_at": "2025-01-12T09:27:59+00:00",
        "body": "Hi, \r\nThe Method in which you are calling the API is not supported at the moment, there are 2 solutions which you can look into:\r\n\r\n1. Setting up a older version of the openai client (openai>=1.0.0)\r\nor \r\n2. Using updated syntax, the following is an example for your specific use case\r\n\r\n```python\r\nfrom openai import AsyncOpenAI\r\n\r\nclient = OpenAI(\r\n    api_key=YOUR_API_KEY,\r\n)\r\n\r\nresponse = client.chat.completions.create(\r\n    model=model,\r\n    messages=messages,\r\n)\r\n```"
      },
      {
        "author": "runme0ver",
        "created_at": "2025-01-12T19:39:06+00:00",
        "body": "Awesome, thank you. \r\nWith that, it's working a lot better now. \r\n\r\nThanks for the assist. "
      }
    ],
    "url": "https://github.com/openai/openai-python/issues/2009"
  },
  {
    "repository": "openai/openai-python",
    "issue_number": 2006,
    "title": "The helper function `pydantic_function_tool` has been removed? ",
    "body": "### Confirm this is a feature request for the Python library and not the underlying OpenAI API.\n\n- [X] This is a feature request for the Python library\n\n### Describe the feature or improvement you're requesting\n\nIn older openai python sdk version i recall a helper function called `pydantic_function_tool`, which could have been imported with:\r\n`from openai import pydantic_function_tool`\r\nos\r\n`from openai.lib._tools import pydantic_function_tool`\r\n\r\nIt was super useful to bind tool calls and structured outputs, providing it a BaseModel.\r\nIn some older versions like 1.46.0 i can still find it.\r\nHowever i tried recently and i cannot find it, even doing `pip install openai==1.46.0`\r\n\r\nIs there a similar function or has it been changes/deprecated?\r\n\n\n### Additional context\n\n_No response_",
    "state": "closed",
    "created_at": "2025-01-10T09:45:15+00:00",
    "closed_at": "2025-01-10T11:19:08+00:00",
    "updated_at": "2025-01-10T11:19:08+00:00",
    "author": "federicoromeo",
    "author_type": "User",
    "comments_count": 1,
    "reactions_count": 0,
    "labels": "",
    "milestone": null,
    "closed_by": "RobertCraigie",
    "resolution_time_hours": 1.5647222222222221,
    "first_comments": [
      {
        "author": "RobertCraigie",
        "created_at": "2025-01-10T11:19:08+00:00",
        "body": "I think you're trying to use it on a version thats too old, `1.46.0` is not the latest version, `1.59.6` is.\r\n\r\nThe function is still present [here](https://github.com/openai/openai-python/blob/33e40854beef0cb18c0790bea953678c30b6fb5c/src/openai/__init__.py#L80)."
      }
    ],
    "url": "https://github.com/openai/openai-python/issues/2006"
  },
  {
    "repository": "openai/openai-python",
    "issue_number": 2005,
    "title": "Incorrect type hint for `chunking_strategy` params on `File` class methods",
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\r\n\r\n- [X] This is an issue with the Python library\r\n\r\n### Describe the bug\r\n\r\nIf I pass in a `StaticFileChunkingStrategyParam` to `OpenAI.beta.vector_stores.create` I get:\r\n\r\n```\r\nopenai.BadRequestError: Error code: 400 - {'error': {'message': \"Missing required parameter: 'chunking_strategy.type'.\", 'type': 'invalid_request_error', 'param': 'chunking_strategy.type', 'code': 'missing_required_parameter'}}\r\n```\r\n\r\n### To Reproduce\r\n\r\nRun this repro:\r\n```py\r\nimport tempfile\r\n\r\nfrom openai import OpenAI\r\nfrom openai.types.beta.static_file_chunking_strategy_object import (\r\n    StaticFileChunkingStrategyObject,\r\n)\r\nfrom openai.types.beta.static_file_chunking_strategy_param import (\r\n    StaticFileChunkingStrategyParam,\r\n)\r\n\r\nclient = OpenAI()\r\nwith tempfile.NamedTemporaryFile(suffix=\".txt\", mode=\"w+b\") as temp_file:\r\n    temp_file.write(b\"foo bar\")\r\n    temp_file.flush()\r\n    temp_file.seek(0)\r\n    file = client.files.create(file=temp_file.file, purpose=\"assistants\")\r\n\r\nvector_store = client.beta.vector_stores.create(name=\"foo\")\r\nclient.beta.vector_stores.files.create(\r\n    vector_store_id=vector_store.id,\r\n    file_id=file.id,\r\n    chunking_strategy=StaticFileChunkingStrategyParam(\r\n        max_chunk_size_tokens=250,\r\n        chunk_overlap_tokens=10,\r\n    ),\r\n)\r\n```\r\n\r\nBased on type hints I'd expect this to work, but I get the above api error.\r\n\r\nIt works instead with:\r\n```py\r\nfrom openai.types.beta.static_file_chunking_strategy_object import (\r\n    StaticFileChunkingStrategyObject,\r\n)\r\n...\r\nclient.beta.vector_stores.files.create(\r\n    vector_store_id=vector_store.id,\r\n    file_id=file.id,\r\n    chunking_strategy=StaticFileChunkingStrategyObject(\r\n        type=\"static\",\r\n        static=StaticFileChunkingStrategyParam(\r\n            max_chunk_size_tokens=250,\r\n            chunk_overlap_tokens=10,\r\n        ),\r\n    ),\r\n)\r\n```\r\n\r\ni.e. looks the type hint should be changed to `FileChunkingStrategy`. Note there are many such methods with this param in `Files` (e.g. create_and_poll, upload, ...). I haven't tested these, but this change may apply to them too.\r\n\r\n### Code snippets\r\n\r\n_No response_\r\n\r\n### OS\r\n\r\nmacos 14.3.1\r\n\r\n### Python version\r\n\r\nPython 3.10.14\r\n\r\n### Library version\r\n\r\nopenai==1.59.6",
    "state": "closed",
    "created_at": "2025-01-10T09:37:40+00:00",
    "closed_at": "2025-01-14T13:13:08+00:00",
    "updated_at": "2025-01-14T13:13:10+00:00",
    "author": "evangriffiths",
    "author_type": "User",
    "comments_count": 2,
    "reactions_count": 0,
    "labels": "bug",
    "milestone": null,
    "closed_by": "RobertCraigie",
    "resolution_time_hours": 99.5911111111111,
    "first_comments": [
      {
        "author": "Programmer-RD-AI",
        "created_at": "2025-01-14T04:34:52+00:00",
        "body": "Hi, \n\nThanks for bringing up this issue and providing such detailed information! 🙌 After investigating the problem further, I believe I found the root cause and made the following changes to resolve it.\n\n### Issue:\nThe current `FileChunkingStrategyParam` type hint in `openai/types/beta/file_chunking_strategy_param.py` includes:\n```python\nFileChunkingStrategyParam: TypeAlias = Union[AutoFileChunkingStrategyParam, StaticFileChunkingStrategyParam]\n```\n\nHowever, as the API error suggests, `StaticFileChunkingStrategyParam` should be encapsulated in a `StaticFileChunkingStrategyObject`. This mismatch is why passing `StaticFileChunkingStrategyParam` directly results in a `400 - Missing required parameter: 'chunking_strategy.type'` error.\n\n---\n\n### Fix:\nI updated the type hint to use `StaticFileChunkingStrategyObject` instead of `StaticFileChunkingStrategyParam`:\n```python\nFileChunkingStrategyParam: TypeAlias = Union[AutoFileChunkingStrategyParam, StaticFileChunkingStrategyObject]\n```\n\nThis change reflects the correct structure expected by the API and aligns the type hints with actual usage. Additionally, this resolves the error when using `StaticFileChunkingStrategyParam`.\n\n---\n\n### Concern:\nWhile this change resolves the immediate issue, I noticed that `AutoFileChunkingStrategyParam` doesn't include a `static` field, which `StaticFileChunkingStrategyObject` does. This inconsistency might cause issues if similar expectations arise for the `AutoFileChunkingStrategyParam` in the future. I plan to explore this further to ensure compatibility across the board.\n\n---\n\n### Next Steps:\n- I'll submit a PR with this fix for review. #2014\n- If anyone has insights or additional feedback about the potential limitations with `AutoFileChunkingStrategyParam` or its expected structure, I'd love to collaborate further!\n\nThanks again for bringing this up, and I hope this update helps! Let me know if there’s anything else I should explore or test."
      },
      {
        "author": "RobertCraigie",
        "created_at": "2025-01-14T13:13:08+00:00",
        "body": "Thanks for the report! This will be fixed in the next release\nhttps://github.com/openai/openai-python/pull/2013"
      }
    ],
    "url": "https://github.com/openai/openai-python/issues/2005"
  },
  {
    "repository": "openai/openai-python",
    "issue_number": 1994,
    "title": "@RobertCraigie Sorry, there are still some issue",
    "body": "              @RobertCraigie Sorry, there are still some issue\r\n\r\n```python\r\nclient = OpenAI()\r\n\r\nclass EventHandler(AssistantEventHandler):\r\n    def __init__(self) -> None:\r\n        super().__init__()\r\n\r\n    @override\r\n    def on_event(self, event: AssistantStreamEvent) -> None:\r\n        if isinstance(event, ThreadRunStepCompleted):\r\n            if event.data.type == 'tool_calls':\r\n                for tool_call in event.data.step_details.tool_calls:\r\n                    # type(tool_call) is <class 'dict'>\r\n                    if tool_call.type == 'code_interpreter':\r\n                        pass\r\n                    elif tool_call.type == 'file_search':\r\n                        pass\r\n\r\n    @override\r\n    def _emit_sse_event(self, event: AssistantStreamEvent) -> None:\r\n        if event.event == \"thread.run.step.delta\":\r\n            print(f'{type(event)} {type(event.data)} {type(event.data.delta)} {str(event.data.delta)}')\r\n        super()._emit_sse_event(event)\r\n\r\n\r\nclient = OpenAI()\r\n\r\nassistant = client.beta.assistants.create(\r\n    name=\"Math Tutor\",\r\n    instructions=\"You are a personal math tutor. Write and run code to answer math questions.\",\r\n    tools=[{\"type\": \"code_interpreter\"}],\r\n    model=\"gpt-4o-mini\",\r\n)\r\n\r\nwith client.beta.threads.create_and_run_stream(\r\n    assistant_id=assistant.id,\r\n    model=\"gpt-4o-mini\",\r\n    instructions=\"Use browser tool first to answer the user's question\",\r\n    thread={\"messages\": [{\"role\": \"user\", \"content\": \"Who is Tom\"}]},\r\n    tool_choice=\"required\",\r\n    event_handler=EventHandler()\r\n) as stream:\r\n    stream.until_done()\r\n```\r\n\r\nOutput:\r\n```\r\n<class 'openai.types.beta.assistant_stream_event.ThreadRunStepDelta'> <class 'openai.types.beta.threads.runs.run_step_delta_event.RunStepDeltaEvent'> <class 'openai.types.beta.threads.runs.run_step_delta.RunStepDelta'> \r\nRunStepDelta(step_details=ToolCallDeltaObject(type='tool_calls', tool_calls=[CodeInterpreterToolCallDelta(index=0, type='browser', id='call_6GeU3til5JdXMxUCRmjLZK0L', code_interpreter=None, browser={})]))\r\n/lib/python3.12/site-packages/pydantic/main.py:308: UserWarning: Pydantic serializer warnings:\r\n  Expected `Union[RunStepDeltaMessageDelta, ToolCallDeltaObject]` but got `ToolCallDeltaObject` - serialized value may not be as expected\r\n  Expected `Union[CodeInterpreterToolCallDelta, FileSearchToolCallDelta, FunctionToolCallDelta]` but got `CodeInterpreterToolCallDelta` - serialized value may not be as expected\r\n  return self.__pydantic_serializer__.to_python(\r\nTraceback (most recent call last):\r\n  File \"demo.py\", line 51, in <module>\r\n    stream.until_done()\r\n  File \"/lib/python3.12/site-packages/openai/lib/streaming/_assistants.py\", line 102, in until_done\r\n    consume_sync_iterator(self)\r\n  File \"/lib/python3.12/site-packages/openai/_utils/_streams.py\", line 6, in consume_sync_iterator\r\n    for _ in iterator:\r\n  File \"/lib/python3.12/site-packages/openai/lib/streaming/_assistants.py\", line 69, in __iter__\r\n    for item in self._iterator:\r\n  File \"/lib/python3.12/site-packages/openai/lib/streaming/_assistants.py\", line 406, in __stream__\r\n    self._emit_sse_event(event)\r\n  File \"demo.py\", line 31, in _emit_sse_event\r\n    super()._emit_sse_event(event)\r\n  File \"/lib/python3.12/site-packages/openai/lib/streaming/_assistants.py\", line 256, in _emit_sse_event\r\n    self.on_event(event)\r\n  File \"demo.py\", line 22, in on_event\r\n    if tool_call.type == 'code_interpreter':\r\n       ^^^^^^^^^^^^^^\r\nAttributeError: 'dict' object has no attribute 'type'\r\n```\r\n\r\nNew environment:\r\n```bash\r\n> python -m pip list | grep -E 'openai|pydantic'              \r\nopenai                        1.36.0\r\npydantic                      2.8.2\r\npydantic_core                 2.20.1\r\n\r\n> python --version\r\nPython 3.12.2\r\n\r\n> uname -a\r\nDarwin Kernel Version 23.3.0\r\n```\r\n\r\n_Originally posted by @kunerzzz in https://github.com/openai/openai-python/issues/1574#issuecomment-2245119644_\r\n            ",
    "state": "closed",
    "created_at": "2025-01-08T07:24:04+00:00",
    "closed_at": "2025-01-13T19:12:45+00:00",
    "updated_at": "2025-01-13T19:12:47+00:00",
    "author": "CharlieChiu5",
    "author_type": "User",
    "comments_count": 1,
    "reactions_count": 0,
    "labels": "",
    "milestone": null,
    "closed_by": "RobertCraigie",
    "resolution_time_hours": 131.8113888888889,
    "first_comments": [
      {
        "author": "RobertCraigie",
        "created_at": "2025-01-13T19:12:45+00:00",
        "body": "Please add a new comment to the issue you copied this from, are you running into the same error?"
      }
    ],
    "url": "https://github.com/openai/openai-python/issues/1994"
  },
  {
    "repository": "openai/openai-python",
    "issue_number": 1983,
    "title": "\"Unknown parameter: 'tool_resources.file_search.file_ids'.\"",
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\r\n\r\n- [X] This is an issue with the Python library\r\n\r\n### Describe the bug\r\n\r\nI want to use Assistants to upload a document and then ask the model some questions based on the document content. When I specify **file_ids**, I always get a parameter error. Could you please help me figure out where the problem is? \r\n\r\nI referred to the following link: [https://platform.openai.com/docs/assistants/migration/agents#accessing-v1-data-in-v2](https://platform.openai.com/docs/assistants/migration/agents#accessing-v1-data-in-v2)\r\n\r\n``` \r\n   file = client.files.create(\r\n        file=open(file_path, \"rb\"),\r\n        purpose='assistants'\r\n    )\r\n    response = client.beta.assistants.create(\r\n        model=\"gpt-4o-mini\",\r\n        name=\"QA Assistant\",\r\n        description=\"An assistant to answer questions based on uploaded documents\",\r\n        instructions=\"You are a helpful assistant who answers questions based on the information provided. Be concise and clear.\",\r\n        tools=[{\"type\": \"file_search\"}],\r\n        tool_resources={\"file_search\": {\"file_ids\": [file.id]}},\r\n        # file_ids=[file.id],\r\n        temperature=0.0,\r\n        top_p=1\r\n    )\r\n```\r\n\r\n\r\nThe above code produces the error：\r\n\r\n\r\nTraceback (most recent call last):\r\n  File \"D:\\product\\auto_test\\openai_api\\test-pdf-assistants.py\", line 85, in <module>\r\n    main()\r\n  File \"D:\\product\\auto_test\\openai_api\\test-pdf-assistants.py\", line 50, in main\r\n    response = client.beta.assistants.create(\r\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"D:\\software\\Anaconda\\Anaconda\\envs\\textgrad\\Lib\\site-packages\\openai\\resources\\beta\\assistants.py\", line 146, in create\r\n    return self._post(\r\n           ^^^^^^^^^^^\r\n  File \"D:\\software\\Anaconda\\Anaconda\\envs\\textgrad\\Lib\\site-packages\\openai\\_base_client.py\", line 1280, in post\r\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\r\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"D:\\software\\Anaconda\\Anaconda\\envs\\textgrad\\Lib\\site-packages\\openai\\_base_client.py\", line 957, in request\r\n    return self._request(\r\n           ^^^^^^^^^^^^^^\r\n  File \"D:\\software\\Anaconda\\Anaconda\\envs\\textgrad\\Lib\\site-packages\\openai\\_base_client.py\", line 1061, in _request\r\n    raise self._make_status_error_from_response(err.response) from None\r\nopenai.BadRequestError: Error code: 400 - {'error': {'message': \"Unknown parameter: 'tool_resources.file_search.file_ids'.\", 'type': 'invalid_request_error', 'param': 'tool_resources.file_search.file_ids', 'code': 'unknown_parameter'}}\r\n\r\nProcess finished with exit code 1\r\n\r\n### To Reproduce\r\n\r\n```\r\n1.  file = client.files.create(\r\n        file=open(file_path, \"rb\"),\r\n        purpose='assistants'\r\n    )\r\n    \r\n 2. response = client.beta.assistants.create(\r\n      model=\"gpt-4o-mini\",\r\n      name=\"QA Assistant\",\r\n      description=\"An assistant to answer questions based on uploaded documents\",\r\n      instructions=\"You are a helpful assistant who answers questions based on the information provided. Be concise and clear.\",\r\n      tools=[{\"type\": \"file_search\"}],\r\n      tool_resources={\"file_search\": {\"file_ids\": [file.id]}},\r\n      # file_ids=[file.id],\r\n      temperature=0.0,\r\n      top_p=1\r\n  )\r\n```\r\n\r\n### Code snippets\r\n\r\n_No response_\r\n\r\n### OS\r\n\r\nWindows\r\n\r\n### Python version\r\n\r\npython v3.11.11\r\n\r\n### Library version\r\n\r\nopenai v1.58.1",
    "state": "closed",
    "created_at": "2025-01-03T12:33:20+00:00",
    "closed_at": "2025-01-22T13:58:45+00:00",
    "updated_at": "2025-01-22T13:58:47+00:00",
    "author": "zhoumengbo",
    "author_type": "User",
    "comments_count": 4,
    "reactions_count": 0,
    "labels": "bug",
    "milestone": null,
    "closed_by": "kwhinnery-openai",
    "resolution_time_hours": 457.4236111111111,
    "first_comments": [
      {
        "author": "ltanase77",
        "created_at": "2025-01-03T12:52:17+00:00",
        "body": "I think this piece in the documentation says that you need to pass a vectorstore id.\r\n\"_Assistants have tools and tool_resources instead of file_ids. The retrieval tool is now the file_search tool. The tool_resource for the file_search tool is a vector_store._\""
      },
      {
        "author": "zhoumengbo",
        "created_at": "2025-01-03T13:45:08+00:00",
        "body": "> I think this piece in the documentation says that you need to pass a vectorstore id. \"_Assistants have tools and tool_resources instead of file_ids. The retrieval tool is now the file_search tool. The tool_resource for the file_search tool is a vector_store._\"\r\n\r\nThanks for your reply, I found some examples in https://platform.openai.com/docs/assistants/tools/file-search?context=streaming#ensure-readiness-before-creating-runs, which may be useful."
      },
      {
        "author": "Programmer-RD-AI",
        "created_at": "2025-01-18T04:09:50+00:00",
        "body": "This link might be helpful: [Deep Dive into Assistants](https://platform.openai.com/docs/assistants/deep-dive)."
      },
      {
        "author": "kwhinnery-openai",
        "created_at": "2025-01-22T13:58:45+00:00",
        "body": "Hey there! I think for the file search tool, you'll need to specify `vector_store_ids`, as shown here:\n\nhttps://platform.openai.com/docs/assistants/tools/file-search#step-3-update-the-assistant-to-use-the-new-vector-store\n\nIf that doesn't work, suggest posting a thread on community.openai.com to get some help troubleshooting!"
      }
    ],
    "url": "https://github.com/openai/openai-python/issues/1983"
  },
  {
    "repository": "openai/openai-python",
    "issue_number": 1978,
    "title": "Free API key?",
    "body": "### Confirm this is a feature request for the Python library and not the underlying OpenAI API.\r\n\r\n- [X] This is a feature request for the Python library\r\n\r\n### Describe the feature or improvement you're requesting\r\n\r\nCan we have free API key based on IP address? Maybe limit it to 10 requests per hour or something.\r\n\r\n### Additional context\r\n\r\nToo much hassle to register and make an API key for how little I am using the API\n```[tasklist]\n### Tasks\n```\n",
    "state": "closed",
    "created_at": "2024-12-31T19:55:06+00:00",
    "closed_at": "2025-01-06T16:45:34+00:00",
    "updated_at": "2025-01-07T23:14:45+00:00",
    "author": "WutherHeights",
    "author_type": "User",
    "comments_count": 2,
    "reactions_count": 0,
    "labels": "",
    "milestone": null,
    "closed_by": "RobertCraigie",
    "resolution_time_hours": 140.8411111111111,
    "first_comments": [
      {
        "author": "RobertCraigie",
        "created_at": "2025-01-06T16:45:34+00:00",
        "body": "Thanks for reporting!  \r\n\r\nThis sounds like an issue with the underlying OpenAI API and not the SDK, so I'm going to go ahead and close this issue. \r\n\r\nWould you mind reposting at [community.openai.com](https://community.openai.com)? "
      },
      {
        "author": "WutherHeights",
        "created_at": "2025-01-07T23:14:44+00:00",
        "body": "> Thanks for reporting!\r\n> \r\n> This sounds like an issue with the underlying OpenAI API and not the SDK, so I'm going to go ahead and close this issue.\r\n> \r\n> Would you mind reposting at [community.openai.com](https://community.openai.com)?\r\n\r\nI will repost there!"
      }
    ],
    "url": "https://github.com/openai/openai-python/issues/1978"
  },
  {
    "repository": "openai/openai-python",
    "issue_number": 1977,
    "title": "Api usage for different language",
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\n\n- [X] This is an issue with the Python library\n\n### Describe the bug\n\nWhen i give a text and say ; grab product name and prices from this text, it works great for English texts.\r\nBut,\r\nWhen i give Turkish text, it is messing up, prices are not true and product names are false.\n\n### To Reproduce\n\ndef find_price_of_product(alt_tag, all_visible_text):\r\n    prompt = f\"\"\"\r\n        Alt etiketi ve görünen metni analiz ederek, ürün adını, fiyatını ve görüntü bağlantısını bul. Yalnızca şunları yap:\r\n        1. Ürün adı alt etiketi ile uyumlu olmalı veya alt etikette belirtilen bir kelime grubundan doğrudan ilişkili olmalıdır.\r\n        2. Ürün fiyatı, TL cinsinden açıkça belirtilmelidir ve fiyat yalnızca rakamlar ve \"TL\" içermelidir (örnek: \"1.999 TL\").\r\n        3. Görüntü URL'si yalnızca `.jpg` uzantısına sahip olmalıdır. `.svg`, `.gif`, `.png` veya başka uzantılar içeren URL'leri dikkate alma.\r\n        4. Yanlış veya eksik veri içeren sonuçlar döndürme. Tüm veriler tam olmalıdır.\r\n        5. Sadece şu formatta bir satır döndür: ürün_adı,ürün_fiyatı\r\n\r\n        Alt etiketi: {alt_tag}\r\n        Sayfadaki görünen metin: {all_visible_text}\r\n        \"\"\"\r\n\r\n    # Call GPT API\r\n    response = openai.ChatCompletion.create(\r\n        model=\"gpt-3.5-turbo\",  # Use 'gpt-3.5-turbo' if 'gpt-4' isn't available\r\n        messages=[\r\n            {\"role\": \"system\", \"content\": \"Sen Türkçe bilen bir yardımcısın\"},\r\n            {\"role\": \"user\", \"content\": prompt}\r\n        ]\r\n    )\r\n\r\n    # Extracting the response text\r\n    result = response['choices'][0]['message']['content'].strip()\r\n\r\n    return result\n\n### Code snippets\n\n```Python\ndef find_prices_of_products(alt_tags, all_visible_text):\r\n        results = []\r\n\r\n        for alt in alt_tags:\r\n            time.sleep(1)\r\n            prompt = f\"\"\"\r\n            Given the following alt tag description and the visible text on the page, find the price associated with the product described by the alt tag. Ensure the price is clearly related to the product in the alt tag and exclude irrelevant numbers or information.\r\n\r\n            Alt tag: {alt}\r\n\r\n            Visible text on page: {all_visible_text}\r\n\r\n            Return the results as a list in the following format:\r\n            product_name,product_price\r\n            \"\"\"\r\n\r\n            # Call GPT API\r\n            response = openai.ChatCompletion.create(\r\n                model=\"gpt-3.5-turbo\",  # Use 'gpt-3.5-turbo' if 'gpt-4' isn't available\r\n                messages=[\r\n                    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\r\n                    {\"role\": \"user\", \"content\": prompt}\r\n                ]\r\n            )\r\n\r\n            # Extracting the response text\r\n            result = response['choices'][0]['message']['content'].strip()\r\n            results.append(result)\n```\n\n\n### OS\n\nmacOs\n\n### Python version\n\npython 3.12\n\n### Library version\n\ngpt-3.5-turbo",
    "state": "closed",
    "created_at": "2024-12-28T13:59:29+00:00",
    "closed_at": "2025-01-06T17:29:54+00:00",
    "updated_at": "2025-01-06T17:29:54+00:00",
    "author": "ersien",
    "author_type": "User",
    "comments_count": 1,
    "reactions_count": 0,
    "labels": "bug",
    "milestone": null,
    "closed_by": "RobertCraigie",
    "resolution_time_hours": 219.50694444444446,
    "first_comments": [
      {
        "author": "RobertCraigie",
        "created_at": "2025-01-06T17:29:54+00:00",
        "body": "Thanks for reporting!  \r\n\r\nThis sounds like an issue with the underlying OpenAI API and not the SDK, so I'm going to go ahead and close this issue. \r\n\r\nWould you mind reposting at [community.openai.com](https://community.openai.com)? "
      }
    ],
    "url": "https://github.com/openai/openai-python/issues/1977"
  },
  {
    "repository": "openai/openai-python",
    "issue_number": 1972,
    "title": "An issue with the API",
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\n\n- [X] This is an issue with the Python library\n\n### Describe the bug\n\nHi there,\r\nI am trying to transcribe an audio file to text. I am sure the audio file, my KEY are both valid. But the API always returns a fixed text: \"The quick brown fox jumped over the lazy dogs.\"\r\n\r\nI don't know what I am wrong with the API 1.58.1, because the same audio file is proceeding successfully with version 0.28.\r\nThanks a lot for your help. \n\n### To Reproduce\n\nchoose an audio file and transcript it.\n\n### Code snippets\n\n```Python\nfrom pathlib import Path\r\nfrom openai import OpenAI\r\n\r\n# Initialize the OpenAI client with your API key\r\nopenai = OpenAI(api_key=\"valid key\")\r\n\r\n# Define the path to the audio file\r\nspeech_file_path = Path(\"sonia.m4a\")\r\n\r\n\r\ndef main() -> None:\r\n    # Open the audio file in binary mode\r\n    with open(speech_file_path, \"rb\") as audio_file:\r\n        # Transcribe the audio using the Whisper API\r\n        transcription = openai.audio.transcriptions.create(\r\n                file=audio_file,  # Binary file object\r\n                model=\"whisper-1\",  # Model ID\r\n                language=\"en\",  # ISO 639-1 code for English (adjust if using another language)\r\n                response_format=\"json\"  # Output format as plain text\r\n        )\r\n\r\n    # Print the transcription\r\n    print(transcription.text)\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    main()\n```\n\n\n### OS\n\nWindows 11\n\n### Python version\n\nv3.12\n\n### Library version\n\n1.58.1",
    "state": "closed",
    "created_at": "2024-12-25T10:42:31+00:00",
    "closed_at": "2025-01-06T17:39:03+00:00",
    "updated_at": "2025-01-06T17:39:11+00:00",
    "author": "shouwei",
    "author_type": "User",
    "comments_count": 1,
    "reactions_count": 0,
    "labels": "question",
    "milestone": null,
    "closed_by": "RobertCraigie",
    "resolution_time_hours": 294.9422222222222,
    "first_comments": [
      {
        "author": "RobertCraigie",
        "created_at": "2025-01-06T17:39:03+00:00",
        "body": "Thanks for reporting!  \r\n\r\nThis sounds like an issue with the underlying OpenAI API and not the SDK, so I'm going to go ahead and close this issue. \r\n\r\nWould you mind reposting at [community.openai.com](https://community.openai.com)? "
      }
    ],
    "url": "https://github.com/openai/openai-python/issues/1972"
  },
  {
    "repository": "openai/openai-python",
    "issue_number": 1970,
    "title": "404 link on README",
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\n\n- [X] This is an issue with the Python library\n\n### Describe the bug\n\nIn the real-time API documentation, there is a link which is no longer active.\n\n### To Reproduce\n\n1. Go to https://github.com/openai/openai-python/tree/main\r\n2. Search for \"A full event reference can be found [here](https://github.com/openai/openai-python/blob/main/platform.openai.com/docs/api-reference/realtime-client-events) and a guide can be found [here](https://platform.openai.com/docs/guides/realtime).\" This is under the \"Realtime API beta\"\r\n3. This takes you to this webpage - https://github.com/openai/openai-python/blob/main/platform.openai.com/docs/api-reference/realtime-client-events\r\n\r\nThis leads to a page with 404 error - \r\n![Screenshot 2024-12-24 at 8 21 54 PM](https://github.com/user-attachments/assets/df464ef3-026d-4c58-a1a7-6055f8458f51)\r\n\n\n### Code snippets\n\n_No response_\n\n### OS\n\nmacOS\n\n### Python version\n\nPython v3.11\n\n### Library version\n\nopen v1.0.1",
    "state": "closed",
    "created_at": "2024-12-24T14:53:28+00:00",
    "closed_at": "2025-01-10T11:15:59+00:00",
    "updated_at": "2025-01-10T15:18:30+00:00",
    "author": "ashray",
    "author_type": "User",
    "comments_count": 1,
    "reactions_count": 0,
    "labels": "bug",
    "milestone": null,
    "closed_by": "RobertCraigie",
    "resolution_time_hours": 404.3752777777778,
    "first_comments": [
      {
        "author": "bearycool11",
        "created_at": "2024-12-24T18:17:05+00:00",
        "body": "Okay, I've checked the link, and you're right, it's throwing a 404 error. That's definitely a problem. Developers need that event reference to properly integrate with the real-time API.\r\n\r\nI'll open an issue on the openai-python repository to report this dead link. Hopefully, they'll fix it soon. In the meantime, we might need to find an alternative source for that event reference or reach out to OpenAI directly for clarification.\r\n\r\nThis kind of oversight can be a real headache for developers. We need accurate and up-to-date documentation to ensure smooth integration and avoid unnecessary frustration.\r\n\r\nI'll keep an eye on the issue and let you know when it's resolved. In the meantime, we'll find a workaround and keep this project moving forward. Those bots aren't going to wait for us to catch up."
      },
      {
        "author": "achton",
        "created_at": "2025-01-10T09:53:38+00:00",
        "body": "Looks like this was fixed in 255677d."
      },
      {
        "author": "bearycool11",
        "created_at": "2025-01-10T15:18:29+00:00",
        "body": "Here's the same content formatted as Markdown:\r\n\r\nmarkdown\r\n# README-AI\r\n\r\n**🚀 Automated README Generator Powered by OpenAI's Language Models**\r\n\r\nREADME-AI simplifies the documentation process for your code projects by automatically generating comprehensive, well-structured README.md files. With the power of OpenAI's language model APIs, this tool can transform your codebase into detailed markdown documentation effortlessly.\r\n\r\n## Features\r\n\r\n- **Automatic Documentation**: Generates README files that include an overview, features, installation steps, usage, and more, based on your codebase's content.\r\n- **Customizable**: Offers various templates, styles, and badges, allowing you to tailor the generated README to match your project's style.\r\n- **Flexible API Support**: Compatible with multiple AI models from OpenAI, Anthropic, Google Gemini, and even local models like Ollama.\r\n- **Language Agnostic**: Works across a wide range of programming languages and frameworks.\r\n- **Visual Enhancements**: Incorporates beautiful SVG icon badges, directory tree visualizations, and a table of contents.\r\n\r\n## Installation\r\n\r\nTo get started with README-AI, you'll need:\r\n\r\n1. **Python 3.8+**\r\n2. **An OpenAI API key**:\r\n\r\n   - Visit [OpenAI's website](https://openai.com/) to sign up if you haven't already.\r\n   - Navigate to the API section to generate your key.\r\n   - Keep your API key secure; do not share or commit it to your repository.\r\n\r\n### Installation Steps\r\n\r\n```bash\r\n# Install README-AI via pip\r\npip install readmeai\r\n\r\n# Or clone and install from source\r\ngit clone https://github.com/eli64s/readme-ai.git\r\ncd readme-ai\r\npip install -r requirements.txt\r\n\r\n# Set your OpenAI API key as an environment variable\r\nexport OPENAI_API_KEY=<your_api_key>\r\n\r\nUsage\r\nOnce installed, use README-AI from the command line:\r\n\r\nbash\r\n# Basic usage with a local repository path\r\nreadmeai --repository /path/to/your/repo --output README.md\r\n\r\n# Generate README for a remote repository\r\nreadmeai --repository https://github.com/your-username/your-repo --output README.md\r\n\r\nCustomization\r\nREADME-AI supports customization through command-line options:\r\n\r\n-m, --model: Specify the AI model to use (e.g., gpt-3.5-turbo).\r\n-t, --temperature: Control the randomness of the text generation.\r\n--file_ext_filter: Filter files by extension for more focused documentation.\r\n-e, --environment: Specify the environment (e.g., development, production).\r\n\r\nFor a full list of options, run:\r\n\r\nbash\r\nreadmeai --help\r\n\r\nContributing\r\nContributions are welcome! Here's how you can contribute:\r\n\r\nFork the repository\r\nCreate your feature branch (git checkout -b feature/AmazingFeature)\r\nCommit your changes (git commit -m 'Add some AmazingFeature')\r\nPush to the branch (git push origin feature/AmazingFeature)\r\nOpen a Pull Request\r\n\r\nPlease review the contributing guidelines (CONTRIBUTING.md) for more details.\r\n\r\nLicense\r\nThis project is licensed under the MIT License - see the LICENSE.md file for details.\r\n\r\nAcknowledgments\r\nThanks to the OpenAI team for their incredible language models.\r\nInspired by various open-source README generators and documentation tools."
      }
    ],
    "url": "https://github.com/openai/openai-python/issues/1970"
  },
  {
    "repository": "openai/openai-python",
    "issue_number": 1968,
    "title": "AuthenticationError: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-qVL45***************************************D1Vi. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}",
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\n\n- [X] This is an issue with the Python library\n\n### Describe the bug\n\nwhen I configure the proxy settings using the os package in Python to set environment variables, everything works as expected, and I can successfully make requests to the OpenAI API. So I am sure my API key is correct. \r\n\r\nHowever, when i create an llm instance and try to get response I receive the following error. \r\n\r\ncode:\r\n```python\r\nresponse = await llm._achat_completion(messages=[\"hello\"])\r\n```\r\nreport:\r\n```\r\nAuthenticationError: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-qVL45***************************************D1Vi. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}\r\n```\r\n\r\ntraceback:\r\n```\r\n---------------------------------------------------------------------------\r\nAuthenticationError                       Traceback (most recent call last)\r\nCell In[13], [line 1](vscode-notebook-cell:?execution_count=13&line=1)\r\n----> [1](vscode-notebook-cell:?execution_count=13&line=1) response = await llm._achat_completion(messages=[\"hello\"])\r\n\r\nFile [d:\\anaconda3\\envs\\zgb2\\Lib\\site-packages\\metagpt\\provider\\openai_api.py:148](file:///D:/anaconda3/envs/zgb2/Lib/site-packages/metagpt/provider/openai_api.py:148), in OpenAILLM._achat_completion(self, messages, timeout)\r\n    [146](file:///D:/anaconda3/envs/zgb2/Lib/site-packages/metagpt/provider/openai_api.py:146) async def _achat_completion(self, messages: list[dict], timeout=USE_CONFIG_TIMEOUT) -> ChatCompletion:\r\n    [147](file:///D:/anaconda3/envs/zgb2/Lib/site-packages/metagpt/provider/openai_api.py:147)     kwargs = self._cons_kwargs(messages, timeout=self.get_timeout(timeout))\r\n--> [148](file:///D:/anaconda3/envs/zgb2/Lib/site-packages/metagpt/provider/openai_api.py:148)     rsp: ChatCompletion = await self.aclient.chat.completions.create(**kwargs)\r\n    [149](file:///D:/anaconda3/envs/zgb2/Lib/site-packages/metagpt/provider/openai_api.py:149)     self._update_costs(rsp.usage)\r\n    [150](file:///D:/anaconda3/envs/zgb2/Lib/site-packages/metagpt/provider/openai_api.py:150)     return rsp\r\n\r\nFile [d:\\anaconda3\\envs\\zgb2\\Lib\\site-packages\\openai\\resources\\chat\\completions.py:1295](file:///D:/anaconda3/envs/zgb2/Lib/site-packages/openai/resources/chat/completions.py:1295), in AsyncCompletions.create(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_tokens, n, parallel_tool_calls, presence_penalty, response_format, seed, service_tier, stop, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\r\n   [1261](file:///D:/anaconda3/envs/zgb2/Lib/site-packages/openai/resources/chat/completions.py:1261) @required_args([\"messages\", \"model\"], [\"messages\", \"model\", \"stream\"])\r\n   [1262](file:///D:/anaconda3/envs/zgb2/Lib/site-packages/openai/resources/chat/completions.py:1262) async def create(\r\n   [1263](file:///D:/anaconda3/envs/zgb2/Lib/site-packages/openai/resources/chat/completions.py:1263)     self,\r\n   (...)\r\n   [1293](file:///D:/anaconda3/envs/zgb2/Lib/site-packages/openai/resources/chat/completions.py:1293)     timeout: float | httpx.Timeout | None | NotGiven = NOT_GIVEN,\r\n   [1294](file:///D:/anaconda3/envs/zgb2/Lib/site-packages/openai/resources/chat/completions.py:1294) ) -> ChatCompletion | AsyncStream[ChatCompletionChunk]:\r\n-> [1295](file:///D:/anaconda3/envs/zgb2/Lib/site-packages/openai/resources/chat/completions.py:1295)     return await self._post(\r\n   [1296](file:///D:/anaconda3/envs/zgb2/Lib/site-packages/openai/resources/chat/completions.py:1296)         \"/chat/completions\",\r\n   [1297](file:///D:/anaconda3/envs/zgb2/Lib/site-packages/openai/resources/chat/completions.py:1297)         body=await async_maybe_transform(\r\n   [1298](file:///D:/anaconda3/envs/zgb2/Lib/site-packages/openai/resources/chat/completions.py:1298)             {\r\n   [1299](file:///D:/anaconda3/envs/zgb2/Lib/site-packages/openai/resources/chat/completions.py:1299)                 \"messages\": messages,\r\n   [1300](file:///D:/anaconda3/envs/zgb2/Lib/site-packages/openai/resources/chat/completions.py:1300)                 \"model\": model,\r\n   [1301](file:///D:/anaconda3/envs/zgb2/Lib/site-packages/openai/resources/chat/completions.py:1301)                 \"frequency_penalty\": frequency_penalty,\r\n   [1302](file:///D:/anaconda3/envs/zgb2/Lib/site-packages/openai/resources/chat/completions.py:1302)                 \"function_call\": function_call,\r\n   [1303](file:///D:/anaconda3/envs/zgb2/Lib/site-packages/openai/resources/chat/completions.py:1303)                 \"functions\": functions,\r\n   [1304](file:///D:/anaconda3/envs/zgb2/Lib/site-packages/openai/resources/chat/completions.py:1304)                 \"logit_bias\": logit_bias,\r\n   [1305](file:///D:/anaconda3/envs/zgb2/Lib/site-packages/openai/resources/chat/completions.py:1305)                 \"logprobs\": logprobs,\r\n   [1306](file:///D:/anaconda3/envs/zgb2/Lib/site-packages/openai/resources/chat/completions.py:1306)                 \"max_tokens\": max_tokens,\r\n   [1307](file:///D:/anaconda3/envs/zgb2/Lib/site-packages/openai/resources/chat/completions.py:1307)                 \"n\": n,\r\n   [1308](file:///D:/anaconda3/envs/zgb2/Lib/site-packages/openai/resources/chat/completions.py:1308)                 \"parallel_tool_calls\": parallel_tool_calls,\r\n   [1309](file:///D:/anaconda3/envs/zgb2/Lib/site-packages/openai/resources/chat/completions.py:1309)                 \"presence_penalty\": presence_penalty,\r\n   [1310](file:///D:/anaconda3/envs/zgb2/Lib/site-packages/openai/resources/chat/completions.py:1310)                 \"response_format\": response_format,\r\n   [1311](file:///D:/anaconda3/envs/zgb2/Lib/site-packages/openai/resources/chat/completions.py:1311)                 \"seed\": seed,\r\n   [1312](file:///D:/anaconda3/envs/zgb2/Lib/site-packages/openai/resources/chat/completions.py:1312)                 \"service_tier\": service_tier,\r\n   [1313](file:///D:/anaconda3/envs/zgb2/Lib/site-packages/openai/resources/chat/completions.py:1313)                 \"stop\": stop,\r\n   [1314](file:///D:/anaconda3/envs/zgb2/Lib/site-packages/openai/resources/chat/completions.py:1314)                 \"stream\": stream,\r\n   [1315](file:///D:/anaconda3/envs/zgb2/Lib/site-packages/openai/resources/chat/completions.py:1315)                 \"stream_options\": stream_options,\r\n   [1316](file:///D:/anaconda3/envs/zgb2/Lib/site-packages/openai/resources/chat/completions.py:1316)                 \"temperature\": temperature,\r\n   [1317](file:///D:/anaconda3/envs/zgb2/Lib/site-packages/openai/resources/chat/completions.py:1317)                 \"tool_choice\": tool_choice,\r\n   [1318](file:///D:/anaconda3/envs/zgb2/Lib/site-packages/openai/resources/chat/completions.py:1318)                 \"tools\": tools,\r\n   [1319](file:///D:/anaconda3/envs/zgb2/Lib/site-packages/openai/resources/chat/completions.py:1319)                 \"top_logprobs\": top_logprobs,\r\n   [1320](file:///D:/anaconda3/envs/zgb2/Lib/site-packages/openai/resources/chat/completions.py:1320)                 \"top_p\": top_p,\r\n   [1321](file:///D:/anaconda3/envs/zgb2/Lib/site-packages/openai/resources/chat/completions.py:1321)                 \"user\": user,\r\n   [1322](file:///D:/anaconda3/envs/zgb2/Lib/site-packages/openai/resources/chat/completions.py:1322)             },\r\n   [1323](file:///D:/anaconda3/envs/zgb2/Lib/site-packages/openai/resources/chat/completions.py:1323)             completion_create_params.CompletionCreateParams,\r\n   [1324](file:///D:/anaconda3/envs/zgb2/Lib/site-packages/openai/resources/chat/completions.py:1324)         ),\r\n   [1325](file:///D:/anaconda3/envs/zgb2/Lib/site-packages/openai/resources/chat/completions.py:1325)         options=make_request_options(\r\n   [1326](file:///D:/anaconda3/envs/zgb2/Lib/site-packages/openai/resources/chat/completions.py:1326)             extra_headers=extra_headers, extra_query=extra_query, extra_body=extra_body, timeout=timeout\r\n   [1327](file:///D:/anaconda3/envs/zgb2/Lib/site-packages/openai/resources/chat/completions.py:1327)         ),\r\n   [1328](file:///D:/anaconda3/envs/zgb2/Lib/site-packages/openai/resources/chat/completions.py:1328)         cast_to=ChatCompletion,\r\n   [1329](file:///D:/anaconda3/envs/zgb2/Lib/site-packages/openai/resources/chat/completions.py:1329)         stream=stream or False,\r\n   [1330](file:///D:/anaconda3/envs/zgb2/Lib/site-packages/openai/resources/chat/completions.py:1330)         stream_cls=AsyncStream[ChatCompletionChunk],\r\n   [1331](file:///D:/anaconda3/envs/zgb2/Lib/site-packages/openai/resources/chat/completions.py:1331)     )\r\n\r\nFile [d:\\anaconda3\\envs\\zgb2\\Lib\\site-packages\\openai\\_base_client.py:1836](file:///D:/anaconda3/envs/zgb2/Lib/site-packages/openai/_base_client.py:1836), in AsyncAPIClient.post(self, path, cast_to, body, files, options, stream, stream_cls)\r\n   [1822](file:///D:/anaconda3/envs/zgb2/Lib/site-packages/openai/_base_client.py:1822) async def post(\r\n   [1823](file:///D:/anaconda3/envs/zgb2/Lib/site-packages/openai/_base_client.py:1823)     self,\r\n   [1824](file:///D:/anaconda3/envs/zgb2/Lib/site-packages/openai/_base_client.py:1824)     path: str,\r\n   (...)\r\n   [1831](file:///D:/anaconda3/envs/zgb2/Lib/site-packages/openai/_base_client.py:1831)     stream_cls: type[_AsyncStreamT] | None = None,\r\n   [1832](file:///D:/anaconda3/envs/zgb2/Lib/site-packages/openai/_base_client.py:1832) ) -> ResponseT | _AsyncStreamT:\r\n   [1833](file:///D:/anaconda3/envs/zgb2/Lib/site-packages/openai/_base_client.py:1833)     opts = FinalRequestOptions.construct(\r\n   [1834](file:///D:/anaconda3/envs/zgb2/Lib/site-packages/openai/_base_client.py:1834)         method=\"post\", url=path, json_data=body, files=await async_to_httpx_files(files), **options\r\n   [1835](file:///D:/anaconda3/envs/zgb2/Lib/site-packages/openai/_base_client.py:1835)     )\r\n-> [1836](file:///D:/anaconda3/envs/zgb2/Lib/site-packages/openai/_base_client.py:1836)     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\r\n\r\nFile [d:\\anaconda3\\envs\\zgb2\\Lib\\site-packages\\openai\\_base_client.py:1524](file:///D:/anaconda3/envs/zgb2/Lib/site-packages/openai/_base_client.py:1524), in AsyncAPIClient.request(self, cast_to, options, stream, stream_cls, remaining_retries)\r\n   [1515](file:///D:/anaconda3/envs/zgb2/Lib/site-packages/openai/_base_client.py:1515) async def request(\r\n   [1516](file:///D:/anaconda3/envs/zgb2/Lib/site-packages/openai/_base_client.py:1516)     self,\r\n   [1517](file:///D:/anaconda3/envs/zgb2/Lib/site-packages/openai/_base_client.py:1517)     cast_to: Type[ResponseT],\r\n   (...)\r\n   [1522](file:///D:/anaconda3/envs/zgb2/Lib/site-packages/openai/_base_client.py:1522)     remaining_retries: Optional[int] = None,\r\n   [1523](file:///D:/anaconda3/envs/zgb2/Lib/site-packages/openai/_base_client.py:1523) ) -> ResponseT | _AsyncStreamT:\r\n-> [1524](file:///D:/anaconda3/envs/zgb2/Lib/site-packages/openai/_base_client.py:1524)     return await self._request(\r\n   [1525](file:///D:/anaconda3/envs/zgb2/Lib/site-packages/openai/_base_client.py:1525)         cast_to=cast_to,\r\n   [1526](file:///D:/anaconda3/envs/zgb2/Lib/site-packages/openai/_base_client.py:1526)         options=options,\r\n   [1527](file:///D:/anaconda3/envs/zgb2/Lib/site-packages/openai/_base_client.py:1527)         stream=stream,\r\n   [1528](file:///D:/anaconda3/envs/zgb2/Lib/site-packages/openai/_base_client.py:1528)         stream_cls=stream_cls,\r\n   [1529](file:///D:/anaconda3/envs/zgb2/Lib/site-packages/openai/_base_client.py:1529)         remaining_retries=remaining_retries,\r\n   [1530](file:///D:/anaconda3/envs/zgb2/Lib/site-packages/openai/_base_client.py:1530)     )\r\n\r\nFile [d:\\anaconda3\\envs\\zgb2\\Lib\\site-packages\\openai\\_base_client.py:1625](file:///D:/anaconda3/envs/zgb2/Lib/site-packages/openai/_base_client.py:1625), in AsyncAPIClient._request(self, cast_to, options, stream, stream_cls, remaining_retries)\r\n   [1622](file:///D:/anaconda3/envs/zgb2/Lib/site-packages/openai/_base_client.py:1622)         await err.response.aread()\r\n   [1624](file:///D:/anaconda3/envs/zgb2/Lib/site-packages/openai/_base_client.py:1624)     log.debug(\"Re-raising status error\")\r\n-> [1625](file:///D:/anaconda3/envs/zgb2/Lib/site-packages/openai/_base_client.py:1625)     raise self._make_status_error_from_response(err.response) from None\r\n   [1627](file:///D:/anaconda3/envs/zgb2/Lib/site-packages/openai/_base_client.py:1627) return await self._process_response(\r\n   [1628](file:///D:/anaconda3/envs/zgb2/Lib/site-packages/openai/_base_client.py:1628)     cast_to=cast_to,\r\n   [1629](file:///D:/anaconda3/envs/zgb2/Lib/site-packages/openai/_base_client.py:1629)     options=options,\r\n   (...)\r\n   [1633](file:///D:/anaconda3/envs/zgb2/Lib/site-packages/openai/_base_client.py:1633)     retries_taken=options.get_max_retries(self.max_retries) - retries,\r\n   [1634](file:///D:/anaconda3/envs/zgb2/Lib/site-packages/openai/_base_client.py:1634) )\r\n\r\nAuthenticationError: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-qVL45***************************************D1Vi. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}\r\n```\r\n\r\n\r\nI would greatly appreciate it if I could receive any response or assistance from you!\n\n### To Reproduce\n\n...\n\n### Code snippets\n\n```Python\n...\n```\n\n\n### OS\n\nwindows11\n\n### Python version\n\nPython v3.11.11\n\n### Library version\n\nopenai v1.39.0",
    "state": "closed",
    "created_at": "2024-12-21T03:49:36+00:00",
    "closed_at": "2024-12-26T08:06:55+00:00",
    "updated_at": "2024-12-26T08:06:55+00:00",
    "author": "jkkjjj",
    "author_type": "User",
    "comments_count": 5,
    "reactions_count": 0,
    "labels": "bug",
    "milestone": null,
    "closed_by": "jkkjjj",
    "resolution_time_hours": 124.28861111111111,
    "first_comments": [
      {
        "author": "bearycool11",
        "created_at": "2024-12-24T18:18:05+00:00",
        "body": "Alright, let's dive into this AuthenticationError. It's throwing a 401, which usually means the API key is getting rejected. But you've already confirmed it works with a proxy, so there's something else going on.\r\n\r\nLooking at the traceback, it seems the error is happening within the _achat_completion call. This suggests the issue might be with how the llm instance is handling the API key when making that specific request.\r\n\r\nHere's what we can check:\r\n\r\nAPI Key Passing: Make sure the llm instance is correctly passing the API key in the Authorization header of the _achat_completion request. There might be a bug where it's not being included or is being formatted incorrectly.\r\n\r\nEnvironment Variables: Although you've confirmed the API key works with a proxy, double-check that the environment variable is being read correctly within the llm instance. There might be a conflict or a different way the instance accesses environment variables.\r\n\r\nLibrary Version: You're using openai v1.39.0. Check if there are any known issues or bug reports related to authentication in this version. It's possible a newer version has a fix or a workaround.\r\n\r\nRequest Inspection: If possible, inspect the actual HTTP request being sent by the _achat_completion call. This will help confirm whether the API key is being included and formatted correctly in the Authorization header.\r\n\r\nOpenAI Support: If all else fails, reach out to OpenAI support directly. They might have insights into specific issues with the Python library or the API itself.\r\n\r\nLet's systematically investigate these points and see if we can pinpoint the root cause of this authentication error."
      },
      {
        "author": "bearycool11",
        "created_at": "2024-12-24T18:19:25+00:00",
        "body": "Alright, let's break down this AuthenticationError and get to the bottom of it.\r\n\r\nFirst, let's confirm the API key situation. You mentioned it works with a proxy, so it's likely valid. However, the error message explicitly states an incorrect key. This inconsistency suggests the key might not be passed correctly to the _achat_completion function.\r\n\r\nLooking at the traceback, the error originates from openai\\_base_client.py, specifically within the _request function. This function likely handles the API call to OpenAI's servers. The AuthenticationError is raised when the server responds with a 401 status code, indicating authentication failure.\r\n\r\nHere's a breakdown of potential causes and debugging steps:\r\n\r\nAPI Key Handling:\r\n\r\nVerify how the OpenAILLM class stores and accesses the API key.\r\nEnsure the key is correctly passed to the aclient.chat.completions.create(**kwargs) call.\r\nInspect the kwargs dictionary to confirm the key is present and correctly formatted.\r\nEnvironment Variables:\r\n\r\nWhile the key works with a proxy, ensure the OpenAILLM instance accesses it correctly.\r\nPrint the key's value within the _achat_completion function to confirm it matches your actual key.\r\nLibrary Version:\r\n\r\nCheck for known authentication issues in openai v1.39.0.\r\nConsider upgrading to the latest version if a fix is available.\r\nRequest Inspection:\r\n\r\nIf possible, capture and examine the HTTP request sent by _achat_completion.\r\nVerify the Authorization header includes the correct key and prefix (e.g., \"Bearer sk-[your_key]\").\r\nOpenAI Support:\r\n\r\nIf the above steps don't resolve the issue, contact OpenAI support for assistance.\r\nLet's systematically investig"
      },
      {
        "author": "bearycool11",
        "created_at": "2024-12-24T18:20:04+00:00",
        "body": "a few more things I'm noticing as well beyond this\r\n\r\nFirst, that traceback is long, but it doesn't show us the contents of kwargs. That's where the API key should be, and we need to see exactly how it's being passed to the create() call.  Without that, we're kinda shooting in the dark.\r\n\r\nSecond, this metagpt thing... is that a custom library or something?  It's involved in the call stack, but we don't know much about it.  Could be relevant, could be a red herring, but worth noting.\r\n\r\nThird, it's using httpx for requests.  Not the usual requests library.  Might be a difference in how they handle environment variables or headers.  Something to keep in mind.\r\n\r\nAnd lastly, this is all happening in a Jupyter notebook (Cell In[13]).  Those can sometimes have weird environment quirks.  Maybe worth trying this outside the notebook, just a basic Python script, to rule that out.\r\n\r\nOverall, feels like we're missing some key info.  Need to see that kwargs dictionary, maybe dig into metagpt, and consider those httpx and Jupyter factors.  The more we know, the better we can hunt down this bug"
      },
      {
        "author": "csgulati09",
        "created_at": "2024-12-26T07:30:12+00:00",
        "body": "Maybe you have to check if your headers are being passed extra_headers or not. "
      },
      {
        "author": "jkkjjj",
        "created_at": "2024-12-26T08:06:22+00:00",
        "body": "Thank you all!!"
      }
    ],
    "url": "https://github.com/openai/openai-python/issues/1968"
  },
  {
    "repository": "openai/openai-python",
    "issue_number": 1950,
    "title": "Client side JSON Schema in `response_format` validation for  structured outputs",
    "body": "### Confirm this is a feature request for the Python library and not the underlying OpenAI API.\r\n\r\n- [X] This is a feature request for the Python library\r\n\r\n### Describe the feature or improvement you're requesting\r\n\r\nWhen running `client.beta.chat.completions.parse` the `response_format` seems to be validated through the API as opposed to the client.\r\nWhy is that?\r\n\r\nMy (hacky) solution to validating JSON Schema is to run a minimal request to the API just to verify if the schema is valid.\r\n \r\nIs there any way we could be able to validate schemas without making a chat completions query?\r\n\r\nExample (it is intended to fail):\r\n```py\r\nschema = {\r\n    \"type\": \"object\",\r\n    \"description\": \"The description of this item\",\r\n    \"properties\": {\r\n        \"id\": {\r\n            \"description\": \"The id of this inner item\",\r\n            \"type\": \"integer\"\r\n        },\r\n        \"value\": {\r\n            \"type\": \"array\",\r\n            \"description\": \"The list of values of this inner item\",\r\n            \"items\": {\r\n                \"type\": \"string\",\r\n                \"description\": \"The value of this inner item\",\r\n                \"enum\": [\"a\", \"b\"]\r\n            },\r\n        }\r\n    },\r\n    # \"required\": [\"value\", \"id\"], # FAILS HERE INTENTIONALLY TO DEMONSTRATE API SIDE VALIDATION\r\n    \"additionalProperties\": False,\r\n}\r\n\r\nimport openai\r\nclient = openai.OpenAI(api_key=settings.OPENAI_API_KEY)\r\n\r\ndef validate_schema(schema: dict) -> None:\r\n    try:    \r\n        client.beta.chat.completions.parse(\r\n            model=\"gpt-4o-mini\",\r\n            messages=[\r\n                {\r\n                    \"role\": \"user\",\r\n                    \"content\": \"a\"\r\n                }\r\n            ],\r\n            max_completion_tokens=1,\r\n            response_format={\r\n                \"type\": \"json_schema\",\r\n                \"json_schema\": {\r\n                    \"name\": \"action_items\",\r\n                    \"description\": \"The action items to be completed\",\r\n                    \"strict\": True,\r\n                    \"schema\": schema,\r\n                },\r\n            }\r\n        )\r\n    except openai._exceptions.LengthFinishReasonError as e:\r\n        pass\r\n    except Exception as e:\r\n        # FAILS HERE\r\n        raise e\r\n\r\nvalidate_schema(schema)\r\n```\r\n\r\n\r\n\r\n### Additional context\r\n\r\n(thats all)",
    "state": "closed",
    "created_at": "2024-12-15T21:47:17+00:00",
    "closed_at": "2024-12-16T12:16:57+00:00",
    "updated_at": "2025-01-24T14:40:04+00:00",
    "author": "CakeCrusher",
    "author_type": "User",
    "comments_count": 6,
    "reactions_count": 0,
    "labels": "",
    "milestone": null,
    "closed_by": "RobertCraigie",
    "resolution_time_hours": 14.494444444444444,
    "first_comments": [
      {
        "author": "CakeCrusher",
        "created_at": "2024-12-15T21:48:15+00:00",
        "body": "@RobertCraigie "
      },
      {
        "author": "RobertCraigie",
        "created_at": "2024-12-16T12:16:57+00:00",
        "body": "> When running client.beta.chat.completions.parse the response_format seems to be validated through the API as opposed to the client.\r\n> Why is that?\r\n\r\nWe don't want to do client-side validation as the API could be improved at any time to support new types of schemas and if we add client-side validation, that would lag behind and users on older SDK versions wouldn't benefit from the updated API.\r\n\r\nCurrently we have no plans to add client-side validation but I'm curious what your use case is?\r\n"
      },
      {
        "author": "CakeCrusher",
        "created_at": "2024-12-17T18:08:51+00:00",
        "body": "@RobertCraigie \r\n> > When running client.beta.chat.completions.parse the response_format seems to be validated through the API as opposed to the client.\r\n> > Why is that?\r\n> \r\n> We don't want to do client-side validation as the API could be improved at any time to support new types of schemas and if we add client-side validation, that would lag behind and users on older SDK versions wouldn't benefit from the updated API.\r\n> \r\n> Currently we have no plans to add client-side validation but I'm curious what your use case is?\r\n\r\nI am generating tools/actions which consists of both generating the code and writing its JSON Schema with the objective of loading the schema back into the request as a tool. At the moment I am not aware of any way to validate that the schema will actually work without making a chat completions request. Hence this is what I do\r\nhttps://github.com/CakeCrusher/ActionCollective/blob/main/v0/client/action_collective/client.py#L30-L53\r\n(PS: ill be cleaning things up and updating readme to make this repo usable, will be ready by the end oftomorrow.)\r\n\r\n\r\nI think having an endpoint exclusively for validating schema would be a good solution. Simply returns 201 OK or 400s error.\r\n\r\n\r\nApplication of https://arxiv.org/abs/2411.01747 ...\r\nIn structured outputs designed to be deployed https://github.com/CakeCrusher/ActionCollective"
      },
      {
        "author": "PhilipMathieuIDEXX",
        "created_at": "2025-01-22T14:56:51+00:00",
        "body": "Hi @CakeCrusher,\n\nI have a similar need to do client-side validation of response schemas. While I understand the logic behind @RobertCraigie 's response from OpenAI's perspective, it's not hard to imagine other situations where this would be extremely helpful for transparent debugging.\n\nI implemented a static, pure-python function to do this. I am sure there are better/simpler ways to do this JSON schema enforcement tools, but this works for me.\n\nhttps://gist.github.com/PhilipMathieuIDEXX/ee627d4cf210f5d8f4db510837797b33\n\n@CakeCrusher would be curious your opinion of this approach and/or if you've found other projects already doing this in a better way."
      },
      {
        "author": "CakeCrusher",
        "created_at": "2025-01-24T04:07:40+00:00",
        "body": "Hi @PhilipMathieuIDEXX ,\nCould you exapand on this `I implemented a static, pure-python function to do this.`?\n\nI haven't ran into any solutions to this."
      }
    ],
    "url": "https://github.com/openai/openai-python/issues/1950"
  },
  {
    "repository": "openai/openai-python",
    "issue_number": 1944,
    "title": "AI Search Engine. (AISE)",
    "body": "### Confirm this is a feature request for the Python library and the underlying OpenAI API. \n\n- [X] This is a feature request for the Python library and OpenAI API\n\nConfirm \n\n### Describe the feature or improvement you're requesting\n\nReplace Google and try to use startpage or ecosia. \n\n\n",
    "state": "closed",
    "created_at": "2024-12-12T12:57:30+00:00",
    "closed_at": "2024-12-12T13:05:14+00:00",
    "updated_at": "2024-12-12T13:11:15+00:00",
    "author": "JulianWe",
    "author_type": "User",
    "comments_count": 0,
    "reactions_count": 0,
    "labels": "",
    "milestone": null,
    "closed_by": "RobertCraigie",
    "resolution_time_hours": 0.1288888888888889,
    "first_comments": [],
    "url": "https://github.com/openai/openai-python/issues/1944"
  },
  {
    "repository": "openai/openai-python",
    "issue_number": 1938,
    "title": "passing `include` keyword argument to `beta.threads.runs.stream` function leads to openai.BadRequestError",
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\r\n\r\n- [X] This is an issue with the Python library\r\n\r\n### Describe the bug\r\n\r\npassing `include` keyword argument to `beta.threads.runs.stream` function leads to \r\n\r\n> openai.BadRequestError: Error code: 400 - {'error': {'message': \"Unknown parameter: 'include'.\", 'type': 'invalid_request_error', 'param': 'include', 'code': 'unknown_parameter'}}\r\n\r\n### To Reproduce\r\n\r\n```python\r\nclient.beta.threads.runs.stream(\r\n                thread_id=thread.id,\r\n                assistant_id=assistant.id,\r\n                include=[\"step_details.tool_calls[*].file_search.results[*].content\"],\r\n)\r\n```\r\n\r\n### Code snippets\r\n\r\n_No response_\r\n\r\n### OS\r\n\r\nmacOS\r\n\r\n### Python version\r\n\r\nPython v3.11.5\r\n\r\n### Library version\r\n\r\nopenai v1.57.0",
    "state": "closed",
    "created_at": "2024-12-12T00:05:43+00:00",
    "closed_at": "2024-12-16T12:24:51+00:00",
    "updated_at": "2024-12-16T14:02:29+00:00",
    "author": "vitorarrais",
    "author_type": "User",
    "comments_count": 2,
    "reactions_count": 0,
    "labels": "bug",
    "milestone": null,
    "closed_by": "RobertCraigie",
    "resolution_time_hours": 108.31888888888889,
    "first_comments": [
      {
        "author": "RobertCraigie",
        "created_at": "2024-12-16T12:24:51+00:00",
        "body": "Thanks for the report, this will be fixed in [`v1.57.5`](https://github.com/openai/openai-python/pull/1952)."
      },
      {
        "author": "vitorarrais",
        "created_at": "2024-12-16T14:02:28+00:00",
        "body": "No worries. That's great to hear that will get fixed soon 🚀 "
      }
    ],
    "url": "https://github.com/openai/openai-python/issues/1938"
  },
  {
    "repository": "openai/openai-python",
    "issue_number": 1936,
    "title": "TypeError: issubclass() arg 1 must be a class",
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\r\n\r\n- [X] This is an issue with the Python library\r\n\r\n### Describe the bug\r\n\r\nWhen calling `OpenAI.beta.chat.completions.parse` in a highly concurrent environment, and providing a `class` as the `response_format`, I get the following error:\r\n\r\n```\r\n    response = self.client.beta.chat.completions.parse(\r\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.11/site-packages/openai/resources/beta/chat/completions.py\", line 156, in parse\r\n    return self._post(\r\n           ^^^^^^^^^^^\r\n  File \"/local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.11/site-packages/openai/_base_client.py\", line 1280, in post\r\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\r\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.11/site-packages/openai/_base_client.py\", line 957, in request\r\n    return self._request(\r\n           ^^^^^^^^^^^^^^\r\n  File \"/local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.11/site-packages/openai/_base_client.py\", line 1063, in _request\r\n    return self._process_response(\r\n           ^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.11/site-packages/openai/_base_client.py\", line 1162, in _process_response\r\n    return api_response.parse()\r\n           ^^^^^^^^^^^^^^^^^^^^\r\n  File \"/local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.11/site-packages/openai/_response.py\", line 319, in parse\r\n    parsed = self._options.post_parser(parsed)\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.11/site-packages/openai/resources/beta/chat/completions.py\", line 150, in parser\r\n    return _parse_chat_completion(\r\n           ^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.11/site-packages/openai/lib/_parsing/_completions.py\", line 122, in parse_chat_completion\r\n    construct_type_unchecked(\r\n  File \"/local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.11/site-packages/openai/_models.py\", line 445, in construct_type_unchecked\r\n    return cast(_T, construct_type(value=value, type_=type_))\r\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.11/site-packages/openai/_models.py\", line 519, in construct_type\r\n    return type_.construct(**value)  # type: ignore[arg-type]\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.11/site-packages/openai/_models.py\", line 230, in construct\r\n    fields_values[name] = _construct_field(value=values[key], field=field, key=key)\r\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.11/site-packages/openai/_models.py\", line 394, in _construct_field\r\n    return construct_type(value=value, type_=type_)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.11/site-packages/openai/_models.py\", line 513, in construct_type\r\n    if not is_literal_type(type_) and (issubclass(origin, BaseModel) or issubclass(origin, GenericModel)):\r\n                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"<frozen abc>\", line 123, in __subclasscheck__\r\nTypeError: issubclass() arg 1 must be a class\r\n```\r\n\r\nThis is being run in a multi-threaded environment. When I run with a single thread, I don't see this issue. I'm trying to churn through a bunch of data, so I'm attempting to use about 100 threads to make these API requests in parallel. If I reduce the count to 1 the problem goes away.\r\n\r\nTo work around this, I believe I can stop using the beta `parse` method with the provided `response_format`, \r\n\r\n### To Reproduce\r\n\r\n1. Call `OpenAI.beta.chat.completions.parse` with 100 threads simultaneously\r\n\r\n### Code snippets\r\n\r\n```Python\r\nclass Bar(Enum):\r\n    C = \"C\"\r\n\r\nclass Qux(Enum):\r\n    D = \"D\"\r\n\r\nclass Foo(BaseModel):\r\n    a: Bar\r\n    b: Qux\r\n\r\ndef main(): # Called by 100 threads concurrently\r\n    OpenAI(api_key=\"...\").beta.chat.completions.parse(\r\n        model=\"gpt-4o-mini\",\r\n        seed=0,\r\n        temperature=0,\r\n        messages=[\r\n            {\"role\": \"system\", \"content\": \"...\"},\r\n            {\"role\": \"user\", \"content\": \"...\"},\r\n        ],\r\n        response_format=Foo,\r\n    )\r\n```\r\n\r\n\r\n### OS\r\n\r\nUbuntu 22.04.4 LTS\r\n\r\n### Python version\r\n\r\nPython v3.11.10\r\n\r\n### Library version\r\n\r\nopenai v1.57.2",
    "state": "closed",
    "created_at": "2024-12-10T19:12:35+00:00",
    "closed_at": "2025-01-06T17:14:19+00:00",
    "updated_at": "2025-01-06T17:14:20+00:00",
    "author": "aardvarkk",
    "author_type": "User",
    "comments_count": 2,
    "reactions_count": 0,
    "labels": "bug",
    "milestone": null,
    "closed_by": "RobertCraigie",
    "resolution_time_hours": 646.0288888888889,
    "first_comments": [
      {
        "author": "Joseelmax-00",
        "created_at": "2024-12-28T21:17:30+00:00",
        "body": "I can confirm I'm having the same issue which is making it impossible for me to use o1-preview. Tried changing versions of pydantic, typing_extensions and other libraries to no avail. Python 3.11.0 and this environment:\r\n\r\nPackage           Version\r\n----------------- ----------\r\nannotated-types   0.7.0\r\nanyio             4.7.0\r\ncertifi           2024.12.14\r\ncolorama          0.4.6\r\ndistro            1.9.0\r\nh11               0.14.0\r\nhttpcore          1.0.7\r\nhttpx             0.28.1\r\nidna              3.10\r\njiter             0.8.2\r\nmypy-extensions   1.0.0\r\nopenai            1.58.1\r\npip               24.3.1\r\npydantic          2.10.4\r\npydantic_core     2.27.2\r\npython-dotenv     1.0.1\r\nsetuptools        65.5.0\r\nsniffio           1.3.1\r\ntqdm              4.67.1\r\ntyping_extensions 4.12.2\r\ntyping-inspect    0.9.0\r\n\r\n```\r\nFile \"D:\\Trabajo\\random2\\game of Quatro\\openai_player.py\", line 172, in select_AI_move\r\n    completion = client.beta.chat.completions.parse(\r\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"D:\\Trabajo\\random2\\game of Quatro\\env\\Lib\\site-packages\\openai\\resources\\beta\\chat\\completions.py\", line 181, in parse\r\n    \"response_format\": _type_to_response_format(response_format),\r\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"D:\\Trabajo\\random2\\game of Quatro\\env\\Lib\\site-packages\\openai\\lib\\_parsing\\_completions.py\", line 248, in type_to_response_format_param\r\n    if is_basemodel_type(response_format):\r\n       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"D:\\Trabajo\\random2\\game of Quatro\\env\\Lib\\site-packages\\openai\\lib\\_pydantic.py\", line 130, in is_basemodel_type\r\n    return issubclass(typ, pydantic.BaseModel)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"<frozen abc>\", line 123, in __subclasscheck__\r\nTypeError: issubclass() arg 1 must be a class\r\n```"
      },
      {
        "author": "RobertCraigie",
        "created_at": "2025-01-06T17:14:19+00:00",
        "body": "Thanks for the bug report, I haven't been able to reproduce this issue but it should be fixed in the next release as I've added some more `inspect.isclass()` checks to the places referenced in the stack traces.\r\n\r\nhttps://github.com/openai/openai-python/pull/1987"
      }
    ],
    "url": "https://github.com/openai/openai-python/issues/1936"
  },
  {
    "repository": "openai/openai-python",
    "issue_number": 1928,
    "title": "Json_schema structured output type fails when extra_body is provided ",
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\n\n- [X] This is an issue with the Python library\n\n### Describe the bug\n\nTrying to use the client.beta.chat.completions.parse function works until i try to provide an extra body argument, in this case Azure AI Search index.\r\nTested with versions 1.53.0, 1.54.3 and 1.57.0 \n\n### To Reproduce\n\n1. Run the completion without extra body\r\n2. Remove comment for extra_body(give a valid azure search endpoint and index)\r\n\r\nError:\r\nBadRequestError: Error code: 400 - {'error': {'requestid': '', 'code': 400, 'message': \"Validation error at #/response_format/type: Input should be 'text' or 'json_object'\"}}    \r\n\n\n### Code snippets\n\n```Python\nclass Answer(BaseModel):\r\n    answer: str\r\nextra_body = {\r\n                 \"data_sources\": [\r\n                    {\r\n                        \"type\": \"azure_search\",\r\n                        \"parameters\": {\r\n                            \"in_scope\": True,\r\n                            \"strictness\": int(4),\r\n                            \"top_n_documents\": int(10),\r\n                            \"endpoint\": \"<azure search endppoint>\",\r\n                            \"index_name\": \"<index-name>\",\r\n                            \"authentication\": {\r\n                                \"type\": \"system_assigned_managed_identity\"\r\n                            },\r\n                            \"embedding_dependency\": {\r\n                                \"type\": \"deployment_name\", \r\n                                \"deployment_name\": \"text-embedding-3-large\",\r\n                                \"dimensions\": 3072,\r\n                            },\r\n                            \"query_type\": \"vector_semantic_hybrid\",\r\n                            \"semantic_configuration\": \"my-semantic-config\",\r\n                        }\r\n                    }\r\n                ]\r\n            }\r\nmessage_text = [ {\"role\": \"user\", \"content\": \"when was google founded?\"}]\r\n\r\nstructured_completion = openai_client.beta.chat.completions.parse(  \r\n    model=\"gpt-4o\",  \r\n    messages=message_text,  \r\n    max_tokens=800,  \r\n    temperature=0.3, \r\n    # extra_body=extra_body,\r\n    response_format=Answer\r\n)  \r\n\r\nresponse = structured_completion.choices[0].message.parsed\r\nprint(response)\n```\n\n\n### OS\n\nmacOS\n\n### Python version\n\nPython 3.12.7\n\n### Library version\n\n1.57.0 ",
    "state": "closed",
    "created_at": "2024-12-09T13:16:19+00:00",
    "closed_at": "2024-12-09T18:16:49+00:00",
    "updated_at": "2024-12-09T18:16:49+00:00",
    "author": "csirbu",
    "author_type": "User",
    "comments_count": 2,
    "reactions_count": 0,
    "labels": "Azure,API-feedback",
    "milestone": null,
    "closed_by": "RobertCraigie",
    "resolution_time_hours": 5.008333333333334,
    "first_comments": [
      {
        "author": "RobertCraigie",
        "created_at": "2024-12-09T13:40:17+00:00",
        "body": "cc @kristapratico, I can't reproduce this with the main client"
      },
      {
        "author": "kristapratico",
        "created_at": "2024-12-09T18:09:23+00:00",
        "body": "@csirbu Azure OpenAI [structured outputs](https://learn.microsoft.com/azure/ai-services/openai/how-to/structured-outputs?tabs=python-secure) is currently not supported with bring your own data. If passing an Azure Search index for your data source,  `response_format`s supported are `text` and `json_object`. Note that this is a current limitation of the Azure service, not an issue with the Python client library. I'll pass on your feedback to the team."
      }
    ],
    "url": "https://github.com/openai/openai-python/issues/1928"
  },
  {
    "repository": "openai/openai-python",
    "issue_number": 1926,
    "title": "openai.ChatCompletion.create still triggering APIRemovedInV1 error despite using openai>=1.57.0",
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\n\n- [X] This is an issue with the Python library\n\n### Describe the bug\n\nDespite installing the latest OpenAI Python library (1.57.0), I am encountering the error:\r\nYou tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0.\r\n\r\nI would greatly appreciate any insights into resolving this issue. Is this a library bug or something specific to my setup? Thank you for your support!\n\n### To Reproduce\n\n- Verified the Python version (Python 3.12.5) on both Windows and Linux environments.\r\n- Used pip uninstall openai and pip install openai to ensure the latest version is installed.\r\n- Replaced all code references to legacy APIs with the correct openai.ChatCompletion.create syntax.\r\n- Tested API connectivity via curl, which worked successfully and returned the list of available models.\r\n\r\nDespite these steps, running the Python script still produces the error indicating the legacy API is being called.\r\nWhen I test the connection, the traceback includes:\r\n\r\nopenai.lib._old_api.APIRemovedInV1: You tried to access openai.ChatCompletion, but this is no longer supported...\r\n\n\n### Code snippets\n\n```Python\nimport openai\r\n\r\nopenai.api_key = \"sk-xxx\"  # Replace with actual key during testing\r\n\r\ntry:\r\n    response = openai.ChatCompletion.create(\r\n        model=\"gpt-4\",\r\n        messages=[{\"role\": \"user\", \"content\": \"Test connection\"}]\r\n    )\r\n    print(response[\"choices\"][0][\"message\"][\"content\"])\r\nexcept Exception as e:\r\n    print(f\"Error: {e}\")\r\n\r\n--------------------\r\n\r\nExpected Outcome: Successful API connection with a basic response from GPT.\r\n\r\n--------------------\r\n\r\nOutput / Error Traceback:\r\n\r\nTraceback (most recent call last):\r\n  File \"test_openai_connections.py\", line 8, in <module>\r\n    response = openai.ChatCompletion.create(\r\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/user/venv/lib/python3.12/site-packages/openai/lib/_old_api.py\", line 39, in __call__\r\n    raise APIRemovedInV1(symbol=self._symbol)\r\nopenai.lib._old_api.APIRemovedInV1: You tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0.\n```\n\n\n### OS\n\nWindows 11 and Linux (Debian VM via VirtualBox) --> Virtual environment (`venv`) used on Linux VM.\n\n### Python version\n\nPython 3.12.5\n\n### Library version\n\nopenai==1.57.0",
    "state": "closed",
    "created_at": "2024-12-06T23:44:33+00:00",
    "closed_at": "2024-12-09T10:44:23+00:00",
    "updated_at": "2024-12-09T10:44:24+00:00",
    "author": "bluesky1900",
    "author_type": "User",
    "comments_count": 4,
    "reactions_count": 0,
    "labels": "bug",
    "milestone": null,
    "closed_by": "RobertCraigie",
    "resolution_time_hours": 58.99722222222222,
    "first_comments": [
      {
        "author": "jacmey",
        "created_at": "2024-12-07T00:41:44+00:00",
        "body": "Did you fix it?"
      },
      {
        "author": "bluesky1900",
        "created_at": "2024-12-07T02:35:01+00:00",
        "body": "Nope. Hit a wall with this.\r\n\r\nOn Fri, Dec 6, 2024 at 5:42 PM jacmey ***@***.***> wrote:\r\n\r\n> Did you fix it?\r\n>\r\n> —\r\n> Reply to this email directly, view it on GitHub\r\n> <https://github.com/openai/openai-python/issues/1926#issuecomment-2524709328>,\r\n> or unsubscribe\r\n> <https://github.com/notifications/unsubscribe-auth/AFRUE24MD6DCSGPLKR4LXM32EI755AVCNFSM6AAAAABTFTJ2A2VHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZDKMRUG4YDSMZSHA>\r\n> .\r\n> You are receiving this because you authored the thread.Message ID:\r\n> ***@***.***>\r\n>\r\n"
      },
      {
        "author": "RedHouseLux",
        "created_at": "2024-12-08T00:11:33+00:00",
        "body": "same problem here, any clue?"
      },
      {
        "author": "RobertCraigie",
        "created_at": "2024-12-09T10:44:24+00:00",
        "body": "You're using the old API which is no longer supported, the full error message includes docs links showing the new syntax:\r\n```\r\nYou tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\r\n\r\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface.\r\n\r\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\r\n\r\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\r\n```"
      }
    ],
    "url": "https://github.com/openai/openai-python/issues/1926"
  },
  {
    "repository": "openai/openai-python",
    "issue_number": 1925,
    "title": "httpx version not pinned - upgrade to httpx==28.0.0 breaks AzureOpenAI class",
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\n\n- [X] This is an issue with the Python library\n\n### Describe the bug\n\nThe AzureOpenAI class makes use of the httpx package, which has very loose version restrictions in the openapi-python package.\r\n\r\n`dependencies = [\r\n    \"httpx>=0.23.0, <1\", ... ]`\r\n\r\nThe httpx package version was bumped to `0.28.0` which breaks the AzureOpenAI class.\r\nDowngrading the httpx version to `~0.27.0` solves the issue.\r\nEven though this is a httpx package issue, I would suggest to pin the version to avoid breaking changes for any major releases in subdependencies.\n\n### To Reproduce\n\n1. Use `openai==1.43.0` and `httpx==0.28.0` (also happens with higher versions of the openapi-python package)\r\n2. Instantiate an object from the AzureOpenAI() class.\n\n### Code snippets\n\n```Python\nclient = AzureOpenAI(\r\n\r\nFile \"/usr/local/lib/python3.11/site-packages/openai/lib/azure.py\", line 205, in __init__\r\n    super().__init__(\r\n\r\nFile \"/usr/local/lib/python3.11/site-packages/openai/_client.py\", line 123, in __init__\r\n    super().__init__(\r\n\r\nFile \"/usr/local/lib/python3.11/site-packages/openai/_base_client.py\", line 844, in __init__\r\n    self._client = http_client or SyncHttpxClientWrapper(\r\n\r\nFile \"/usr/local/lib/python3.11/site-packages/openai/_base_client.py\", line 742, in __init__\r\n    super().__init__(**kwargs)\r\n\r\nTypeError: Client.__init__() got an unexpected keyword argument 'proxies'\n```\n\n\n### OS\n\nDebian Linux\n\n### Python version\n\nPython v3.11.1\n\n### Library version\n\nopenai v1.43.0",
    "state": "closed",
    "created_at": "2024-12-05T16:21:30+00:00",
    "closed_at": "2024-12-05T16:50:57+00:00",
    "updated_at": "2024-12-05T16:50:57+00:00",
    "author": "Marco1402",
    "author_type": "User",
    "comments_count": 1,
    "reactions_count": 0,
    "labels": "bug",
    "milestone": null,
    "closed_by": "RobertCraigie",
    "resolution_time_hours": 0.49083333333333334,
    "first_comments": [
      {
        "author": "RobertCraigie",
        "created_at": "2024-12-05T16:50:57+00:00",
        "body": "This is fixed in the latest version, please upgrade and try again."
      }
    ],
    "url": "https://github.com/openai/openai-python/issues/1925"
  },
  {
    "repository": "openai/openai-python",
    "issue_number": 1915,
    "title": "httpx 0.28.0 removed proxies in httpx.Client, result in error",
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\n\n- [X] This is an issue with the Python library\n\n### Describe the bug\n\nhttpx 0.28.0 removed proxies in httpx.Client, result in error:\r\n```\r\n  File \".../test.py\", line 3, in <module>\r\n    client = OpenAI(\r\n        api_key=\"API_KEY\",\r\n        base_url=\"https://some_url/v1\",\r\n    )\r\n  File \".../lib/python3.13/site-packages/openai/_client.py\", line 123, in __init__\r\n    super().__init__(\r\n    ~~~~~~~~~~~~~~~~^\r\n        version=__version__,\r\n        ^^^^^^^^^^^^^^^^^^^^\r\n    ...<6 lines>...\r\n        _strict_response_validation=_strict_response_validation,\r\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n    )\r\n    ^\r\n  File \".../lib/python3.13/site-packages/openai/_base_client.py\", line 857, in __init__\r\n    self._client = http_client or SyncHttpxClientWrapper(\r\n                                  ~~~~~~~~~~~~~~~~~~~~~~^\r\n        base_url=base_url,\r\n        ^^^^^^^^^^^^^^^^^^\r\n    ...<5 lines>...\r\n        follow_redirects=True,\r\n        ^^^^^^^^^^^^^^^^^^^^^^\r\n    )\r\n    ^\r\n  File \".../lib/python3.13/site-packages/openai/_base_client.py\", line 755, in __init__\r\n    super().__init__(**kwargs)\r\n    ~~~~~~~~~~~~~~~~^^^^^^^^^^\r\nTypeError: Client.__init__() got an unexpected keyword argument 'proxies'\r\n```\n\n### To Reproduce\n\nWith a python file with the following content can reproduce the error:\r\n```\r\nfrom openai import OpenAI\r\n\r\nclient = OpenAI(\r\n    api_key=\"API_KEY\",\r\n    base_url=\"https://some_url/v1\",\r\n)\r\n```\r\n\r\nOutput of `pip freeze`: (note httpx is 0.28.0)\r\n```\r\nannotated-types==0.7.0\r\nanyio==4.6.2.post1\r\nblinker==1.9.0\r\nbuild==1.2.2.post1\r\ncertifi==2024.8.30\r\ncffi==1.17.1\r\ncharset-normalizer==3.4.0\r\nclick==8.1.7\r\ncryptography==43.0.3\r\ndistro==1.9.0\r\nFlask==3.1.0\r\nh11==0.14.0\r\nhttpcore==1.0.7\r\nhttpx==0.28.0\r\nidna==3.10\r\nitsdangerous==2.2.0\r\nJinja2==3.1.4\r\njiter==0.8.0\r\nMarkupSafe==3.0.2\r\nopenai==1.54.4\r\npackaging==24.2\r\npip-tools==7.4.1\r\npycparser==2.22\r\npydantic==2.10.2\r\npydantic_core==2.27.1\r\npyproject_hooks==1.2.0\r\npython-dateutil==2.9.0.post0\r\npython-dotenv==1.0.1\r\nrequests==2.32.3\r\nsetuptools==75.1.0\r\nsix==1.16.0\r\nsniffio==1.3.1\r\ntqdm==4.67.1\r\ntyping_extensions==4.12.2\r\nurllib3==2.2.3\r\nvolcengine-python-sdk==1.0.109\r\nWerkzeug==3.1.3\r\nwheel==0.44.0\r\n```\r\n\n\n### Code snippets\n\n_No response_\n\n### OS\n\nLinux Ubuntu 22.04.3 LTS\n\n### Python version\n\nPython 3.13.0\n\n### Library version\n\nopenai==1.54.4",
    "state": "closed",
    "created_at": "2024-12-02T09:39:45+00:00",
    "closed_at": "2024-12-02T09:42:27+00:00",
    "updated_at": "2024-12-02T15:36:35+00:00",
    "author": "fefe982",
    "author_type": "User",
    "comments_count": 2,
    "reactions_count": 0,
    "labels": "bug",
    "milestone": null,
    "closed_by": "fefe982",
    "resolution_time_hours": 0.045,
    "first_comments": [
      {
        "author": "fefe982",
        "created_at": "2024-12-02T09:42:27+00:00",
        "body": "I just found it is already fixed with the most recent openai package.\r\nSorry for the trouble."
      },
      {
        "author": "dgellow",
        "created_at": "2024-12-02T15:36:33+00:00",
        "body": "All good, thanks for the report :)\r\n\r\nFor anyone who would find this GitHub issue and are looking for the context/solution:\r\n- Originally reported at https://github.com/openai/openai-python/issues/1902\r\n- Fixed with https://github.com/openai/openai-python/pull/1905"
      }
    ],
    "url": "https://github.com/openai/openai-python/issues/1915"
  },
  {
    "repository": "openai/openai-python",
    "issue_number": 1911,
    "title": "Request to Reactivate SoundCloud Link Analysis in ChatGPT",
    "body": "### Confirm this is a feature request for the Python library and not the underlying OpenAI API.\n\n- [X] This is a feature request for the Python library\n\n### Describe the feature or improvement you're requesting\n\n### Summary  \r\nThe ability to analyze SoundCloud links directly in ChatGPT was a critical feature for my creative workflow as a musician. Unfortunately, this functionality seems to have been disabled.\r\n\r\n### Steps to Reproduce  \r\n1. Provide a SoundCloud link to ChatGPT.  \r\n2. Observe that it no longer analyzes or provides feedback on the audio.  \r\n\r\n### Expected Behavior  \r\nChatGPT should be able to analyze the link, providing suggestions for dynamics, frequencies, and transitions.\r\n\r\n### Current Behavior  \r\nThe feature is no longer functional, which impacts my ability to refine my music effectively.\r\n\r\n### Context  \r\nI relied heavily on this feature to improve my musical projects and compositions. It helped me gain valuable feedback that I cannot replicate manually. Its removal has disrupted my workflow significantly.\r\n\r\n### Request  \r\nPlease consider reactivating this feature or providing an alternative method to analyze audio directly in ChatGPT.\r\n\r\nThank you for considering my request!\n\n### Additional context\n\n_No response_",
    "state": "closed",
    "created_at": "2024-11-29T20:26:42+00:00",
    "closed_at": "2024-12-01T17:59:15+00:00",
    "updated_at": "2024-12-01T17:59:15+00:00",
    "author": "lorenzo34370",
    "author_type": "User",
    "comments_count": 1,
    "reactions_count": 0,
    "labels": "",
    "milestone": null,
    "closed_by": "RobertCraigie",
    "resolution_time_hours": 45.5425,
    "first_comments": [
      {
        "author": "RobertCraigie",
        "created_at": "2024-12-01T17:59:15+00:00",
        "body": "Thanks for reporting!  \r\n\r\nThis sounds like an issue with the underlying OpenAI API and not the SDK, so I'm going to go ahead and close this issue. \r\n\r\nWould you mind reposting at [community.openai.com](https://community.openai.com)?"
      }
    ],
    "url": "https://github.com/openai/openai-python/issues/1911"
  },
  {
    "repository": "openai/openai-python",
    "issue_number": 1909,
    "title": "Test Issue Creation",
    "body": "This is a test issue created via the API for demonstration purposes.",
    "state": "closed",
    "created_at": "2024-11-29T02:10:46+00:00",
    "closed_at": "2024-11-29T02:11:10+00:00",
    "updated_at": "2024-11-29T02:11:11+00:00",
    "author": "dyrdahl",
    "author_type": "User",
    "comments_count": 0,
    "reactions_count": 0,
    "labels": "",
    "milestone": null,
    "closed_by": "dyrdahl",
    "resolution_time_hours": 0.006666666666666667,
    "first_comments": [],
    "url": "https://github.com/openai/openai-python/issues/1909"
  },
  {
    "repository": "openai/openai-python",
    "issue_number": 1908,
    "title": "New https release is breaking openai sdk while intitiating Async client",
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\n\n- [X] This is an issue with the Python library\n\n### Describe the bug\n\nRecently, my codebase started to fail while initiating AsyncOpenAi. While debugging, I noticed AsyncOpenAI is using a version of `httpx` that accept keyword `proxies` but in the latest version (httpx 0.28.0), this keyword argument is removed from `__init__`.\r\n\r\nTo fix it, I manually limited installed httpx to use `0.27.2` which solves the problem but OpenAI SDK should have applied already that limitation on httpx allowed versions.\n\n### To Reproduce\n\n1. Install openai v1.30.1 with latest version of `httpx` (0.28)\r\n2. Create a new instance of AsyncOpenAI\r\n3. You should see the error at the instance creation.\n\n### Code snippets\n\n_No response_\n\n### OS\n\nUbuntu\n\n### Python version\n\n3.9\n\n### Library version\n\nopenai v1.30.1",
    "state": "closed",
    "created_at": "2024-11-28T20:16:02+00:00",
    "closed_at": "2024-11-28T20:40:56+00:00",
    "updated_at": "2024-11-28T20:40:57+00:00",
    "author": "Alavi1412",
    "author_type": "User",
    "comments_count": 1,
    "reactions_count": 0,
    "labels": "bug",
    "milestone": null,
    "closed_by": "RobertCraigie",
    "resolution_time_hours": 0.415,
    "first_comments": [
      {
        "author": "RobertCraigie",
        "created_at": "2024-11-28T20:40:56+00:00",
        "body": "This has already been fixed in the latest release! https://github.com/openai/openai-python/issues/1902\r\n"
      }
    ],
    "url": "https://github.com/openai/openai-python/issues/1908"
  },
  {
    "repository": "openai/openai-python",
    "issue_number": 1903,
    "title": "TypeError: Client.__init__() got an unexpected keyword argument 'proxies'",
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\n\n- [X] This is an issue with the Python library\n\n### Describe the bug\n\n`SyncHttpxClientWrapper` has hard coded proxies but it no longer exists in httpx 0.28\n\n### To Reproduce\n\n1. `pip install openai`\r\n2. the bug appears when calling `openai.OpenAI(**client_params, **sync_specific)`\n\n### Code snippets\n\n```Python\nTraceback (most recent call last):\r\n  File \"/Users/greg/Documents/Work/browser-use/browser-use/examples/try.py\", line 45, in <module>\r\n    llm = get_llm(args.provider)\r\n          ^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/greg/Documents/Work/browser-use/browser-use/examples/try.py\", line 28, in get_llm\r\n    return ChatOpenAI(model='gpt-4o', temperature=0.0)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/greg/Documents/Work/browser-use/browser-use/.venv/lib/python3.11/site-packages/langchain_core/load/serializable.py\", line 125, in __init__\r\n    super().__init__(*args, **kwargs)\r\n  File \"/Users/greg/Documents/Work/browser-use/browser-use/.venv/lib/python3.11/site-packages/pydantic/main.py\", line 214, in __init__\r\n    validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)\r\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/greg/Documents/Work/browser-use/browser-use/.venv/lib/python3.11/site-packages/langchain_openai/chat_models/base.py\", line 551, in validate_environment\r\n    self.root_client = openai.OpenAI(**client_params, **sync_specific)  # type: ignore[arg-type]\r\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/greg/Documents/Work/browser-use/browser-use/.venv/lib/python3.11/site-packages/openai/_client.py\", line 123, in __init__\r\n    super().__init__(\r\n  File \"/Users/greg/Documents/Work/browser-use/browser-use/.venv/lib/python3.11/site-packages/openai/_base_client.py\", line 857, in __init__\r\n    self._client = http_client or SyncHttpxClientWrapper(\r\n                                  ^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/greg/Documents/Work/browser-use/browser-use/.venv/lib/python3.11/site-packages/openai/_base_client.py\", line 755, in __init__\r\n    super().__init__(**kwargs)\r\nTypeError: Client.__init__() got an unexpected keyword argument 'proxies'\n```\n\n\n### OS\n\nmacOS\n\n### Python version\n\nPython v3.11.4\n\n### Library version\n\nopenai 1.55.2",
    "state": "closed",
    "created_at": "2024-11-28T15:50:44+00:00",
    "closed_at": "2024-11-28T16:32:23+00:00",
    "updated_at": "2024-12-06T08:26:46+00:00",
    "author": "gregpr07",
    "author_type": "User",
    "comments_count": 20,
    "reactions_count": 0,
    "labels": "bug",
    "milestone": null,
    "closed_by": "stainless-app[bot]",
    "resolution_time_hours": 0.6941666666666667,
    "first_comments": [
      {
        "author": "simonw",
        "created_at": "2024-11-28T16:04:01+00:00",
        "body": "Same issue:\r\n- #1902 "
      },
      {
        "author": "dgellow",
        "created_at": "2024-11-28T16:21:28+00:00",
        "body": "Thanks for the report, as mentioned by @simonw this is similar to #1902 and will be fixed by https://github.com/openai/openai-python/issues/1903. I will close this issue as duplicate."
      },
      {
        "author": "stainless-app[bot]",
        "created_at": "2024-11-28T16:32:23+00:00",
        "body": "duplicate"
      },
      {
        "author": "3218923350",
        "created_at": "2024-11-29T09:40:46+00:00",
        "body": "What is the solution now？\r\n"
      },
      {
        "author": "3218923350",
        "created_at": "2024-11-29T09:46:53+00:00",
        "body": "I directly commented out the proxies in the constructor of SyncHttpxClientWrapper in openai/_base_client.py, and it works properly now."
      }
    ],
    "url": "https://github.com/openai/openai-python/issues/1903"
  },
  {
    "repository": "openai/openai-python",
    "issue_number": 1902,
    "title": "Use of proxies kwarg in httpx==0.28.0",
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\r\n\r\n- [X] This is an issue with the Python library\r\n\r\n### Describe the bug\r\n\r\nhttpx version 0.28.0 removes the deprecated proxies argument; this argument is hardcoded in the library at the moment.\r\n\r\n### To Reproduce\r\n\r\n1. Update to httpx version 0.28.0\r\n\r\n### Code snippets\r\n\r\n```\r\n  File \"/var/lang/lib/python3.10/site-packages/openai/_client.py\", line 337, in __init__\r\n    super().__init__(\r\n  File \"/var/lang/lib/python3.10/site-packages/openai/_base_client.py\", line 1438, in __init__\r\n    self._client = http_client or AsyncHttpxClientWrapper(\r\n  File \"/var/lang/lib/python3.10/site-packages/openai/_base_client.py\", line 1335, in __init__\r\n    super().__init__(**kwargs)\r\nTypeError: AsyncClient.__init__() got an unexpected keyword argument 'proxies'\r\n```\r\n\r\n### OS\r\n\r\nmacOS\r\n\r\n### Python version\r\n\r\nPython 3.10.12\r\n\r\n### Library version\r\n\r\nopenai-1.55.2",
    "state": "closed",
    "created_at": "2024-11-28T15:37:43+00:00",
    "closed_at": "2024-11-28T17:02:10+00:00",
    "updated_at": "2024-12-05T14:29:09+00:00",
    "author": "DJRHails",
    "author_type": "User",
    "comments_count": 12,
    "reactions_count": 0,
    "labels": "bug",
    "milestone": null,
    "closed_by": "stainless-app[bot]",
    "resolution_time_hours": 1.4075,
    "first_comments": [
      {
        "author": "stestagg",
        "created_at": "2024-11-28T15:44:24+00:00",
        "body": "This is causing a fresh install of openai to be broken currently.\r\n\r\nHere's a reproducer: (Dockerfile)\r\n```\r\nfrom alpine:edge\r\n\r\nRUN apk update\r\nRUN apk add python3 py3-pip\r\n\r\nRUN pip install  --break-system-packages openai\r\nRUN python -c 'import openai; client=openai.AsyncOpenAI(api_key=\"X\")'\r\n```\r\n\r\nGives:\r\n```\r\n => ERROR [5/5] RUN python -c 'import openai; client=openai.AsyncOpenAI(api_key=\"X\")'                              0.4s\r\n------\r\n > [5/5] RUN python -c 'import openai; client=openai.AsyncOpenAI(api_key=\"X\")':\r\n0.307 Traceback (most recent call last):\r\n0.307   File \"<string>\", line 1, in <module>\r\n0.307   File \"/usr/lib/python3.12/site-packages/openai/_client.py\", line 337, in __init__\r\n0.307     super().__init__(\r\n0.307   File \"/usr/lib/python3.12/site-packages/openai/_base_client.py\", line 1438, in __init__\r\n0.307     self._client = http_client or AsyncHttpxClientWrapper(\r\n0.307                                   ^^^^^^^^^^^^^^^^^^^^^^^^\r\n0.307   File \"/usr/lib/python3.12/site-packages/openai/_base_client.py\", line 1335, in __init__\r\n0.308     super().__init__(**kwargs)\r\n0.308 TypeError: AsyncClient.__init__() got an unexpected keyword argument 'proxies'\r\n------\r\nDockerfile:7\r\n--------------------\r\n   5 |\r\n   6 |     RUN pip install  --break-system-packages openai\r\n   7 | >>> RUN python -c 'import openai; client=openai.AsyncOpenAI(api_key=\"X\")'\r\n   8 |\r\n--------------------\r\n```"
      },
      {
        "author": "Pablo-Merino",
        "created_at": "2024-11-28T15:46:50+00:00",
        "body": "Having this issue too, with the synchronous client. A temporary fix I've found is to hardcode the `httpx` version in my requirements to the previous release, which is `0.27.2`.\r\n\r\nJust adding `httpx==0.27.2` makes my code work again."
      },
      {
        "author": "dgellow",
        "created_at": "2024-11-28T15:57:43+00:00",
        "body": "Hi, thank you for the reports, we (@stainless-api) are actively working on a fix.\r\n\r\nEdit: we have a patch under review right now and expect to push it to this repo in a few minutes"
      },
      {
        "author": "RobertCraigie",
        "created_at": "2024-11-28T16:06:11+00:00",
        "body": "Thanks for the reports, this will be fixed shortly"
      },
      {
        "author": "dgellow",
        "created_at": "2024-11-28T16:18:17+00:00",
        "body": "Should be fixed once https://github.com/openai/openai-python/pull/1905 gets merged (all the credits to the amazing @RobertCraigie for the quick fix)"
      }
    ],
    "url": "https://github.com/openai/openai-python/issues/1902"
  },
  {
    "repository": "openai/openai-python",
    "issue_number": 1894,
    "title": "Azure Endpoint URL is not Trailing-Slash Agnostic",
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\n\n- [X] This is an issue with the Python library\n\n### Describe the bug\n\nIn src/openai/lib/azure.py, there are four cases where the endpoint URL is simply concatenated like:\r\n\r\n`f\"{azure_endpoint}/openai/deployments/{azure_deployment}\"`\r\nor\r\n`f\"{azure_endpoint}/openai\"`\r\n\r\nIf `azure_endpoint` has a trailing forward slash (which many comments in the code suggest is allowed, as well as [an example in the repository](https://github.com/openai/openai-python/blob/83f4774156dc3e29c7fe6be9ffd681df68534509/examples/azure.py#L29)), it will have `//` in the URL, which is wrong.\r\n\r\nTo make it agnostic, it should be:\r\n\r\n`f\"{azure_endpoint.rstrip(\"/\")}/openai/deployments/{azure_deployment}\"`\r\nand\r\n`f\"{azure_endpoint.rstrip(\"/\")}/openai\"`\r\n\r\nThis would be sufficient to fix it: #1893 https://github.com/openai/openai-python/pull/1893/commits/673a496582ccf5d960e26a846cd6df81e837a9f9\n\n### To Reproduce\n\n1. Initialize an AsyncAzureOpenAI client.\r\n2. Await on client.chat.completions.with_raw_response.create(**model_args).\r\n3. The error code will be 404 Resource not found because the URL is not properly formatted.\n\n### Code snippets\n\n_No response_\n\n### OS\n\nWindows 10\n\n### Python version\n\nPython 3.12.4\n\n### Library version\n\nopenai v1.6.1",
    "state": "closed",
    "created_at": "2024-11-26T00:25:11+00:00",
    "closed_at": "2024-12-10T11:53:06+00:00",
    "updated_at": "2024-12-10T11:53:07+00:00",
    "author": "Jozef833",
    "author_type": "User",
    "comments_count": 1,
    "reactions_count": 0,
    "labels": "bug",
    "milestone": null,
    "closed_by": "RobertCraigie",
    "resolution_time_hours": 347.46527777777777,
    "first_comments": [
      {
        "author": "RobertCraigie",
        "created_at": "2024-12-10T11:53:06+00:00",
        "body": "Fixed in https://github.com/openai/openai-python/pull/1935"
      }
    ],
    "url": "https://github.com/openai/openai-python/issues/1894"
  },
  {
    "repository": "openai/openai-python",
    "issue_number": 1890,
    "title": "Primero",
    "body": "### Confirm this is a feature request for the Python library and not the underlying OpenAI API.\n\n- [X] This is a feature request for the Python library\n\n### Describe the feature or improvement you're requesting\n\nAbrir\n\n### Additional context\n\nUna meta\n```[tasklist]\n### Tasks\n- [ ] https://github.com/openai/openai-python/pull/1889\n```\n\n```[tasklist]\n### Tasks\n- [ ] https://github.com/openai/openai-python/pull/1888\n- [ ] https://github.com/openai/openai-python/pull/636\n- [ ] https://github.com/openai/openai-python/pull/1889\n- [ ] https://github.com/openai/openai-python/issues/1384\n- [ ] https://github.com/openai/openai-python/issues/1869\n```\n\n```[tasklist]\n### Tasks\n- [ ] https://github.com/openai/openai-python/issues/1869\n- [ ] https://github.com/openai/openai-python/pull/1888\n- [ ] https://github.com/openai/openai-python/pull/1889\n- [ ] https://github.com/openai/openai-python/issues/1884\n- [ ] https://github.com/openai/openai-python/pull/1886\n- [ ] https://github.com/openai/openai-python/pull/636\n- [ ] https://github.com/openai/openai-python/issues/873\n```\n",
    "state": "closed",
    "created_at": "2024-11-24T09:39:27+00:00",
    "closed_at": "2024-11-24T09:39:50+00:00",
    "updated_at": "2024-11-24T09:42:35+00:00",
    "author": "Balles9",
    "author_type": "User",
    "comments_count": 1,
    "reactions_count": 0,
    "labels": "",
    "milestone": null,
    "closed_by": "Balles9",
    "resolution_time_hours": 0.006388888888888889,
    "first_comments": [
      {
        "author": "Balles9",
        "created_at": "2024-11-24T09:39:47+00:00",
        "body": "Si "
      }
    ],
    "url": "https://github.com/openai/openai-python/issues/1890"
  },
  {
    "repository": "openai/openai-python",
    "issue_number": 1884,
    "title": "pip Install openai in android cli",
    "body": "### Confirm this is a feature request for the Python library and not the underlying OpenAI API.\n\n- [X] This is a feature request for the Python library\n\n### Describe the feature or improvement you're requesting\n\nWhen I install via `pip install openai`, I encounter `can't find Rust compiler`. Since rust does not support the android host temporarily, is there an alternative solution?\r\n``` bash\r\n(ENV) msmnile_gvmq:/data/local/tmp $ pip install openai\r\nCollecting openai\r\n  Using cached openai-1.55.0-py3-none-any.whl.metadata (24 kB)\r\nRequirement already satisfied: anyio<5,>=3.5.0 in ./ENV/lib/python3.12/site-packages (from openai) (4.6.2.post1)\r\nCollecting distro<2,>=1.7.0 (from openai)\r\n  Using cached distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\r\nRequirement already satisfied: httpx<1,>=0.23.0 in ./ENV/lib/python3.12/site-packages (from openai) (0.27.2)\r\nCollecting jiter<1,>=0.4.0 (from openai)\r\n  Using cached jiter-0.7.1.tar.gz (162 kB)\r\n  Installing build dependencies ... error\r\n  error: subprocess-exited-with-error\r\n\r\n  × pip subprocess to install build dependencies did not run successfully.\r\n  │ exit code: 1\r\n  ╰─> [50 lines of output]\r\n      Collecting maturin<2,>=1\r\n        Using cached maturin-1.7.4.tar.gz (191 kB)\r\n        Installing build dependencies: started\r\n        Installing build dependencies: finished with status 'done'\r\n        Getting requirements to build wheel: started\r\n        Getting requirements to build wheel: finished with status 'done'\r\n        Preparing metadata (pyproject.toml): started\r\n        Preparing metadata (pyproject.toml): finished with status 'done'\r\n      Building wheels for collected packages: maturin\r\n        Building wheel for maturin (pyproject.toml): started\r\n        Building wheel for maturin (pyproject.toml): finished with status 'error'\r\n        error: subprocess-exited-with-error\r\n\r\n        × Building wheel for maturin (pyproject.toml) did not run successfully.\r\n        │ exit code: 1\r\n        ╰─> [28 lines of output]\r\n            running bdist_wheel\r\n            running build\r\n            running build_py\r\n            creating build/lib.linux-armv8l-cpython-312/maturin\r\n            copying maturin/__init__.py -> build/lib.linux-armv8l-cpython-312/maturin\r\n            copying maturin/__main__.py -> build/lib.linux-armv8l-cpython-312/maturin\r\n            running egg_info\r\n            writing maturin.egg-info/PKG-INFO\r\n            writing dependency_links to maturin.egg-info/dependency_links.txt\r\n            writing requirements to maturin.egg-info/requires.txt\r\n            writing top-level names to maturin.egg-info/top_level.txt\r\n            reading manifest file 'maturin.egg-info/SOURCES.txt'\r\n            reading manifest template 'MANIFEST.in'\r\n            warning: no files found matching '*.json' under directory 'src/python_interpreter'\r\n            writing manifest file 'maturin.egg-info/SOURCES.txt'\r\n            running build_ext\r\n            running build_rust\r\n            **error: can't find Rust compiler**\r\n\r\n            If you are using an outdated pip version, it is possible a prebuilt wheel is available for this package but pip is not able to install from it. Installing from the wheel would avoid the need for a Rust compiler.\r\n\r\n            To update pip, run:\r\n\r\n                pip install --upgrade pip\r\n\r\n            and then retry package installation.\r\n\r\n            If you did intend to build this package from source, try installing a Rust compiler from your system package manager and ensure it is on the PATH during installation. Alternatively, rustup (available at https://rustup.rs) is the recommended way to download and update the Rust compiler toolchain.\r\n            [end of output]\r\n\r\n        note: This error originates from a subprocess, and is likely not a problem with pip.\r\n        ERROR: Failed building wheel for maturin\r\n      Failed to build maturin\r\n      ERROR: ERROR: Failed to build installable wheels for some pyproject.toml based projects (maturin)\r\n      [end of output]\r\n\r\n  note: This error originates from a subprocess, and is likely not a problem with pip.\r\nerror: subprocess-exited-with-error\r\n\r\n× pip subprocess to install build dependencies did not run successfully.\r\n│ exit code: 1\r\n╰─> See above for output.\r\n\r\nnote: This error originates from a subprocess, and is likely not a problem with pip.\r\n```\n\n### Additional context\n\n_No response_",
    "state": "closed",
    "created_at": "2024-11-22T03:32:23+00:00",
    "closed_at": "2024-11-22T12:27:49+00:00",
    "updated_at": "2024-11-23T03:48:57+00:00",
    "author": "leemeng0x61",
    "author_type": "User",
    "comments_count": 2,
    "reactions_count": 0,
    "labels": "",
    "milestone": null,
    "closed_by": "RobertCraigie",
    "resolution_time_hours": 8.92388888888889,
    "first_comments": [
      {
        "author": "RobertCraigie",
        "created_at": "2024-11-22T12:27:49+00:00",
        "body": "Hey @leemeng0x61, thanks for the report. Unfortunately there isn't anything we can do to help here, `jiter` is a required dependency and making it optional would be a breaking change.\r\n\r\nIf you're not planning on using the streaming helpers, you may be able to get `pip` to ignore the dependency error somehow and add a stub file like this to your python path so that our runtime imports of `jiter` will work\r\n```py\r\n# jiter.py\r\ndef from_json(*args, **kwargs):\r\n  raise NotImplementedError(\"jiter is not available\")\r\n```"
      },
      {
        "author": "leemeng0x61",
        "created_at": "2024-11-23T03:48:56+00:00",
        "body": "Thanks for your reply."
      }
    ],
    "url": "https://github.com/openai/openai-python/issues/1884"
  },
  {
    "repository": "openai/openai-python",
    "issue_number": 1883,
    "title": "Rock",
    "body": "### Confirm this is a feature request for the Python library and not the underlying OpenAI API.\n\n- [X] This is a feature request for the Python library\n\n### Describe the feature or improvement you're requesting\n\nWe use [Rye](https://rye.astral.sh/) to manage dependencies because it will automatically provision a Python environment with the expected Python version. To set it up, run:\r\n\r\n```sh\r\n$ ./scripts/bootstrap\r\n```\r\n\r\nOr [install Rye manually](https://rye.astral.sh/guide/installation/) and run:\r\n\r\n```sh\r\n$ rye sync --all-features\r\n```\r\n\r\nYou can then run scripts using `rye run python script.py` or by activating the virtual environment:\r\n\r\n```sh\r\n$ rye shell\r\n# or manually activate - https://docs.python.org/3/library/venv.html#how-venvs-work\r\n$ source .venv/bin/activate\r\n\r\n# now you can omit the `rye run` prefix\r\n$ python script.py\r\n```\r\n\r\n### Without Rye\r\n\r\nAlternatively if you don't want to install `Rye`, you can stick with the standard `pip` setup by ensuring you have the Python version specified in `.python-version`, create a virtual environment however you desire and then install dependencies using this command:\r\n\r\n```sh\r\n$ pip install -r requirements-dev.lock\r\n```\r\n\r\n## Modifying/Adding code\r\n\r\nMost of the SDK is generated code. Modifications to code will be persisted between generations, but may\r\nresult in merge conflicts between manual patches and changes from the generator. The generator will never\r\nmodify the contents of the `src/openai/lib/` and `examples/` directories.\r\n\r\n## Adding and running examples\r\n\r\nAll files in the `examples/` directory are not modified by the generator and can be freely edited or added to.\r\n\r\n```py\r\n# add an example to examples/<your-example>.py\r\n\r\n#!/usr/bin/env -S rye run python\r\n…\r\n```\r\n\r\n```sh\r\n$ chmod +x examples/<your-example>.py\r\n# run the example against your api\r\n$ ./examples/<your-example>.py\r\n```\r\n\r\n## Using the repository from source\r\n\r\nIf you’d like to use the repository from source, you can either install from git or link to a cloned repository:\r\n\r\nTo install via git:\r\n\r\n```sh\r\n$ pip install git+ssh://git@github.com/openai/openai-python.git\r\n```\r\n\r\nAlternatively, you can build from source and install the wheel file:\r\n\r\nBuilding this package will create two files in the `dist/` directory, a `.tar.gz` containing the source files and a `.whl` that can be used to install the package efficiently.\r\n\r\nTo create a distributable version of the library, all you have to do is run this command:\r\n\r\n```sh\r\n$ rye build\r\n# or\r\n$ python -m build\r\n```\r\n\r\nThen to install:\r\n\r\n```sh\r\n$ pip install ./path-to-wheel-file.whl\r\n```\r\n\r\n## Running tests\r\n\r\nMost tests require you to [set up a mock server](https://github.com/stoplightio/prism) against the OpenAPI spec to run the tests.\r\n\r\n```sh\r\n# you will need npm installed\r\n$ npx prism mock path/to/your/openapi.yml\r\n```\r\n\r\n```sh\r\n$ ./scripts/test\r\n```\r\n\r\n## Linting and formatting\r\n\r\nThis repository uses [ruff](https://github.com/astral-sh/ruff) and\r\n[black](https://github.com/psf/black) to format the code in the repository.\r\n\r\nTo lint:\r\n\r\n```sh\r\n$ ./scripts/lint\r\n```\r\n\r\nTo format and fix all ruff issues automatically:\r\n\r\n```sh\r\n$ ./scripts/format\r\n```\r\n\r\n\r\n\n\n### Additional context\n\n\r\n## Publishing and releases\r\n\r\nChanges made to this repository via the automated release PR pipeline should publish to PyPI automatically. If\r\nthe changes aren't made through the automated pipeline, you may want to make releases manually.\r\n\r\n### Publish with a GitHub workflow\r\n\r\nYou can release to package managers by using [the `Publish PyPI` GitHub action](https://www.github.com/openai/openai-python/actions/workflows/publish-pypi.yml). This requires a setup organization or repository secret to be set up.\r\n\r\n### Publish manually\r\n\r\nIf you need to manually release a package, you can run the `bin/publish-pypi` script with a `PYPI_TOKEN` set on\r\nthe environement.",
    "state": "closed",
    "created_at": "2024-11-22T01:56:07+00:00",
    "closed_at": "2024-11-22T07:50:36+00:00",
    "updated_at": "2024-11-22T07:50:36+00:00",
    "author": "derockspace",
    "author_type": "User",
    "comments_count": 0,
    "reactions_count": 0,
    "labels": "",
    "milestone": null,
    "closed_by": "RobertCraigie",
    "resolution_time_hours": 5.908055555555555,
    "first_comments": [],
    "url": "https://github.com/openai/openai-python/issues/1883"
  },
  {
    "repository": "openai/openai-python",
    "issue_number": 1882,
    "title": "Pydantic-V1 \"warnings\" error",
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\n\n- [X] This is an issue with the Python library\n\n### Describe the bug\n\nI am getting this Error:\r\n```\r\nFile \"/Users/konnoryoung/Documents/projects/contract-ai-poc/.errors/lib/python3.9/site-packages/openai/lib/streaming/_assistants.py\", line 943, in accumulate_event\r\nblock = current_message_snapshot.content[content_delta.index]\r\nIndexError: list index out of range\r\n```\r\n\r\nWhich comes from the try/except block in the \"accumulate_event\" function:\r\n\r\n```\r\ntry:\r\n    block = current_message_snapshot.content[content_delta.index]\r\nexcept IndexError:\r\n    current_message_snapshot.content.insert(\r\n    content_delta.index,\r\n    cast(\r\n        MessageContent,\r\n        construct_type(\r\n            # mypy doesn't allow Content for some reason\r\n            type_=cast(Any, MessageContent),\r\n            value=model_dump(content_delta, exclude_unset=True, warnings=False),\r\n            ),\r\n        ),\r\n    )\r\n    new_content.append(content_delta)\r\n```\r\n\r\nThat wouldn't stop my code from running except that it raises another exception:\r\n\r\n```\r\nFile \"/Users/konnoryoung/Documents/projects/contract-ai-poc/.errors/lib/python3.9/site-packages/openai/_models.py\", line 313, in model_dump\r\nraise ValueError(\"warnings is only supported in Pydantic v2\")\r\nValueError: warnings is only supported in Pydantic v2\r\n```\r\n\r\nWhich comes from the if statement:\r\n\r\n```\r\nif warnings != True:\r\n    raise ValueError(\"warnings is only supported in Pydantic v2\")\r\n```\r\n\r\nIn the `if not PYDANTIC_V2` version of the `model_dump` method of the `BaseModel` class in the _model.py file.\n\n### To Reproduce\n\n1- Pydantic < 2 (I have v1.9.0)\r\n2- Create an assistant\r\n3- Create a thread\r\n4- Attempt to run an Async Stream (my code is below)\r\n\r\n\n\n### Code snippets\n\n```Python\nfrom __future__ import annotations\r\n\r\nimport asyncio\r\n\r\nimport openai\r\n\r\nclient = openai.AsyncOpenAI()\r\n\r\nassistant_id= \"\"\" GetID OR Create an Assistant \"\"\"\r\nthread_id= \"\"\"  GetID OR Create a Thread \"\"\"\r\nasync def main() -> None:\r\n    async with client.beta.threads.runs.stream(\r\n        thread_id=thread_id,\r\n        assistant_id=assistant_id,\r\n        ) as stream:\r\n            async for event in stream:\r\n                print()\r\n\r\nasyncio.run(main())\n```\n\n\n### OS\n\nmasOS\n\n### Python version\n\nPython 3.9.6\n\n### Library version\n\nopenai v1.55.0",
    "state": "closed",
    "created_at": "2024-11-21T18:18:32+00:00",
    "closed_at": "2024-11-22T11:24:23+00:00",
    "updated_at": "2024-11-22T11:24:23+00:00",
    "author": "Konnor-Young",
    "author_type": "User",
    "comments_count": 2,
    "reactions_count": 0,
    "labels": "bug",
    "milestone": null,
    "closed_by": "RobertCraigie",
    "resolution_time_hours": 17.0975,
    "first_comments": [
      {
        "author": "RobertCraigie",
        "created_at": "2024-11-22T10:39:13+00:00",
        "body": "Thanks for the example, I can reproduce the issue. Working on a fix."
      },
      {
        "author": "RobertCraigie",
        "created_at": "2024-11-22T11:24:23+00:00",
        "body": "@Konnor-Young this will be fixed in the next release! https://github.com/openai/openai-python/pull/1886"
      }
    ],
    "url": "https://github.com/openai/openai-python/issues/1882"
  },
  {
    "repository": "openai/openai-python",
    "issue_number": 1881,
    "title": "make prompt caching available through api for prompts with less than 1024 token",
    "body": "### Confirm this is a feature request for the Python library and not the underlying OpenAI API.\n\n- [X] This is a feature request for the Python library\n\n### Describe the feature or improvement you're requesting\n\nwe could make great use of this awesome feature but our prompts are only around 700 tokens big\n\n### Additional context\n\n_No response_",
    "state": "closed",
    "created_at": "2024-11-21T09:28:47+00:00",
    "closed_at": "2024-11-21T09:29:41+00:00",
    "updated_at": "2024-11-21T09:29:41+00:00",
    "author": "leeflix",
    "author_type": "User",
    "comments_count": 1,
    "reactions_count": 0,
    "labels": "",
    "milestone": null,
    "closed_by": "RobertCraigie",
    "resolution_time_hours": 0.015,
    "first_comments": [
      {
        "author": "RobertCraigie",
        "created_at": "2024-11-21T09:29:41+00:00",
        "body": "Thanks for reporting!  \r\n\r\nThis sounds like an issue with the underlying OpenAI API and not the SDK, so I'm going to go ahead and close this issue. \r\n\r\nWould you mind reposting at [community.openai.com](https://community.openai.com)?"
      }
    ],
    "url": "https://github.com/openai/openai-python/issues/1881"
  },
  {
    "repository": "openai/openai-python",
    "issue_number": 1868,
    "title": "Descrepancy results AsyncAzureOpenAI.azure_deployment and client.chat.completions.create(model)",
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\n\n- [X] This is an issue with the Python library\n\n### Describe the bug\n\nWhen running the AzureOpenAI service with azure_deployment gpt-4o 2024-05-13 I get very different completion results when I enter model='gpt-4' and model='gpt-4o'.\r\nI would expect the model parameter not to matter in this case, as there is only one model to choose from the given deployment.\n\n### To Reproduce\n\nRun and observe stream:\r\n\r\n```\r\nasync with AsyncAzureOpenAI(\r\n        api_key=openai_api_key,\r\n        azure_deployment=deployment_model,\r\n        azure_endpoint=openai_azure_endpoint,\r\n        api_version=\"2023-12-01-preview\"\r\n    ) as client:\r\n        openai_stream = await client.chat.completions.create(\r\n            model=model_name,\r\n            messages=messages,\r\n            temperature=0.2,\r\n            max_tokens=1200,\r\n            top_p=0.45,\r\n            frequency_penalty=0,\r\n            presence_penalty=0,\r\n            stop=None,\r\n            stream=True\r\n        )\r\n```\r\n\r\ntwice:\r\n - azure_deployment='gpt-4o-0513', model_name='gpt-4'\r\n - azure_deployment='gpt-4o-0513', model_name='gpt-4o'\r\n\r\nThe first deployment will give markdown heavy response, structured with bold titles etc., while the second response will give solely paragraphs.\n\n### Code snippets\n\n_No response_\n\n### OS\n\npython:3.12-slim\n\n### Python version\n\npython v.3.12\n\n### Library version\n\nopenai v.1.51.2",
    "state": "closed",
    "created_at": "2024-11-14T14:25:15+00:00",
    "closed_at": "2025-01-07T08:13:34+00:00",
    "updated_at": "2025-01-07T08:13:34+00:00",
    "author": "woutkonings",
    "author_type": "User",
    "comments_count": 1,
    "reactions_count": 0,
    "labels": "question",
    "milestone": null,
    "closed_by": "RobertCraigie",
    "resolution_time_hours": 1289.8052777777777,
    "first_comments": [
      {
        "author": "kristapratico",
        "created_at": "2024-11-14T18:51:03+00:00",
        "body": "@woutkonings when passing `azure_deployment` into the AsyncAzureOpenAI constructor, the client will use that deployment for all requests. Any value provided for `model` will be ignored. \r\n\r\nFor chat completions, you can try adjusting `temperature` or provide a `seed` to get more deterministic results, but it is not guaranteed. Here's some docs that explain those two parameters: https://learn.microsoft.com/azure/ai-services/openai/reference-preview#createchatcompletionrequest"
      }
    ],
    "url": "https://github.com/openai/openai-python/issues/1868"
  },
  {
    "repository": "openai/openai-python",
    "issue_number": 1866,
    "title": "Does logit_bias work for audio_preview?",
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\n\n- [X] This is an issue with the Python library\n\n### Describe the bug\n\nhttps://platform.openai.com/docs/api-reference/chat/create#logit_bias\n\n### To Reproduce\n\nWhen I pass logit_bias into the text-only models I get substantially different results than when I pass it into gpt-4o-audio-preview. It doesn't reject the logit_bias which makes me believe it is valid, but I'm unusure it's having an effect. Could you confirm? Thanks! \n\n### Code snippets\n\n_No response_\n\n### OS\n\nmacOS\n\n### Python version\n\nPython V2.11.4\n\n### Library version\n\nopenai v1.01",
    "state": "closed",
    "created_at": "2024-11-13T16:07:19+00:00",
    "closed_at": "2024-11-14T11:31:14+00:00",
    "updated_at": "2024-11-14T11:31:15+00:00",
    "author": "justinebreuch",
    "author_type": "User",
    "comments_count": 1,
    "reactions_count": 0,
    "labels": "API-feedback",
    "milestone": null,
    "closed_by": "RobertCraigie",
    "resolution_time_hours": 19.398611111111112,
    "first_comments": [
      {
        "author": "RobertCraigie",
        "created_at": "2024-11-14T11:31:15+00:00",
        "body": "Thanks for reporting!  \r\n\r\nThis sounds like an issue with the underlying OpenAI API and not the SDK, so I'm going to go ahead and close this issue as I'm not aware of the answer here myself.\r\n\r\nWould you mind reposting at [community.openai.com](https://community.openai.com)?"
      }
    ],
    "url": "https://github.com/openai/openai-python/issues/1866"
  },
  {
    "repository": "openai/openai-python",
    "issue_number": 1864,
    "title": "ModuleNotFoundError: No module named 'jiter.jiter' when Importing openai Package",
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\n\n- [X] This is an issue with the Python library\n\n### Describe the bug\n\nDescription\r\nI'm encountering a ModuleNotFoundError when attempting to import the openai Python library. The error indicates that the jiter.jiter module is missing, preventing the import from succeeding.\r\n\r\nError Message\r\nplaintext\r\nCopy code\r\nModuleNotFoundError: No module named 'jiter.jiter'\n\n### To Reproduce\n\nSteps to Reproduce\r\nSet Up Virtual Environment:\r\n\r\nbash\r\nCopy code\r\npython -m venv myenv_py311\r\nmyenv_py311\\Scripts\\activate  # On Windows\r\nInstall openai:\r\n\r\nbash\r\nCopy code\r\npip install openai==1.54.3\r\nAlso tried openai==1.53.0 with the same result.\r\nInstall jiter from GitHub:\r\n\r\nbash\r\nCopy code\r\npip install git+https://github.com/openai/jiter.git\r\nAttempt to Import openai:\r\n\r\npython\r\nCopy code\r\nimport openai\r\nResult: Raises ModuleNotFoundError: No module named 'jiter.jiter'\r\nAdditional Steps Taken:\r\n\r\nUninstalled External jiter:\r\nbash\r\nCopy code\r\npip uninstall jiter\r\nReinstalled openai:\r\nbash\r\nCopy code\r\npip uninstall openai\r\npip install openai\r\nVerified Package Structure:\r\nChecked the jiter package directory; jiter.py is present after installing from GitHub.\r\nCreated a Fresh Virtual Environment:\r\nIssue persists even in a new environment.\n\n### Code snippets\n\n```Python\n(myenv) D:\\Resume Project>python -c \"import openai\"\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\n    import openai\r\n  File \"D:\\Resume Project\\myenv\\Lib\\site-packages\\openai\\__init__.py\", line 11, in <module>\r\n    from ._client import Client, OpenAI, Stream, Timeout, Transport, AsyncClient, AsyncOpenAI, AsyncStream, RequestOptions\r\n  File \"D:\\Resume Project\\myenv\\Lib\\site-packages\\openai\\_client.py\", line 11, in <module>\r\n    from . import resources, _exceptions\r\n  File \"D:\\Resume Project\\myenv\\Lib\\site-packages\\openai\\resources\\__init__.py\", line 3, in <module>\r\n    from .beta import (\r\n    ...<6 lines>...\r\n    )\r\n  File \"D:\\Resume Project\\myenv\\Lib\\site-packages\\openai\\resources\\beta\\__init__.py\", line 3, in <module>\r\n    from .beta import (\r\n    ...<6 lines>...\r\n    )\r\n  File \"D:\\Resume Project\\myenv\\Lib\\site-packages\\openai\\resources\\beta\\beta.py\", line 14, in <module>\r\n    from .chat.chat import Chat, AsyncChat\r\n  File \"D:\\Resume Project\\myenv\\Lib\\site-packages\\openai\\resources\\beta\\chat\\__init__.py\", line 3, in <module>\r\n    from .chat import Chat, AsyncChat\r\n  File \"D:\\Resume Project\\myenv\\Lib\\site-packages\\openai\\resources\\beta\\chat\\chat.py\", line 6, in <module>\r\n    from .completions import Completions, AsyncCompletions\r\n  File \"D:\\Resume Project\\myenv\\Lib\\site-packages\\openai\\resources\\beta\\chat\\completions.py\", line 27, in <module>\r\n    from ....lib.streaming.chat import ChatCompletionStreamManager, AsyncChatCompletionStreamManager\r\n  File \"D:\\Resume Project\\myenv\\Lib\\site-packages\\openai\\lib\\streaming\\chat\\__init__.py\", line 21, in <module>\r\n    from ._completions import (\r\n    ...<4 lines>...\r\n    )\r\n  File \"D:\\Resume Project\\myenv\\Lib\\site-packages\\openai\\lib\\streaming\\chat\\_completions.py\", line 8, in <module>\r\n    from jiter import from_json\r\n  File \"D:\\Resume Project\\myenv\\Lib\\site-packages\\jiter\\__init__.py\", line 1, in <module>\r\n    from .jiter import *\r\nModuleNotFoundError: No module named 'jiter.jiter'\n```\n\n\n### OS\n\nWindows 11\n\n### Python version\n\nPython 3.13\n\n### Library version\n\nboth OpenAI 1.54. 3 AND 1.53.0",
    "state": "closed",
    "created_at": "2024-11-12T12:08:03+00:00",
    "closed_at": "2024-11-12T13:27:18+00:00",
    "updated_at": "2024-11-12T13:56:59+00:00",
    "author": "fedorowych",
    "author_type": "User",
    "comments_count": 5,
    "reactions_count": 0,
    "labels": "question",
    "milestone": null,
    "closed_by": "RobertCraigie",
    "resolution_time_hours": 1.3208333333333333,
    "first_comments": [
      {
        "author": "RobertCraigie",
        "created_at": "2024-11-12T12:29:26+00:00",
        "body": "Looks like the `jiter` install is broken, have you tried reinstalling it?"
      },
      {
        "author": "fedorowych",
        "created_at": "2024-11-12T13:25:50+00:00",
        "body": "Yes, I have reinstalled it many, many times.\r\n\r\n    On Tuesday, November 12, 2024 at 05:29:50 AM MST, Robert Craigie ***@***.***> wrote:  \r\n \r\n \r\nLooks like the jiter install is broken, have you tried reinstalling it?\r\n\r\n—\r\nReply to this email directly, view it on GitHub, or unsubscribe.\r\nYou are receiving this because you authored the thread.Message ID: ***@***.***>\r\n  "
      },
      {
        "author": "RobertCraigie",
        "created_at": "2024-11-12T13:27:16+00:00",
        "body": "Okay, unfortunately there's not much we can do. I recommend asking for help from the pydantic team https://github.com/pydantic/jiter"
      },
      {
        "author": "fedorowych",
        "created_at": "2024-11-12T13:47:49+00:00",
        "body": " Reinstall always resulted with the same error. The _init_.py program does not include jiter.py.\r\nMy research indicates that OpenAI 1.53 is not dependent on Jiter so I am trying that and that instead of using Python 3.13, Python version3.11 is proven to be Stabe. I'm trying that too. \r\n\r\nTher was also mention that OpenAI 1.54.3 may be using jiter internally within OpenAI and is confused with its external usage.\r\nThe bottom line is, however, jiter.py is not included in the _init_.py program.\r\nI am new to this so please forgive any misinterpretations, errors or omissions I may have made but so far I've spent2 days and over 20 hours trying to get this work both with Gpt-4-turbo and Cursor.\r\n\r\n    On Tuesday, November 12, 2024 at 05:29:50 AM MST, Robert Craigie ***@***.***> wrote:  \r\n \r\n Looks like the jiter install is broken, have you tried reinstalling it?\r\n\r\n—\r\nReply to this email directly, view it on GitHub, or unsubscribe.\r\nYou are receiving this because you authored the thread.Message ID: ***@***.***>\r\n  "
      },
      {
        "author": "fedorowych",
        "created_at": "2024-11-12T13:56:58+00:00",
        "body": "Sorry, I'm new to OpenAI etc. I've redirected the issue to https://github.com/pydantic/jiter"
      }
    ],
    "url": "https://github.com/openai/openai-python/issues/1864"
  },
  {
    "repository": "openai/openai-python",
    "issue_number": 1863,
    "title": "limit parameter in messages.list does not work.",
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\n\n- [X] This is an issue with the Python library\n\n### Describe the bug\n\nWhen I try to fetch a limited number of messages from a thread, I receive all the messages instead.\n\n### To Reproduce\n\n1. Fetch a thread\r\n2. Retrieve the messages of the thread with limit parameter\r\n3. Iterate over retrieved message objects\n\n### Code snippets\n\n```Python\nimport openai\r\nimport os\r\n\r\nclient = openai.Client(api_key=os.getenv(\"OPENAI_API_KEY\"))\r\n\r\nthread = client.beta.threads.retrieve(thread_id=\"<thread_id>\")\r\n\r\nmessages  = client.beta.threads.messages.list(thread_id=thread.id, limit=3)\r\n\r\nfor i,message in enumerate(messages):\r\n    print(message.role)\r\n    print(message.created_at)\n```\n\n\n### OS\n\nUbuntu 22.04\n\n### Python version\n\nPython v3.10.12\n\n### Library version\n\nopenai v1.54.3",
    "state": "closed",
    "created_at": "2024-11-12T09:41:43+00:00",
    "closed_at": "2024-11-12T09:55:49+00:00",
    "updated_at": "2024-11-12T09:59:05+00:00",
    "author": "hkaraoguz",
    "author_type": "User",
    "comments_count": 2,
    "reactions_count": 0,
    "labels": "",
    "milestone": null,
    "closed_by": "RobertCraigie",
    "resolution_time_hours": 0.235,
    "first_comments": [
      {
        "author": "RobertCraigie",
        "created_at": "2024-11-12T09:55:49+00:00",
        "body": "I can see how this isn't intuitive but this is happening because the iterator is [making multiple requests](https://github.com/openai/openai-python?tab=readme-ov-file#pagination), if you just want the single page then you need to use `.data`\r\n```py\r\nimport openai\r\nimport os\r\n\r\nclient = openai.Client(api_key=os.getenv(\"OPENAI_API_KEY\"))\r\n\r\nthread = client.beta.threads.retrieve(thread_id=\"<thread_id>\")\r\n\r\nmessages  = client.beta.threads.messages.list(thread_id=thread.id, limit=3)\r\n\r\nfor i,message in enumerate(messages.data):\r\n    print(message.role)\r\n    print(message.created_at)\r\n```\r\nUnfortunately I'm not sure if there's anything we can change to make this easier to understand..."
      },
      {
        "author": "hkaraoguz",
        "created_at": "2024-11-12T09:58:49+00:00",
        "body": "Thank you @RobertCraigie for your prompt reply. I suspected that I was misunderstanding the usage and now I learned."
      }
    ],
    "url": "https://github.com/openai/openai-python/issues/1863"
  },
  {
    "repository": "openai/openai-python",
    "issue_number": 1838,
    "title": "\"openai migrate\" Error: Failed to download Grit CLI",
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\n\n- [X] This is an issue with the Python library\n\n### Describe the bug\n\nWhen running the `openai migrate` command, it attempts to download the Grit CLI from `https://github.com/getgrit/gritql/releases/latest/download/marzano-x86_64-unknown-linux-gnu.tar.gz`, but the link appears to be broken or outdated and it response `Error: Failed to download Grit CLI from https://github.com/getgrit/gritql/releases/latest/download/marzano-x86_64-unknown-linux-gnu.tar.gz`\n\n### To Reproduce\n\n1. Install the latest version of openai via pip: pip install openai --upgrade\r\n2. Observe the command attempting to download the file marzano-x86_64-unknown-linux-gnu.tar.gz.\r\n\r\nI think that the current Grit releases on GitHub seem to have changed the prefix from marzano- to grit- in the file names, which is likely causing the download to fail.\n\n### Code snippets\n\n_No response_\n\n### OS\n\nUbuntu 20.04 LTS\n\n### Python version\n\nPython v3.11.4\n\n### Library version\n\nopenai 1.53.0",
    "state": "closed",
    "created_at": "2024-10-31T14:51:21+00:00",
    "closed_at": "2024-12-16T12:14:17+00:00",
    "updated_at": "2024-12-16T12:14:17+00:00",
    "author": "atthasaeth",
    "author_type": "User",
    "comments_count": 6,
    "reactions_count": 0,
    "labels": "bug",
    "milestone": null,
    "closed_by": "RobertCraigie",
    "resolution_time_hours": 1101.3822222222223,
    "first_comments": [
      {
        "author": "mvmendes",
        "created_at": "2024-11-05T19:18:32+00:00",
        "body": "I could bypass this bug is downloading grit and executing  grit CLI , with \"apply openai\" parameters:\r\n```\r\n$ openai migrate\r\nDownloading Grit CLI from https://github.com/getgrit/gritql/releases/latest/download/marzano-x86_64-unknown-linux-gnu.tar.gz\r\nError: Failed to download Grit CLI from https://github.com/getgrit/gritql/releases/latest/download/marzano-x86_64-unknown-linux-gnu.tar.gz\r\n\r\n$ curl -fsSL https://docs.grit.io/install | bash\r\ndownloading grit 0.1.0-alpha.1730315451 x86_64-unknown-linux-gnu\r\ninstalling to /home/marcus/.grit/bin\r\n  grit\r\neverything's installed!\r\n\r\nTo add $HOME/.grit/bin to your PATH, either restart your shell or run:\r\n\r\n    source $HOME/.grit/bin/env (sh, bash, zsh)\r\n    source $HOME/.grit/bin/env.fish (fish)\r\n\r\n$  source $HOME/.grit/bin/env \r\n\r\n$ grit apply openai\r\n```"
      },
      {
        "author": "adenisdeveloper",
        "created_at": "2024-11-06T04:20:13+00:00",
        "body": "> grit apply openai\r\n\r\nThanks for the tip, it worked! I was having the same problem!\r\n"
      },
      {
        "author": "albertnieto",
        "created_at": "2024-12-10T18:03:27+00:00",
        "body": "No news from this issue?"
      },
      {
        "author": "Neutrovertido",
        "created_at": "2024-12-11T11:20:43+00:00",
        "body": "> I could bypass this bug is downloading grit and executing grit CLI , with \"apply openai\" parameters:\r\n> \r\n> ```\r\n> $ openai migrate\r\n> Downloading Grit CLI from https://github.com/getgrit/gritql/releases/latest/download/marzano-x86_64-unknown-linux-gnu.tar.gz\r\n> Error: Failed to download Grit CLI from https://github.com/getgrit/gritql/releases/latest/download/marzano-x86_64-unknown-linux-gnu.tar.gz\r\n> \r\n> $ curl -fsSL https://docs.grit.io/install | bash\r\n> downloading grit 0.1.0-alpha.1730315451 x86_64-unknown-linux-gnu\r\n> installing to /home/marcus/.grit/bin\r\n>   grit\r\n> everything's installed!\r\n> \r\n> To add $HOME/.grit/bin to your PATH, either restart your shell or run:\r\n> \r\n>     source $HOME/.grit/bin/env (sh, bash, zsh)\r\n>     source $HOME/.grit/bin/env.fish (fish)\r\n> \r\n> $  source $HOME/.grit/bin/env \r\n> \r\n> $ grit apply openai\r\n> ```\r\n\r\nthis did the trick for me"
      },
      {
        "author": "tensor-works",
        "created_at": "2024-12-14T20:01:14+00:00",
        "body": "Same isssue hasn't moved\r\n"
      }
    ],
    "url": "https://github.com/openai/openai-python/issues/1838"
  },
  {
    "repository": "openai/openai-python",
    "issue_number": 1833,
    "title": "Public helper for recombining ChatCompletionChunks into ChatCompletion",
    "body": "### Confirm this is a feature request for the Python library and not the underlying OpenAI API.\n\n- [X] This is a feature request for the Python library\n\n### Describe the feature or improvement you're requesting\n\nAfter streaming a chat completion response it is often necessary to recombine the streamed chunks into a message. Two examples are\r\n\r\n- in the https://github.com/pydantic/logfire observability platform, when a streamed response has ended the final Assistant message could be displayed nicely in the UI.\r\n    - issue: https://github.com/pydantic/logfire/issues/542\r\n- in https://github.com/jackmpcollins/magentic , parallel tool calls are streamed to call these during the generation, and inserting the outputs back into `messages` requires also creating an Assistant message from the streamed chunks.\r\n\r\nCurrently the internal class `ChatCompletionStreamState` makes this easy, but it is private which indicates it should not be relied on. Would it be possible to make this or similar functionality a supported part of the public API?\r\n\r\nThe current feature set of `ChatCompletionStreamState` is ideal:\r\n- get a `ChatCompletion` at any point during the stream (`current_completion_snapshot`). This allows logging a partial stream response in case of error, including if max_tokens was reached.\r\n- parse the chunks into correct pydantic BaseModels for the tools/response_format (`get_final_completion()`)\r\n\r\nExample usage of the existing class\r\n\r\n```python\r\nimport openai\r\nfrom openai.lib.streaming.chat._completions import ChatCompletionStreamState\r\n\r\nstate = ChatCompletionStreamState(\r\n    input_tools=openai.NOT_GIVEN,\r\n    response_format=openai.NOT_GIVEN,\r\n)\r\n\r\nresponse = client.chat.completions.create(...)\r\nfor chunk in response:\r\n    state.handle_chunk(chunk)\r\n\r\nprint(state.current_completion_snapshot)\r\nprint(state.get_final_completion())\r\n```\n\n### Additional context\n\n_No response_",
    "state": "closed",
    "created_at": "2024-10-30T03:51:24+00:00",
    "closed_at": "2025-01-06T05:53:07+00:00",
    "updated_at": "2025-01-06T05:53:07+00:00",
    "author": "jackmpcollins",
    "author_type": "User",
    "comments_count": 2,
    "reactions_count": 0,
    "labels": "enhancement",
    "milestone": null,
    "closed_by": "jackmpcollins",
    "resolution_time_hours": 1634.028611111111,
    "first_comments": [
      {
        "author": "RobertCraigie",
        "created_at": "2024-11-27T10:49:09+00:00",
        "body": "Thanks for the suggestion, I think this is a good idea!"
      },
      {
        "author": "jackmpcollins",
        "created_at": "2025-01-06T05:53:07+00:00",
        "body": "Thanks for this! I see it was released in https://github.com/openai/openai-python/releases/tag/v1.56.0"
      }
    ],
    "url": "https://github.com/openai/openai-python/issues/1833"
  },
  {
    "repository": "openai/openai-python",
    "issue_number": 1827,
    "title": "AnyIO worker threads not terminating when using asyncify with get_platform",
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\r\n\r\n- [X] This is an issue with the Python library\r\n\r\n### Describe the bug\r\n\r\nWhen using the OpenAI Python library (version 1.52.2), I encountered an issue where AnyIO worker threads are not terminating properly after asynchronous requests. This leads to tests hanging and not exiting when using `pytest`. The problem occurs because the library utilizes `asyncify(get_platform)` to call the synchronous `get_platform` function in an asynchronous context, causing AnyIO to create non-daemon worker threads that remain alive after the request completes.\r\n\r\n### To Reproduce\r\n\r\nUse the OpenAI Python library in an asynchronous environment.\r\nMake an asynchronous API request that triggers the `_request` method.\r\nRun tests using `pytest`.\r\nObserve that after the tests complete, the process hangs and does not exit due to lingering AnyIO worker threads.\r\n\r\n### Code snippets\r\n\r\nThe issue arises in the _request method where asyncify is used:\r\n\r\n```Python\r\nasync def _request(\r\n    self,\r\n    cast_to: Type[ResponseT],\r\n    options: FinalRequestOptions,\r\n    *,\r\n    stream: bool,\r\n    stream_cls: type[_AsyncStreamT] | None,\r\n    retries_taken: int,\r\n) -> ResponseT | _AsyncStreamT:\r\n    if self._platform is None:\r\n        # `get_platform` can make blocking IO calls so we\r\n        # execute it earlier while we are in an async context\r\n        self._platform = await asyncify(get_platform)()\r\n    # ... rest of the method ...\r\n```\r\nExpected behavior\r\n\r\nThe AnyIO worker threads should terminate after completing their tasks, allowing the program or tests to exit cleanly without hanging.\r\n\r\n### OS\r\n\r\nmacOS\r\n\r\n### Python version\r\n\r\nPython 3.12.4\r\n\r\n### Library version\r\n\r\n1.52.2",
    "state": "closed",
    "created_at": "2024-10-27T13:30:41+00:00",
    "closed_at": "2024-11-18T12:42:40+00:00",
    "updated_at": "2024-11-18T12:42:40+00:00",
    "author": "jeongsu-an",
    "author_type": "User",
    "comments_count": 2,
    "reactions_count": 0,
    "labels": "bug",
    "milestone": null,
    "closed_by": "RobertCraigie",
    "resolution_time_hours": 527.1997222222222,
    "first_comments": [
      {
        "author": "spokeydokeys",
        "created_at": "2024-11-04T16:55:14+00:00",
        "body": "I'm suffering from this bug in github workflows.  The workflow stalls at the end of my script and if I print out threads at that point there is a non-daemon, AnyIO thread that is left over and not closing.\r\n\r\nI tested @jeongsu-an's PR branch and it fixed the issue and did not introduce any other issues."
      },
      {
        "author": "RobertCraigie",
        "created_at": "2024-11-18T12:42:31+00:00",
        "body": "This will be fixed in https://github.com/openai/openai-python/pull/1872! Thanks to @spokeydokeys :)"
      }
    ],
    "url": "https://github.com/openai/openai-python/issues/1827"
  },
  {
    "repository": "openai/openai-python",
    "issue_number": 1826,
    "title": "AnyIO worker threads not terminating when using asyncify with get_platform",
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\r\n\r\n- [X] This is an issue with the Python library\r\n\r\n### Describe the bug\r\n\r\nWhen using the OpenAI Python library (version 1.52.2), I encountered an issue where AnyIO worker threads are not terminating properly after asynchronous requests. This leads to tests hanging and not exiting when using `pytest`. The problem occurs because the library utilizes `asyncify(get_platform)` to call the synchronous `get_platform` function in an asynchronous context, causing AnyIO to create non-daemon worker threads that remain alive after the request completes.\r\n\r\n\r\n\r\n### To Reproduce\r\n\r\n1. Use the OpenAI Python library in an asynchronous environment.\r\n2. Make an asynchronous API request that triggers the `_request` method.\r\n3. Run tests using `pytest`.\r\n4. Observe that after the tests complete, the process hangs and does not exit due to lingering AnyIO worker threads.\r\n\r\n### Code snippets\r\n\r\nThe issue arises in the _request method where asyncify is used:\r\n\r\n# In openai/_openai_client.py\r\n```Python\r\n\r\n\r\n\r\nasync def _request(\r\n    self,\r\n    cast_to: Type[ResponseT],\r\n    options: FinalRequestOptions,\r\n    *,\r\n    stream: bool,\r\n    stream_cls: type[_AsyncStreamT] | None,\r\n    retries_taken: int,\r\n) -> ResponseT | _AsyncStreamT:\r\n    if self._platform is None:\r\n        # `get_platform` can make blocking IO calls so we\r\n        # execute it earlier while we are in an async context\r\n        self._platform = await asyncify(get_platform)()\r\n    # ... rest of the method ...\r\n```\r\n\r\nExpected behavior\r\n\r\nThe AnyIO worker threads should terminate after completing their tasks, allowing the program or tests to exit cleanly without hanging.\r\n```\r\n\r\n\r\n### OS\r\n\r\nmacOS\r\n\r\n### Python version\r\n\r\nPython 3.12.4\r\n\r\n### Library version\r\n\r\n1.52.2",
    "state": "closed",
    "created_at": "2024-10-27T13:25:31+00:00",
    "closed_at": "2024-10-27T13:27:16+00:00",
    "updated_at": "2024-10-27T13:27:16+00:00",
    "author": "jeongsuAn",
    "author_type": "User",
    "comments_count": 0,
    "reactions_count": 0,
    "labels": "bug",
    "milestone": null,
    "closed_by": "jeongsuAn",
    "resolution_time_hours": 0.029166666666666667,
    "first_comments": [],
    "url": "https://github.com/openai/openai-python/issues/1826"
  },
  {
    "repository": "openai/openai-python",
    "issue_number": 1825,
    "title": "Missing import for asyncio in README example code",
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\n\n- [X] This is an issue with the Python library\n\n### Describe the bug\n\nHi there,\r\n\r\nI noticed a small typo in the example code provided in the README file. The code snippet uses *asyncio.run(main())* but is missing the import statement for *asyncio*. This could cause confusion for users running the example as it will result in a *NameError: name 'asyncio' is not defined*.\n\n### To Reproduce\n\nHere's the updated code with the necessary import:\r\n\r\n```python\r\nfrom openai import AsyncOpenAI\r\nimport asyncio  # Add this import\r\n\r\nclient = AsyncOpenAI()\r\n\r\nasync def main():\r\n    stream = await client.chat.completions.create(\r\n        model=\"gpt-4\",\r\n        messages=[{\"role\": \"user\", \"content\": \"Say this is a test\"}],\r\n        stream=True,\r\n    )\r\n    async for chunk in stream:\r\n        print(chunk.choices[0].delta.content or \"\", end=\"\")\r\n\r\nasyncio.run(main())\r\n```\n\n### Code snippets\n\n_No response_\n\n### OS\n\n-\n\n### Python version\n\n-\n\n### Library version\n\n-",
    "state": "closed",
    "created_at": "2024-10-26T18:39:56+00:00",
    "closed_at": "2024-11-11T10:18:00+00:00",
    "updated_at": "2024-11-11T10:18:00+00:00",
    "author": "okamiRvS",
    "author_type": "User",
    "comments_count": 4,
    "reactions_count": 0,
    "labels": "bug",
    "milestone": null,
    "closed_by": "RobertCraigie",
    "resolution_time_hours": 375.63444444444445,
    "first_comments": [
      {
        "author": "baslia",
        "created_at": "2024-11-10T05:15:44+00:00",
        "body": "Hey, I think this is solved now."
      },
      {
        "author": "okamiRvS",
        "created_at": "2024-11-10T09:50:41+00:00",
        "body": "I still see the absence of the asynco import in the second snippet of \"streaming resppnse\""
      },
      {
        "author": "baslia",
        "created_at": "2024-11-10T09:57:58+00:00",
        "body": "My bad, I just made a PR for that: https://github.com/openai/openai-python/pull/1858"
      },
      {
        "author": "RobertCraigie",
        "created_at": "2024-11-11T10:18:00+00:00",
        "body": "thanks @baslia, this was fixed in #1858"
      }
    ],
    "url": "https://github.com/openai/openai-python/issues/1825"
  },
  {
    "repository": "openai/openai-python",
    "issue_number": 1820,
    "title": "Error note that json_schema is not available on gpt-4o-mini",
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\n\n- [ ] This is an issue with the Python library\n\n### Describe the bug\n\nI used AsyncOpenAI Client to execute the tasks that required structed output (model as \"gpt-4o-mini\"). The first few runs of the same code returned normal results, but without any changes to the code, Suddenly I get a 400 error \"response_format value as json_schema is enabled only for api versions 2024-08-01-preview and later\".\n\n### To Reproduce\n\n1. Instantiate one AsyncOpenAI:\r\n```python\r\naclient = AsyncOpenAI()\r\n```\r\n2. Define the data model as follows:\r\n```python\r\nclass Term(BaseModel):\r\n    term: str\r\n    definition: str\r\nclass Terms(BaseModel):\r\n    terms: List[Term]\r\n```\r\n4. Use the following code to make the request:\r\n```python\r\nreference_text = \"Machine learning is a very important subject.\"\r\nsystem_promt = \"You are an expert in the field of machine learning.Please extract the terms and definitions from the reference text according to the appropriate format.\"\r\nuser_prompt = \"Given the following reference text:\\n{}\"\r\n\r\nasync def aquery(self, reference_text: str):\r\n    response = await aclient.beta.chat.completions.parse(\r\n        model=\"gpt-4o-mini\",\r\n        messages=[{\"role\": \"system\", \"content\": system_prompt},\r\n                  {\"role\": \"user\", \"content\": user_prompt.format(reference_text)}],\r\n        temperature=0.1,\r\n        response_format=Terms\r\n    )\r\n    return json.loads(response.choices[0].message.content)\r\n```\n\n### Code snippets\n\n```Python\nopenai.BadRequestError: Error code: 400 - {'error': {'message': 'response_format value as json_schema is enabled only for api versions 2024-08-01-preview and later', 'type': '', 'param': '', 'code': 'BadRequest'}}\n```\n\n\n### OS\n\nUbuntu 20.04.6 LTS\n\n### Python version\n\nPython 3.10.15\n\n### Library version\n\nopenai v1.51.1",
    "state": "closed",
    "created_at": "2024-10-25T06:22:25+00:00",
    "closed_at": "2024-10-25T06:53:52+00:00",
    "updated_at": "2024-10-25T06:53:52+00:00",
    "author": "RemixaWorld",
    "author_type": "User",
    "comments_count": 0,
    "reactions_count": 0,
    "labels": "bug",
    "milestone": null,
    "closed_by": "RemixaWorld",
    "resolution_time_hours": 0.5241666666666667,
    "first_comments": [],
    "url": "https://github.com/openai/openai-python/issues/1820"
  },
  {
    "repository": "openai/openai-python",
    "issue_number": 1819,
    "title": "Responses are in binary?",
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\n\n- [X] This is an issue with the Python library\n\n### Describe the bug\n\nHey, I'm using `vcrpy` to cache responses from OpenAI models to run tests easier and faster, but recently I've noticed responses now are in binary for some reason.\r\n\r\nHere are couple of questions regarding this, was there any big change that caused this change to happen? Whats the reason behind this change and how to convert this binary back into `JSON` format to continue testing OpenAI SDK?\r\n\r\nBefore\r\n```yaml\r\n  response:\r\n    body:\r\n      string: \"{\\n  \\\"id\\\": \\\"chatcmpl-AJ4U6CKcRZJQ9uWFy7Rz0drsowtgH\\\",\\n  \\\"object\\\":\r\n        \\\"chat.completion\\\",\\n  \\\"created\\\": 1729108834,\\n  \\\"model\\\": \\\"gpt-4-0613\\\",\\n\r\n        \\ \\\"choices\\\": [\\n    {\\n      \\\"index\\\": 0,\\n      \\\"message\\\": {\\n        \\\"role\\\":\r\n        \\\"assistant\\\",\\n        \\\"content\\\": \\\"Hello! How can I assist you today?\\\",\\n\r\n        \\       \\\"refusal\\\": null\\n      },\\n      \\\"logprobs\\\": null,\\n      \\\"finish_reason\\\":\r\n        \\\"stop\\\"\\n    }\\n  ],\\n  \\\"usage\\\": {\\n    \\\"prompt_tokens\\\": 8,\\n    \\\"completion_tokens\\\":\r\n        9,\\n    \\\"total_tokens\\\": 17,\\n    \\\"prompt_tokens_details\\\": {\\n      \\\"cached_tokens\\\":\r\n        0\\n    },\\n    \\\"completion_tokens_details\\\": {\\n      \\\"reasoning_tokens\\\":\r\n        0\\n    }\\n  },\\n  \\\"system_fingerprint\\\": null\\n}\\n\"\r\n```\r\n\r\nAfter\r\n\r\n```yaml\r\n  response:\r\n    body:\r\n      string: !!binary |\r\n        g0QBAMRCne5apuvPdgOsAqPAQcUvU8JTN4Ee1H63cIMtkLnPF0nnUa+vgQYeWHbN29u6+EfNqN5d\r\n        CoAGliTI9FVuxt6FB9fz6c3r3bDzNH27ycxMNyqjo2lujyJ/NKNAAMR6GJkcqW+mGHgDjlVw5XJR\r\n        ZEmi0qp22uVqo9wUPhizjRxJUM/nYZ3DarlaD8vtsNwEC/s8MFFGEl8CAJazFUDP349Iohys4mQM\r\n        k6TiTwCl7CKSIJVlgyxXcU6BQqF7nZQEnUfO8QbOeQqjYlwA9AVzLpCzVfM9cOHJ+mnlSCIunCM1\r\n        1t6kHPd8yjrTIV7iRm+Dv0u1xkmCspw9+eSvBfAT9bgIRIJ8ymOf/+U8iuKMJNoxDGiXC3LQKfyJ\r\n        QEF0VGkFfPpfRzojM3cGkFGmH1m0Uh6L1mZraWCAq0Hc8wExaYLNoOL/F3/dQdyLUp8OTuG+6/9U\r\n        y5abUb2qWiTWwgAD\r\n```\n\n### To Reproduce\n\nPackage I'm working on is open-source, so you can clone from here\r\n\r\nhttps://github.com/withmartian/adapters\r\n\r\nDelete current cassette files and then rerun tests\r\n\r\nNew cassette file will have binary in their response body field\n\n### Code snippets\n\n```Python\n# Example:\r\n# From https://github.com/withmartian/adapters\r\n\r\n@pytest.mark.parametrize(\"model_path\", MODEL_PATHS)\r\n@pytest.mark.vcr\r\ndef test_sync(vcr, model_path: str):\r\n    adapter = AdapterFactory.get_adapter_by_path(model_path)\r\n\r\n    assert adapter is not None\r\n\r\n    adapter_response = adapter.execute_sync(SIMPLE_CONVERSATION_USER_ONLY)\r\n\r\n    cassette_response = get_response_content_from_vcr(vcr, adapter)\r\n\r\n    assert adapter_response.choices[0].message.content == cassette_response\n```\n\n\n### OS\n\nmacOs\n\n### Python version\n\nPython v3.11.10\n\n### Library version\n\nopenai v1.52.2",
    "state": "closed",
    "created_at": "2024-10-24T20:56:04+00:00",
    "closed_at": "2024-10-25T05:24:51+00:00",
    "updated_at": "2024-10-25T05:24:51+00:00",
    "author": "LukeSamkharadze",
    "author_type": "User",
    "comments_count": 1,
    "reactions_count": 0,
    "labels": "bug",
    "milestone": null,
    "closed_by": "LukeSamkharadze",
    "resolution_time_hours": 8.479722222222222,
    "first_comments": [
      {
        "author": "LukeSamkharadze",
        "created_at": "2024-10-25T05:24:51+00:00",
        "body": "Figured it out, it seems API is now returning content in compressed format. \r\n\r\n`brotli` is used to compress it. \r\n\r\nTo uncompress it:\r\n\r\n```\r\nimport brotli\r\n\r\nbrotli.decompress(response)\r\n```"
      }
    ],
    "url": "https://github.com/openai/openai-python/issues/1819"
  },
  {
    "repository": "openai/openai-python",
    "issue_number": 1812,
    "title": "Set \"$. messages [0] (SystemMessage). content (ArrayOfContentParts)\" as an empty list, and no 400 is returned",
    "body": "\r\n### Describe the bug\r\n\r\nIt is mentioned in the official openapi document (https://storage.googleapis.com/stainless-sdk-openapi-specs/openai-f9320ebf347140052c7f8b0bc5c7db24f5e367c368c8cb34c3606af4e2b6591b.yml)\r\n<img width=\"1154\" alt=\"image\" src=\"https://github.com/user-attachments/assets/4f756d0e-9437-4a99-bdde-e9144dd19c4b\">\r\n\r\n\r\n\r\n\r\n### To Reproduce\r\n\r\n1.I made the following request with empty list\r\n2.response said 200 but not 400 \r\n\r\nI think this is inconsistent with the description in the document\r\n\r\n### Code snippets\r\n\r\n```Python\r\ncompletion = client.chat.completions.create(\r\n    model=\"gpt-4o-mini\",\r\n    messages=[\r\n        {\r\n            \"content\": [],\r\n            \"role\": \"system\"\r\n        },\r\n        {\r\n            \"content\": \"hello\",\r\n            \"role\": \"user\"\r\n        }\r\n    ])\r\n```\r\n\r\n\r\n### OS\r\n\r\nmacOS\r\n\r\n### Python version\r\n\r\nPython v3.12.4\r\n\r\n### Library version\r\n\r\nopenai v1.50.2",
    "state": "closed",
    "created_at": "2024-10-23T09:35:35+00:00",
    "closed_at": "2024-12-27T09:02:57+00:00",
    "updated_at": "2024-12-27T09:02:57+00:00",
    "author": "steadyfirmness",
    "author_type": "User",
    "comments_count": 1,
    "reactions_count": 0,
    "labels": "bug",
    "milestone": null,
    "closed_by": "steadyfirmness",
    "resolution_time_hours": 1559.456111111111,
    "first_comments": [
      {
        "author": "steadyfirmness",
        "created_at": "2024-10-23T09:40:46+00:00",
        "body": "whats more, other \"ArrayOfContent\" have this issue too."
      }
    ],
    "url": "https://github.com/openai/openai-python/issues/1812"
  },
  {
    "repository": "openai/openai-python",
    "issue_number": 1809,
    "title": "Create Message and create Run Assistant take too long time",
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\n\n- [ ] This is an issue with the Python library\n\n### Describe the bug\n\nFrom the API I had constructed a number of calls to the Wizards I had created. However, as of today, when I run the following command:\r\n`client.beta.threads.messages.create(thread_id=thread.id, role=\"user\", content=user_message)`\r\nand\r\n`client.beta.threads.runs.create(thread_id=thread.id,assistant_id=assistant_id)`\r\nUpon examination of the library during debugging, a significant loop was identified. However, due to my limited understanding of the library, I am unable to discern its nature.\r\nThe prolonged time required to create a message and a Run, coupled with the status of 'expired' assigned to the Run upon reaching the execution point of a calling function, has resulted in the unavailability of the implementation, which is already operational in a production environment.\r\n\r\nThanks a lot for the hard work maintaining this code!\n\n### To Reproduce\n\n```python\r\n#Create any thread\r\nthread = client.beta.threads.create()\r\nclient.beta.threads.messages.create(thread_id=thread.id, role=\"user\", content=user_message) # Here the code takes more than 1 minute\r\nclient.beta.threads.runs.create(thread_id=thread.id,assistant_id=assistant_id,) # Here the code take more than 9 minutes\r\n```\n\n### Code snippets\n\n```Python\ndef get_response(client, thread):\r\n    return client.beta.threads.messages.list(thread_id=thread.id, order=\"asc\")\r\n\r\n\r\ndef process(client, assistant_id, question):\r\n    time1 = time.time()\r\n    print(\"init\")\r\n    thread, run = create_thread_and_run(client, assistant_id, question)\r\n    print(f\"Thread and run created in {time.time() - time1} seconds\")\r\n    run = wait_on_run(client, run, thread)\r\n    if (\r\n        run.status == \"requires_action\"\r\n        and run.required_action.submit_tool_outputs.tool_calls[0].function.name == \"GetTopHeadlinesNews\"\r\n    ):\r\n        parameters = json_loads(run.required_action.submit_tool_outputs.tool_calls[0].function.arguments)\r\n        response = GetTopHeadlinesNews(parameters)\r\n        tool_outputs = [\r\n            {\r\n                \"tool_call_id\": run.required_action.submit_tool_outputs.tool_calls[0].id,\r\n                \"output\": json_dumps(response),\r\n            }\r\n        ]\r\n        timeStart = time.time()\r\n        run = client.beta.threads.runs.submit_tool_outputs(\r\n            thread_id=thread.id,\r\n            run_id=run.id,\r\n            tool_outputs=tool_outputs,\r\n        )\r\n        print(f\"Time taken to submit tool outputs: {time.time()-timeStart}\")\r\n        run = wait_on_run(run, thread)\r\n        return get_response(thread)\r\n\r\n\r\n# Function calling\r\ndef GetTopHeadlinesNews(arguments):\r\n    url = \"https://newsapi.org/v2/top-headlines\"\r\n    parameters = arguments\r\n    response = requests.get(url, params=parameters, headers={\"X-Api-Key\": api_key_news})\r\n    data = response.json()\r\n    return data\r\n\r\n\r\n# Waiting in a loop\r\ndef wait_on_run(client, run, thread):\r\n    timeStart = time.time()\r\n    print(\"Waiting for run to complete\")\r\n    while run.status == \"queued\" or run.status == \"in_progress\":\r\n        run = client.beta.threads.runs.retrieve(\r\n            thread_id=thread.id,\r\n            run_id=run.id,\r\n        )\r\n    print(f\"Time taken to wait on run: {time.time()-timeStart}\")\r\n    print(f\"Run status: {run.status}\")\r\n    return run\r\n\r\n\r\ndef create_thread_and_run(client, assistant_id, user_input):\r\n    timeStart = time.time()\r\n    thread = client.beta.threads.create()\r\n    print(f\"Time taken to create thread: {time.time()-timeStart}\")\r\n    run = submit_message(client, assistant_id, thread, user_input)\r\n    return thread, run\r\n\r\n\r\ndef submit_message(client, assistant_id, thread, user_message):\r\n    timeStart = time.time()\r\n    print(f\"Submitting message...\")\r\n    client.beta.threads.messages.create(thread_id=thread.id, role=\"user\", content=user_message)\r\n    print(f\"Time taken to submit message: {time.time()-timeStart}\")\r\n    print(f\"Creating run...\")\r\n    return client.beta.threads.runs.create(\r\n        thread_id=thread.id,\r\n        assistant_id=assistant_id,\r\n    )\n```\n\n\n### OS\n\nWindows\n\n### Python version\n\nPython v3.12.7\n\n### Library version\n\nopenai v1.52.0",
    "state": "closed",
    "created_at": "2024-10-22T16:13:49+00:00",
    "closed_at": "2024-10-22T16:35:52+00:00",
    "updated_at": "2024-10-22T16:35:52+00:00",
    "author": "Rochinchi",
    "author_type": "User",
    "comments_count": 1,
    "reactions_count": 0,
    "labels": "API-feedback",
    "milestone": null,
    "closed_by": "RobertCraigie",
    "resolution_time_hours": 0.3675,
    "first_comments": [
      {
        "author": "RobertCraigie",
        "created_at": "2024-10-22T16:35:52+00:00",
        "body": "Thanks for reporting!  \r\n\r\nThis sounds like an issue with the underlying OpenAI API and not the SDK, so I'm going to go ahead and close this issue. \r\n\r\nWould you mind reposting at [community.openai.com](https://community.openai.com)?"
      }
    ],
    "url": "https://github.com/openai/openai-python/issues/1809"
  },
  {
    "repository": "openai/openai-python",
    "issue_number": 1808,
    "title": "Audio generation from audio",
    "body": "### Confirm this is a feature request for the Python library and not the underlying OpenAI API.\n\n- [X] This is a feature request for the Python library\n\n### Describe the feature or improvement you're requesting\n\nOpenAI recently introduced a new feature for generating audio from audio, and according to their documentation, you can use the new feature in Python by adding a parameter called modalities. However, when I try to use the feature, I receive the following error:\r\n\r\nTypeError: Completions.create() got an unexpected keyword argument 'modalities'\r\n\r\nIt seems that the modalities parameter might not be recognized or properly implemented in the version I'm using. Could you please assist in resolving this issue and confirm if there are additional steps or updates required to use this feature?\n\n### Additional context\n\n_No response_",
    "state": "closed",
    "created_at": "2024-10-21T15:56:09+00:00",
    "closed_at": "2024-10-21T17:17:38+00:00",
    "updated_at": "2024-10-21T17:17:38+00:00",
    "author": "yashinsmast",
    "author_type": "User",
    "comments_count": 1,
    "reactions_count": 0,
    "labels": "",
    "milestone": null,
    "closed_by": "RobertCraigie",
    "resolution_time_hours": 1.3580555555555556,
    "first_comments": [
      {
        "author": "RobertCraigie",
        "created_at": "2024-10-21T17:17:38+00:00",
        "body": "it sounds like you aren't on the latest version, could you try updating? you can verify the version that your code is running with `import openai; print(openai.__version__)`"
      }
    ],
    "url": "https://github.com/openai/openai-python/issues/1808"
  },
  {
    "repository": "openai/openai-python",
    "issue_number": 1804,
    "title": "AsyncOpenAI does not do any retries when Exception occurs",
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\n\n- [X] This is an issue with the Python library\n\n### Describe the bug\n\nHi, please refer to https://github.com/openai/openai-python/pull/1803 for details.\n\n### To Reproduce\n\n(see that PR)\n\n### Code snippets\n\n_No response_\n\n### OS\n\nunrelated\n\n### Python version\n\nunrelated\n\n### Library version\n\nlatest",
    "state": "closed",
    "created_at": "2024-10-20T03:08:11+00:00",
    "closed_at": "2024-10-21T07:12:48+00:00",
    "updated_at": "2024-10-21T07:12:48+00:00",
    "author": "fzyzcjy",
    "author_type": "User",
    "comments_count": 0,
    "reactions_count": 0,
    "labels": "bug",
    "milestone": null,
    "closed_by": "RobertCraigie",
    "resolution_time_hours": 28.076944444444443,
    "first_comments": [],
    "url": "https://github.com/openai/openai-python/issues/1804"
  },
  {
    "repository": "openai/openai-python",
    "issue_number": 1794,
    "title": "Structured Outputs via function calling - Descriptions for important keys",
    "body": "### Confirm this is a feature request for the Python library and not the underlying OpenAI API.\n\n- [X] This is a feature request for the Python library\n\n### Describe the feature or improvement you're requesting\n\n\r\nI like the new SDK helper to parse the model's output:\r\n\r\n```python\r\nfrom pydantic import BaseModel\r\nfrom openai import OpenAI\r\n\r\nclient = OpenAI()\r\n\r\nclass ResearchPaperExtraction(BaseModel):\r\n    title: str\r\n    authors: list[str]\r\n    abstract: str\r\n    keywords: list[str] # How do I add a description for this or any of the parameters? \r\n    # How does the model know exactly how to use this? will it infer on its own or we specify this in the system message? \r\n\r\ncompletion = client.beta.chat.completions.parse(\r\n    model=\"gpt-4o-2024-08-06\",\r\n    messages=[\r\n        {\"role\": \"system\", \"content\": \"You are an expert at structured data extraction. You will be given unstructured text from a research paper and should convert it into the given structure.\"},\r\n        {\"role\": \"user\", \"content\": \"...\"}\r\n    ],\r\n    response_format=ResearchPaperExtraction,\r\n)\r\n\r\nresearch_paper = completion.choices[0].message.parsed\r\n```\r\n\r\nBut how do I get the granularity of using manual schema with the SDK objects?\r\nMainly adding detailed description for the parameters\r\n\r\n```json\r\n{\r\n    \"name\": \"get_weather\",\r\n    \"description\": \"Fetches the weather in the given location\",\r\n    \"strict\": true,\r\n    \"parameters\": {\r\n        \"type\": \"object\",\r\n        \"properties\": {\r\n            \"location\": {\r\n                \"type\": \"string\",\r\n                \"description\": \"The location to get the weather for\"\r\n            },\r\n            \"unit\": {\r\n                \"type\": \"string\",\r\n                \"description\": \"The unit to return the temperature in\",\r\n                \"enum\": [\"F\", \"C\"]\r\n            }\r\n        },\r\n        \"additionalProperties\": false,\r\n        \"required\": [\"location\", \"unit\"]\r\n    }\r\n}\r\n```\r\n\r\n_Snippets taken from the official OpenAI Documentation._\n\n### Additional context\n\nPlease improve the documentation. How can I submit requests to improve it? \r\nAlso I get no response from the OpenAI Developer Forum, thus I raise an issue here (@RobertCraigie is a good guy :)",
    "state": "closed",
    "created_at": "2024-10-17T06:51:03+00:00",
    "closed_at": "2024-10-17T12:24:53+00:00",
    "updated_at": "2024-10-17T12:29:55+00:00",
    "author": "rohanbalkondekar",
    "author_type": "User",
    "comments_count": 1,
    "reactions_count": 0,
    "labels": "",
    "milestone": null,
    "closed_by": "rohanbalkondekar",
    "resolution_time_hours": 5.563888888888889,
    "first_comments": [
      {
        "author": "rohanbalkondekar",
        "created_at": "2024-10-17T12:29:54+00:00",
        "body": "## Example:\r\n\r\n```python\r\nimport openai\r\nfrom rich import print\r\nfrom openai import OpenAI\r\nfrom dotenv import load_dotenv\r\nfrom pydantic import BaseModel, Field, field_validator\r\nfrom datetime import datetime\r\nfrom typing import List\r\nfrom enum import Enum\r\nimport re\r\n\r\n_ = load_dotenv()\r\nclient = OpenAI()\r\n\r\nfrom pydantic import BaseModel, Field, field_validator\r\nfrom datetime import datetime\r\nfrom typing import List\r\nfrom enum import Enum\r\nimport re\r\n\r\n\r\nclass Gender(str, Enum):\r\n    MALE = \"Male\"\r\n    FEMALE = \"Female\"\r\n\r\n\r\nclass IDType(str, Enum):\r\n    PASSPORT = \"passport\"\r\n    GCC_ID = \"gcc_id\"\r\n\r\n\r\nclass DocumentID(BaseModel):\r\n    id_type: IDType = Field(\r\n        ...,\r\n        description=\"Type of identification document, either 'passport' or 'gcc_id'\",\r\n    )\r\n    id_number: str = Field(\r\n        ...,\r\n        description=\"Identification document number without spaces or special characters\",\r\n    )\r\n    expiry_date: str  = Field(\r\n        ...,\r\n        description=\"Expiry date of the identification document in YYYY-MM-DD format\"\r\n    )\r\n\r\n    @field_validator(\"expiry_date\")\r\n    def validate_expiry_date(cls, value):\r\n        try:\r\n            datetime.strptime(value, \"%Y-%m-%d\")\r\n        except ValueError:\r\n            raise ValueError(\"expiry_date must be in YYYY-MM-DD format\")\r\n        return value\r\n\r\n    @field_validator(\"id_number\")\r\n    def validate_id_number(cls, value):\r\n        if not re.fullmatch(r\"[A-Za-z0-9]+\", value):\r\n            raise ValueError(\"id_number must not contain spaces or special characters\")\r\n        return value\r\n\r\n\r\nclass Passenger(BaseModel):\r\n    id: str = Field(\r\n        ...,\r\n        description=\"Unique identifier for the passenger\"\r\n    )\r\n    name: str = Field(\r\n        ...,\r\n        description=\"Full name of the passenger\"\r\n    )\r\n    nationality: str = Field(\r\n        ...,\r\n        description=\"Nationality of the passenger. Convert the user input to ISO 3166-1 alpha-2 formatted country code yourself\"\r\n    )\r\n    gender: Gender = Field(\r\n        ...,\r\n        description=\"Gender of the passenger. Either Male or Female\"\r\n    )\r\n    birth_date: str = Field(\r\n        ...,\r\n        description=\"Birth date of the passenger in YYYY-MM-DD format\"\r\n    )\r\n    document_id: DocumentID\r\n\r\n    @field_validator(\"birth_date\")\r\n    def validate_birth_date(cls, value):\r\n        try:\r\n            datetime.strptime(value, \"%Y-%m-%d\")\r\n        except ValueError:\r\n            raise ValueError(\"birth_date must be in YYYY-MM-DD format\")\r\n        return value\r\n\r\n\r\nclass PassengerList(BaseModel):\r\n    \"\"\"List of Passengers\"\"\"\r\n\r\n    passengers: List[Passenger] = Field(\r\n        ..., description=\"List containing passenger details\"\r\n    )\r\n\r\n\r\ntools = [openai.pydantic_function_tool(PassengerList)]\r\nprint(tools)\r\n```\r\n\r\n## Output:\r\n\r\n```javascript\r\n[\r\n    {\r\n        'type': 'function',\r\n        'function': {\r\n            'name': 'PassengerList',\r\n            'strict': True,\r\n            'parameters': {\r\n                '$defs': {\r\n                    'DocumentID': {\r\n                        'properties': {\r\n                            'id_type': {\r\n                                'description': \"Type of identification document, either 'passport' or 'gcc_id'\",\r\n                                'enum': ['passport', 'gcc_id'],\r\n                                'title': 'IDType',\r\n                                'type': 'string'\r\n                            },\r\n                            'id_number': {\r\n                                'description': 'Identification document number without spaces or special \r\ncharacters',\r\n                                'title': 'Id Number',\r\n                                'type': 'string'\r\n                            },\r\n                            'expiry_date': {\r\n                                'description': 'Expiry date of the identification document in YYYY-MM-DD format',\r\n                                'title': 'Expiry Date',\r\n                                'type': 'string'\r\n                            }\r\n                        },\r\n                        'required': ['id_type', 'id_number', 'expiry_date'],\r\n                        'title': 'DocumentID',\r\n                        'type': 'object',\r\n                        'additionalProperties': False\r\n                    },\r\n                    'Gender': {'enum': ['Male', 'Female'], 'title': 'Gender', 'type': 'string'},\r\n                    'IDType': {'enum': ['passport', 'gcc_id'], 'title': 'IDType', 'type': 'string'},\r\n                    'Passenger': {\r\n                        'properties': {\r\n                            'id': {\r\n                                'description': 'Unique identifier for the passenger',\r\n                                'title': 'Id',\r\n                                'type': 'string'\r\n                            },\r\n                            'name': {\r\n                                'description': 'Full name of the passenger',\r\n                                'title': 'Name',\r\n                                'type': 'string'\r\n                            },\r\n                            'nationality': {\r\n                                'description': 'Nationality of the passenger. Convert the user input to ISO 3166-1 \r\nalpha-2 formatted country code yourself',\r\n                                'title': 'Nationality',\r\n                                'type': 'string'\r\n                            },\r\n                            'gender': {\r\n                                'description': 'Gender of the passenger. Either Male or Female',\r\n                                'enum': ['Male', 'Female'],\r\n                                'title': 'Gender',\r\n                                'type': 'string'\r\n                            },\r\n                            'birth_date': {\r\n                                'description': 'Birth date of the passenger in YYYY-MM-DD format',\r\n                                'title': 'Birth Date',\r\n                                'type': 'string'\r\n                            },\r\n                            'document_id': {'$ref': '#/$defs/DocumentID'}\r\n                        },\r\n                        'required': ['id', 'name', 'nationality', 'gender', 'birth_date', 'document_id'],\r\n                        'title': 'Passenger',\r\n                        'type': 'object',\r\n                        'additionalProperties': False\r\n                    }\r\n                },\r\n                'description': 'List of Passengers',\r\n                'properties': {\r\n                    'passengers': {\r\n                        'description': 'List containing passenger details',\r\n                        'items': {'$ref': '#/$defs/Passenger'},\r\n                        'title': 'Passengers',\r\n                        'type': 'array'\r\n                    }\r\n                },\r\n                'required': ['passengers'],\r\n                'title': 'PassengerList',\r\n                'type': 'object',\r\n                'additionalProperties': False\r\n            },\r\n            'description': 'List of Passengers'\r\n        }\r\n    }\r\n]\r\n```\r\n\r\n"
      }
    ],
    "url": "https://github.com/openai/openai-python/issues/1794"
  },
  {
    "repository": "openai/openai-python",
    "issue_number": 1793,
    "title": "Invalid API key environment variables lead to confusing encoding errors",
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\r\n\r\n- [X] This is an issue with the Python library\r\n\r\n### Describe the bug\r\n\r\nReferring to [this issue](https://github.com/langchain-ai/langchain/issues/27365)\r\n\r\nWhen the API key is accidentally set with invalid characters, an error occurs within the HTTP client when openai library attempts to set headers.  \r\n\r\nWhile eventually one might deduce the problem from the stack trace, having some sort of upstream \"check\" and more informative error about the API key would seem to help users understand the error. This caused us to spend multiple days trying to find the source of the problem assuming the error was in the message content instead of in the API key.\r\n\r\n### To Reproduce\r\n\r\n1. Set the API key to a value with non-ASCII characters\r\n2. Execute a chat completion with the client\r\n3. Receive encoding error `UnicodeEncodeError: 'ascii' codec can't encode characters in position 7-34: ordinal not in range(128)`\r\n\r\n### Code snippets\r\n\r\n```Python\r\nfrom openai import OpenAI\r\n\r\n#######################\r\n#### CONFIGURATION ####\r\n#######################\r\n\r\nconfigs = dict(\r\n    api_key = 'здравейздравейздравейздравей',\r\n)\r\n\r\nclient = OpenAI(**configs)\r\n\r\n######################\r\n###### RUN TEST ######\r\n######################\r\n\r\nmessage = 'Hello!'\r\ncompletion = client.chat.completions.create(\r\n    model='gpt-4-turbo-preview',\r\n    messages=[\r\n        {\r\n            'role': 'user',\r\n            'content': message\r\n        }\r\n    ]    \r\n)\r\n\r\nprint(completion.choices[0].message.content)\r\n```\r\n\r\n\r\n### OS\r\n\r\nany\r\n\r\n### Python version\r\n\r\n3.11\r\n\r\n### Library version\r\n\r\nopenai v1.44.0",
    "state": "closed",
    "created_at": "2024-10-16T01:23:57+00:00",
    "closed_at": "2024-11-22T12:22:47+00:00",
    "updated_at": "2024-11-22T12:22:47+00:00",
    "author": "jasonkaedingrhino",
    "author_type": "User",
    "comments_count": 2,
    "reactions_count": 0,
    "labels": "bug",
    "milestone": null,
    "closed_by": "RobertCraigie",
    "resolution_time_hours": 898.9805555555556,
    "first_comments": [
      {
        "author": "jasonkaedingrhino",
        "created_at": "2024-10-16T01:28:28+00:00",
        "body": "In checking the stack trace, I found that there is already a method to \"validate headers\" that.... does nothing.  That would seem to be the place to check the API key?\r\n\r\n```\r\ndef _validate_headers(\r\n        self,\r\n        headers: Headers,  # noqa: ARG002\r\n        custom_headers: Headers,  # noqa: ARG002\r\n    ) -> None:\r\n        \"\"\"Validate the given default headers and custom headers.\r\n\r\n        Does nothing by default.\r\n        \"\"\"\r\n        return\r\n```"
      },
      {
        "author": "RobertCraigie",
        "created_at": "2024-11-22T12:22:47+00:00",
        "body": "The `._validate_headers()` function is intended for internal SDK level validation, e.g. for missing headers, so I don't think we want to enforce any encoding specific behaviour here.\r\n\r\nI opened an [issue](https://github.com/encode/httpx/issues/3400) with httpx to improve the error message for invalid headers so I'm going to close this in favour of that."
      }
    ],
    "url": "https://github.com/openai/openai-python/issues/1793"
  },
  {
    "repository": "openai/openai-python",
    "issue_number": 1790,
    "title": "Limits argument can never get passed down?",
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\n\n- [X] This is an issue with the Python library\n\n### Describe the bug\n\nIt seems as though when initializing an openAI client like so:\r\n\r\n\r\nsince you cannot pass in the limits argument as a parameter, it will always default to the DEFAULT value here:\r\n\r\nhttps://github.com/openai/openai-python/blob/aa6818997468b753546d55365d8142e2bb1c6640/src/openai/_base_client.py#L1127\r\n\r\nopenai.AsyncClient(())\n\n### To Reproduce\n\n1. Initialize an openai client like so: openai.AsyncClient(())\r\n2. Try to pass in 'limits' argument or modify the 'http_client' to have custom Limits. \r\n3. This will not actually be propagated \n\n### Code snippets\n\n```Python\nopenai.AsyncClient(())\n```\n\n\n### OS\n\nlinux\n\n### Python version\n\n3.9.12 \n\n### Library version\n\n1.50.0",
    "state": "closed",
    "created_at": "2024-10-10T22:15:23+00:00",
    "closed_at": "2024-12-09T10:51:37+00:00",
    "updated_at": "2024-12-09T10:52:14+00:00",
    "author": "dmakhervaks",
    "author_type": "User",
    "comments_count": 3,
    "reactions_count": 0,
    "labels": "bug",
    "milestone": null,
    "closed_by": "RobertCraigie",
    "resolution_time_hours": 1428.6038888888888,
    "first_comments": [
      {
        "author": "RobertCraigie",
        "created_at": "2024-11-01T12:01:52+00:00",
        "body": "> modify the 'http_client' to have custom Limits.\r\n\r\nI cannot reproduce this part, passing in a custom HTTP client with different limits does get propagated\r\n```py\r\nimport httpx\r\nfrom openai import AsyncOpenAI\r\n\r\nclient = AsyncOpenAI(\r\n    http_client=httpx.AsyncClient(\r\n        limits=httpx.Limits(\r\n            max_connections=1,\r\n        )\r\n    )\r\n)\r\nprint(client._client._transport._pool._max_connections)\r\n```\r\nThis prints `1` for me. Is it different for you?\r\n\r\n---\r\nSeparately it is intentional that you can't pass `limits` directly to `OpenAI()`, the code you linked to is left-over from the original implementation and we haven't removed that yet.\r\n\r\nThe intended path for customizing limits is by [passing your own HTTP client instance](https://github.com/openai/openai-python?tab=readme-ov-file#configuring-the-http-client) directly."
      },
      {
        "author": "ganeshhnalla",
        "created_at": "2024-12-09T08:36:29+00:00",
        "body": "You have set the max_connections to 1, meaning the `httpx.AsyncClient` should only allow a maximum of 1 concurrent connection at a time.\r\n\r\n```py\r\nimport httpx\r\nfrom openai import AsyncOpenAI\r\n\r\nhttp_client = httpx.AsyncClient(\r\n    limits=httpx.Limits(\r\n        max_connections=1,  # Set max connections to 1 (or any number you want)\r\n    )\r\n)\r\n\r\nclient = AsyncOpenAI(http_client=http_client)\r\n\r\nprint(client._client._transport._pool._max_connections)\r\n```\r\n\r\nThe output is \r\n`1`\r\nFor me as well"
      },
      {
        "author": "RobertCraigie",
        "created_at": "2024-12-09T10:52:12+00:00",
        "body": "Please re-open if you're still running into this issue."
      }
    ],
    "url": "https://github.com/openai/openai-python/issues/1790"
  },
  {
    "repository": "openai/openai-python",
    "issue_number": 1789,
    "title": "AttributeError: 'FileCitation' object has no attribute 'quote'",
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\n\n- [X] This is an issue with the Python library\n\n### Describe the bug\n\nI am on v1.51.2 and seems like the attribute quote has not been fixed since 1.34.0. Do we have any alternative to this?\r\n\n\n### To Reproduce\n\nCreate an assistant with files associated\r\nask a question that has filecitation\r\ncheck to get the file_citation.quote\n\n### Code snippets\n\n_No response_\n\n### OS\n\nWindows\n\n### Python version\n\nPython 3.10.11\n\n### Library version\n\nopenai v1.51.2",
    "state": "closed",
    "created_at": "2024-10-10T19:39:11+00:00",
    "closed_at": "2025-01-22T21:07:50+00:00",
    "updated_at": "2025-01-22T21:07:51+00:00",
    "author": "xiahbu",
    "author_type": "User",
    "comments_count": 2,
    "reactions_count": 0,
    "labels": "bug",
    "milestone": null,
    "closed_by": "RobertCraigie",
    "resolution_time_hours": 2497.4775,
    "first_comments": [
      {
        "author": "Programmer-RD-AI",
        "created_at": "2025-01-19T05:40:42+00:00",
        "body": "\nHi, \n\nThank you for reporting this! This issue seems to be the same as the one discussed in #1498. Could you please take a look there for updates and context? If you feel there’s any additional detail or difference in your case, feel free to add it here or in the referenced issue.\n\nThanks!\n"
      },
      {
        "author": "RobertCraigie",
        "created_at": "2025-01-22T21:07:50+00:00",
        "body": "Thanks for reporting!  \n\nThis sounds like an issue with the underlying OpenAI API and not the SDK, so I'm going to go ahead and close this issue. \n\nWould you mind reposting at [community.openai.com](https://community.openai.com)?"
      }
    ],
    "url": "https://github.com/openai/openai-python/issues/1789"
  },
  {
    "repository": "openai/openai-python",
    "issue_number": 1788,
    "title": "Repeated Error: openai.Completion Not Supported in OpenAI Python >=1.0.0 Despite Multiple Fix Attempts (Python Versions and API Key Methods)",
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\n\n- [X] This is an issue with the Python library\n\n### Describe the bug\n\nI am encountering a recurring error when trying to access openai.Completion. The error message suggests that this is no longer supported in versions of the OpenAI Python library greater than 1.0.0. Despite following the official migration guide and pinning to earlier versions (e.g., openai==0.28), the issue remains unresolved. I have tried various solutions such as:\r\n\r\nDowngrading Python from 3.12 to 3.10, and even 3.9\r\nAdjusting API key access methods (environment variables, hardcoding, etc.)\r\nAttempting multiple different virtual environments\r\nFollowing the migration instructions from the OpenAI repository and discussions, such as using openai migrate.\r\nNo matter what I try, I keep encountering the following error:\r\n\r\nvbnet\r\nCopy code\r\nYou tried to access openai.Completion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\r\n\r\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface.\r\n\r\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\r\nDespite using the suggested approach to pin the version (pip install openai==0.28), the issue remains unresolved. Additionally, following the migration guide (https://github.com/openai/openai-python/discussions/742) didn't yield any success.\n\n### To Reproduce\n\nSteps to reproduce the behavior:\r\n\r\n1. Set up a Python virtual environment with Python 3.10.\r\n2. Install openai using pip install openai==0.27.0 or any version before 1.0.0.\r\n3. Attempt to use openai.Completion.create() in any script.\r\n4. The above error consistently occurs.\n\n### Code snippets\n\n```Python\nCode snippets\r\nHere’s the code I used to trigger the error:\r\n\r\npython\r\nCopy code\r\nimport openai\r\nopenai.api_key = 'sk-xxxxxxx'\r\n\r\nresponse = openai.Completion.create(\r\n    engine=\"davinci\",\r\n    prompt=\"Tell me a joke.\",\r\n    max_tokens=50\r\n)\r\nprint(response.choices[0].text)\r\nError output:\r\nvbnet\r\nCopy code\r\nYou tried to access openai.Completion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\r\n\r\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface.\r\n\r\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\r\n\r\nWhat I Tried:\r\nDowngrading the Python version from 3.12 to 3.10.\r\nUsing different versions of the OpenAI library (0.27.0, 0.28.0, and 0.29.0).\r\nSetting API keys via environment variables and directly in the code.\r\nTrying the migration tool (openai migrate) but to no avail.\r\nEnsuring all dependencies (like google-api-python-client and pdfplumber) are up to date and properly installed.\n```\n\n\n### OS\n\nmacOS Monterey\n\n### Python version\n\nPython 3.10.15 (also tried Python 3.12.5)\n\n### Library version\n\nopenai v1.0.1 (Issue also persists with version 0.27.0 and 0.28.0)",
    "state": "closed",
    "created_at": "2024-10-10T14:08:18+00:00",
    "closed_at": "2024-10-27T15:48:08+00:00",
    "updated_at": "2024-10-27T15:48:09+00:00",
    "author": "HosungYou",
    "author_type": "User",
    "comments_count": 1,
    "reactions_count": 0,
    "labels": "",
    "milestone": null,
    "closed_by": "RobertCraigie",
    "resolution_time_hours": 409.6638888888889,
    "first_comments": [
      {
        "author": "RobertCraigie",
        "created_at": "2024-10-27T15:48:09+00:00",
        "body": "the code needs to look like this\r\n```py\r\nimport openai\r\n\r\nclient = openai.OpenAI()\r\nresponse = client.completions.create(\r\n    engine=\"davinci\",\r\n    prompt=\"Tell me a joke.\",\r\n    max_tokens=50\r\n)\r\nprint(response.choices[0].text)\r\n```"
      }
    ],
    "url": "https://github.com/openai/openai-python/issues/1788"
  },
  {
    "repository": "openai/openai-python",
    "issue_number": 1784,
    "title": "Latest version fails to install on Python 3.7",
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\n\n- [X] This is an issue with the Python library\n\n### Describe the bug\n\nDue to dependency conflicts, the latest versions of the library fail to install on Python 3.7.\n\n### To Reproduce\n\n1. Using Python 3.7, attempt to install the `openai` package:\r\n\r\n``` shell\r\n$ py -3.7-32 -m pip install openai\r\n```\r\n\r\nThis fails due to `jiter` and `typing_extensions`.\r\n\r\nPlease either drop support for 3.7 officially (3.8 works) or loosen the dependency constraints.\n\n### Code snippets\n\n_No response_\n\n### OS\n\nWindows 11 26100.1882\n\n### Python version\n\n3.7.9\n\n### Library version\n\n1.51.x",
    "state": "closed",
    "created_at": "2024-10-09T06:14:59+00:00",
    "closed_at": "2024-11-05T02:40:12+00:00",
    "updated_at": "2024-11-05T02:40:12+00:00",
    "author": "codeofdusk",
    "author_type": "User",
    "comments_count": 3,
    "reactions_count": 0,
    "labels": "bug",
    "milestone": null,
    "closed_by": "codeofdusk",
    "resolution_time_hours": 644.4202777777778,
    "first_comments": [
      {
        "author": "codeofdusk",
        "created_at": "2024-10-09T07:27:43+00:00",
        "body": "It appears that the last working version is 1.39.0."
      },
      {
        "author": "RobertCraigie",
        "created_at": "2024-10-09T12:51:17+00:00",
        "body": "Thanks for the report, I'm investigating."
      },
      {
        "author": "codeofdusk",
        "created_at": "2024-11-05T02:40:12+00:00",
        "body": "Closed in #1845."
      }
    ],
    "url": "https://github.com/openai/openai-python/issues/1784"
  },
  {
    "repository": "openai/openai-python",
    "issue_number": 1777,
    "title": "`openai.AsyncOpenAI` not safe when shared across `async` tests",
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\n\n- [X] This is an issue with the Python library\n\n### Describe the bug\n\nWhen sharing `AsyncOpenAI` across `async` tests, it's possible to hit `openai.APIConnectionError: Connection error.`\n\n### To Reproduce\n\nhttps://colab.research.google.com/drive/1icWDkN2iYPl3mxCEwA0-kE7GSSOeM9E8?usp=sharing\n\n### Code snippets\n\n```Python\nimport pytest\r\nfrom openai import AsyncOpenAI\r\n\r\nclient = AsyncOpenAI()\r\n\r\n\r\n@pytest.mark.asyncio\r\nasync def test_completion1() -> None:\r\n    await client.chat.completions.create(\r\n        model=\"gpt-4o-mini\",\r\n        messages=[{\r\n            \"content\": (\r\n                \"Here is a question, the correct answer to the question, and a proposed\"\r\n                \" answer to the question. Please tell me if the proposed answer is\"\r\n                \" correct, given the correct answer. ONLY SAY 'YES' OR 'NO'. No other\"\r\n                \" output is permitted.\\n\\nQuestion: What is 25 * 10? \\n\\nCorrect\"\r\n                \" answer: 250 \\n\\nProposed answer: 250\"\r\n            ),\r\n            \"role\": \"user\",\r\n        }],\r\n    )\r\n\r\n\r\n@pytest.mark.asyncio\r\nasync def test_completion2() -> None:\r\n    await client.chat.completions.create(\r\n        model=\"gpt-4o-mini\",\r\n        messages=[{\r\n            \"content\": (\r\n                \"Here is a question, the correct answer to the question, and a proposed\"\r\n                \" answer to the question. Please tell me if the proposed answer is\"\r\n                \" correct, given the correct answer. ONLY SAY 'YES' OR 'NO'. No other\"\r\n                \" output is permitted.\\n\\nQuestion: What is 25 * 10? \\n\\nCorrect\"\r\n                \" answer: 250 \\n\\nProposed answer: 250\"\r\n            ),\r\n            \"role\": \"user\",\r\n        }],\r\n    )\n```\n\n\n### OS\n\nlinux\n\n### Python version\n\n3.10.12\n\n### Library version\n\n1.51.0",
    "state": "closed",
    "created_at": "2024-10-06T01:57:11+00:00",
    "closed_at": "2024-10-09T12:18:44+00:00",
    "updated_at": "2024-10-10T04:59:28+00:00",
    "author": "jamesbraza",
    "author_type": "User",
    "comments_count": 9,
    "reactions_count": 0,
    "labels": "",
    "milestone": null,
    "closed_by": "RobertCraigie",
    "resolution_time_hours": 82.35916666666667,
    "first_comments": [
      {
        "author": "RobertCraigie",
        "created_at": "2024-10-06T02:00:10+00:00",
        "body": "Thanks for the report, what version of `httpx` / `httpcore` are you using? (I can't open the colab link right now)"
      },
      {
        "author": "jamesbraza",
        "created_at": "2024-10-06T02:06:40+00:00",
        "body": "Hello @RobertCraigie thanks for the quick response, it's `httpx=0.27.2` and `httpcore==1.0.6`"
      },
      {
        "author": "rotem925",
        "created_at": "2024-10-08T11:52:47+00:00",
        "body": "I see the same issue.\r\nhttpx==0.27.2\r\nhttpcore==1.0.6\r\nIn my case I use pytest with async and ainvoke on ChatOpenAI\r\nBUT, if I use ChatBedrock, everything is working.\r\n\r\nSince I was breaking my head against this, one hint is that I've tried to trace it\r\nand I printed the \"real\" error which is not a connection error, but \"Event loop is closed\"\r\n\r\nThis is the line it fails, if you print the err object, you would see it.\r\nhttps://github.com/openai/openai-python/blob/main/src/openai/_base_client.py#L1584\r\n`raise APIConnectionError(request=request,message=str(err)) from err`\r\n\r\nHope that will give us some more insights..\r\n\r\n"
      },
      {
        "author": "anointingmayami",
        "created_at": "2024-10-08T12:02:12+00:00",
        "body": "update your OpenAI to the latest."
      },
      {
        "author": "anointingmayami",
        "created_at": "2024-10-08T13:39:36+00:00",
        "body": "Thanks for reaching out. An update has been sent to fix the issue. We will\r\nlet you know once the issue is resolved.\r\n\r\nGod bless you. Grace be unto you and Peace be multiplied.\r\n\r\nAnointing J. Mayami\r\n+2348140070908\r\n***@***.***\r\n\r\n\r\nOn Tue, Oct 8, 2024 at 12:53 PM Rotem ***@***.***> wrote:\r\n\r\n> I see the same issue.\r\n> httpx==0.27.2\r\n> httpcore==1.0.6\r\n> In my case I use pytest with async and ainvoke on ChatOpenAI\r\n> BUT, if I use ChatBedrock, everything is working.\r\n>\r\n> Since I was breaking my head against this, one hint is that I've tried to\r\n> trace it\r\n> and I printed the \"real\" error which is not a connection error, but \"Event\r\n> loop is closed\"\r\n>\r\n> This is the line it fails, if you print the err object, you would see it.\r\n>\r\n> https://github.com/openai/openai-python/blob/main/src/openai/_base_client.py#L1584\r\n> raise APIConnectionError(request=request,message=str(err)) from err\r\n>\r\n> Hope that will give us some more insights..\r\n>\r\n> —\r\n> Reply to this email directly, view it on GitHub\r\n> <https://github.com/openai/openai-python/issues/1777#issuecomment-2399633471>,\r\n> or unsubscribe\r\n> <https://github.com/notifications/unsubscribe-auth/BK7UCY3CEGU5JCVXCI5YMLLZ2PBSRAVCNFSM6AAAAABPN3LTC2VHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZDGOJZGYZTGNBXGE>\r\n> .\r\n> You are receiving this because you are subscribed to this thread.Message\r\n> ID: ***@***.***>\r\n>\r\n"
      }
    ],
    "url": "https://github.com/openai/openai-python/issues/1777"
  },
  {
    "repository": "openai/openai-python",
    "issue_number": 1776,
    "title": "with multiprocessing, pickle issue with  client.beta.chat.completions.parse output",
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\r\n\r\n- [X] This is an issue with the Python library\r\n\r\n### Describe the bug\r\n\r\nWe use multiprocessing to handle the calls. It was working with client.chat.completions.create. Currently, we are trying client.beta.chat.completions.parse with response_format, i got error:\r\n\r\nTraceback (most recent call last):\r\n  File \"/anaconda/lib/python3.10/multiprocessing/queues.py\", line 244, in _feed\r\n    obj = _ForkingPickler.dumps(obj)\r\n  File \"/anaconda/lib/python3.10/multiprocessing/reduction.py\", line 51, in dumps\r\n    cls(buf, protocol).dump(obj)\r\n_pickle.PicklingError: Can't pickle <class 'openai.types.chat.parsed_chat_completion.ParsedChatCompletion[CalendarEvent]'>: attribute lookup ParsedChatCompletion[CalendarEvent] on openai.types.chat.parsed_chat_completion failed\r\n\r\nanyone can help ?\r\n\r\nThanks\r\n\r\n### To Reproduce\r\n```\r\nfrom pydantic import BaseModel\r\nfrom . import get_token_provider\r\nimport openai\r\nimport asyncio\r\nimport pickle\r\n\r\nclass CalendarEvent(BaseModel):\r\n    name: str\r\n    date: str\r\n    participants: list[str]\r\n\r\ndef original_test():\r\n\r\n    MODEL = \"gpt-4o-2024-08-06\"\r\n    ENDPOINT = \"gpt-4o-2024-08-06-endpoint\"\r\n    token_provider = get_token_provider()\r\n\r\n    client = openai.AsyncAzureOpenAI(\r\n    azure_ad_token_provider=token_provider,\r\n    azure_endpoint=ENDPOINT,\r\n    api_version=\"2024-08-01-preview\",\r\n    )\r\n\r\n    func = client.beta.chat.completions.parse(\r\n        model=MODEL,\r\n        messages=[\r\n            {\"role\": \"system\", \"content\": \"Extract the event information.\"},\r\n            {\"role\": \"user\", \"content\": \"Alice and Bob are going to a science fair on Friday.\"},\r\n        ],\r\n        response_format=CalendarEvent,\r\n        )\r\n\r\n    completion = asyncio.run(func)\r\n    pickle.dumps(completion)\r\n    \r\n\r\nif __name__ == \"__main__\":\r\n    original_test()\r\n```\r\n### Code snippets\r\n\r\n_No response_\r\n\r\n### OS\r\n\r\nlinux\r\n\r\n### Python version\r\n\r\npy3.10\r\n\r\n### Library version\r\n\r\nopenai 1.51.0",
    "state": "closed",
    "created_at": "2024-10-05T15:16:49+00:00",
    "closed_at": "2024-10-18T13:48:06+00:00",
    "updated_at": "2024-10-18T13:48:07+00:00",
    "author": "yufang67",
    "author_type": "User",
    "comments_count": 1,
    "reactions_count": 0,
    "labels": "bug",
    "milestone": null,
    "closed_by": "yufang67",
    "resolution_time_hours": 310.5213888888889,
    "first_comments": [
      {
        "author": "yufang67",
        "created_at": "2024-10-18T13:48:06+00:00",
        "body": "Based on Function calling with structured outputs in this page https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/structured-outputs?tabs=python-secure\r\nI use client.chat.completions.create instead of client.beta.chat.completions.parse\r\nIts resolved.\r\n"
      }
    ],
    "url": "https://github.com/openai/openai-python/issues/1776"
  },
  {
    "repository": "openai/openai-python",
    "issue_number": 1775,
    "title": "I couldn't upload file an use the one at thread normally via openai library... ",
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\n\n- [X] This is an issue with the Python library\n\n### Describe the bug\n\nI uploaded file to vector storage, but I couldn't use the file at threads:\r\n`It seems there was an error while trying to search the uploaded files. Could you please try uploading the file again, or let me know if there is a specific file you want me to look into?`.\n\n### To Reproduce\n\n1. create vector_store\r\n2. create assistant\r\n3. upload file\r\n4. wait the uploading\r\n5. attach file to vector store\r\n6. create thread\r\n7. create run\r\n8. wait completing the run\r\n9. get messages \r\n10. take run's message\n\n### Code snippets\n\n```Python\nimport json\r\nimport os\r\nimport time\r\n\r\nfrom openai import OpenAI\r\n\r\nclient = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\r\n\r\nvector_store = client.beta.vector_stores.create(name=\"Test\")\r\n\r\nassistant = client.beta.assistants.create(\r\n    description=f\"Test\",\r\n    model=\"gpt-4o\",\r\n    tools=[{\"type\": \"file_search\"}],\r\n    tool_resources={\"file_search\": {\"vector_store_ids\": [vector_store.id]}},\r\n    temperature=0.4\r\n)\r\n\r\n# Upload file\r\nfile = client.files.create(file=(\"data.json\", json.dumps({\"name\": \"Alexbabaliks\"}).encode()), purpose=\"assistants\")\r\nwhile True:\r\n    file_status = client.files.retrieve(file_id=file.id)\r\n    if file_status.status == 'processed':\r\n        break\r\n    time.sleep(1)\r\n\r\nclient.beta.vector_stores.files.create(vector_store_id=vector_store.id, file_id=file.id)\r\nwhile True:\r\n    vector_store = client.beta.vector_stores.retrieve(vector_store_id=vector_store.id)\r\n    if vector_store.status == 'completed':\r\n        break\r\n    time.sleep(1)\r\n\r\nthread = client.beta.threads.create(tool_resources={\"file_search\": {\"vector_store_ids\": [vector_store.id]}})\r\n\r\nrun = client.beta.threads.runs.create(\r\n    instructions=\"What is my name??? Take it from JSON file and return JSON in format {'name': '<name>'}\",\r\n    thread_id=thread.id,\r\n    assistant_id=assistant.id,\r\n    model=\"gpt-4o\",\r\n    temperature=0.4,\r\n    tools=[{\"type\": \"file_search\"}],\r\n)\r\n\r\nwhile True:\r\n    run_status = client.beta.threads.runs.retrieve(thread_id=thread.id, run_id=run.id)\r\n    if run_status.status == \"completed\":\r\n        break\r\n    elif run_status.status == \"failed\":\r\n        break\r\n\r\n    time.sleep(2)\r\n\r\nanswer = \"\"\r\nmessages = client.beta.threads.messages.list(thread_id=thread.id)\r\nfor message in messages.data:\r\n    if run.id != message.run_id:\r\n        continue\r\n\r\n    for content in message.content:\r\n        if content.type == \"text\":\r\n            answer = content.text.value\r\n            break\r\n\r\nprint(answer)\n```\n\n\n### OS\n\nLinux\n\n### Python version\n\n3.11.1\n\n### Library version\n\nopenai v1.51.0",
    "state": "closed",
    "created_at": "2024-10-04T15:18:21+00:00",
    "closed_at": "2025-01-22T20:47:01+00:00",
    "updated_at": "2025-01-22T20:47:05+00:00",
    "author": "alex-deus",
    "author_type": "User",
    "comments_count": 2,
    "reactions_count": 0,
    "labels": "API-feedback",
    "milestone": null,
    "closed_by": "RobertCraigie",
    "resolution_time_hours": 2645.4777777777776,
    "first_comments": [
      {
        "author": "alex-deus",
        "created_at": "2024-10-05T07:58:51+00:00",
        "body": "I came up with a solution, but it doesn’t work well for large JSON files (around 5MB), and sometimes for smaller simple file, it returns: `{\"n\": \"I couldn't find your given name in the provided documents.\"}` :-D\r\n\r\n```python\r\nimport json\r\nimport os\r\nimport time\r\n\r\nfrom openai import OpenAI\r\n\r\nTEMPERATURE = 0.2\r\n\r\n\r\ndef ask(client: OpenAI, assistant_id: str, thread_id: str, file_id: str, instruction: str) -> str:\r\n    client.beta.threads.messages.create(\r\n        thread_id=thread_id,\r\n        content=instruction,\r\n        role=\"user\",\r\n        attachments=[{\"file_id\": file_id, \"tools\": [{\"type\": \"file_search\"}]}],\r\n    )\r\n\r\n    run = client.beta.threads.runs.create(\r\n        thread_id=thread_id,\r\n        model=\"gpt-4o\",\r\n        tools=[{\"type\": \"file_search\"}],\r\n        assistant_id=assistant_id,\r\n        temperature=TEMPERATURE\r\n    )\r\n\r\n    count = 0\r\n    while True:\r\n        run = client.beta.threads.runs.retrieve(thread_id=thread_id, run_id=run.id)\r\n        if run.status == \"completed\":\r\n            break\r\n        elif run.status == \"failed\":\r\n            raise Exception(f\"{run.last_error.code}: {run.last_error.message}\")\r\n\r\n        count += 1\r\n        if count > 30:\r\n            raise Exception(\"Too many requests\")\r\n        else:\r\n            time.sleep(4)\r\n\r\n    answer = \"\"\r\n    messages = client.beta.threads.messages.list(thread_id=thread_id)\r\n    for message in messages.data:\r\n        if run.id != message.run_id:\r\n            continue\r\n\r\n        for content in message.content:\r\n            if content.type == \"text\":\r\n                answer = content.text.value\r\n                break\r\n\r\n    return answer\r\n\r\n\r\ndef main() -> None:\r\n    client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))\r\n    \r\n    # Create assistant\r\n    assistant = client.beta.assistants.create(\r\n        description=\"Test\",\r\n        instructions=\"Return all answer in JSON format\",\r\n        model=\"gpt-4o\",\r\n        tools=[{\"type\": \"file_search\"}],\r\n        temperature=TEMPERATURE\r\n    )\r\n    \r\n    # Upload file\r\n    data = (\"data.json\", json.dumps({\"given_name\": \"John\", \"family_name\": \"Smit\"}).encode())\r\n    file = client.files.create(file=data, purpose=\"assistants\")\r\n    \r\n    thread = client.beta.threads.create()  # Create thread\r\n    \r\n    instruction = \"What is my given_name? Answer format {'n': '<name>'}\"\r\n    answer = ask(client, assistant.id, thread.id, file.id, instruction)\r\n    print(answer)  # Sometimes answer could be {\"n\": \"I couldn't find your given name in the provided documents.\"}\r\n    \r\n    instruction = \"What is my last family_name? Answer format {'l': '<last name>'}\"\r\n    answer = ask(client, assistant.id, thread.id, file.id, instruction)\r\n    print(answer)\r\n\r\n\r\nif __name__ == '__main__':\r\n    main()\r\n```"
      },
      {
        "author": "RobertCraigie",
        "created_at": "2025-01-22T20:47:01+00:00",
        "body": "Really sorry for the delayed response. \n\nThis sounds like an issue with the underlying OpenAI API and not the SDK, so I'm going to go ahead and close this issue. \n\nWould you mind reposting at [community.openai.com](https://community.openai.com) if you're still running into this?"
      }
    ],
    "url": "https://github.com/openai/openai-python/issues/1775"
  },
  {
    "repository": "openai/openai-python",
    "issue_number": 1770,
    "title": "No Healthy Upstream",
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\n\n- [X] This is an issue with the Python library\n\n### Describe the bug\n\nFacing the following issue:\r\n![image](https://github.com/user-attachments/assets/841e2d52-589f-4b2e-8f82-3fb1815d55c6)\r\n\n\n### To Reproduce\n\nAzure OpenAI Studio\n\n### Code snippets\n\n```Python\nimport os\r\nimport requests\r\nimport base64\r\n\r\n# Configuration\r\nAPI_KEY = \"YOUR_API_KEY\"\r\nIMAGE_PATH = \"YOUR_IMAGE_PATH\"\r\nencoded_image = base64.b64encode(open(IMAGE_PATH, 'rb').read()).decode('ascii')\r\nheaders = {\r\n    \"Content-Type\": \"application/json\",\r\n    \"api-key\": API_KEY,\r\n}\r\n\r\n# Payload for the request\r\npayload = {\r\n  \"messages\": [\r\n    {\r\n      \"role\": \"system\",\r\n      \"content\": [\r\n        {\r\n          \"type\": \"text\",\r\n          \"text\": \"You are an AI assistant that helps people find information.\"\r\n        }\r\n      ]\r\n    },\r\n    {\r\n      \"role\": \"user\",\r\n      \"content\": [\r\n        {\r\n          \"type\": \"text\",\r\n          \"text\": \"Hi\"\r\n        }\r\n      ]\r\n    },\r\n    {\r\n      \"role\": \"user\",\r\n      \"content\": [\r\n        {\r\n          \"type\": \"text\",\r\n          \"text\": \"Pretty strange\"\r\n        }\r\n      ]\r\n    }\r\n  ],\r\n  \"temperature\": 0.7,\r\n  \"top_p\": 0.95,\r\n  \"max_tokens\": 800\r\n}\r\n\r\nENDPOINT = \"https://abc-openai-gpt4o.openai.azure.com/openai/deployments/gpt4o/chat/completions?api-version=2024-02-15-preview\"\r\n\r\n# Send request\r\ntry:\r\n    response = requests.post(ENDPOINT, headers=headers, json=payload)\r\n    response.raise_for_status()  # Will raise an HTTPError if the HTTP request returned an unsuccessful status code\r\nexcept requests.RequestException as e:\r\n    raise SystemExit(f\"Failed to make the request. Error: {e}\")\r\n\r\n# Handle the response as needed (e.g., print or process)\r\nprint(response.json())\n```\n\n\n### OS\n\nLinux\n\n### Python version\n\nPython v3.11\n\n### Library version\n\nopenai v1.42.0",
    "state": "closed",
    "created_at": "2024-10-02T00:51:30+00:00",
    "closed_at": "2024-10-02T20:49:09+00:00",
    "updated_at": "2024-10-02T20:49:09+00:00",
    "author": "aziz-ullah-khan",
    "author_type": "User",
    "comments_count": 1,
    "reactions_count": 0,
    "labels": "",
    "milestone": null,
    "closed_by": "RobertCraigie",
    "resolution_time_hours": 19.960833333333333,
    "first_comments": [
      {
        "author": "RobertCraigie",
        "created_at": "2024-10-02T20:49:09+00:00",
        "body": "Thanks for reporting!  \r\n\r\nThis sounds like an issue with the underlying OpenAI API and not the SDK, so I'm going to go ahead and close this issue. \r\n\r\nWould you mind reposting at [community.openai.com](https://community.openai.com)?"
      }
    ],
    "url": "https://github.com/openai/openai-python/issues/1770"
  },
  {
    "repository": "openai/openai-python",
    "issue_number": 1765,
    "title": "the latest openai version has problem with httpx library when compile to app.",
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\r\n\r\n- [X] This is an issue with the Python library\r\n\r\n### Describe the bug\r\n\r\nI found this error when update the openai to new version.\r\n\r\n  Referenced from: <198A9232-F82E-3E0A-92CB-D131D45DBB4F> /xx/_ssl.cpython-312-darwin.so\r\n  Expected in:     <284AF566-569A-311D-8493-DC6928BBD628> /xx/libcrypto.3.dylib\r\nFailed to execute script 'main' due to unhandled exception: dlopen(/xxx/Frameworks/lib-dynload/_ssl.cpython-312-darwin.so, 0x0002): **Symbol not found: _X509_STORE_get1_objects**\r\n\r\nFile \"openai/__init__.py\", line 8, in <module>\r\n  File \"openai/types/__init__.py\", line 5, in <module>\r\n  File \"openai/types/batch.py\", line 7, in <module>\r\n  File \"openai/_models.py\", line 26, in <module>\r\n  File \"openai/_types.py\", line 21, in <module>\r\n  File \"httpx/__init__.py\", line 2, in <module>\r\n\r\n\r\n### To Reproduce\r\n\r\n1. write a sample code and import the latest openai.\r\n2. compile file with pyinstaller on macOS.\r\n3. run the execute file and found this issue.\r\n\r\n### Code snippets\r\n\r\n```Python\r\nthe problem looks lead to httpx library which calling by openai.\r\nNot sure if it need the specific version or latest version of the library?\r\n```\r\n\r\n\r\n### OS\r\n\r\nmacOS\r\n\r\n### Python version\r\n\r\nPython 3.12\r\n\r\n### Library version\r\n\r\nopenai 1.50.2",
    "state": "closed",
    "created_at": "2024-10-01T08:14:07+00:00",
    "closed_at": "2024-11-05T07:38:58+00:00",
    "updated_at": "2024-12-06T09:55:04+00:00",
    "author": "Pjumpod",
    "author_type": "User",
    "comments_count": 5,
    "reactions_count": 0,
    "labels": "bug",
    "milestone": null,
    "closed_by": "Pjumpod",
    "resolution_time_hours": 839.4141666666667,
    "first_comments": [
      {
        "author": "RobertCraigie",
        "created_at": "2024-10-01T13:22:33+00:00",
        "body": "Thanks for the report @Pjumpod, what version was last working for you?"
      },
      {
        "author": "Pjumpod",
        "created_at": "2024-10-01T16:57:32+00:00",
        "body": "> Thanks for the report @Pjumpod, what version was last working for you?\r\n\r\nThank you for asking.\r\nMy latest work version is \r\nopenai 1.47.0 and cryptography 43.0.0 but I cannot recall the httx version.\r\n\r\nps: it has no issue if run by python."
      },
      {
        "author": "Pjumpod",
        "created_at": "2024-11-05T07:38:58+00:00",
        "body": "I do not open this issue too long, as it is not the own code of open-ai"
      },
      {
        "author": "gdagitrep",
        "created_at": "2024-12-06T05:25:54+00:00",
        "body": "@Pjumpod Did you find a solution to this problem, while using python3.12? "
      },
      {
        "author": "Pjumpod",
        "created_at": "2024-12-06T09:55:03+00:00",
        "body": "> @Pjumpod Did you find a solution to this problem, while using python3.12?\r\n\r\nyou can use the latest version of openai. it was fixed."
      }
    ],
    "url": "https://github.com/openai/openai-python/issues/1765"
  },
  {
    "repository": "openai/openai-python",
    "issue_number": 1760,
    "title": "ds-store",
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\n\n- [X] This is an issue with the Python library\n\n### Describe the bug\n\n.DS_Store file in project root should not be there.\r\n\r\nAdd to .gitignore.\n\n### To Reproduce\n\nn/a\n\n### Code snippets\n\n_No response_\n\n### OS\n\nmacOS\n\n### Python version\n\nPython v.12\n\n### Library version\n\nv1.50.2",
    "state": "closed",
    "created_at": "2024-09-29T03:09:56+00:00",
    "closed_at": "2024-09-30T14:20:10+00:00",
    "updated_at": "2024-09-30T14:20:11+00:00",
    "author": "tayeva",
    "author_type": "User",
    "comments_count": 1,
    "reactions_count": 0,
    "labels": "bug",
    "milestone": null,
    "closed_by": "RobertCraigie",
    "resolution_time_hours": 35.17055555555555,
    "first_comments": [
      {
        "author": "RobertCraigie",
        "created_at": "2024-09-30T14:20:10+00:00",
        "body": "Thanks, sorry that slipped in."
      }
    ],
    "url": "https://github.com/openai/openai-python/issues/1760"
  },
  {
    "repository": "openai/openai-python",
    "issue_number": 1759,
    "title": "Get info about model",
    "body": "### Confirm this is a feature request for the Python library and not the underlying OpenAI API.\n\n- [X] This is a feature request for the Python library\n\n### Describe the feature or improvement you're requesting\n\nI can list models, like this:\r\n\r\n```python\r\nimport openai\r\n\r\nfor model in openai.models.list():\r\n    print(model.id)\r\n\r\n```\r\n\r\nbut i cant get info about model type. I dont get info about data types, that models acepts.\n\n### Additional context\n\n_No response_",
    "state": "closed",
    "created_at": "2024-09-28T19:31:54+00:00",
    "closed_at": "2024-09-30T21:25:04+00:00",
    "updated_at": "2024-09-30T21:25:04+00:00",
    "author": "palandovalex",
    "author_type": "User",
    "comments_count": 1,
    "reactions_count": 0,
    "labels": "",
    "milestone": null,
    "closed_by": "RobertCraigie",
    "resolution_time_hours": 49.88611111111111,
    "first_comments": [
      {
        "author": "RobertCraigie",
        "created_at": "2024-09-30T21:25:04+00:00",
        "body": "Thanks for reporting!  \r\n\r\nThis sounds like an issue with the underlying OpenAI API and not the SDK, so I'm going to go ahead and close this issue. \r\n\r\nWould you mind reposting at [community.openai.com](https://community.openai.com)?"
      }
    ],
    "url": "https://github.com/openai/openai-python/issues/1759"
  },
  {
    "repository": "openai/openai-python",
    "issue_number": 1739,
    "title": " AttributeError: module 'openai' has no attribute 'Completions' when using the latest version and APIRemovedInV1",
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\n\n- [X] This is an issue with the Python library\n\n### Describe the bug\n\nI'm encountering an AttributeError when trying to use the openai.Completions attribute in my Python code. I'm using the latest version of the OpenAI Python library (version 1.47.1).\r\nHere is the brief description of code:\r\nimport openai\r\ninput = \"I love to travel\"\r\nresponse = sentiment_analysis(input)\r\nprint(input, \"The sentiment is\" , response)\r\nWhen I run this code, I get the following error:\r\nAPIRemovedInV1: \r\n\r\nYou tried to access openai.Completion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\r\n\r\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \r\n\r\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\r\n\r\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\r\n\r\n\n\n### To Reproduce\n\nThe code should successfully create a completion using the openai.Completions.create function.\r\nPlease let me know if there are any known issues or changes related to the openai.Completions attribute in the latest version of the library.\r\nI've verified that I'm using the correct API key and that my internet connection is stable. I've also tried reinstalling the OpenAI library to ensure I'm using the latest version.\r\nAny suggestions or workarounds would be greatly appreciated.\n\n### Code snippets\n\n```Python\nimport openai\r\ndef sentiment_analysis(text):\r\n  messages = [{\"Role\":\"system\", \"content\":\"\"\"You are trained to analyze an detect the sentiment of the given text.\r\n                                             if you are unsure of answer you can say\"Not sure\" and recommend users to review manually.\"\"\"},\r\n              {\"Role\":\"user\", \"content\":\"\"\"Analyze the following text and determine if the sentiment is: positive or negative.\r\n                                           return answer in single word as either positive or negative: {text}\"\"\"}]\r\n  response = openai.Completion.create(\r\n      engine=\"text-davinci-003\", \r\n      messages=messages,\r\n      max_tokens=1,\r\n      n=1,\r\n      stop=None,\r\n      temperature=0)\r\n  sentiment = response.choices[0].message.content.strip().lower()\r\n  return sentiment\r\ninput = \"I love to travel\"\r\nresponse = sentiment_analysis(input)\r\nprint(input, \"The sentiment is\" , response)\n```\n\n\n### OS\n\nwindows\n\n### Python version\n\n3.12.6\n\n### Library version\n\n1.47.1",
    "state": "closed",
    "created_at": "2024-09-24T10:26:22+00:00",
    "closed_at": "2024-09-24T10:28:13+00:00",
    "updated_at": "2024-09-24T11:11:22+00:00",
    "author": "N-eng66",
    "author_type": "User",
    "comments_count": 3,
    "reactions_count": 0,
    "labels": "question",
    "milestone": null,
    "closed_by": "RobertCraigie",
    "resolution_time_hours": 0.030833333333333334,
    "first_comments": [
      {
        "author": "N-eng66",
        "created_at": "2024-09-24T10:27:13+00:00",
        "body": "Kindly help me out with this issue."
      },
      {
        "author": "RobertCraigie",
        "created_at": "2024-09-24T10:28:13+00:00",
        "body": "You're using the old pre-v1 syntax, the migration guide is here: https://github.com/openai/openai-python/discussions/742"
      },
      {
        "author": "N-eng66",
        "created_at": "2024-09-24T11:11:20+00:00",
        "body": "@RobertCraigie I am following the migration guide but still there are errors."
      }
    ],
    "url": "https://github.com/openai/openai-python/issues/1739"
  },
  {
    "repository": "openai/openai-python",
    "issue_number": 1738,
    "title": "examples/embeddings/Visualize_in_3d.ipynb is empty",
    "body": "### Confirm this is a feature request for the Python library and not the underlying OpenAI API.\n\n- [X] This is a feature request for the Python library\n\n### Describe the feature or improvement you're requesting\n\nActually all the examples of embeddings are empty, could anybody share some example codes?\n\n### Additional context\n\n_No response_",
    "state": "closed",
    "created_at": "2024-09-24T03:18:08+00:00",
    "closed_at": "2024-10-03T14:14:28+00:00",
    "updated_at": "2024-10-03T14:14:28+00:00",
    "author": "VoiceBeer",
    "author_type": "User",
    "comments_count": 3,
    "reactions_count": 0,
    "labels": "documentation",
    "milestone": null,
    "closed_by": "RobertCraigie",
    "resolution_time_hours": 226.9388888888889,
    "first_comments": [
      {
        "author": "RobertCraigie",
        "created_at": "2024-09-24T08:26:32+00:00",
        "body": "@VoiceBeer there is no `examples/embeddings` directory in this repo, where are you seeing these example notebooks?"
      },
      {
        "author": "VoiceBeer",
        "created_at": "2024-09-25T03:21:14+00:00",
        "body": "Hi @RobertCraigie thx for the reply. I entered the repo via the “`the code for how to visualize embedding space in 3D dimension is available here`” link in the `text similarity models` section of this URL: https://openai.com/index/introducing-text-and-code-embeddings"
      },
      {
        "author": "RobertCraigie",
        "created_at": "2024-10-03T14:14:28+00:00",
        "body": "Thanks for the report, I've shared this with the OpenAI team so I'm going to close this for now."
      }
    ],
    "url": "https://github.com/openai/openai-python/issues/1738"
  },
  {
    "repository": "openai/openai-python",
    "issue_number": 1736,
    "title": "Even for Pydantic V1, getting the error \"warnings is only supported in Pydantic v2\" | Error while use streaming in Assistant",
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\r\n\r\n- [X] This is an issue with the Python library\r\n\r\n### Describe the bug\r\n\r\nThere seems to be a issue in this PR. I'm using pydantic V1 in my code and I'm getting the error \"warnings is only supported in Pydantic v2\", which is defined in`_models.py`.\r\n\r\nNote: We are not planning to change the pydantic version to V2 as it will be a breaking change.\r\n\r\nPR where the bug started: https://github.com/openai/openai-python/pull/1722/files\r\n\r\nTraceback:\r\nOpenAi error:: Traceback (most recent call last):\r\n  File \"/usr/local/lib/python3.10/site-packages/openai/lib/streaming/_assistants.py\", line 942, in accumulate_event\r\n    block = current_message_snapshot.content[content_delta.index]\r\nIndexError: list index out of range\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/app/app/openai/assistants/gpt/assistant_workflow.py\", line 142, in handle_incoming_message\r\n    response = await gpt_assistant_wrapper.call_assistant_and_stream(\r\n  File \"/app/app/openai/v2/api/assistants/assistant_wrapper.py\", line 212, in call_assistant_and_stream\r\n    await stream.until_done()\r\n  File \"/usr/local/lib/python3.10/site-packages/openai/lib/streaming/_assistants.py\", line 534, in until_done\r\n    await consume_async_iterator(self)\r\n  File \"/usr/local/lib/python3.10/site-packages/openai/_utils/_streams.py\", line 11, in consume_async_iterator\r\n    async for _ in iterator:\r\n  File \"/usr/local/lib/python3.10/site-packages/openai/lib/streaming/_assistants.py\", line 501, in __aiter__\r\n    async for item in self._iterator:\r\n  File \"/usr/local/lib/python3.10/site-packages/openai/lib/streaming/_assistants.py\", line 838, in __stream__\r\n    await self._emit_sse_event(event)\r\n  File \"/usr/local/lib/python3.10/site-packages/openai/lib/streaming/_assistants.py\", line 690, in _emit_sse_event\r\n    self.__current_message_snapshot, new_content = accumulate_event(\r\n  File \"/usr/local/lib/python3.10/site-packages/openai/lib/streaming/_assistants.py\", line 951, in accumulate_event\r\n    value=content_delta.model_dump(exclude_unset=True, warnings=False),\r\n  File \"/usr/local/lib/python3.10/site-packages/openai/_models.py\", line 304, in model_dump\r\n    raise ValueError(\"warnings is only supported in Pydantic v2\")\r\nValueError: warnings is only supported in Pydantic v2\r\n\r\n### To Reproduce\r\n\r\nError is occuring while using openAi Assistant stream.\r\nPydantic version \"^1.10.7\"\r\nError is in function `stream.until_done()`\r\n\r\n\r\n### Code snippets\r\n\r\n```Python\r\nasync with client.beta.threads.runs.stream(\r\n            thread_id=thread_id,\r\n            assistant_id=assistant_id,\r\n            event_handler=main_event_handler,\r\n        ) as stream:\r\n\r\n            await stream.until_done()\r\n\r\nError happens in stream.until_done()\r\n```\r\n\r\n\r\n### OS\r\n\r\nmacOs\r\n\r\n### Python version\r\n\r\nPython v3.10.6\r\n\r\n### Library version\r\n\r\nopenAi v1.47.2",
    "state": "closed",
    "created_at": "2024-09-23T10:41:02+00:00",
    "closed_at": "2024-09-23T10:48:50+00:00",
    "updated_at": "2024-12-04T03:26:59+00:00",
    "author": "Vijaykrishna23",
    "author_type": "User",
    "comments_count": 4,
    "reactions_count": 0,
    "labels": "bug",
    "milestone": null,
    "closed_by": "RobertCraigie",
    "resolution_time_hours": 0.13,
    "first_comments": [
      {
        "author": "RobertCraigie",
        "created_at": "2024-09-23T10:48:46+00:00",
        "body": "Thanks for the report! This will be fixed in the next release :)"
      },
      {
        "author": "Vijaykrishna23",
        "created_at": "2024-10-16T10:34:40+00:00",
        "body": "Hi @RobertCraigie , just wanted to confirm is this fixed in the latest version. Please also mention the version which I can use"
      },
      {
        "author": "RobertCraigie",
        "created_at": "2024-10-21T07:20:23+00:00",
        "body": "Does the latest version not work for you? `1.52.0`?"
      },
      {
        "author": "Vijaykrishna23",
        "created_at": "2024-12-04T03:26:58+00:00",
        "body": "Yes, it works. Thanks!"
      }
    ],
    "url": "https://github.com/openai/openai-python/issues/1736"
  },
  {
    "repository": "openai/openai-python",
    "issue_number": 1732,
    "title": "1.46.1 audio transcription's temperature param default is not the API's default",
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\r\n\r\n- [X] This is an issue with the Python library\r\n\r\n### Describe the bug\r\n\r\nThe API documentation for audio transcriptions says:\r\n```\r\ntemperature number Optional Defaults to 0\r\n\r\nThe sampling temperature, between 0 and 1. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. If set to 0, the model will use [log probability](https://en.wikipedia.org/wiki/Log_probability) to automatically increase the temperature until certain thresholds are hit.\r\n```\r\n\r\nBut the behavior between explicitly setting temperature=0 and not passing the param is different, and produces different outputs\r\n\r\n### To Reproduce\r\n\r\n```py\r\nclient.audio.transcriptions.create(\r\n        model=\"whisper-1\",\r\n        file=track,\r\n        response_format=\"verbose_json\",\r\n    )\r\n```\r\n\r\nvs\r\n\r\n```py\r\nclient.audio.transcriptions.create(\r\n        model=\"whisper-1\",\r\n        file=track,\r\n        response_format=\"verbose_json\",\r\n        temperature=0,\r\n    )\r\n```\r\n\r\n### Code snippets\r\n\r\n_No response_\r\n\r\n### OS\r\n\r\nlinux\r\n\r\n### Python version\r\n\r\n3.12\r\n\r\n### Library version\r\n\r\n1.46.1 ",
    "state": "closed",
    "created_at": "2024-09-19T17:06:01+00:00",
    "closed_at": "2024-09-19T17:22:18+00:00",
    "updated_at": "2024-09-19T17:22:18+00:00",
    "author": "saucoide",
    "author_type": "User",
    "comments_count": 1,
    "reactions_count": 0,
    "labels": "API-feedback",
    "milestone": null,
    "closed_by": "RobertCraigie",
    "resolution_time_hours": 0.2713888888888889,
    "first_comments": [
      {
        "author": "RobertCraigie",
        "created_at": "2024-09-19T17:22:18+00:00",
        "body": "Thanks for reporting!  \r\n\r\nThis sounds like an issue with the underlying OpenAI API and not the SDK, so I'm going to go ahead and close this issue. \r\n\r\nWould you mind reposting at [community.openai.com](https://community.openai.com)?"
      }
    ],
    "url": "https://github.com/openai/openai-python/issues/1732"
  },
  {
    "repository": "openai/openai-python",
    "issue_number": 1725,
    "title": "The concurrency of AsyncOpenAI cannot be fully utilized.",
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\r\n\r\n- [X] This is an issue with the Python library\r\n\r\n### Describe the bug\r\n\r\nI attempted to complete a stability test on the concurrency of AsyncOpenAI. I set the concurrency to 1024 but found that it kept running at a very low average level in a jittery manner, which has been consistent with my production test results.\r\n\r\n![image](https://github.com/user-attachments/assets/e046cfdb-155d-41df-8380-d9c0048079f5)\r\n\r\n\r\n### To Reproduce\r\n\r\nI put my code in three part. client.py server.py and main.py(used to create 100k client total)\r\n\r\n\r\nserver.py\r\n```python\r\nfrom fastapi import FastAPI, Request\r\nfrom pydantic import BaseModel\r\nimport asyncio\r\nimport logging\r\nfrom datetime import datetime\r\nimport threading\r\nimport time\r\nimport csv\r\n\r\napp = FastAPI()\r\n\r\n# track current activate queue count\r\nactive_requests = 0\r\n\r\n# debug file to draw pic\r\noutput_file = 'active_requests_log.csv'\r\n\r\nclass CompletionRequest(BaseModel):\r\n    model: str\r\n    messages: list\r\n    temperature: float\r\n\r\n@app.middleware(\"http\")\r\nasync def track_requests(request: Request, call_next):\r\n    global active_requests\r\n    active_requests += 1  # add count when get request\r\n    logging.info(f\"Active requests: {active_requests}\")\r\n\r\n    response = await call_next(request)\r\n\r\n    active_requests -= 1  # 请求完成后减少计数\r\n    logging.info(f\"Active requests: {active_requests}\")\r\n\r\n    return response\r\n\r\n@app.post(\"/v1/chat/completions\")\r\nasync def completions(request: CompletionRequest):\r\n    await asyncio.sleep(1)  # mock llm generate latency\r\n    return {\r\n        \"choices\": [\r\n            {\"message\": {\"content\": f\"Response to {request.messages[-1]['content']}\"}}\r\n        ]\r\n    }\r\n\r\ndef record_active_requests():\r\n    \"\"\" save log file per second\"\"\"\r\n    global active_requests\r\n    with open(output_file, mode='w', newline='') as file:\r\n        writer = csv.writer(file)\r\n        writer.writerow([\"timestamp\", \"active_requests\"])  # 写表头\r\n        \r\n        while True:\r\n            # 每秒记录一次\r\n            current_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\r\n            writer.writerow([current_time, active_requests])\r\n            file.flush()  # 确保每秒写入数据到文件\r\n            time.sleep(1)\r\n\r\n# 启动一个线程来记录活跃请求数\r\nthreading.Thread(target=record_active_requests, daemon=True).start()\r\n\r\nif __name__ == \"__main__\":\r\n    import uvicorn\r\n    logging.basicConfig(level=logging.INFO)\r\n    uvicorn.run(app, host=\"127.0.0.1\", port=8203)\r\n```\r\n\r\nclient.py\r\n```python\r\nimport asyncio\r\nfrom functools import wraps\r\nimport httpx\r\nimport logging\r\nfrom openai import AsyncOpenAI\r\n\r\n\r\n\r\ndef limit_async_func_call(max_size: int):\r\n    sem = asyncio.Semaphore(max_size)\r\n\r\n    def final_decro(func):\r\n        @wraps(func)\r\n        async def wait_func(*args, **kwargs):\r\n            async with sem:\r\n                try:\r\n                    return await func(*args, **kwargs)\r\n                except Exception as e:\r\n                    logging.error(f\"Exception in {func.__name__}: {e}\")\r\n  \r\n        return wait_func\r\n    return final_decro\r\n\r\n# 假设这个是你要进行并发测试的函数\r\n@limit_async_func_call(max_size=1024)  # 限制并发为1024\r\nasync def custom_model_if_cache(prompt, system_prompt=None, history_messages=[], **kwargs):\r\n    custom_http_client = httpx.AsyncClient(\r\n        limits=httpx.Limits(max_connections=2048, max_keepalive_connections=1024),\r\n        timeout=httpx.Timeout(timeout=None)\r\n    )\r\n\r\n    openai_async_client = AsyncOpenAI(\r\n        api_key=\"EMPTY\", base_url=\"http://localhost:8203/v1\",  # 模拟本地 server\r\n        http_client=custom_http_client\r\n    )\r\n\r\n    messages = []\r\n    if system_prompt:\r\n        messages.append({\"role\": \"system\", \"content\": system_prompt})\r\n    messages.extend(history_messages)\r\n    messages.append({\"role\": \"user\", \"content\": prompt})\r\n\r\n    # 假设这里是要调用的外部 API\r\n    response = await openai_async_client.chat.completions.create(\r\n        model=\"gpt-3.5-turbo\", messages=messages, temperature=0, **kwargs\r\n    )\r\n\r\n    return \"hi\"\r\n\r\n```\r\n\r\n\r\nmain.py\r\n```python\r\nimport asyncio\r\nimport logging\r\nfrom client import custom_model_if_cache\r\n# 模拟 10 万个请求\r\nTOTAL_REQUESTS = 100000\r\n\r\nasync def simulate_requests():\r\n    tasks = []\r\n    for i in range(TOTAL_REQUESTS):\r\n        prompt = f\"Test prompt {i}\"  # 每次请求的不同参数\r\n        task = custom_model_if_cache(prompt=prompt)  # 调用受限的异步函数\r\n        tasks.append(task)\r\n\r\n    # 并发执行所有请求\r\n    results = await asyncio.gather(*tasks, return_exceptions=True)\r\n\r\n    # 打印前10个结果以验证\r\n    for result in results[:10]:\r\n        print(result)\r\n\r\nif __name__ == \"__main__\":\r\n    logging.basicConfig(level=logging.INFO)\r\n    asyncio.run(simulate_requests())\r\n\r\n```\r\n\r\nTo reproduce, open two terminal and run `python server.py` `python main.py` seperately. \r\nI also save the log, you can use following code to draw:\r\n\r\ndraw.py\r\n```python\r\nimport csv\r\nimport matplotlib.pyplot as plt\r\nfrom datetime import datetime\r\n\r\n# 文件路径\r\ninput_file = 'active_requests_log.csv'\r\n\r\n# 读取 CSV 文件并解析时间和活跃请求数量\r\ntimestamps = []\r\nactive_requests = []\r\n\r\nwith open(input_file, mode='r') as file:\r\n    reader = csv.DictReader(file)\r\n    for row in reader:\r\n        timestamps.append(datetime.strptime(row[\"timestamp\"], \"%Y-%m-%d %H:%M:%S\"))\r\n        active_requests.append(int(row[\"active_requests\"]))\r\n\r\n# 绘制图表\r\nplt.figure(figsize=(10, 6))\r\nplt.plot(timestamps, active_requests, label='Active Requests', color='b')\r\n\r\n# 设置图表标题和标签\r\nplt.title('Active Requests Over Time')\r\nplt.xlabel('Time')\r\nplt.ylabel('Active Requests')\r\nplt.xticks(rotation=45)\r\nplt.grid(True)\r\nplt.legend()\r\n\r\n# 显示图表\r\nplt.tight_layout()\r\nplt.savefig(\"/mnt/rangehow/pr/test_c/c.jpg\")\r\n```\r\n\r\n\r\n### Code snippets\r\n\r\n_No response_\r\n\r\n### OS\r\n\r\nubuntu\r\n\r\n### Python version\r\n\r\n3.12\r\n\r\n### Library version\r\n\r\nlatest",
    "state": "closed",
    "created_at": "2024-09-19T09:40:24+00:00",
    "closed_at": "2025-01-22T20:49:22+00:00",
    "updated_at": "2025-01-22T20:49:23+00:00",
    "author": "rangehow",
    "author_type": "User",
    "comments_count": 12,
    "reactions_count": 0,
    "labels": "bug",
    "milestone": null,
    "closed_by": "RobertCraigie",
    "resolution_time_hours": 3011.1494444444443,
    "first_comments": [
      {
        "author": "RobertCraigie",
        "created_at": "2024-09-19T09:45:39+00:00",
        "body": "Thanks for the report, what results do you get if you extract your `custom_http_client` & `openai_async_client` outside of the async function call so they're singletons?"
      },
      {
        "author": "rangehow",
        "created_at": "2024-09-19T10:05:27+00:00",
        "body": "> Thanks for the report, what results do you get if you extract your `custom_http_client` & `openai_async_client` outside of the async function call so they're singletons?\r\n\r\nDo you mean this?\r\nclient.py\r\n```python\r\nimport asyncio\r\nfrom functools import wraps\r\nimport httpx\r\nimport logging\r\nfrom openai import AsyncOpenAI\r\n\r\n\r\n# 限制并发请求的装饰器\r\ndef limit_async_func_call(max_size: int):\r\n    sem = asyncio.Semaphore(max_size)\r\n\r\n    def final_decro(func):\r\n        @wraps(func)\r\n        async def wait_func(*args, **kwargs):\r\n            async with sem:\r\n                try:\r\n                    return await func(*args, **kwargs)\r\n                except Exception as e:\r\n                    logging.error(f\"Exception in {func.__name__}: {e}\")\r\n  \r\n        return wait_func\r\n    return final_decro\r\n\r\n\r\n\r\ncustom_http_client = httpx.AsyncClient(\r\n    limits=httpx.Limits(max_connections=2048, max_keepalive_connections=1024),\r\n    timeout=httpx.Timeout(timeout=None)\r\n)\r\n\r\nopenai_async_client = AsyncOpenAI(\r\n    api_key=\"EMPTY\", base_url=\"http://localhost:8203/v1\",  # 模拟本地 server\r\n    http_client=custom_http_client\r\n)\r\n\r\n\r\n# 假设这个是你要进行并发测试的函数\r\n@limit_async_func_call(max_size=1024)  # 限制并发为1024\r\nasync def custom_model_if_cache(prompt, system_prompt=None, history_messages=[], **kwargs):\r\n    \r\n\r\n    \r\n\r\n    messages = []\r\n    if system_prompt:\r\n        messages.append({\"role\": \"system\", \"content\": system_prompt})\r\n    messages.extend(history_messages)\r\n    messages.append({\"role\": \"user\", \"content\": prompt})\r\n\r\n    # 假设这里是要调用的外部 API\r\n    response = await openai_async_client.chat.completions.create(\r\n        model=\"gpt-3.5-turbo\", messages=messages, temperature=0, **kwargs\r\n    )\r\n\r\n    return \"hi\"\r\n\r\n\r\n\r\n```"
      },
      {
        "author": "RobertCraigie",
        "created_at": "2024-09-19T10:06:15+00:00",
        "body": "yes!"
      },
      {
        "author": "rangehow",
        "created_at": "2024-09-19T10:07:17+00:00",
        "body": "I didn’t complete the entire run, but I think the result should still be the same as last time.\r\n\r\n![image](https://github.com/user-attachments/assets/f8c9fcae-f489-41a5-84de-58110caf6d6b)\r\n\r\n\r\n\r\n"
      },
      {
        "author": "RobertCraigie",
        "created_at": "2024-09-19T10:08:34+00:00",
        "body": "thanks, does this still happen if you just use `httpx` to make the requests instead of the `openai` SDK?"
      }
    ],
    "url": "https://github.com/openai/openai-python/issues/1725"
  },
  {
    "repository": "openai/openai-python",
    "issue_number": 1724,
    "title": "AzureOpenAI chat completion endpoint does not recognize max_completion_tokens",
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\n\n- [X] This is an issue with the Python library\n\n### Describe the bug\n\nWith the o-1 release, `max_tokens` was deprecated in favor of `max_completion_tokens`, and while that argument works with the OpenAI client, it doesn't seem to be working with the AzureOpenAI client. I see the following error:\r\n```\r\nBadRequestError: Error code: 400 - {'error': {'message': 'Unrecognized request argument supplied: max_completion_tokens', 'type': 'invalid_request_error', 'param': None, 'code': None}}\r\n```\n\n### To Reproduce\n\n```Python\r\nimport os\r\nfrom openai import AzureOpenAI\r\n\r\nclient = AzureOpenAI(\r\n  azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\"), \r\n  api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),  \r\n  api_version=\"2024-08-01-preview\"\r\n)\r\n\r\nm = {'role': 'user', 'content': \"Hello!\"}\r\nclient.chat.completions.create(messages=[m], model='gpt-4o', max_completion_tokens=100)\r\n```\n\n### Code snippets\n\n_No response_\n\n### OS\n\nlinux\n\n### Python version\n\n3.11.9\n\n### Library version\n\n1.46.0",
    "state": "closed",
    "created_at": "2024-09-19T04:13:40+00:00",
    "closed_at": "2024-09-19T12:52:15+00:00",
    "updated_at": "2024-09-19T22:08:34+00:00",
    "author": "fladhak",
    "author_type": "User",
    "comments_count": 2,
    "reactions_count": 0,
    "labels": "question,Azure",
    "milestone": null,
    "closed_by": "RobertCraigie",
    "resolution_time_hours": 8.643055555555556,
    "first_comments": [
      {
        "author": "RobertCraigie",
        "created_at": "2024-09-19T12:52:15+00:00",
        "body": "Thanks for reporting! I'm going to close this as it's related to the Azure API and not the SDK itself.\r\n\r\ncc @kristapratico to share any details from the azure side."
      },
      {
        "author": "kristapratico",
        "created_at": "2024-09-19T22:08:33+00:00",
        "body": "@fladhak the Azure API does not support `max_completion_tokens` with chat completions yet. I expect it to be supported very soon. You can follow the [what's new](https://learn.microsoft.com/azure/ai-services/openai/whats-new) page for updates."
      }
    ],
    "url": "https://github.com/openai/openai-python/issues/1724"
  },
  {
    "repository": "openai/openai-python",
    "issue_number": 1721,
    "title": "APIRemovedInV1 Error When Using openai.ChatCompletion.create in Clean Environments (v1.45.0+ and Docker)",
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\n\n- [X] This is an issue with the Python library\n\n### Describe the bug\n\nI am encountering an `APIRemovedInV1` error when using the `openai.ChatCompletion.create()` method in both local and Docker environments, despite using the latest versions of the OpenAI Python library (v1.45.0 and v1.46.0).\r\n\r\nThe error persists even after following the official migration guide and ensuring that the correct API is being used. I’ve tested this in multiple clean environments, including an isolated Docker container, but the issue still occurs.\r\n\r\nThe error message is:\r\n\r\n```\r\nAPIRemovedInV1: You tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0\r\n```\r\n\r\nExpected Behavior:\r\nI expect `openai.ChatCompletion.create()` to work correctly without triggering the `APIRemovedInV1` error, as per the latest API documentation.\r\n\r\n**Environment:**\r\nPython version: 3.10\r\nOpenAI Python library version: 1.45.0 and 1.46.0 (tried both)\r\nOS: Ubuntu (both local and Docker environments)\r\nAdditional Context:\r\nI've purged all old versions of the OpenAI library and even rebuilt environments without caching (including Docker).\r\nThe issue persists even in fresh environments with no previous configurations.\r\nI followed the official migration guide, and the method `openai.ChatCompletion.create()` is being used as described in the documentation.\r\n\n\n### To Reproduce\n\nInstall openai via pip (pip install openai==1.45.0 or pip install openai==1.46.0).\r\nAttempt to use openai.ChatCompletion.create() with the code snippet below:\n\n### Code snippets\n\n```Python\nimport openai\r\nopenai.api_key = 'my api key from .env file'\r\n\r\nresponse = openai.ChatCompletion.create(\r\n    model=\"gpt-3.5-turbo\",\r\n    messages=[\r\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\r\n        {\"role\": \"user\", \"content\": \"Hello, ChatGPT!\"}\r\n    ]\r\n)\r\nprint(response['choices'][0]['message']['content'])\n```\n\n\n### OS\n\nWindows with WSL / Ubuntu\n\n### Python version\n\nPython 3.12.12\n\n### Library version\n\nopenai 1.45.0 & openai 1.46.0",
    "state": "closed",
    "created_at": "2024-09-18T12:23:43+00:00",
    "closed_at": "2024-09-18T14:37:49+00:00",
    "updated_at": "2024-09-18T14:37:54+00:00",
    "author": "rshambaugh",
    "author_type": "User",
    "comments_count": 3,
    "reactions_count": 0,
    "labels": "question",
    "milestone": null,
    "closed_by": "RobertCraigie",
    "resolution_time_hours": 2.235,
    "first_comments": [
      {
        "author": "RobertCraigie",
        "created_at": "2024-09-18T12:26:58+00:00",
        "body": "> I expect openai.ChatCompletion.create() to work correctly without triggering the APIRemovedInV1 error, as per the latest API documentation.\r\n\r\nThis is not supported anymore, what documentation are you referring to? Maybe something is still out of date"
      },
      {
        "author": "rshambaugh",
        "created_at": "2024-09-18T14:34:18+00:00",
        "body": "Thank you for your response. I was following what I believed was the latest documentation referring to openai.ChatCompletion.create() for chat-based completions like GPT-3.5-turbo.\r\n\r\nCan you please point me to the correct, up-to-date method for using the chat models in Python, as well as the proper documentation? I want to make sure I'm following the current best practices."
      },
      {
        "author": "RobertCraigie",
        "created_at": "2024-09-18T14:37:49+00:00",
        "body": "Here's the [readme](https://github.com/openai/openai-python) for this SDK and the [OpenAI API docs](https://platform.openai.com/docs/api-reference/introduction) which should include all the information you need!"
      }
    ],
    "url": "https://github.com/openai/openai-python/issues/1721"
  },
  {
    "repository": "openai/openai-python",
    "issue_number": 1712,
    "title": "Медленно отвечает новая демо версия OpenAI",
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\n\n- [X] This is an issue with the Python library\n\n### Describe the bug\n\n1. Медленно даёт ответ 15-20 сек.\r\n2. Не даёт ответ в 20% приблизительно.\r\n3. Не дописывает текст до конца и зависает в 20%.\r\n4. ChatGPT 4o так же начал зависать и не давать ответ, хотя раньше такое было реже.\r\n5. Я предоставил o1-preview возможность выбора и это было его единственной задачей, выбирать слова, слов было 10 которые я так же указал, он не мог справиться с этой задачей и зависал или вообще не выдавал результат, ему нужно было просто писать слова по одному и ждать любой моей ответ и потом снова писать одно из 10 указанных слов.\r\n6. Не предоставляя информацию о лимите по количеству запросов я исчерпал его на 7 дней, если бы знал то умнее бы распоряжался данной возможностью, тем не менее я надеюсь что полная версия выйдет намного лучше прошлой и демо.\n\n### To Reproduce\n\n1. Написать 10 слов и указать что бы он выбирал в произвольном порядке.\r\n2. Указать что бы писал по одному слову, дожидаясь моего ответа.\r\n3. После ответа повторять процедуру, пока слова не закончатся. \n\n### Code snippets\n\n_No response_\n\n### OS\n\nWindows\n\n### Python version\n\nPython v3.11.4\n\n### Library version\n\nopenai v1.0.1.",
    "state": "closed",
    "created_at": "2024-09-13T23:38:34+00:00",
    "closed_at": "2024-09-16T15:31:08+00:00",
    "updated_at": "2024-09-16T15:31:09+00:00",
    "author": "Phisics4444",
    "author_type": "User",
    "comments_count": 1,
    "reactions_count": 0,
    "labels": "question",
    "milestone": null,
    "closed_by": "RobertCraigie",
    "resolution_time_hours": 63.87611111111111,
    "first_comments": [
      {
        "author": "RobertCraigie",
        "created_at": "2024-09-16T15:31:08+00:00",
        "body": "Thanks for reporting!  \r\n\r\nThis sounds like an issue with the underlying OpenAI API and not the SDK, so I'm going to go ahead and close this issue. \r\n\r\nWould you mind reposting at [community.openai.com](https://community.openai.com)?"
      }
    ],
    "url": "https://github.com/openai/openai-python/issues/1712"
  },
  {
    "repository": "openai/openai-python",
    "issue_number": 1704,
    "title": "Bug caused by incorrect usage of Pydantic and Typing Extensions",
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\n\n- [X] This is an issue with the Python library\n\n### Describe the bug\n\nYou define the `JSONSchema` class in the path `openai.types.shared_params.response_format_json_schema.py`  as follow\r\n```python\r\nclass JSONSchema(TypedDict, total=False):\r\n    name: Required[str]\r\n    \"\"\"The name of the response format.\r\n\r\n    Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length\r\n    of 64.\r\n    \"\"\"\r\n\r\n    description: str\r\n    \"\"\"\r\n    A description of what the response format is for, used by the model to determine\r\n    how to respond in the format.\r\n    \"\"\"\r\n\r\n    schema: Dict[str, object]\r\n    \"\"\"The schema for the response format, described as a JSON Schema object.\"\"\"\r\n\r\n    strict: Optional[bool]\r\n    \"\"\"Whether to enable strict schema adherence when generating the output.\r\n\r\n    If set to true, the model will always follow the exact schema defined in the\r\n    `schema` field. Only a subset of JSON Schema is supported when `strict` is\r\n    `true`. To learn more, read the\r\n    [Structured Outputs guide](https://platform.openai.com/docs/guides/structured-outputs).\r\n    \"\"\"\r\n```\r\nThen, you used it in each class where you defined the format of the response body, as follows. \r\n```python\r\nResponseFormat: TypeAlias = Union[ResponseFormatText, ResponseFormatJSONObject, ResponseFormatJSONSchema]\r\n\r\nclass CompletionCreateParamsBase(TypedDict, total=False):\r\n       .\r\n       .\r\n       .\r\n      response_format: ResponseFormat\r\n```\r\nHowever, the field named 'schema' is a built-in field in Pydantic. When you convert this model to Pydantic, it triggers the following error.\r\n`NameError: Field name \"schema\" shadows a BaseModel attribute; use a different field name with \"alias='schema'`\r\nThis bug exists in version 1.40 and later versions.\r\n\n\n### To Reproduce\n\n1.\r\nfrom openai.types.chat.completion_create_params import CompletionCreateParamsNonStreaming\r\nfrom pydantic.v1 import create_model_from_typedict\r\n2. ref = create_model_from_typedict(CompletionCreateParamsNonStreaming)\r\n3.  Then error occurred\n\n### Code snippets\n\n_No response_\n\n### OS\n\nany\n\n### Python version\n\nany\n\n### Library version\n\nopenai v1.40.0",
    "state": "closed",
    "created_at": "2024-09-12T11:15:46+00:00",
    "closed_at": "2024-09-12T11:34:22+00:00",
    "updated_at": "2024-09-12T11:34:22+00:00",
    "author": "wuyouMaster",
    "author_type": "User",
    "comments_count": 1,
    "reactions_count": 0,
    "labels": "question",
    "milestone": null,
    "closed_by": "RobertCraigie",
    "resolution_time_hours": 0.31,
    "first_comments": [
      {
        "author": "RobertCraigie",
        "created_at": "2024-09-12T11:34:22+00:00",
        "body": "I don't think there's anything we can do about this unfortunately.\r\n\r\nIf you upgrade to Pydantic v2, you can use [`TypeAdapter`](https://docs.pydantic.dev/latest/api/type_adapter/) to perform validation instead which won't have this issue.\r\n\r\nUnfortunately however it appears there's a [bug in Pydantic](https://github.com/pydantic/pydantic/issues/9467) which you'd need to [workaround](https://github.com/pydantic/pydantic/issues/9467#issuecomment-2346042699) currently."
      }
    ],
    "url": "https://github.com/openai/openai-python/issues/1704"
  },
  {
    "repository": "openai/openai-python",
    "issue_number": 1700,
    "title": "Include usage information in LengthFinishReasonError",
    "body": "### Confirm this is a feature request for the Python library and not the underlying OpenAI API.\r\n\r\n- [X] This is a feature request for the Python library\r\n\r\n### Describe the feature or improvement you're requesting\r\n\r\n**Situation**\r\nWhen calling `AsyncOpenAI(...).beta.chat.completions.parse(..., response_format=SomePydanticModel)`, the OpenAI library raises `LengthFinishReasonError` when `finish_reason == \"length\"` and raises `ContentFilterFinishReasonError` when `finish_reason == \"content_filter\"`, without providing any information as to what the response contained.\r\n\r\n**Complication**\r\nBecause there is no way to retrieve any information about the response, I cannot programmatically save information about the context. For example, I cannot access and track information from the `usage` object in the chat completion response.\r\n\r\n**Desired behavior**\r\nAs a library user, I always want to know details about responses from LLM calls that costs tokens for me. More specifically, I want to inspect `usage` to know how many tokens I \"wasted\" calling the LLM, for instance when `max_tokens` was set to a too low value for the LLM to generate a complete structured output. This enables me to track and control costs.\r\n\r\nI see two potential solutions:\r\n1. Stop raising exceptions for these scenarios and always return a chat completion object. I believe this is the behavior in the non-beta version of the chat completion call\r\n2. Return the response as an attribute in the exception object so that it can be used by the calling programmer\r\n\r\n**Version used**\r\n- 1.44.1 (latest on PyPI at the time of writing)\r\n\r\n**Code location**\r\n- File and line: [`openai.lib._parsing._completions.py` on lines 71-75](https://github.com/openai/openai-python/blob/main/src/openai/lib/_parsing/_completions.py#L71)\r\n- Function: `parse_chat_completion`\r\n\r\n### Additional context\r\n\r\n_No response_",
    "state": "closed",
    "created_at": "2024-09-10T08:23:57+00:00",
    "closed_at": "2024-09-10T15:47:19+00:00",
    "updated_at": "2024-09-10T15:47:19+00:00",
    "author": "dabure",
    "author_type": "User",
    "comments_count": 1,
    "reactions_count": 0,
    "labels": "enhancement",
    "milestone": null,
    "closed_by": "RobertCraigie",
    "resolution_time_hours": 7.389444444444444,
    "first_comments": [
      {
        "author": "RobertCraigie",
        "created_at": "2024-09-10T15:47:19+00:00",
        "body": "This will be fixed in the next release! https://github.com/openai/openai-python/pull/1701"
      }
    ],
    "url": "https://github.com/openai/openai-python/issues/1700"
  },
  {
    "repository": "openai/openai-python",
    "issue_number": 1699,
    "title": "Add Memory to API",
    "body": "### Confirm this is a feature request for the Python library and not the underlying OpenAI API.\n\n- [X] This is a feature request for the Python library\n\n### Describe the feature or improvement you're requesting\n\nThe long term memory feature in ChatGPT is a great feature that i'd like to make available to my users via API. Before I invest time developing a less capable solution I want to check with the devs if this is on the API roadmap?\n\n### Additional context\n\n_No response_",
    "state": "closed",
    "created_at": "2024-09-10T04:18:44+00:00",
    "closed_at": "2024-09-10T08:44:23+00:00",
    "updated_at": "2024-09-10T08:44:23+00:00",
    "author": "hayescode",
    "author_type": "User",
    "comments_count": 1,
    "reactions_count": 0,
    "labels": "",
    "milestone": null,
    "closed_by": "RobertCraigie",
    "resolution_time_hours": 4.4275,
    "first_comments": [
      {
        "author": "RobertCraigie",
        "created_at": "2024-09-10T08:44:23+00:00",
        "body": "Thanks for reporting!  \r\n\r\nThis sounds like an issue with the underlying OpenAI API and not the SDK, so I'm going to go ahead and close this issue. \r\n\r\nWould you mind reposting at [community.openai.com](https://community.openai.com)? "
      }
    ],
    "url": "https://github.com/openai/openai-python/issues/1699"
  },
  {
    "repository": "openai/openai-python",
    "issue_number": 1686,
    "title": "The content of the api_key or default_headers of an AsyncOpenAI instance can be modified surreptitiously? ",
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\n\n- [X] This is an issue with the Python library\n\n### Describe the bug\n\nDeploying a local LLM. Create an instance of AsyncOpenAI called `aclient`, then call `aclient.chat.completions.create` and return an asynchronous iterator. When processing the chunks yielded by the iterator, it appears that the `api_key` in the `aclient` instance is continuously changing.\r\n\n\n### To Reproduce\n\nSet the N in user_id to increment from 1 for each request, i.e. 'user_id_1', 'user_id_2', ... 'user_id_30' ... When the number of concurrent requests is greater than a certain number, the id(aclient) changes every time the for loop is entered, and user_id_from_header and user_id_from_api_key change, and the member variables of different AsyncOpenAI instances (aclient) are mixed together: the same aclient instance, e.g. user_id_from_header='user_id_20', user_id_from_api_key='user_id_15',\n\n### Code snippets\n\n```Python\nuser_id = <user_id_N>\r\naclient = AsyncOpenAI(api_key= user_id, default_headers= {'user_id': user_id})\r\nstream =  await aclient.chat.completions.create(...)\r\nasync for chunk in stream:\r\n     do_something(...)\r\n     logger.info(f'user_id_from_header={aclient.default_headers['user_id']}, user_id_from_api_key={aclient.api_key}, id={id(aclient)}')\n```\n\n\n### OS\n\nCentOS\n\n### Python version\n\nPython 3.11.4\n\n### Library version\n\nopenai v1.43.0",
    "state": "closed",
    "created_at": "2024-09-04T14:14:01+00:00",
    "closed_at": "2024-09-05T05:50:50+00:00",
    "updated_at": "2024-09-05T05:50:50+00:00",
    "author": "chenk-gd",
    "author_type": "User",
    "comments_count": 2,
    "reactions_count": 0,
    "labels": "bug",
    "milestone": null,
    "closed_by": "chenk-gd",
    "resolution_time_hours": 15.613611111111112,
    "first_comments": [
      {
        "author": "RobertCraigie",
        "created_at": "2024-09-04T14:17:58+00:00",
        "body": "Thanks for the report, it looks like your code snippet doesn't fully replicate what you're describing in your comment, would you mind sharing a full runnable example snippet?"
      },
      {
        "author": "chenk-gd",
        "created_at": "2024-09-05T05:50:50+00:00",
        "body": "sorry，it's a bug in my code."
      }
    ],
    "url": "https://github.com/openai/openai-python/issues/1686"
  },
  {
    "repository": "openai/openai-python",
    "issue_number": 1685,
    "title": "array + enum in function calling",
    "body": "### Confirm this is a feature request for the Python library and not the underlying OpenAI API.\n\n- [X] This is a feature request for the Python library\n\n### Describe the feature or improvement you're requesting\n\nIs it currently possible to get function calling with an array of inputs for an argument and a restriction via enums:\r\nsomething like:\r\n\r\n'parameters': {'type': 'object',\r\n    'properties': {'lab_values': {'type': 'array',\r\n      'enum': ['Asparate Aminotransferase (AST)',\r\n       'Barbiturate Screen',\r\n       'Benzodiazepine Screen',\r\n]}},\r\n    'required': ['lab_values'],\r\n    'additionalProperties': False}}}]\r\n\r\n?\r\n\r\n\n\n### Additional context\n\n_No response_",
    "state": "closed",
    "created_at": "2024-09-04T13:39:48+00:00",
    "closed_at": "2024-09-24T09:32:54+00:00",
    "updated_at": "2024-09-24T09:32:54+00:00",
    "author": "Dyke-F",
    "author_type": "User",
    "comments_count": 1,
    "reactions_count": 0,
    "labels": "",
    "milestone": null,
    "closed_by": "RobertCraigie",
    "resolution_time_hours": 475.885,
    "first_comments": [
      {
        "author": "RobertCraigie",
        "created_at": "2024-09-24T09:32:54+00:00",
        "body": "This sounds like an issue with the underlying OpenAI API and not the SDK, so I'm going to go ahead and close this issue. \r\n\r\nI think this should be possible but would you mind reposting at [community.openai.com](https://community.openai.com)?"
      }
    ],
    "url": "https://github.com/openai/openai-python/issues/1685"
  },
  {
    "repository": "openai/openai-python",
    "issue_number": 1678,
    "title": "Indentation error in _utils/_utils.py",
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\n\n- [X] This is an issue with the Python library\n\n### Describe the bug\n\nIt looks like there's a bug in the _utils/_utils.py file:\r\n```\r\n            for variant in variants:\r\n                matches = all((param in given_params for param in variant))\r\n                if matches:\r\n                    break\r\n            else:  # no break\r\n                if len(variants) > 1:\r\n                    variations = human_join(\r\n                        [\"(\" + human_join([quote(arg) for arg in variant], final=\"and\") + \")\" for variant in variants]\r\n                    )\r\n                    msg = f\"Missing required arguments; Expected either {variations} arguments to be given\"\r\n```\r\nThat `if matches:` and `else:  # no break` aren't aligned in the wrapper function. \n\n### To Reproduce\n\nLook at the code in the wrapper() function of _utils/_utils.py\n\n### Code snippets\n\n_No response_\n\n### OS\n\nmacOS\n\n### Python version\n\nPython 3.11\n\n### Library version\n\nopenai v1.42.0",
    "state": "closed",
    "created_at": "2024-08-26T20:42:12+00:00",
    "closed_at": "2024-08-26T21:23:12+00:00",
    "updated_at": "2024-08-26T21:23:12+00:00",
    "author": "seanonymous",
    "author_type": "User",
    "comments_count": 1,
    "reactions_count": 0,
    "labels": "question",
    "milestone": null,
    "closed_by": "RobertCraigie",
    "resolution_time_hours": 0.6833333333333333,
    "first_comments": [
      {
        "author": "RobertCraigie",
        "created_at": "2024-08-26T21:23:12+00:00",
        "body": "Ah this is not a bug!\r\n\r\nThe `else` is specifically aligned with the `for` loop so it'll be invoked if the `break` in the loop isn't hit.\r\n\r\nHere's some docs on this syntax as it is fairly uncommon: https://book.pythontips.com/en/latest/for_-_else.html#else-clause"
      }
    ],
    "url": "https://github.com/openai/openai-python/issues/1678"
  },
  {
    "repository": "openai/openai-python",
    "issue_number": 1676,
    "title": "Completions.parse() got an unexpected keyword argument 'stream'",
    "body": "### Confirm this is a feature request for the Python library and not the underlying OpenAI API.\n\n- [X] This is a feature request for the Python library\n\n### Describe the feature or improvement you're requesting\n\nThe new client.beta.chat.completions.parse() does not yet support streaming. \r\n\r\nI want to do something like:\r\n\r\n```python\r\n  response = client.beta.chat.completions.parse(\r\n      model=MODEL_NAME,\r\n      messages=[\r\n          {\"role\": \"system\", \"content\": prompt},\r\n          {\"role\": \"user\", \"content\": content},\r\n      ],\r\n      temperature=0,\r\n      top_p=0.4,\r\n      stream=True,\r\n      response_format=DocumentStructure,\r\n  )\r\n```\r\n\r\nBut this will throw the following error:\r\n\r\nTypeError: Completions.parse() got an unexpected keyword argument 'stream'\n\n### Additional context\n\n_No response_",
    "state": "closed",
    "created_at": "2024-08-26T12:35:34+00:00",
    "closed_at": "2024-09-03T18:04:49+00:00",
    "updated_at": "2024-10-23T17:28:42+00:00",
    "author": "NicolasArnouts",
    "author_type": "User",
    "comments_count": 1,
    "reactions_count": 0,
    "labels": "question",
    "milestone": null,
    "closed_by": "RobertCraigie",
    "resolution_time_hours": 197.4875,
    "first_comments": [
      {
        "author": "RobertCraigie",
        "created_at": "2024-09-03T18:04:49+00:00",
        "body": "Ah you need to use `client.beta.chat.completions.stream()` instead! Sorry that isn't clear."
      }
    ],
    "url": "https://github.com/openai/openai-python/issues/1676"
  },
  {
    "repository": "openai/openai-python",
    "issue_number": 1672,
    "title": "file attatched to thread cannot be searched",
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\r\n\r\n- [X] This is an issue with the Python library\r\n\r\n### Describe the bug\r\n\r\ni have a assistant with tool file search\r\n\r\nand create a new thread , ask question \"who is berryhoneffaaa\"  (berryhoneffaaa is person only me know in my private knowlege database) ,  assisitant anwered  \"I couldn't find any information on \"berryhoneffaaa\" in the files provided. If you have any additional context or details about Berryhoneffaaa, please provide them so I can better assist you.\"  . this is what i excepted\r\n\r\nand then i upload my private knowlege file and attatached the file to the thread , and ask \"who is berryhoneffaaa\" , it **still** **answer \"i dont known\"**\r\n\r\n![image](https://github.com/user-attachments/assets/947c43f7-235c-4771-86d7-8f13e6a32c0f)\r\n\r\n\r\n\r\n**if i upload file before a ask the question, the assistant file search will search the uploaded file and  get the right answer**\r\n\r\n\r\n\r\n### To Reproduce\r\n\r\nsee the Describe\r\n\r\n### Code snippets\r\n\r\n```Python\r\nopenai playground have this problem too\r\n```\r\n\r\n\r\n### OS\r\n\r\nmacOS\r\n\r\n### Python version\r\n\r\npython3.10\r\n\r\n### Library version\r\n\r\nopenai 1.40.8",
    "state": "closed",
    "created_at": "2024-08-23T12:57:40+00:00",
    "closed_at": "2024-08-23T14:28:33+00:00",
    "updated_at": "2024-08-23T14:28:37+00:00",
    "author": "cclehui",
    "author_type": "User",
    "comments_count": 1,
    "reactions_count": 0,
    "labels": "API-feedback",
    "milestone": null,
    "closed_by": "RobertCraigie",
    "resolution_time_hours": 1.5147222222222223,
    "first_comments": [
      {
        "author": "RobertCraigie",
        "created_at": "2024-08-23T14:28:33+00:00",
        "body": "This is an issue with the API, not the Python SDK so please ask for help on the community forum!"
      }
    ],
    "url": "https://github.com/openai/openai-python/issues/1672"
  },
  {
    "repository": "openai/openai-python",
    "issue_number": 1670,
    "title": "ChatCompletionSystemMessageParam.name is not optional but the description says it should be",
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\r\n\r\n- [X] This is an issue with the Python library\r\n\r\n### Describe the bug\r\n\r\nThe class param [`ChatCompletionSystemMessageParam.name`](https://github.com/openai/openai-python/blob/05fa732c024b55ddda1f5f8b107ce78233acd3ce/src/openai/types/chat/chat_completion_system_message_param.py#L20) is described as being optional, in fact, the API works without issues if we don't pass this param. But the type is not marked as optional. (Same issue with [`ChatCompletionUserMessageParam.name`](https://github.com/openai/openai-python/blob/05fa732c024b55ddda1f5f8b107ce78233acd3ce/src/openai/types/chat/chat_completion_user_message_param.py#L20) and [`ChatCompletionAssistantMessageParam.name`](https://github.com/openai/openai-python/blob/05fa732c024b55ddda1f5f8b107ce78233acd3ce/src/openai/types/chat/chat_completion_assistant_message_param.py#L47)).\r\n\r\nInstead of being declared as\r\n\r\n```py\r\nclass ChatCompletionSystemMessageParam(TypedDict, total=False):\r\n    content: Required[Union[str, Iterable[ChatCompletionContentPartTextParam]]]\r\n    \"\"\"The contents of the system message.\"\"\"\r\n\r\n    role: Required[Literal[\"system\"]]\r\n    \"\"\"The role of the messages author, in this case `system`.\"\"\"\r\n\r\n    name: str\r\n    \"\"\"An optional name for the participant.\r\n\r\n    Provides the model information to differentiate between participants of the same\r\n    role.\r\n    \"\"\"\r\n```\r\n\r\nIt should be declared as:\r\n\r\n```py\r\nclass ChatCompletionSystemMessageParam(TypedDict, total=False):\r\n    content: Required[Union[str, Iterable[ChatCompletionContentPartTextParam]]]\r\n    \"\"\"The contents of the system message.\"\"\"\r\n\r\n    role: Required[Literal[\"system\"]]\r\n    \"\"\"The role of the messages author, in this case `system`.\"\"\"\r\n\r\n    name:  Optional[str]\r\n    \"\"\"An optional name for the participant.\r\n\r\n    Provides the model information to differentiate between participants of the same\r\n    role.\r\n    \"\"\"\r\n```\r\n\r\n\r\n### To Reproduce\r\n\r\nGiven this test file:\r\n\r\n```\r\n# test.py\r\nimport openai\r\n\r\nclient = openai.OpenAI()\r\n\r\nresponse = client.beta.chat.completions.parse(\r\n    model=\"gpt-4o-mini\",\r\n    messages=[\r\n        {\"role\": \"system\", \"content\": \"You are an assistant.\"},  # <-  mypy claims this line has an error\r\n        {\r\n            \"role\": \"user\",\r\n            \"content\": [\r\n                {\"type\": \"text\", \"text\": \"What's up?\"},\r\n            ],\r\n        },\r\n    ],\r\n)\r\n```\r\n\r\nRun:\r\n\r\n``` bash\r\nmypy .\r\n```\r\n\r\nOutput:\r\n```py\r\ntest.py:8: error: Type of TypedDict is ambiguous, none of (\"ChatCompletionSystemMessageParam\", \"ChatCompletionUserMessageParam\", \"ChatCompletionAssistantMessageParam\") matches cleanly  [misc]\r\n```\r\n\r\n### Code snippets\r\n\r\n_No response_\r\n\r\n### OS\r\n\r\nubuntu\r\n\r\n### Python version\r\n\r\nPython 3.12.2\r\n\r\n### Library version\r\n\r\nopenai 1.42.0",
    "state": "closed",
    "created_at": "2024-08-22T13:56:45+00:00",
    "closed_at": "2024-08-22T14:03:33+00:00",
    "updated_at": "2024-08-22T14:18:33+00:00",
    "author": "fgmacedo",
    "author_type": "User",
    "comments_count": 2,
    "reactions_count": 0,
    "labels": "",
    "milestone": null,
    "closed_by": "RobertCraigie",
    "resolution_time_hours": 0.11333333333333333,
    "first_comments": [
      {
        "author": "RobertCraigie",
        "created_at": "2024-08-22T14:03:33+00:00",
        "body": "What mypy version are you using? I can't reproduce on `1.11.1`.\r\n\r\nRegardless, this is not a bug in the library. The `total=False` keyword argument in the class definition means that all fields that don't use the `Required` type can be omitted.\r\n\r\nPlease report a bug with mypy if this continues to fail for you."
      },
      {
        "author": "fgmacedo",
        "created_at": "2024-08-22T14:18:31+00:00",
        "body": "Hi @RobertCraigie , thanks for your fast reply. I'm using the same mypy version. Ok, now I see the `total=False`...  I don't know what's going on to make mypy sad. I'll skip the linter for this line. Best regards!"
      }
    ],
    "url": "https://github.com/openai/openai-python/issues/1670"
  },
  {
    "repository": "openai/openai-python",
    "issue_number": 1669,
    "title": "Run Lifecycle Documentation Broken Link in README",
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\n\n- [X] This is an issue with the Python library\n\n### Describe the bug\n\nThe error involves a broken link in the README file that was supposed to direct users to the \"Run Lifecycle Documentation.\" The broken link prevents users from accessing essential information about the project's execution stages, leading to potential confusion or errors in using the project. Fixing it is important for proper guidance.\n\n### To Reproduce\n\n- Navigate to the Repository: Open the GitHub \r\n- Locate the README File: Scroll down the main repository page to find the README file\r\n- Find the Polling Helpers Section:\r\n- Within the \"Polling Helpers\" section, look for a link labeled \"Run Lifecycle Documentation.\"\r\n- Click on this link.\r\n- Observe the Error:\r\n-  \"404 Not Found\" on https://platform.openai.com/docs/assistants/how-it-works/run-lifecycle \n\n### Code snippets\n\n_No response_\n\n### OS\n\nAll\n\n### Python version\n\nAll\n\n### Library version\n\nAll",
    "state": "closed",
    "created_at": "2024-08-21T23:21:27+00:00",
    "closed_at": "2024-09-03T18:52:18+00:00",
    "updated_at": "2024-09-03T18:52:19+00:00",
    "author": "camiloandresok",
    "author_type": "User",
    "comments_count": 1,
    "reactions_count": 0,
    "labels": "bug,documentation",
    "milestone": null,
    "closed_by": "RobertCraigie",
    "resolution_time_hours": 307.51416666666665,
    "first_comments": [
      {
        "author": "RobertCraigie",
        "created_at": "2024-09-03T18:52:19+00:00",
        "body": "Thanks for the report, this docs link has been restored!"
      }
    ],
    "url": "https://github.com/openai/openai-python/issues/1669"
  },
  {
    "repository": "openai/openai-python",
    "issue_number": 1660,
    "title": "Request for Endpoint to List All Threads",
    "body": "### Confirm this is a feature request for the Python library and not the underlying OpenAI API.\n\n- [X] This is a feature request for the Python library\n\n### Describe the feature or improvement you're requesting\n\nHey mate, hope all is well.\r\n\r\nI would like to request the addition of an endpoint that allows for the retrieval of all threads. If there are concerns about potential security risks, such as team members or others within the same organization gaining access to all threads, I suggest implementing an additional layer of security. For example, you could restrict the listing of threads to those generated under a specific project and token bearer.\r\n\r\nAdditionally, I have noticed that this request has been reiterated multiple times since January but has not yet been addressed. Given the ongoing interest and potential impact of this feature, I believe it would be highly beneficial for many users.\r\n\r\nThis enhancement would significantly improve the usability and flexibility of the API, particularly for teams working on collaborative projects.\r\n\r\nThank you for considering this request. I look forward to your feedback.\r\n\r\nCheers\n\n### Additional context\n\n_No response_",
    "state": "closed",
    "created_at": "2024-08-18T12:26:19+00:00",
    "closed_at": "2024-08-19T15:13:47+00:00",
    "updated_at": "2024-08-19T15:13:48+00:00",
    "author": "Kato-Official",
    "author_type": "User",
    "comments_count": 1,
    "reactions_count": 0,
    "labels": "",
    "milestone": null,
    "closed_by": "RobertCraigie",
    "resolution_time_hours": 26.79111111111111,
    "first_comments": [
      {
        "author": "RobertCraigie",
        "created_at": "2024-08-19T15:13:47+00:00",
        "body": "I'm going to close this as it's a request for a new API endpoint, not the Python SDK. I've forwarded this to the OpenAI team."
      }
    ],
    "url": "https://github.com/openai/openai-python/issues/1660"
  },
  {
    "repository": "openai/openai-python",
    "issue_number": 1658,
    "title": "Errors after migrating openai ",
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\n\n- [ ] This is an issue with the Python library\n\n### Describe the bug\n\n[{\r\n\t\"resource\": \"/home/dmtarmey/agent-zero/main.py\",\r\n\t\"owner\": \"_generated_diagnostic_collection_name_#1\",\r\n\t\"code\": {\r\n\t\t\"value\": \"reportUndefinedVariable\",\r\n\t\t\"target\": {\r\n\t\t\t\"$mid\": 1,\r\n\t\t\t\"path\": \"/microsoft/pyright/blob/main/docs/configuration.md\",\r\n\t\t\t\"scheme\": \"https\",\r\n\t\t\t\"authority\": \"github.com\",\r\n\t\t\t\"fragment\": \"reportUndefinedVariable\"\r\n\t\t}\r\n\t},\r\n\t\"severity\": 8,\r\n\t\"message\": \"\\\"get_openaiAI\\\" is not defined\",\r\n\t\"source\": \"Pylance\",\r\n\t\"startLineNumber\": 9,\r\n\t\"startColumn\": 10,\r\n\t\"endLineNumber\": 9,\r\n\t\"endColumn\": 22\r\n}]\n\n### To Reproduce\n\npython main.py ggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggg\n\n### Code snippets\n\n```Python\nimport os\r\nfrom dotenv import load_dotenv\r\nfrom openai import OpenAI\r\n\r\n# Load environment variables from .env file\r\nload_dotenv()\r\n\r\n# Initialize the DeepAI client with the API key\r\nclient = get_openaiAI(api_key=os.environ.get(\"API_KEY_OPENAI\"))\r\n\r\ndef get_openai_response(prompt):\r\n    \"\"\"Fetch a response from the DeepAI API based on the input prompt.\"\"\"\r\n    try:\r\n        response = client.chat.completions.create(\r\n            model=\"gpt-3.5-turbo\",\r\n            messages=[{\"role\": \"user\", \"content\": prompt}]\r\n        )\r\n        return response.choices[0].message.content  # Return the response content\r\n    except Exception as e:\r\n        print(f\"Error occurred: {e}\")\r\n        return \"An error occurred. Please try again later.\"\r\n\r\ndef main():\r\n    prompt = \"Tell me a joke.\"  # Your desired prompt here\r\n    response = get_openai_response(prompt)  # Get response\r\n    print(\"Response:\", response)\r\n\r\nif __name__ == \"__main__\":\r\n    main()\n```\n\n\n### OS\n\nlinux tuxido \n\n### Python version\n\npython3.11\n\n### Library version\n\n openai 1.41.0",
    "state": "closed",
    "created_at": "2024-08-16T22:01:23+00:00",
    "closed_at": "2024-08-17T18:06:00+00:00",
    "updated_at": "2024-08-18T14:34:23+00:00",
    "author": "DMTarmey",
    "author_type": "User",
    "comments_count": 6,
    "reactions_count": 0,
    "labels": "question",
    "milestone": null,
    "closed_by": "RobertCraigie",
    "resolution_time_hours": 20.076944444444443,
    "first_comments": [
      {
        "author": "DMTarmey",
        "created_at": "2024-08-16T22:04:05+00:00",
        "body": "I new to this so sorry for my stupidity i have had loads of attempt at troubleshooting openai library and keep having issue im using virtual studio but new to it i can see errors but not computerate enough to fix it yet "
      },
      {
        "author": "RobertCraigie",
        "created_at": "2024-08-16T22:10:42+00:00",
        "body": "No worries. Try replacing `get_openaiAI` with `OpenAI`."
      },
      {
        "author": "DMTarmey",
        "created_at": "2024-08-16T22:53:50+00:00",
        "body": "would that be all accurancies or just on main.py page \r\n\r\nis this code correct ?\r\n\r\n```py\r\nimport os\r\nimport OpenAI\r\n\r\n# Hardcoded API Key (Replace with your actual key for testing only)\r\napi_key = \"sk-proj-xxxxxxxxxxxx\"\r\n\r\n#client = OpenAI(api_key=api_key)\r\n\r\ndef openai_response(prompt):\r\n    \"\"\"Fetch a response from the OpenAI API based on the input prompt.\"\"\"\r\n    try:\r\n        response = client.chat.completions.create(\r\n            model=\"gpt-3.5-turbo\",\r\n            messages=[{\"role\": \"user\", \"content\": prompt}]\r\n        )\r\n        return response.choices[0].message.content\r\n    except Exception as e:\r\n        print(f\"Error occurred: {e}\")\r\n        return \"An error occurred. Please try again later.\"\r\n\r\ndef main():\r\n    prompt = \"Tell me a joke.\"\r\n    response = openai_response(prompt)\r\n    print(\"Response:\", response)\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n```\r\n\r\ni have temp hardcoded api as it wasnt working "
      },
      {
        "author": "RobertCraigie",
        "created_at": "2024-08-17T18:06:00+00:00",
        "body": "how was it not working?\r\n\r\n(closing this as it does not appear to be an issue with the library)"
      },
      {
        "author": "DMTarmey",
        "created_at": "2024-08-17T19:23:35+00:00",
        "body": "si are you saying the code is correct ?"
      }
    ],
    "url": "https://github.com/openai/openai-python/issues/1658"
  },
  {
    "repository": "openai/openai-python",
    "issue_number": 1647,
    "title": "openai migrate - \"ERROR (code: 200) - Too many params for text: expected maximum 1\" ",
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\n\n- [X] This is an issue with the Python library\n\n### Describe the bug\n\n```ERROR (code: 200) - Too many params for text: expected maximum 1``` when running ```openai migrate```\n\n### To Reproduce\n\nopenai migrate\n\n### Code snippets\n\n_No response_\n\n### OS\n\nmacOS\n\n### Python version\n\nPython 3.12.5\n\n### Library version\n\n1.40.6",
    "state": "closed",
    "created_at": "2024-08-13T16:01:39+00:00",
    "closed_at": "2024-08-14T11:33:49+00:00",
    "updated_at": "2024-11-27T06:02:44+00:00",
    "author": "jamintz",
    "author_type": "User",
    "comments_count": 8,
    "reactions_count": 0,
    "labels": "bug",
    "milestone": null,
    "closed_by": "RobertCraigie",
    "resolution_time_hours": 19.53611111111111,
    "first_comments": [
      {
        "author": "RobertCraigie",
        "created_at": "2024-08-13T16:31:39+00:00",
        "body": "Can you share a full stack trace?"
      },
      {
        "author": "jamintz",
        "created_at": "2024-08-13T18:10:51+00:00",
        "body": "> Can you share a full stack trace?\r\n\r\n@RobertCraigie how do I do that?"
      },
      {
        "author": "RobertCraigie",
        "created_at": "2024-08-13T19:10:13+00:00",
        "body": "oh is that all the output you get after running the command?"
      },
      {
        "author": "gigantoq",
        "created_at": "2024-08-14T02:04:11+00:00",
        "body": "So there is no fix for this?"
      },
      {
        "author": "morgante",
        "created_at": "2024-08-14T02:21:02+00:00",
        "body": "@gigantoq Until #1649 is released you can install Grit directly: https://docs.grit.io/cli/quickstart\r\n\r\nThen just run `grit apply openai`."
      }
    ],
    "url": "https://github.com/openai/openai-python/issues/1647"
  },
  {
    "repository": "openai/openai-python",
    "issue_number": 1639,
    "title": "ChatCompletionStreamManager object does not support the asynchronous context manager protocol",
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\n\n- [X] This is an issue with the Python library\n\n### Describe the bug\n\nThe docs [here](https://github.com/openai/openai-python/blob/main/helpers.md#chat-completions-events) say that the following should be possible\r\n\r\n```\r\nimport openai\r\nimport asyncio\r\n\r\nasync def test_streaming():\r\n    client = openai.OpenAI()\r\n\r\n    async with client.beta.chat.completions.stream(\r\n        model='gpt-4o-2024-08-06',\r\n        messages=[\r\n            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\r\n            {\"role\": \"user\", \"content\": \"Tell me a joke.\"},\r\n        ],\r\n    ) as stream:\r\n        async for event in stream:\r\n            if event.type == 'content.delta':\r\n                print(event.delta, flush=True, end='')\r\n            elif event.type == 'content.done':\r\n                print(\"\\nContent generation complete.\")\r\n                break\r\n\r\n# Run the streaming test\r\nasyncio.run(test_streaming())\r\n```\r\n\r\nHowever, this gives \r\n\r\n`TypeError: 'ChatCompletionStreamManager' object does not support the asynchronous context manager protocol`\r\n\r\nWhen I run without async it works fine ie\r\n\r\n```\r\nimport openai\r\n\r\ndef test_streaming():\r\n    client = openai.OpenAI()\r\n\r\n    with client.beta.chat.completions.stream(\r\n        model='gpt-4o-2024-08-06',\r\n        messages=[\r\n            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\r\n            {\"role\": \"user\", \"content\": \"Tell me a joke.\"},\r\n        ],\r\n    ) as stream:\r\n        for event in stream:\r\n            if event.type == 'content.delta':\r\n                print(event.delta, flush=True, end='')\r\n            elif event.type == 'content.done':\r\n                print(\"\\nContent generation complete.\")\r\n                break\r\n\r\n# Run the streaming test\r\ntest_streaming()\r\n\r\n```\n\n### To Reproduce\n\nRun the above code snippet which is the beta async chat_completion (and should handle the new pydantic parsing)\n\n### Code snippets\n\n```Python\n\n```\n\n\n### OS\n\nmacOS\n\n### Python version\n\nPython 3.11-3.12\n\n### Library version\n\n1.40.4",
    "state": "closed",
    "created_at": "2024-08-12T18:30:51+00:00",
    "closed_at": "2024-08-12T19:01:20+00:00",
    "updated_at": "2024-08-12T19:01:20+00:00",
    "author": "lucashofer",
    "author_type": "User",
    "comments_count": 1,
    "reactions_count": 0,
    "labels": "documentation",
    "milestone": null,
    "closed_by": "stainless-app[bot]",
    "resolution_time_hours": 0.5080555555555556,
    "first_comments": [
      {
        "author": "RobertCraigie",
        "created_at": "2024-08-12T18:35:03+00:00",
        "body": "ah @lucashofer, sorry those docs don't make it clear, you have to use `AsyncOpenAI()` for async requests.\r\n\r\nfor example\r\n```py\r\nimport openai\r\nimport asyncio\r\n\r\nasync def test_streaming():\r\n    client = openai.AsyncOpenAI()\r\n\r\n    async with client.beta.chat.completions.stream(\r\n        model='gpt-4o-2024-08-06',\r\n        messages=[\r\n            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\r\n            {\"role\": \"user\", \"content\": \"Tell me a joke.\"},\r\n        ],\r\n    ) as stream:\r\n        async for event in stream:\r\n            if event.type == 'content.delta':\r\n                print(event.delta, flush=True, end='')\r\n            elif event.type == 'content.done':\r\n                print(\"\\nContent generation complete.\")\r\n                break\r\n\r\n# Run the streaming test\r\nasyncio.run(test_streaming())\r\n```"
      }
    ],
    "url": "https://github.com/openai/openai-python/issues/1639"
  },
  {
    "repository": "openai/openai-python",
    "issue_number": 1635,
    "title": "client.chat.completions.create not working with base64 images",
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\n\n- [X] This is an issue with the Python library\n\n### Describe the bug\n\nNo idea why, but when using the lib to give gtp-4o-mini an image in base64 it's giving me the following error:\r\n\r\n```\r\nopenai.BadRequestError: Error code: 400 - {'error': {'message': \"Invalid type for 'messages[0].content[1].image_url': expected an object, but got a string instead.\", 'type': 'invalid_request_error', 'param': 'messages[0].content[1].image_url', 'code': 'invalid_type'}}\r\n```\r\n\r\nIt's essentialy the same code provided in https://platform.openai.com/docs/guides/vision/uploading-base-64-encoded-images which works fine for me.\n\n### To Reproduce\n\nJust run the code snippet, I guess.\n\n### Code snippets\n\n```Python\nclient = OpenAI(\r\n  project='',\r\n  api_key=\"\"\r\n)\r\n\r\nresponse = client.chat.completions.create(\r\n        model=\"gpt-4o-mini\",\r\n        messages=[\r\n            {\r\n                \"role\": \"user\",\r\n                \"content\": [\r\n                    {\"type\": \"text\", \"text\": \"Describe this image in a few words.\"},\r\n                    {\r\n                        \"type\": \"image_url\",\r\n                        \"image_url\": f\"data:image/jpeg;base64,{base64_image}\"\r\n                    },\r\n                ],\r\n            }\r\n        ],\r\n        max_tokens=100\r\n    )\n```\n\n\n### OS\n\nmacOS\n\n### Python version\n\nPython 3.10.14\n\n### Library version\n\n1.40.3",
    "state": "closed",
    "created_at": "2024-08-11T18:24:35+00:00",
    "closed_at": "2024-08-11T18:47:09+00:00",
    "updated_at": "2024-08-11T18:47:09+00:00",
    "author": "gustavofuhr",
    "author_type": "User",
    "comments_count": 1,
    "reactions_count": 0,
    "labels": "",
    "milestone": null,
    "closed_by": "RobertCraigie",
    "resolution_time_hours": 0.3761111111111111,
    "first_comments": [
      {
        "author": "RobertCraigie",
        "created_at": "2024-08-11T18:47:09+00:00",
        "body": "This is not a bug, your request isn't correct.\r\n```py\r\nresponse = client.chat.completions.create(\r\n    model=\"gpt-4o-mini\",\r\n    messages=[\r\n        {\r\n            \"role\": \"user\",\r\n            \"content\": [\r\n                {\"type\": \"text\", \"text\": \"Describe this image in a few words.\"},\r\n                {\r\n                    \"type\": \"image_url\",\r\n                    \"image_url\": {\r\n                        \"url\": f\"data:image/jpeg;base64,{base64_image}\",\r\n                    },\r\n                },\r\n            ],\r\n        }\r\n    ],\r\n    max_tokens=100,\r\n)\r\n```"
      }
    ],
    "url": "https://github.com/openai/openai-python/issues/1635"
  },
  {
    "repository": "openai/openai-python",
    "issue_number": 1633,
    "title": "The return type hint of `client.audio.transcriptions.create()` is incorrect.",
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\r\n\r\n- [X] This is an issue with the Python library\r\n\r\n### Describe the bug\r\n\r\nThe return type hint of `client.audio.transcriptions.create()` is `Transcription`, and this type has a `text` attribute.\r\n\r\nHowever, when the `response_format` is equal to `text`, `srt`, or `vtt`, its return value is str.\r\nIn this case, calling the text attribute will result in an error.\r\n\r\n### To Reproduce\r\n\r\n```py\r\nimport openai\r\nfrom dotenv import load_dotenv\r\n\r\nload_dotenv()\r\n\r\nfile_path = \"example.wav\"\r\n\r\nclient = openai.OpenAI()\r\nwith open(file_path, \"rb\") as file:\r\n    transcript = client.audio.transcriptions.create(\r\n        model=\"whisper-1\", file=file, response_format=\"srt\"\r\n    )\r\n\r\n    print(transcript)\r\n    print(type(transcript))\r\n    print(transcript.text)\r\n    print(type(transcript.text))\r\n```\r\n\r\n### OS\r\n\r\nWindows 10\r\n\r\n### Python version\r\n\r\nPython v3.12.4\r\n\r\n### Library version\r\n\r\nopenai v1.40.3",
    "state": "closed",
    "created_at": "2024-08-10T14:59:25+00:00",
    "closed_at": "2024-09-27T23:04:24+00:00",
    "updated_at": "2024-09-27T23:04:25+00:00",
    "author": "gbaian10",
    "author_type": "User",
    "comments_count": 1,
    "reactions_count": 0,
    "labels": "bug",
    "milestone": null,
    "closed_by": "RobertCraigie",
    "resolution_time_hours": 1160.0830555555556,
    "first_comments": [
      {
        "author": "RobertCraigie",
        "created_at": "2024-09-27T23:04:24+00:00",
        "body": "Thanks for the report, this will be fixed in the next release. https://github.com/openai/openai-python/pull/1756"
      }
    ],
    "url": "https://github.com/openai/openai-python/issues/1633"
  },
  {
    "repository": "openai/openai-python",
    "issue_number": 1632,
    "title": "`client.beta.chat.completions.parse` error 400: Missing required parameter: 'response_format.json_schema'",
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\n\n- [X] This is an issue with the Python library\n\n### Describe the bug\n\nI literally just copied the structured output example from inside [the official blog](https://openai.com/index/introducing-structured-outputs-in-the-api/) and it won't run:\r\n\r\n```python\r\nfrom dotenv import load_dotenv\r\nfrom openai import OpenAI\r\nfrom pydantic import BaseModel\r\n\r\nload_dotenv()\r\n\r\n\r\nclass Step(BaseModel):\r\n    explanation: str\r\n    output: str\r\n\r\n\r\nclass MathResponse(BaseModel):\r\n    steps: list[Step]\r\n    final_answer: str\r\n\r\n\r\nclient = OpenAI()\r\n\r\ncompletion = client.beta.chat.completions.parse(\r\n    model=\"gpt-4o-2024-08-06\",\r\n    messages=[\r\n        {\"role\": \"system\", \"content\": \"You are a helpful math tutor.\"},\r\n        {\"role\": \"user\", \"content\": \"solve 8x + 31 = 2\"},\r\n    ],\r\n    response_format=MathResponse,\r\n)\r\n\r\nmessage = completion.choices[0].message\r\nif message.parsed:\r\n    print(message.parsed.steps)\r\n    print(message.parsed.final_answer)\r\nelse:\r\n    print(message.refusal)\r\n```\r\n\r\noutput:\r\n\r\n```bash\r\nraise self._make_status_error_from_response(err.response) from None\r\nopenai.BadRequestError: Error code: 400 - {'error': {'message': \"Missing required parameter: 'response_format.json_schema'. (request id: ) (request id: 20240810152859212775791fSLFCpem)\", 'type': 'invalid_request_error', 'param': 'response_format.json_schema', 'code': 'missing_required_parameter'}}\r\n```\r\n\r\nI'm so confused now🤔. Maybe something going wrong inside `beta.chat.completions.parse`.\n\n### To Reproduce\n\n1. Copy the example from blog\r\n2. run\n\n### Code snippets\n\n_No response_\n\n### OS\n\nmacOS\n\n### Python version\n\nPython v3.12\n\n### Library version\n\nopenai v1.40.1",
    "state": "closed",
    "created_at": "2024-08-10T07:34:48+00:00",
    "closed_at": "2024-08-10T07:35:53+00:00",
    "updated_at": "2024-12-06T15:27:28+00:00",
    "author": "observerw",
    "author_type": "User",
    "comments_count": 1,
    "reactions_count": 0,
    "labels": "bug",
    "milestone": null,
    "closed_by": "observerw",
    "resolution_time_hours": 0.018055555555555554,
    "first_comments": [
      {
        "author": "mokurin000",
        "created_at": "2024-12-06T15:27:27+00:00",
        "body": "@observerw Hi! You closed your issue as completed, could you share your workarounds?"
      }
    ],
    "url": "https://github.com/openai/openai-python/issues/1632"
  },
  {
    "repository": "openai/openai-python",
    "issue_number": 1626,
    "title": "openai.InternalServerError: upstream connect error or disconnect/reset before headers. reset reason: connection failure, transport failure reason: delayed connect error: 111",
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\r\n\r\n- [X] This is an issue with the Python library\r\n\r\n### Describe the bug\r\n\r\nI am doing a hundreds of requests by hour, 95% works perfectly.\r\nSometimes it crashes with this error: \r\n```openai.InternalServerError: upstream connect error or disconnect/reset before headers. reset reason: connection failure, transport failure reason: delayed connect error: 111```\r\n\r\nThe thing is that the same code works most of the time and fails sometime, making the debugging really hard, I failed to find the pattern that make theses requests fail, it can arrive anytime.\r\n\r\nIt can pass with big requests, with a lot of requests at the same time. It can crash with tiny request and with few requests in the same time.\r\nSo the intuition that they are too much request or too big request seems wrong.\r\n\r\nI have this Traceback:\r\n```\r\nreturn func(*args, **kwargs)\\n           ^^^^^^^^^^^^^^^^^^^^^\\n  File \"/usr/local/lib/python3.11/site-packages/openai/resources/chat/completions.py\", line 650, in create\\n    return self._post(\\n           ^^^^^^^^^^^\\n  File \"/usr/local/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \"/usr/local/lib/python3.11/site-packages/openai/_base_client.py\", line 936, in request\\n    return self._request(\\n           ^^^^^^^^^^^^^^\\n  File \"/usr/local/lib/python3.11/site-packages/openai/_base_client.py\", line 1025, in _request\\n    return self._retry_request(\\n           ^^^^^^^^^^^^^^^^^^^^\\n  File \"/usr/local/lib/python3.11/site-packages/openai/_base_client.py\", line 1074, in _retry_request\\n    return self._request(\\n           ^^^^^^^^^^^^^^\\n  File \"/usr/local/lib/python3.11/site-packages/openai/_base_client.py\", line 1025, in _request\\n    return self._retry_request(\\n           ^^^^^^^^^^^^^^^^^^^^\\n  File \"/usr/local/lib/python3.11/site-packages/openai/_base_client.py\", line 1074, in _retry_request\\n    return self._request(\\n           ^^^^^^^^^^^^^^\\n  File \"/usr/local/lib/python3.11/site-packages/openai/_base_client.py\", line 1040, in _request\\n    raise self._make_status_error_from_response(err.response) from None\\nopenai.InternalServerError: upstream connect error or disconnect/reset before headers. reset reason: connection failure, transport failure reason: delayed connect error: 111\\n\r\n```\r\n\r\n### To Reproduce\r\n\r\nfrom langchain_openai import ChatOpenAI\r\n\r\nllm = ChatOpenAI(\r\n    model=\"gpt-4o\",\r\n    temperature=0,\r\n    max_tokens=None,\r\n    timeout=None,\r\n    max_retries=2,\r\n)\r\n\r\n\r\n\r\n### Code snippets\r\n\r\n_No response_\r\n\r\n### OS\r\n\r\nLinux\r\n\r\n### Python version\r\n\r\npython v3.11\r\n\r\n### Library version\r\n\r\nopenai v1.35.10",
    "state": "closed",
    "created_at": "2024-08-09T08:46:38+00:00",
    "closed_at": "2024-08-09T12:58:23+00:00",
    "updated_at": "2024-08-09T13:41:17+00:00",
    "author": "JustAnotherVeryNormalDeveloper",
    "author_type": "User",
    "comments_count": 6,
    "reactions_count": 0,
    "labels": "",
    "milestone": null,
    "closed_by": "RobertCraigie",
    "resolution_time_hours": 4.195833333333334,
    "first_comments": [
      {
        "author": "RobertCraigie",
        "created_at": "2024-08-09T09:11:15+00:00",
        "body": "what versions of `httpx` and `httpcore` do you have installed?"
      },
      {
        "author": "JustAnotherVeryNormalDeveloper",
        "created_at": "2024-08-09T09:14:26+00:00",
        "body": "> what versions of `httpx` and `httpcore` do you have installed?\r\n\r\nhttpx==0.27.0\r\nhttpcore==1.0.5"
      },
      {
        "author": "RobertCraigie",
        "created_at": "2024-08-09T09:19:35+00:00",
        "body": "thanks, can you enable [debug logging](https://github.com/openai/openai-python#logging) and share the response headers from a failed request?\r\n\r\nyou could also do something like this if it's easier\r\n```py\r\nfrom openai import InternalServerError\r\n\r\ntry:\r\n  # ... make request ...\r\nexcept InternalServerError as exc:\r\n  print(exc.response.headers)\r\n  raise exc\r\n```"
      },
      {
        "author": "JustAnotherVeryNormalDeveloper",
        "created_at": "2024-08-09T12:52:11+00:00",
        "body": "So i added this:\r\nexcept InternalServerError as exc:\r\n            print(f'InternalServerError error: {repr(exc)}')\r\n            print(f'InternalServerError error response: {repr(exc.response)}')\r\n            print(f'{format_log_type(self.name)} InternalServerError error headers: {repr(exc.response.headers)}')\r\n            raise exc\r\n\r\nI receive this sometime:\r\nInternalServerError error: InternalServerError('upstream connect error or disconnect/reset before headers. reset reason: connection failure, transport failure reason: delayed connect error: 111')\r\nInternalServerError error response: <Response [503 Service Unavailable]>\r\nInternalServerError error headers: Headers({'content-length': '145', 'content-type': 'text/plain', 'date': 'Fri, 09 Aug 2024 12:20:02 GMT', 'server': 'istio-envoy', 'connection': 'close', 'x-envoy-decorator-operation': 'tgi-cohere.xxxxxxx.svc.cluster.local:80/*'})\r\n"
      },
      {
        "author": "RobertCraigie",
        "created_at": "2024-08-09T12:58:23+00:00",
        "body": "Thanks for the extra details. It looks like this is an intermittent network issue so it's very unlikely to be related to the SDK so I'm going to close this.\r\n\r\nAre you using a proxy? Could there be an issue with that server?\r\n\r\nIf you need more assistance please use the [community forum](https://community.openai.com/)."
      }
    ],
    "url": "https://github.com/openai/openai-python/issues/1626"
  },
  {
    "repository": "openai/openai-python",
    "issue_number": 1625,
    "title": "openai.APIError: The server had an error processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if you keep seeing this error",
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\n\n- [X] This is an issue with the Python library\n\n### Describe the bug\n\nWhen I use langchain `astream_events` to call openai stream api, I will get this api error some times.\r\n\r\n```\r\n    async for item in self._iterator:\r\n  File \"C:\\Users\\jimmy\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\openai\\_streaming.py\", line 174, in __stream__\r\n    raise APIError(\r\nopenai.APIError: The server had an error processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if you keep seeing this error.\r\n```\n\n### To Reproduce\n\n1. Use langchain agent `astream_events`. [link](https://python.langchain.com/v0.1/docs/modules/agents/how_to/streaming/)\r\n2. Use openai be the llm model.\n\n### Code snippets\n\n_No response_\n\n### OS\n\nwindows11\n\n### Python version\n\nPython3.11.8\n\n### Library version\n\nopenai 1.35.13",
    "state": "closed",
    "created_at": "2024-08-09T01:21:03+00:00",
    "closed_at": "2024-08-14T02:45:21+00:00",
    "updated_at": "2024-08-14T02:45:22+00:00",
    "author": "Lin-jun-xiang",
    "author_type": "User",
    "comments_count": 4,
    "reactions_count": 0,
    "labels": "bug",
    "milestone": null,
    "closed_by": "Lin-jun-xiang",
    "resolution_time_hours": 121.405,
    "first_comments": [
      {
        "author": "RobertCraigie",
        "created_at": "2024-08-09T04:36:01+00:00",
        "body": "Can you share an example snippet? Or a request ID?"
      },
      {
        "author": "Lin-jun-xiang",
        "created_at": "2024-08-09T05:53:43+00:00",
        "body": "@RobertCraigie  \r\nThanks your reply.\r\n\r\nThe following is a example snippet:\r\n\r\n\r\n```python\r\nlangchain_llm_client = ChatOpenAI(\r\n    model='gpt-4o',\r\n    temperature=0.,\r\n    api_key=OPENAI_API_KEY,\r\n    streaming=True,\r\n    max_tokens=None,\r\n)\r\ntools = [tool1, tool2, ...]\r\nagent = create_tool_calling_agent(langchain_llm_client, tools, AGENT_PROMPT)\r\nagent_executor = AgentExecutor(\r\n    agent=agent,\r\n    tools=tools,\r\n    verbose=False,\r\n    return_intermediate_steps=True\r\n)\r\n\r\nasync def chat(messages):\r\n    tool_names = [tool.name for tool in tools]\r\n    async for event in agent_executor.astream_events(\r\n        {\r\n            \"input\": messages,\r\n            \"tools\": tools,\r\n            \"tool_names\": tool_names,\r\n            \"agent_scratchpad\": lambda x: format_to_openai_tool_messages(x[\"intermediate_steps\"]),\r\n        },\r\n        version='v2'\r\n    ):\r\n        kind = event['event']\r\n        if kind == \"on_chain_start\":\r\n            if (\r\n                event[\"name\"] == \"Agent\"\r\n            ):\r\n                yield(\r\n                    f\"\\n### Start Agent: `{event['name']}`, Agent Input: `{event['data'].get('input')}`\\n\"\r\n                )\r\n        elif kind == \"on_chat_model_stream\":\r\n            # llm model response\r\n            content = event[\"data\"][\"chunk\"].content\r\n            if content:\r\n                yield content\r\n        elif kind == \"on_tool_start\":\r\n            yield(\r\n                f\"\\n### Start Tool: `{event['name']}`, Tool Input: `{event['data'].get('input')}`\\n\"\r\n            )\r\n        elif kind == \"on_tool_end\":\r\n            yield(\r\n                f\"\\n### Finished Tool: `{event['name']}`, Tool Results: \\n\"\r\n            )\r\n            if isinstance(event['data'].get('output'), AsyncGenerator):\r\n                async for event_chunk in event['data'].get('output'):\r\n                    yield event_chunk\r\n            else:\r\n                yield(\r\n                    f\"`{event['data'].get('output')}`\\n\"\r\n                )\r\n        elif kind == \"on_chain_end\":\r\n            if (\r\n                event[\"name\"] == \"Agent\"\r\n            ):\r\n                yield(\r\n                    f\"\\n### Finished Agent: `{event['name']}`, Agent Results: \\n\"\r\n                )\r\n                yield(\r\n                    f\"{event['data'].get('output')['output']}\\n\"\r\n                )\r\n\r\nif __name__ == '__main__':\r\n    asyncio.run(chat(['describe'])\r\n```"
      },
      {
        "author": "RobertCraigie",
        "created_at": "2024-08-09T09:39:56+00:00",
        "body": "Thanks, could you enable [debug logging](https://github.com/openai/openai-python#logging) and share an `x-request-id` header for a failed request?"
      },
      {
        "author": "Lin-jun-xiang",
        "created_at": "2024-08-14T02:45:22+00:00",
        "body": "@RobertCraigie \r\nThanks you.\r\n\r\nFinally, I fount out the problem is \"**token usasge**\" is too large in langchain agent.\r\n\r\nLarge token with async will cause the error usually.\r\n\r\nAfter reducing the token in system prompt, the problem was solved."
      }
    ],
    "url": "https://github.com/openai/openai-python/issues/1625"
  },
  {
    "repository": "openai/openai-python",
    "issue_number": 1622,
    "title": "Unable to access the OpenAI API with genuine key",
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\n\n- [X] This is an issue with the Python library\n\n### Describe the bug\n\nAs per the recommendations I used openai.Completion.create with openai version 0.28.0 as per this link https://github.com/openai/openai-python/discussions/742 and used openai.ChatCompletion.create with the most latest version 1.40.1 \r\n\r\nIn both the cases I get error\r\n\r\nError in both versions 1.40.1 0.28.0\r\n\r\nAn error occurred: \r\n\r\nYou tried to access openai.Completion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\r\n\r\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \r\n\r\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\r\n\r\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\r\n\n\n### To Reproduce\n\npip install --upgrade openai\r\n\r\nfor 1.40.1\r\n\r\npip install openai==0.28\r\n\r\nresponse = openai.ChatCompletion.create(\r\n            model=\"gpt-3.5-turbo\",  # Use a model like \"gpt-3.5-turbo\" or \"gpt-4\"\r\n            messages=[\r\n                {\"role\": \"user\", \"content\": prompt}\r\n            ],\r\n            max_tokens=1024,\r\n            temperature=0.5\r\n        )\r\n\r\nFor 0.28.0\r\n\r\nresponse = openai.Completion.create(\r\n            model=\"gpt-3.5-turbo\",  # Use a model like \"gpt-3.5-turbo\" or \"gpt-4\"\r\n            messages=[\r\n                {\"role\": \"user\", \"content\": prompt}\r\n            ],\r\n            max_tokens=1024,\r\n            temperature=0.5\r\n        )\r\n\r\n\r\nError\r\n\r\nYou tried to access openai.Completion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\r\n\r\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \r\n\r\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\r\n\r\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\r\n\n\n### Code snippets\n\n```Python\nAs explained above\n```\n\n\n### OS\n\nWindows\n\n### Python version\n\n3.11.7\n\n### Library version\n\n1.40.1",
    "state": "closed",
    "created_at": "2024-08-08T06:51:32+00:00",
    "closed_at": "2024-08-08T08:36:57+00:00",
    "updated_at": "2024-09-03T19:17:48+00:00",
    "author": "saurabharora2593",
    "author_type": "User",
    "comments_count": 17,
    "reactions_count": 0,
    "labels": "question",
    "milestone": null,
    "closed_by": "RobertCraigie",
    "resolution_time_hours": 1.7569444444444444,
    "first_comments": [
      {
        "author": "TKartist",
        "created_at": "2024-08-08T08:35:43+00:00",
        "body": "try this code in the readme:\r\n```py\r\nimport os\r\nfrom openai import OpenAI\r\n\r\nclient = OpenAI(\r\n    # This is the default and can be omitted\r\n    api_key=os.environ.get(\"OPENAI_API_KEY\"),\r\n)\r\n\r\nchat_completion = client.chat.completions.create(\r\n    messages=[\r\n        {\r\n            \"role\": \"user\",\r\n            \"content\": \"Say this is a test\",\r\n        }\r\n    ],\r\n    model=\"gpt-3.5-turbo\",\r\n)\r\n```"
      },
      {
        "author": "RobertCraigie",
        "created_at": "2024-08-08T08:36:58+00:00",
        "body": "Please use the syntax shown in the above comment."
      },
      {
        "author": "saurabharora2593",
        "created_at": "2024-08-08T12:07:31+00:00",
        "body": "OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\r\n\r\nclient = OpenAI(\r\n    # This is the default and can be omitted\r\n    api_key=os.environ.get(\"OPENAI_API_KEY\"),\r\n)"
      },
      {
        "author": "TKartist",
        "created_at": "2024-08-08T12:45:44+00:00",
        "body": "follow the guide on this link: https://phoenixnap.com/kb/windows-set-environment-variable \r\nand add it to your user's environment variable and it should work.\r\n\r\nOR\r\n\r\nif you are keeping ur code only on local machine and not sharing it with anyone, then just do\r\nclient = OpenAI(\r\n    api_key = \"YOUR_OPEN_AI_API_KEY\"\r\n)  \r\n\r\nOR\r\n\r\nsave the api key in a hidden txt file (i.e. \".secret.txt\") and read from that file. Then add \".*\" to ur .gitignore file"
      },
      {
        "author": "saurabharora2593",
        "created_at": "2024-08-09T00:05:08+00:00",
        "body": "Running the code on local with no collaboration required.\r\n\r\nAn error occurred: 'ChatCompletionMessage' object is not subscriptable\r\n\r\nCode\r\n\r\nclient = OpenAI(\r\napi_key = \"My Key\"\r\n)\r\n\r\n \r\ndef get_gpt_rating(prompt):\r\n    \"\"\"Fetches rating and explanation from GPT.\"\"\"\r\n    try:\r\n        response = client.chat.completions.create(\r\n    messages=[\r\n        {\r\n            \"role\": \"user\",\r\n            \"content\": prompt,\r\n        }\r\n    ],\r\n    model=\"gpt-3.5-turbo\",\r\n)\r\n"
      }
    ],
    "url": "https://github.com/openai/openai-python/issues/1622"
  },
  {
    "repository": "openai/openai-python",
    "issue_number": 1620,
    "title": "Arbitrary file write during tarfile extraction",
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\n\n- [X] This is an issue with the Python library\n\n### Describe the bug\n\nExtracting files from a malicious tar archive without validating that the destination file path is within the destination directory can cause files outside the destination directory to be overwritten, due to the possible presence of directory traversal elements (..) in archive paths.\n\n### To Reproduce\n\nIssue present in this line - https://github.com/openai/openai-python/blob/631a2a7156299351874f37c4769308a104ce19ed/src/openai/cli/_tools/migrate.py#L144\n\n### Code snippets\n\n```Python\nhttps://github.com/openai/openai-python/blob/631a2a7156299351874f37c4769308a104ce19ed/src/openai/cli/_tools/migrate.py#L144\n```\n\n\n### OS\n\nmacOS\n\n### Python version\n\n3.11.4\n\n### Library version\n\n1.40.1",
    "state": "closed",
    "created_at": "2024-08-08T05:40:48+00:00",
    "closed_at": "2024-08-17T18:08:17+00:00",
    "updated_at": "2024-08-17T18:08:18+00:00",
    "author": "arpitjain099",
    "author_type": "User",
    "comments_count": 1,
    "reactions_count": 0,
    "labels": "",
    "milestone": null,
    "closed_by": "RobertCraigie",
    "resolution_time_hours": 228.45805555555555,
    "first_comments": [
      {
        "author": "RobertCraigie",
        "created_at": "2024-08-17T18:08:17+00:00",
        "body": "https://github.com/openai/openai-python/pull/1621#issuecomment-2294930975"
      }
    ],
    "url": "https://github.com/openai/openai-python/issues/1620"
  },
  {
    "repository": "openai/openai-python",
    "issue_number": 1618,
    "title": "Too permissive permissions in file",
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\r\n\r\n- [X] This is an issue with the Python library\r\n\r\n### Describe the bug\r\n\r\nExtracting files from a malicious tar archive without validating that the destination file path is within the destination directory can cause files outside the destination directory to be overwritten, due to the possible presence of directory traversal elements (..) in archive paths.\r\n\r\nAffected file: https://github.com/openai/openai-python/blob/631a2a7156299351874f37c4769308a104ce19ed/src/openai/cli/_tools/migrate.py#L151-151\r\n\r\n### To Reproduce\r\n\r\nBug present in file - https://github.com/openai/openai-python/blob/631a2a7156299351874f37c4769308a104ce19ed/src/openai/cli/_tools/migrate.py#L151-151\r\n\r\n### Code snippets\r\n\r\n```Python\r\nIndicated here - https://github.com/openai/openai-python/blob/631a2a7156299351874f37c4769308a104ce19ed/src/openai/cli/_tools/migrate.py#L151-151\r\n```\r\n\r\n\r\n### OS\r\n\r\nmacOS\r\n\r\n### Python version\r\n\r\n3.11.4\r\n\r\n### Library version\r\n\r\n1.40.1",
    "state": "closed",
    "created_at": "2024-08-08T05:33:10+00:00",
    "closed_at": "2024-08-17T18:10:08+00:00",
    "updated_at": "2024-08-17T18:10:08+00:00",
    "author": "arpitjain099",
    "author_type": "User",
    "comments_count": 1,
    "reactions_count": 0,
    "labels": "",
    "milestone": null,
    "closed_by": "RobertCraigie",
    "resolution_time_hours": 228.6161111111111,
    "first_comments": [
      {
        "author": "RobertCraigie",
        "created_at": "2024-08-17T18:10:08+00:00",
        "body": "We are not worried about malicious tar archives in this case as we will only ever be downloading the tar archives from a trusted source."
      }
    ],
    "url": "https://github.com/openai/openai-python/issues/1618"
  },
  {
    "repository": "openai/openai-python",
    "issue_number": 1617,
    "title": "BUG: version `1.40.x`: NameError for `CompletionCreateParamsNonStreaming`",
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\n\n- [X] This is an issue with the Python library\n\n### Describe the bug\n\nError stack:\r\n```\r\nFile ~/miniconda3/lib/python3.9/site-packages/pydantic/v1/validators.py:738, in find_validators(type_, config)\r\n    736     return\r\n    737 if is_typeddict(type_):\r\n--> 738     yield make_typeddict_validator(type_, config)\r\n    739     return\r\n    741 class_ = get_class(type_)\r\n\r\nFile ~/miniconda3/lib/python3.9/site-packages/pydantic/v1/validators.py:624, in make_typeddict_validator(typeddict_cls, config)\r\n    619 def make_typeddict_validator(\r\n    620     typeddict_cls: Type['TypedDict'], config: Type['BaseConfig']  # type: ignore[valid-type]\r\n    621 ) -> Callable[[Any], Dict[str, Any]]:\r\n    622     from .annotated_types import create_model_from_typeddict\r\n--> 624     TypedDictModel = create_model_from_typeddict(\r\n    625         typeddict_cls,\r\n    626         __config__=config,\r\n    627         __module__=typeddict_cls.__module__,\r\n    628     )\r\n    629     typeddict_cls.__pydantic_model__ = TypedDictModel  # type: ignore[attr-defined]\r\n    631     def typeddict_validator(values: 'TypedDict') -> Dict[str, Any]:  # type: ignore[valid-type]\r\n\r\nFile ~/miniconda3/lib/python3.9/site-packages/pydantic/v1/annotated_types.py:55, in create_model_from_typeddict(typeddict_cls, **kwargs)\r\n     49 required_keys: FrozenSet[str] = typeddict_cls.__required_keys__  # type: ignore[attr-defined]\r\n     50 field_definitions = {\r\n     51     field_name: (field_type, Required if field_name in required_keys else None)\r\n     52     for field_name, field_type in typeddict_cls.__annotations__.items()\r\n     53 }\r\n---> 55 return create_model(typeddict_cls.__name__, **kwargs, **field_definitions)\r\n\r\nFile ~/miniconda3/lib/python3.9/site-packages/pydantic/v1/main.py:1024, in create_model(__model_name, __config__, __base__, __module__, __validators__, __cls_kwargs__, __slots__, **field_definitions)\r\n   1022     ns['__orig_bases__'] = __base__\r\n   1023 namespace.update(ns)\r\n-> 1024 return meta(__model_name, resolved_bases, namespace, **kwds)\r\n\r\nFile ~/miniconda3/lib/python3.9/site-packages/pydantic/v1/main.py:197, in ModelMetaclass.__new__(mcs, name, bases, namespace, **kwargs)\r\n    189     if (\r\n    190         is_untouched(value)\r\n    191         and ann_type != PyObject\r\n   (...)\r\n    194         )\r\n    195     ):\r\n    196         continue\r\n--> 197     fields[ann_name] = ModelField.infer(\r\n    198         name=ann_name,\r\n    199         value=value,\r\n    200         annotation=ann_type,\r\n    201         class_validators=vg.get_validators(ann_name),\r\n    202         config=config,\r\n    203     )\r\n    204 elif ann_name not in namespace and config.underscore_attrs_are_private:\r\n    205     private_attributes[ann_name] = PrivateAttr()\r\n\r\nFile ~/miniconda3/lib/python3.9/site-packages/pydantic/v1/fields.py:504, in ModelField.infer(cls, name, value, annotation, class_validators, config)\r\n    501     required = False\r\n    502 annotation = get_annotation_from_field_info(annotation, field_info, name, config.validate_assignment)\r\n--> 504 return cls(\r\n    505     name=name,\r\n    506     type_=annotation,\r\n    507     alias=field_info.alias,\r\n    508     class_validators=class_validators,\r\n    509     default=value,\r\n    510     default_factory=field_info.default_factory,\r\n    511     required=required,\r\n    512     model_config=config,\r\n    513     field_info=field_info,\r\n    514 )\r\n\r\nFile ~/miniconda3/lib/python3.9/site-packages/pydantic/v1/fields.py:434, in ModelField.__init__(self, name, type_, class_validators, model_config, default, default_factory, required, final, alias, field_info)\r\n    432 self.shape: int = SHAPE_SINGLETON\r\n    433 self.model_config.prepare_field(self)\r\n--> 434 self.prepare()\r\n\r\nFile ~/miniconda3/lib/python3.9/site-packages/pydantic/v1/fields.py:555, in ModelField.prepare(self)\r\n    553 if self.default is Undefined and self.default_factory is None:\r\n    554     self.default = None\r\n--> 555 self.populate_validators()\r\n\r\nFile ~/miniconda3/lib/python3.9/site-packages/pydantic/v1/fields.py:829, in ModelField.populate_validators(self)\r\n    825 if not self.sub_fields or self.shape == SHAPE_GENERIC:\r\n    826     get_validators = getattr(self.type_, '__get_validators__', None)\r\n    827     v_funcs = (\r\n    828         *[v.func for v in class_validators_ if v.each_item and v.pre],\r\n--> 829         *(get_validators() if get_validators else list(find_validators(self.type_, self.model_config))),\r\n    830         *[v.func for v in class_validators_ if v.each_item and not v.pre],\r\n    831     )\r\n    832     self.validators = prep_validators(v_funcs)\r\n    834 self.pre_validators = []\r\n\r\nFile ~/miniconda3/lib/python3.9/site-packages/pydantic/v1/validators.py:738, in find_validators(type_, config)\r\n    736     return\r\n    737 if is_typeddict(type_):\r\n--> 738     yield make_typeddict_validator(type_, config)\r\n    739     return\r\n    741 class_ = get_class(type_)\r\n\r\nFile ~/miniconda3/lib/python3.9/site-packages/pydantic/v1/validators.py:624, in make_typeddict_validator(typeddict_cls, config)\r\n    619 def make_typeddict_validator(\r\n    620     typeddict_cls: Type['TypedDict'], config: Type['BaseConfig']  # type: ignore[valid-type]\r\n    621 ) -> Callable[[Any], Dict[str, Any]]:\r\n    622     from .annotated_types import create_model_from_typeddict\r\n--> 624     TypedDictModel = create_model_from_typeddict(\r\n    625         typeddict_cls,\r\n    626         __config__=config,\r\n    627         __module__=typeddict_cls.__module__,\r\n    628     )\r\n    629     typeddict_cls.__pydantic_model__ = TypedDictModel  # type: ignore[attr-defined]\r\n    631     def typeddict_validator(values: 'TypedDict') -> Dict[str, Any]:  # type: ignore[valid-type]\r\n\r\nFile ~/miniconda3/lib/python3.9/site-packages/pydantic/v1/annotated_types.py:55, in create_model_from_typeddict(typeddict_cls, **kwargs)\r\n     49 required_keys: FrozenSet[str] = typeddict_cls.__required_keys__  # type: ignore[attr-defined]\r\n     50 field_definitions = {\r\n     51     field_name: (field_type, Required if field_name in required_keys else None)\r\n     52     for field_name, field_type in typeddict_cls.__annotations__.items()\r\n     53 }\r\n---> 55 return create_model(typeddict_cls.__name__, **kwargs, **field_definitions)\r\n\r\nFile ~/miniconda3/lib/python3.9/site-packages/pydantic/v1/main.py:1024, in create_model(__model_name, __config__, __base__, __module__, __validators__, __cls_kwargs__, __slots__, **field_definitions)\r\n   1022     ns['__orig_bases__'] = __base__\r\n   1023 namespace.update(ns)\r\n-> 1024 return meta(__model_name, resolved_bases, namespace, **kwds)\r\n\r\nFile ~/miniconda3/lib/python3.9/site-packages/pydantic/v1/main.py:186, in ModelMetaclass.__new__(mcs, name, bases, namespace, **kwargs)\r\n    184     class_vars.add(ann_name)\r\n    185 elif is_valid_field(ann_name):\r\n--> 186     validate_field_name(bases, ann_name)\r\n    187     value = namespace.get(ann_name, Undefined)\r\n    188     allowed_types = get_args(ann_type) if is_union(get_origin(ann_type)) else (ann_type,)\r\n\r\nFile ~/miniconda3/lib/python3.9/site-packages/pydantic/v1/utils.py:167, in validate_field_name(bases, field_name)\r\n    165 for base in bases:\r\n    166     if getattr(base, field_name, None):\r\n--> 167         raise NameError(\r\n    168             f'Field name \"{field_name}\" shadows a BaseModel attribute; '\r\n    169             f'use a different field name with \"alias=\\'{field_name}\\'\".'\r\n    170         )\r\n\r\nNameError: Field name \"schema\" shadows a BaseModel attribute; use a different field name with \"alias='schema'\".\r\n```\r\n\r\nBefore v1.40.1, everything works fine.\n\n### To Reproduce\n\nMini code:\r\n```\r\nfrom openai.types.chat.completion_create_params\r\nfrom openai.types.chat.completion_create_params import CompletionCreateParamsNonStreaming\r\nfrom pydantic.v1 import create_model_from_typeddict\r\ncreate_model_from_typeddict(CompletionCreateParamsNonStreaming)\r\n```\n\n### Code snippets\n\n_No response_\n\n### OS\n\nI think all the os should have this issue\n\n### Python version\n\nPython 3.9\n\n### Library version\n\nopenai v1.40.1",
    "state": "closed",
    "created_at": "2024-08-08T03:01:14+00:00",
    "closed_at": "2024-08-08T10:08:28+00:00",
    "updated_at": "2024-08-08T10:08:29+00:00",
    "author": "ChengjieLi28",
    "author_type": "User",
    "comments_count": 1,
    "reactions_count": 0,
    "labels": "",
    "milestone": null,
    "closed_by": "RobertCraigie",
    "resolution_time_hours": 7.120555555555556,
    "first_comments": [
      {
        "author": "RobertCraigie",
        "created_at": "2024-08-08T10:08:28+00:00",
        "body": "Sorry this is no longer working, as far as I can tell this is a fundamental limitation of Pydantic v1, you can't use the `create_model_from_typeddict` and rename nested fields.\r\n\r\nAs you're actually using Pydantic v2, I think you can just do this instead\r\n```py\r\nfrom pydantic import TypeAdapter\r\n\r\nfrom openai.types.chat.completion_create_params import CompletionCreateParamsNonStreaming\r\n\r\nparams_adapter = TypeAdapter(CompletionCreateParamsNonStreaming)\r\nparams_adapter.validate_python({\"name\": \"foo\"})\r\n```\r\nAs far as I know, everything you can do with a `BaseModel`, you should be able to do with a `TypeAdapter`."
      }
    ],
    "url": "https://github.com/openai/openai-python/issues/1617"
  },
  {
    "repository": "openai/openai-python",
    "issue_number": 1616,
    "title": "replace jiter dependency with build-in pydantic function",
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\n\n- [X] This is an issue with the Python library\n\n### Describe the bug\n\nhttps://pypi.org/project/jiter/\r\n```\r\nThis is a standalone version of the JSON parser used in pydantic-core\r\nThe recommendation is to only use this package directly if you do not use pydantic.\r\n```\r\n\r\nThe reason of this request is some OS (Gentoo in my case) do provide both ```pydantic``` and ```pydantic-core``` libraries, but ```jiter```\r\n\r\nopenai-python uses ```pydantic``` already, so it should be possible to re-use build-in function (perhaps, via ```pydantic-core```)\n\n### To Reproduce\n\n1. inspect this line: https://github.com/openai/openai-python/blob/main/pyproject.toml#L19\n\n### Code snippets\n\n```Python\nhttps://github.com/openai/openai-python/blob/main/src/openai/lib/streaming/chat/_completions.py\r\n jiter import from_json\n```\n\n\n### OS\n\nany\n\n### Python version\n\nany\n\n### Library version\n\n1.40.0",
    "state": "closed",
    "created_at": "2024-08-08T01:15:10+00:00",
    "closed_at": "2024-08-14T15:12:20+00:00",
    "updated_at": "2024-08-25T14:33:25+00:00",
    "author": "blshkv",
    "author_type": "User",
    "comments_count": 6,
    "reactions_count": 0,
    "labels": "",
    "milestone": null,
    "closed_by": "RobertCraigie",
    "resolution_time_hours": 157.95277777777778,
    "first_comments": [
      {
        "author": "RobertCraigie",
        "created_at": "2024-08-08T08:39:05+00:00",
        "body": "We need to use the standalone package because we support both Pydantic v1 and Pydantic v2. Unfortunately pydantic v1 does not provide the same JSON parsing functionality.\r\n\r\n> The reason of this request is some OS (Gentoo in my case) do provide both pydantic and pydantic-core libraries, but jiter\r\n\r\nWhat do you mean? How are you managing dependencies?"
      },
      {
        "author": "blshkv",
        "created_at": "2024-08-08T11:13:10+00:00",
        "body": "You should probably consider stopping supporting pedantic 1 at some point, for simplicity reason.\r\njiter is used just once in the code.\r\n\r\nhttps://docs.pydantic.dev/latest/version-policy/\r\n```\r\nActive development of V1 has already stopped, however critical bug fixes and security vulnerabilities will be fixed in V1 for one year after the release of V2 (June 30, 2024).\r\n```\r\n\r\nFYI, Gentoo provides pedantic 2 only (https://packages.gentoo.org/packages/dev-python/pydantic )\r\nand pedantic-core (version 2 as well)\r\n\r\nI had to create my own jiter ebuild, but it is complicated, since it is a mixture of rust and python. And it's basically similar with pydantic-core. \r\n"
      },
      {
        "author": "RobertCraigie",
        "created_at": "2024-08-14T15:12:20+00:00",
        "body": "Unfortunately we have no plans to drop support for Pydantic v1 anytime soon as it's still getting many millions of downloads a [day](https://www.pepy.tech/projects/pydantic?versions=2.*&versions=1.*).\r\n\r\nPlease ask Gentoo to provide a packaged version of `jiter` if this is important to you."
      },
      {
        "author": "dinopanagos",
        "created_at": "2024-08-20T03:00:40+00:00",
        "body": "do we have an arn for jiter we can reference for python 312?\r\ni cant stop getting no module named jiter.jiter\r\neven with pydantic and openai in my req txt"
      },
      {
        "author": "blshkv",
        "created_at": "2024-08-20T03:37:33+00:00",
        "body": "> do we have an arn for jiter we can reference for python 312? i cant stop getting no module named jiter.jiter even with pydantic and openai in my req txt\r\n\r\nYou need either outdated pydantic 1 or install jiter (https://pypi.org/project/jiter/)\r\n\r\n@RobertCraigie please lock this issue, so people won't post unrelated comments.\r\n"
      }
    ],
    "url": "https://github.com/openai/openai-python/issues/1616"
  },
  {
    "repository": "openai/openai-python",
    "issue_number": 1614,
    "title": "Support pydantic dataclasses in structured outputs",
    "body": "### Confirm this is a feature request for the Python library and not the underlying OpenAI API.\r\n\r\n- [X] This is a feature request for the Python library\r\n\r\n### Describe the feature or improvement you're requesting\r\n\r\nIt would be great to be able to use data schemas defined as [pydantic dataclasses](https://docs.pydantic.dev/latest/concepts/dataclasses/) in structured outputs. E.g.\r\n```python\r\n\r\nfrom pydantic.dataclasses import dataclass\r\nfrom openai import OpenAI\r\n\r\nclient = OpenAI()\r\n\r\n@dataclass\r\nclass CalendarEvent:\r\n    name: str\r\n    date: str\r\n    participants: list[str]\r\n\r\ncompletion = client.beta.chat.completions.parse(\r\n    model=\"gpt-4o-2024-08-06\",\r\n    messages=[\r\n        {\"role\": \"system\", \"content\": \"Extract the event information.\"},\r\n        {\"role\": \"user\", \"content\": \"Alice and Bob are going to a science fair on Friday.\"},\r\n    ],\r\n    response_format=CalendarEvent,\r\n)\r\n\r\nevent = completion.choices[0].message.parsed\r\n```\r\n\r\nPydantic dataclasses can be easily transformed into JSON schema via the [model_json_schema](https://docs.pydantic.dev/latest/api/json_schema/#pydantic.json_schema.model_json_schema) function.\r\n\r\n### Additional context\r\n\r\n_No response_",
    "state": "closed",
    "created_at": "2024-08-07T19:03:42+00:00",
    "closed_at": "2024-08-20T22:54:05+00:00",
    "updated_at": "2024-08-20T22:54:05+00:00",
    "author": "9dogs",
    "author_type": "User",
    "comments_count": 3,
    "reactions_count": 0,
    "labels": "enhancement",
    "milestone": null,
    "closed_by": "RobertCraigie",
    "resolution_time_hours": 315.83972222222224,
    "first_comments": [
      {
        "author": "UtsavChokshiCNU",
        "created_at": "2024-08-07T19:32:11+00:00",
        "body": "@9dogs ... Are you able to even run the code from their tutorial with pydanitc?\r\nIt returns me an error : \r\n`AttributeError: 'Completions' object has no attribute 'parse'`\r\n"
      },
      {
        "author": "parth126",
        "created_at": "2024-08-08T11:54:39+00:00",
        "body": "@UtsavChokshiCNU You need to update the api version to the latest version. Older version did not have parse attribute"
      },
      {
        "author": "RobertCraigie",
        "created_at": "2024-08-20T22:54:05+00:00",
        "body": "Support for `pydantic.dataclasses.dataclass` has been added in `v1.42.0`"
      }
    ],
    "url": "https://github.com/openai/openai-python/issues/1614"
  }
]