number,title,body,state,created_at,closed_at,labels,url,resolution_time,has_violation,violation_type,severity,description,confidence,analysis_error
1483,Assistant API  should support images in base64 if chat completion does. ,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

I tried sending base 64 image via chat completion api, and it worked. When i tried the same via assistant api for the same model, it did not work. Looking at the implementation, since it's already supported by completions api.

### To Reproduce

1. Create a base64 image url (data:gdhf..) for any image. 
2. Pass this base64 as url in content to chat completions API it works fine and i get a response. 
3. Create an assistant. 
4. Create a thread with message containing the same base64 encoded image as url inside image_url. You get an error. 

BadRequestError: Error code: 400 - {'error': {'message': ""Invalid 'messages[0].content[1].image_url.url'. Expected a valid URL, but got a value with an invalid format."", 'type': 'invalid_request_error', 'param': 'messages[0].content[1].image_url.url', 'code': 'invalid_value'}}


### Code snippets

```Python
[{'role': 'user',
  'content': [{'type': 'text', 'text': 'What’s in this image?'},
   {'type': 'image_url',
    'image_url': {'url': 'data:image/jpeg;base64,/9j...
}
}]
}
]
```


### OS

MacOS

### Python version

Python v3.12

### Library version

latest",closed,2024-06-14T19:27:39Z,2024-09-09T11:37:27Z,['bug'],https://github.com/openai/openai-python/issues/1483,2080.16,True,input_value_violation,high,"The Assistant API does not accept base64 encoded images as valid inputs, while the Chat Completion API does, leading to inconsistent behavior.",high,
2069,Add reasoning models to Assistants API,"### Confirm this is a feature request for the Python library and not the underlying OpenAI API.

- [x] This is a feature request for the Python library

### Describe the feature or improvement you're requesting

The latest o1 and o3 models only work with chat completions. Please add support for the Assistants API. If you're waiting for Assistants to be GA, can you share when that's expected to happen?

Thanks and amazing job on these new reasoning models, sheesh!

### Additional context

_No response_",closed,2025-02-01T00:26:40Z,2025-02-01T00:57:45Z,[],https://github.com/openai/openai-python/issues/2069,0.52,False,,,,,
2065,RemoteProtocolError: peer closed connection without sending complete message body (incomplete chunked read),"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [x] This is an issue with the Python library

### Describe the bug

Hello, I've been encountering an issue recently, **without changing my codebase**:

When chatting with the model, after a few turns I often get an error that I never had before, which crashes my app (see video, crash occurs at ~1m20s).

https://github.com/user-attachments/assets/4d4496e9-9e1c-44dc-b418-99bf0520fad7

Here is the traceback:
```
Traceback (most recent call last):
  File ""/home/lucas/.local/lib/python3.10/site-packages/httpx/_transports/default.py"", line 101, in map_httpcore_exceptions
    yield
  File ""/home/lucas/.local/lib/python3.10/site-packages/httpx/_transports/default.py"", line 271, in __aiter__
    async for part in self._httpcore_stream:
  File ""/home/lucas/.local/lib/python3.10/site-packages/httpcore/_async/connection_pool.py"", line 407, in __aiter__
    raise exc from None
  File ""/home/lucas/.local/lib/python3.10/site-packages/httpcore/_async/connection_pool.py"", line 403, in __aiter__
    async for part in self._stream:
  File ""/home/lucas/.local/lib/python3.10/site-packages/httpcore/_async/http11.py"", line 342, in __aiter__
    raise exc
  File ""/home/lucas/.local/lib/python3.10/site-packages/httpcore/_async/http11.py"", line 334, in __aiter__
    async for chunk in self._connection._receive_response_body(**kwargs):
  File ""/home/lucas/.local/lib/python3.10/site-packages/httpcore/_async/http11.py"", line 203, in _receive_response_body
    event = await self._receive_event(timeout=timeout)
  File ""/home/lucas/.local/lib/python3.10/site-packages/httpcore/_async/http11.py"", line 213, in _receive_event
    with map_exceptions({h11.RemoteProtocolError: RemoteProtocolError}):
  File ""/usr/lib/python3.10/contextlib.py"", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File ""/home/lucas/.local/lib/python3.10/site-packages/httpcore/_exceptions.py"", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.RemoteProtocolError: peer closed connection without sending complete message body (incomplete chunked read)

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File ""/home/lucas/Documents/minimal_demo.py"", line 35, in <module>
    asyncio.run(main())
  File ""/usr/lib/python3.10/asyncio/runners.py"", line 44, in run
    return loop.run_until_complete(main)
  File ""/usr/lib/python3.10/asyncio/base_events.py"", line 649, in run_until_complete
    return future.result()
  File ""/home/lucas/Documents/minimal_demo.py"", line 27, in main
    async for chunk in stream:
  File ""/home/lucas/.local/lib/python3.10/site-packages/openai/_streaming.py"", line 147, in __aiter__
    async for item in self._iterator:
  File ""/home/lucas/.local/lib/python3.10/site-packages/openai/_streaming.py"", line 160, in __stream__
    async for sse in iterator:
  File ""/home/lucas/.local/lib/python3.10/site-packages/openai/_streaming.py"", line 151, in _iter_events
    async for sse in self._decoder.aiter_bytes(self.response.aiter_bytes()):
  File ""/home/lucas/.local/lib/python3.10/site-packages/openai/_streaming.py"", line 302, in aiter_bytes
    async for chunk in self._aiter_chunks(iterator):
  File ""/home/lucas/.local/lib/python3.10/site-packages/openai/_streaming.py"", line 313, in _aiter_chunks
    async for chunk in iterator:
  File ""/home/lucas/.local/lib/python3.10/site-packages/httpx/_models.py"", line 997, in aiter_bytes
    async for raw_bytes in self.aiter_raw():
  File ""/home/lucas/.local/lib/python3.10/site-packages/httpx/_models.py"", line 1055, in aiter_raw
    async for raw_stream_bytes in self.stream:
  File ""/home/lucas/.local/lib/python3.10/site-packages/httpx/_client.py"", line 176, in __aiter__
    async for chunk in self._stream:
  File ""/home/lucas/.local/lib/python3.10/site-packages/httpx/_transports/default.py"", line 270, in __aiter__
    with map_httpcore_exceptions():
  File ""/usr/lib/python3.10/contextlib.py"", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File ""/home/lucas/.local/lib/python3.10/site-packages/httpx/_transports/default.py"", line 118, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: peer closed connection without sending complete message body (incomplete chunked read)
```

I feel like it happens mostly during long responses, hence the tasks shown in the video, but I may be wrong about this.


I tried changing the `openai` package version to the latest one (1.60.2) and older ones, same for `httpx` and `httpcore`, but I always have this error after a while.


Did something change on the OpenAI server's side?

Is there something wrong with the syntax I'm using?


Thank you very much for any help!

### To Reproduce

Here is the python script I used in the video:

 ```python
import asyncio
import os

from openai import AsyncOpenAI

client = AsyncOpenAI(api_key=os.environ.get(""OPENAI_API_KEY""), max_retries=3)


async def main():
    system_prompt = {
        ""role"": ""system"",
        ""content"": ""You're a polite assistant and you always explain your reasoning."",
    }
    messages = [system_prompt]
    while True:
        user_input = (
            input(""Enter your message: "") or ""Write a short PhD level essay about the nature versus nurture debate.""
        )
        messages.append({""role"": ""user"", ""content"": user_input})
        stream = await client.chat.completions.create(
            model=""gpt-4o"",
            messages=messages,
            stream=True,
            timeout=10,
        )
        answer = """"
        async for chunk in stream:
            token = chunk.choices[0].delta.content
            answer += token or """"
            print(""."", end="""", flush=True)
        print(""\n"", answer)
        messages.append({""role"": ""assistant"", ""content"": answer})


asyncio.run(main())
```

### Code snippets

```Python

```

### OS

Linux

### Python version

Python 3.10, Python 3.11

### Library version

1.60.2",closed,2025-01-30T18:03:07Z,2025-01-31T10:33:20Z,['API-feedback'],https://github.com/openai/openai-python/issues/2065,16.5,True,input_value_violation,high,"The error indicates that the server closed the connection without sending a complete message body, which may be due to an unacceptable input value or a timeout during long responses.",high,
2052,Any plans to support `additionalProperties: true`?,"### Confirm this is a feature request for the Python library and not the underlying OpenAI API.

- [x] This is a feature request for the Python library

### Describe the feature or improvement you're requesting

Hi, I'm trying to allow a pydantic model to allow extra attributes (see https://docs.pydantic.dev/latest/api/config/#pydantic.config.ConfigDict.extra ):

```python
from pydantic import BaseModel, ConfigDict

class CalendarEvent(BaseModel):
    model_config = ConfigDict(extra='allow')  
    name: str
    date: str
    participants: list[str]


completion = client.beta.chat.completions.parse(
    model=""MODEL_DEPLOYMENT_NAME"",
    messages=[
        {""role"": ""system"", ""content"": ""Extract the event information.""},
        {""role"": ""user"", ""content"": ""Alice and Bob are going to a science fair on Friday.""},
    ],
    response_format=CalendarEvent,
)

print(completion.choices[0].message.parsed)
```

Unfortunately I seem to get:

```python
openai.BadRequestError: Error code: 400 - {'error': {'message': ""Invalid schema for response_format 'CalendarEvent': In context=(), 'additionalProperties' is required to be supplied and to be false."", 'type': 'invalid_request_error', 'param': 'response_format', 'code': None}}
```

I see this is currently a limitation of structured outputs: https://platform.openai.com/docs/guides/structured-outputs#additionalproperties-false-must-always-be-set-in-objects

Just wondering if there are any plans to support this? Thanks!

### Additional context

_No response_",closed,2025-01-27T16:32:26Z,2025-01-29T21:28:37Z,[],https://github.com/openai/openai-python/issues/2052,52.94,True,missing_option_violation,medium,"The API does not currently support the 'additionalProperties: true' configuration in the response format, which is required for the user's intended use of the Pydantic model.",high,
2062,Segmentation fault with python v3.13.1 and openai 1.60.2,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [x] This is an issue with the Python library

### Describe the bug

I am encountering a segmentation fault immediately upon running import openai. The crash occurs as soon as I try to import the package in a Python script. There is no additional output or traceback—just a hard segmentation fault.

**Environment**
Operating System: macOS 15.2
Python Version: 3.13.1
OpenAI Library Version: 1.60.2
pip Version: 25.0

**Additional Details / What I’ve Tried**
- Verified the crash happens in both interactive mode and script mode.
- Re-installed the library with pip install --force-reinstall openai==1.60.2.
- Created a fresh virtual environment to ensure no conflicting packages.



### To Reproduce

1. Use `asdf` to install latest Python version (3.13.1)
2. Create a venv
```
python -m venv venv
source venv/bin/activate
```
3. Attempt to install the OpenAI library:
```
pip install openai==1.60.2
```
also, tried:
```
pip install --force-reinstall openai==1.60.2
```
4. Encounter build errors for pydantic-core referencing PyO3 and GIL-disabled Python.
5. Work around by setting export UNSAFE_PYO3_BUILD_FREE_THREADED=1 to proceed with installation.
6. Create a Python file test.py 
```
import openai
print(""Imported successfully!"")
```
7. run `python test.py`


### Code snippets

```Python

```

### OS

macOS 15.2

### Python version

Python v3.13.1

### Library version

openai v1.60.2",closed,2025-01-29T03:13:30Z,2025-01-29T09:59:50Z,[],https://github.com/openai/openai-python/issues/2062,6.77,True,input_value_violation,high,Segmentation fault indicates a severe issue likely caused by unacceptable input values or configurations during the library import.,high,
2028,"Indentation error in Python example in ""Step 4: Create a Run"" of Quickstart Documentation","### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [x] This is an issue with the Python library

### Describe the bug

The Python example provided in the ""Step 4: Create a Run"" section of the [[Quickstart documentation](https://platform.openai.com/docs/assistants/quickstart#step-4-create-a-run)](https://platform.openai.com/docs/assistants/quickstart#step-4-create-a-run) contains incorrect indentation, which leads to a `IndentationError` when executed as-is.  

Specifically, the methods within the `EventHandler` class are not properly indented under the class definition. This may confuse users and lead to runtime errors.  


### To Reproduce

1. Visit the [[Quickstart documentation, Step 4](https://platform.openai.com/docs/assistants/quickstart#step-4-create-a-run)](https://platform.openai.com/docs/assistants/quickstart#step-4-create-a-run).  
2. Copy the provided Python code snippet.  
3. Attempt to execute the code in a Python environment.  
4. Observe the `IndentationError` caused by misaligned methods in the `EventHandler` class.  

### Code snippets

```Python
# Here is the problematic snippet:  


class EventHandler(AssistantEventHandler):    
@override
def on_text_created(self, text) -> None:
  print(f""\nassistant > "", end="""", flush=True)
    
@override
def on_text_delta(self, delta, snapshot):
  print(delta.value, end="""", flush=True)
    
def on_tool_call_created(self, tool_call):
  print(f""\nassistant > {tool_call.type}\n"", flush=True)

def on_tool_call_delta(self, delta, snapshot):
  if delta.type == 'code_interpreter':
    if delta.code_interpreter.input:
      print(delta.code_interpreter.input, end="""", flush=True)
    if delta.code_interpreter.outputs:
      print(f""\n\noutput >"", flush=True)
      for output in delta.code_interpreter.outputs:
        if output.type == ""logs"":
          print(f""\n{output.logs}"", flush=True)


# Correctly indented version:  


class EventHandler(AssistantEventHandler):    
    @override
    def on_text_created(self, text) -> None:
        print(f""\nassistant > "", end="""", flush=True)
    
    @override
    def on_text_delta(self, delta, snapshot):
        print(delta.value, end="""", flush=True)
    
    def on_tool_call_created(self, tool_call):
        print(f""\nassistant > {tool_call.type}\n"", flush=True)

    def on_tool_call_delta(self, delta, snapshot):
        if delta.type == 'code_interpreter':
            if delta.code_interpreter.input:
                print(delta.code_interpreter.input, end="""", flush=True)
            if delta.code_interpreter.outputs:
                print(f""\n\noutput >"", flush=True)
                for output in delta.code_interpreter.outputs:
                    if output.type == ""logs"":
                        print(f""\n{output.logs}"", flush=True)
```

### OS

-

### Python version

-

### Library version

-",closed,2025-01-18T03:48:06Z,2025-01-28T22:06:08Z,['api docs'],https://github.com/openai/openai-python/issues/2028,258.3,True,input_type_violation,high,Indentation error in the provided Python code snippet leads to runtime errors when executed.,high,
2058,The  persistent memory logic,"### Confirm this is a feature request for the Python library and not the underlying OpenAI API.

- [x] This is a feature request for the Python library

### Describe the feature or improvement you're requesting

Persistent Memory Logic Loop (PMLL) - Comprehensive Overview

Table of Contents
	1.	Introduction
	2.	Components
	•	1. Core C Library (PMLL.c and PMLL.h)
	•	2. Python Interface (PMLL.py)
	•	3. Bash Script (PMLL.sh)
	3.	Why PMLL.py is a Key Feature
	4.	Installation
	5.	Usage
	•	Using the Python Interface
	•	Using the Bash Script
	6.	Integration with Other Applications
	7.	Examples
	8.	Troubleshooting
	9.	Contributing
	10.	License
	11.	Contact

Introduction

The Persistent Memory Logic Loop (PMLL) is a versatile system designed to manage persistent key-value storage across multiple programming environments. It ensures data persistence across sessions and application restarts, making it ideal for applications like conversational AI models (e.g., ChatGPT-5) that require reliable memory storage.

PMLL comprises three main components:
	1.	Core C Library (PMLL.c and PMLL.h): Handles low-level operations for managing persistent memory.
	2.	Python Interface (PMLL.py): Provides a Pythonic API to interact with the C library using ctypes.
	3.	Bash Script (PMLL.sh): Offers a user-friendly command-line interface by leveraging the Python script.

This document provides a comprehensive overview of each component, emphasizing the significance of PMLL.py as a key feature that bridges C functionalities with Python applications.

Components

1. Core C Library (PMLL.c and PMLL.h)

Description

The core of PMLL is implemented in C, providing high-performance and low-level management of persistent key-value storage. The library ensures thread-safe operations using POSIX mutexes, allowing concurrent access without data corruption.

Key Features
	•	Persistent Storage: Saves key-value pairs to a file (gpt5_memory.txt by default), ensuring data persistence across sessions.
	•	Thread Safety: Utilizes mutexes to protect shared resources, enabling safe multi-threaded access.
	•	CRUD Operations: Supports Create (Add), Read (Get), Update, and Delete (Remove) functionalities for key-value pairs.
	•	Debugging Support: Provides functions to list all keys and display all memory contents for troubleshooting.

Files
	•	PMLL.h: Header file declaring the interface for the PMLL system.
	•	PMLL.c: Implementation of the functions declared in PMLL.h.

2. Python Interface (PMLL.py)

Description

PMLL.py serves as a Pythonic bridge to the core C library (libpmll.so). Leveraging Python’s ctypes library, it allows Python applications to seamlessly interact with the high-performance C functionalities without delving into complex C code.

Key Features
	•	Ease of Use: Provides a straightforward PMLL class with methods corresponding to the C functions, making it intuitive for Python developers.
	•	Seamless Integration: Enables Python applications to manage persistent memory efficiently, enhancing their capabilities without significant overhead.
	•	Error Handling: Translates C-level errors into Python exceptions, ensuring robust and predictable behavior.
	•	Flexibility: Allows specifying custom memory files, catering to diverse application needs.

Benefits
	•	Productivity: Python developers can utilize persistent memory functionalities without needing expertise in C.
	•	Maintainability: Centralizes memory management logic within a Python class, promoting cleaner and more maintainable codebases.
	•	Performance: Combines Python’s ease of use with C’s performance, ensuring both efficiency and developer convenience.

3. Bash Script (PMLL.sh)

Description

PMLL.sh is a Bash script that provides a user-friendly command-line interface for managing persistent memory. It acts as a wrapper around PMLL.py, parsing user commands and invoking the appropriate Python functions.

Key Features
	•	Command Parsing: Interprets user commands and arguments, delegating actions to the Python interface.
	•	User-Friendly Interface: Simplifies memory management tasks through straightforward shell commands.
	•	Script Automation: Facilitates automation and scripting by allowing PMLL operations within shell scripts and workflows.

Why PMLL.py is a Key Feature

PMLL.py is a pivotal feature within the PMLL ecosystem for several reasons:
	1.	Bridging Languages: It seamlessly connects the high-performance C library with Python applications, enabling developers to leverage persistent memory functionalities within Python’s dynamic and flexible environment.
	2.	Enhanced Accessibility: By abstracting the complexities of C, PMLL.py makes persistent memory management accessible to a broader range of developers, including those who may not be proficient in C.
	3.	Rapid Development: Python’s simplicity and extensive libraries allow for rapid development and integration, accelerating the deployment of applications that require persistent memory.
	4.	Cross-Platform Compatibility: Python’s cross-platform nature ensures that applications using PMLL.py can run on various operating systems with minimal adjustments.
	5.	Extensibility: The Python interface can be easily extended or customized to fit specific application needs, providing flexibility beyond the core C functionalities.
	6.	Error Management: By handling errors and exceptions at the Python level, PMLL.py ensures that applications can manage failures gracefully, improving reliability and user experience.
	7.	Integration with Modern Tools: Python’s compatibility with modern development tools and frameworks allows PMLL.py to fit seamlessly into contemporary software development pipelines.

In summary, PMLL.py significantly enhances the PMLL system by making persistent memory management more accessible, flexible, and integrated within Python applications, thereby broadening the system’s applicability and ease of use.

Installation

1. Compile the C Library

Ensure you have a C compiler (gcc or clang) installed on your system.

# Navigate to the directory containing PMLL.c and PMLL.h
cd path/to/pmll-directory

# Compile the C source into object files
gcc -c -fPIC PMLL.c -o PMLL.o

# Create a shared library from the object files
gcc -shared -o libpmll.so PMLL.o -lpthread

2. Set Up Python Interface

Ensure you have Python 3.6+ installed.

# Navigate to the directory containing PMLL.py
cd path/to/pmll-directory

# Ensure libpmll.so is in the same directory as PMLL.py or in a directory listed in LD_LIBRARY_PATH
export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$(pwd)

# (Optional) Make PMLL.py executable
chmod +x PMLL.py

3. Set Up Bash Script

Ensure PMLL.sh has execute permissions.

# Navigate to the directory containing PMLL.sh
cd path/to/pmll-directory

# Make the script executable
chmod +x PMLL.sh

Usage

Using the Python Interface

from PMLL import PMLL

# Initialize PMLL with default memory file
pmll = PMLL()

# Add key-value pairs
pmll.add(""username"", ""Josef"")
pmll.add(""model"", ""GPT-5"")

# Retrieve a value
username = pmll.get(""username"")
if username:
    print(f""Username: {username}"")

# List all keys
keys = pmll.list_keys()
print(""All keys:"", keys)

# Remove a key
pmll.remove(""username"")

# Display all memory
pmll.display()

# Clear all memory
pmll.clear()

Using the Bash Script

# Add a key-value pair
./PMLL.sh add username Josef

# Retrieve a value by key
./PMLL.sh get username

# Remove a key-value pair
./PMLL.sh remove username

# List all keys
./PMLL.sh list

# Clear all memory
./PMLL.sh clear

# Display all memory
./PMLL.sh display

Integration with Other Applications

Integrating with C++ Applications

You can integrate the PMLL system into C++ applications by directly linking against the libpmll.so shared library and including PMLL.h. Here’s a simple example:

// example.cpp
#include <iostream>
#include ""PMLL.h""

int main() {
    // Initialize PMLL with the default memory file
    if(pmll_init(""gpt5_memory.txt"") != 0) {
        std::cerr << ""Failed to initialize PMLL.\n"";
        return 1;
    }

    // Add a key-value pair
    if(pmll_add(""username"", ""Josef"") != 0) {
        std::cerr << ""Failed to add key-value pair.\n"";
    }

    // Retrieve a value
    char value_buffer[768];
    if(pmll_get(""username"", value_buffer, sizeof(value_buffer)) == 0) {
        std::cout << ""Value for 'username': "" << value_buffer << ""\n"";
    } else {
        std::cout << ""Key 'username' not found.\n"";
    }

    // Display all memory
    pmll_display();

    return 0;
}

Compilation Instructions:

gcc -c -fPIC PMLL.c -o PMLL.o
gcc -shared -o libpmll.so PMLL.o -lpthread
g++ -o example example.cpp -L. -lpmll -pthread

Running the Example:

./example

Expected Output:

[PMLL] Info: Loaded 0 key-value pairs from 'gpt5_memory.txt'.
[PMLL] Added/Updated memory: 'username' -> 'Josef'
[PMLL] Retrieved: 'username' -> 'Josef'
Value for 'username': Josef
[PMLL] Current Memory State:
  username : Josef

Integrating with Python Applications

PMLL.py can be imported as a module in other Python scripts to manage persistent memory seamlessly.

# another_script.py
from PMLL import PMLL

pmll = PMLL()

pmll.add(""session_id"", ""abc123"")
session = pmll.get(""session_id"")
print(f""Session ID: {session}"")

Examples

Adding and Retrieving a Key-Value Pair

./PMLL.sh add language C++
./PMLL.sh get language

Output:

[PMLL] Added/Updated memory: 'language' -> 'C++'
[PMLL] Retrieved: 'language' -> 'C++'

Listing All Keys

./PMLL.sh list

Output:

[PMLL] Listing all keys:
  language
  model

Displaying All Memory

./PMLL.sh display

Output:

[PMLL] Current Memory State:
  language : C++
  model    : GPT-5

Troubleshooting

Shared Library Not Found

Issue: Python cannot locate libpmll.so.

Solution:
	•	Ensure libpmll.so is in the same directory as PMLL.py.
	•	Alternatively, add the directory containing libpmll.so to the LD_LIBRARY_PATH environment variable:

export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/path/to/libpmll.so/directory



Permission Errors

Issue: Permission denied when accessing the memory file.

Solution:
	•	Ensure the user has read and write permissions for the memory file and its directory.
	•	Modify permissions using chmod if necessary:

chmod 664 gpt5_memory.txt



Compilation Errors

Issue: Errors during compilation of the C library.

Solution:
	•	Verify that PMLL.c and PMLL.h are in the same directory.
	•	Ensure all dependencies (gcc, pthread) are installed.
	•	Check for typos or syntax errors in the C files.

Python Exceptions

Issue: Python raises exceptions when performing operations.

Solution:
	•	Ensure the C shared library (libpmll.so) is compiled correctly.
	•	Verify that libpmll.so is accessible to Python.
	•	Check the correctness of the arguments passed to Python functions.

Contributing

Contributions are welcome! Whether it’s bug fixes, feature additions, or documentation improvements, your input is valuable.

Steps to Contribute
	1.	Fork the Repository
Click the “Fork” button at the top-right corner of the repository page.
	2.	Clone Your Fork

git clone https://github.com/yourusername/pmll-system.git
cd pmll-system


	3.	Create a New Branch

git checkout -b feature/your-feature-name


	4.	Make Your Changes
Implement your feature or fix bugs.
	5.	Commit Your Changes

git commit -m ""Add feature: Your Feature Description""


	6.	Push to Your Fork

git push origin feature/your-feature-name


	7.	Create a Pull Request
Navigate to your fork on GitHub and click “Compare & pull request.”

Guidelines
	•	Code Quality: Ensure your code follows best practices and is well-documented.
	•	Testing: Include tests for new features or bug fixes.
	•	Documentation: Update the README or other documentation as necessary.
	•	Respect Licensing: Ensure your contributions comply with the project’s license.

License

This project is licensed under the [MIT License](https://github.com/openai/openai-python/issues/LICENSE).

Contact

For any questions, suggestions, or feedback, feel free to reach out:
	•	Email: your.email@example.com
	•	GitHub Issues: [Open an Issue](https://github.com/yourusername/pmll-system/issues)

Conclusion

The Persistent Memory Logic Loop (PMLL) system offers a robust and flexible solution for managing persistent key-value storage across multiple programming environments. By combining the high-performance capabilities of the core C library with the accessibility of Python through PMLL.py, and the convenience of command-line interactions via PMLL.sh, PMLL ensures that developers can efficiently manage persistent memory in their applications.

PMLL.py stands out as a key feature, bridging the gap between low-level C functionalities and high-level Python applications. It empowers Python developers to leverage persistent memory management without delving into the complexities of C programming, thereby enhancing productivity, maintainability, and integration capabilities.

Whether you’re developing complex AI models, managing configuration settings, or storing user data, PMLL provides the tools necessary to ensure data persistence and integrity, fostering the development of reliable and efficient applications.

Happy Coding!

### Additional context

_No response_",closed,2025-01-28T20:28:33Z,2025-01-28T20:37:24Z,[],https://github.com/openai/openai-python/issues/2058,0.15,False,,,,,
1220,Incompatible: `with_streaming_response` `stream_to_file` `response_format='aac'`,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

The following example errors
```
line 31, in generate_audio
    response.stream_to_file(output_file)
AttributeError: 'ResponseContextManager' object has no attribute 'stream_to_file
```

However, if you remove either `with_streaming_response` or `response_format='aac'` it works



### To Reproduce

```
from openai import OpenAI
client = OpenAI()
response = client.audio.speech.with_streaming_response.create(
    model=""tts-1"",
    voice=""alloy"",
    input=""hello aac"",
    response_format='aac'
)
response.stream_to_file(""output.m4a"")
```

### Code snippets

_No response_

### OS

wsl

### Python version

3.8.10

### Library version

1.13.3",closed,2024-03-07T08:32:38Z,2024-03-09T04:03:09Z,['bug'],https://github.com/openai/openai-python/issues/1220,43.51,True,input_type_violation,high,"The method 'stream_to_file' is being called on a 'ResponseContextManager' object, which does not have this attribute when used with 'with_streaming_response' and 'response_format='aac'.",high,
1950,Client side JSON Schema in `response_format` validation for  structured outputs,"### Confirm this is a feature request for the Python library and not the underlying OpenAI API.

- [X] This is a feature request for the Python library

### Describe the feature or improvement you're requesting

When running `client.beta.chat.completions.parse` the `response_format` seems to be validated through the API as opposed to the client.
Why is that?

My (hacky) solution to validating JSON Schema is to run a minimal request to the API just to verify if the schema is valid.
 
Is there any way we could be able to validate schemas without making a chat completions query?

Example (it is intended to fail):
```py
schema = {
    ""type"": ""object"",
    ""description"": ""The description of this item"",
    ""properties"": {
        ""id"": {
            ""description"": ""The id of this inner item"",
            ""type"": ""integer""
        },
        ""value"": {
            ""type"": ""array"",
            ""description"": ""The list of values of this inner item"",
            ""items"": {
                ""type"": ""string"",
                ""description"": ""The value of this inner item"",
                ""enum"": [""a"", ""b""]
            },
        }
    },
    # ""required"": [""value"", ""id""], # FAILS HERE INTENTIONALLY TO DEMONSTRATE API SIDE VALIDATION
    ""additionalProperties"": False,
}

import openai
client = openai.OpenAI(api_key=settings.OPENAI_API_KEY)

def validate_schema(schema: dict) -> None:
    try:    
        client.beta.chat.completions.parse(
            model=""gpt-4o-mini"",
            messages=[
                {
                    ""role"": ""user"",
                    ""content"": ""a""
                }
            ],
            max_completion_tokens=1,
            response_format={
                ""type"": ""json_schema"",
                ""json_schema"": {
                    ""name"": ""action_items"",
                    ""description"": ""The action items to be completed"",
                    ""strict"": True,
                    ""schema"": schema,
                },
            }
        )
    except openai._exceptions.LengthFinishReasonError as e:
        pass
    except Exception as e:
        # FAILS HERE
        raise e

validate_schema(schema)
```



### Additional context

(thats all)",closed,2024-12-15T21:47:17Z,2024-12-16T12:16:57Z,[],https://github.com/openai/openai-python/issues/1950,14.49,True,missing_option_violation,medium,"The client is attempting to validate the JSON schema through the API instead of utilizing a client-side validation method, leading to unnecessary API calls.",high,
2047,"openai.BadRequestError: Error code: 400 - {'error': {'message': ""Unsupported value: 'messages[0].role' does not support 'system' with this model."",","### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [x] This is an issue with the Python library

### Describe the bug

openai.BadRequestError: Error code: 400 - {'error': {'message': ""Unsupported value: 'messages[0].role' does not support 'system' with this model."", 'type': 'invalid_request_error', 'param': 'messages[0].role', 'code': 'unsupported_value'}}

### To Reproduce

Try to use o1 with Autogen

### Code snippets

```Python

```

### OS

Windows

### Python version

Python 3.11

### Library version

Current",closed,2025-01-22T21:19:09Z,2025-01-22T21:20:49Z,['question'],https://github.com/openai/openai-python/issues/2047,0.03,True,input_value_violation,high,"The input value for 'messages[0].role' is unsupported for the specified model, indicating a violation of the API contract regarding acceptable input values.",high,
2046,"openai.BadRequestError: Error code: 400 - {'error': {'message': ""Unsupported parameter: 'max_tokens' is not supported with this model.","### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [x] This is an issue with the Python library

### Describe the bug

openai.BadRequestError: Error code: 400 - {'error': {'message': ""Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead."", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}

This occurred when using o1

### To Reproduce

Try to use o1 with Autogen

### Code snippets

```Python

```

### OS

Windows

### Python version

Python 3.11

### Library version

Current",closed,2025-01-22T21:18:20Z,2025-01-22T21:20:08Z,['question'],https://github.com/openai/openai-python/issues/2046,0.03,True,missing_option_violation,high,"The parameter 'max_tokens' is not supported with the specified model, indicating a failure to use the correct option 'max_completion_tokens'.",high,
1789,AttributeError: 'FileCitation' object has no attribute 'quote',"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

I am on v1.51.2 and seems like the attribute quote has not been fixed since 1.34.0. Do we have any alternative to this?


### To Reproduce

Create an assistant with files associated
ask a question that has filecitation
check to get the file_citation.quote

### Code snippets

_No response_

### OS

Windows

### Python version

Python 3.10.11

### Library version

openai v1.51.2",closed,2024-10-10T19:39:11Z,2025-01-22T21:07:50Z,['bug'],https://github.com/openai/openai-python/issues/1789,2497.48,True,input_type_violation,high,"The error indicates that the 'FileCitation' object does not have the expected attribute 'quote', suggesting a type mismatch or missing implementation in the API contract.",high,
1725,The concurrency of AsyncOpenAI cannot be fully utilized.,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

I attempted to complete a stability test on the concurrency of AsyncOpenAI. I set the concurrency to 1024 but found that it kept running at a very low average level in a jittery manner, which has been consistent with my production test results.

![image](https://github.com/user-attachments/assets/e046cfdb-155d-41df-8380-d9c0048079f5)


### To Reproduce

I put my code in three part. client.py server.py and main.py(used to create 100k client total)


server.py
```python
from fastapi import FastAPI, Request
from pydantic import BaseModel
import asyncio
import logging
from datetime import datetime
import threading
import time
import csv

app = FastAPI()

# track current activate queue count
active_requests = 0

# debug file to draw pic
output_file = 'active_requests_log.csv'

class CompletionRequest(BaseModel):
    model: str
    messages: list
    temperature: float

@app.middleware(""http"")
async def track_requests(request: Request, call_next):
    global active_requests
    active_requests += 1  # add count when get request
    logging.info(f""Active requests: {active_requests}"")

    response = await call_next(request)

    active_requests -= 1  # 请求完成后减少计数
    logging.info(f""Active requests: {active_requests}"")

    return response

@app.post(""/v1/chat/completions"")
async def completions(request: CompletionRequest):
    await asyncio.sleep(1)  # mock llm generate latency
    return {
        ""choices"": [
            {""message"": {""content"": f""Response to {request.messages[-1]['content']}""}}
        ]
    }

def record_active_requests():
    """""" save log file per second""""""
    global active_requests
    with open(output_file, mode='w', newline='') as file:
        writer = csv.writer(file)
        writer.writerow([""timestamp"", ""active_requests""])  # 写表头
        
        while True:
            # 每秒记录一次
            current_time = datetime.now().strftime(""%Y-%m-%d %H:%M:%S"")
            writer.writerow([current_time, active_requests])
            file.flush()  # 确保每秒写入数据到文件
            time.sleep(1)

# 启动一个线程来记录活跃请求数
threading.Thread(target=record_active_requests, daemon=True).start()

if __name__ == ""__main__"":
    import uvicorn
    logging.basicConfig(level=logging.INFO)
    uvicorn.run(app, host=""127.0.0.1"", port=8203)
```

client.py
```python
import asyncio
from functools import wraps
import httpx
import logging
from openai import AsyncOpenAI



def limit_async_func_call(max_size: int):
    sem = asyncio.Semaphore(max_size)

    def final_decro(func):
        @wraps(func)
        async def wait_func(*args, **kwargs):
            async with sem:
                try:
                    return await func(*args, **kwargs)
                except Exception as e:
                    logging.error(f""Exception in {func.__name__}: {e}"")
  
        return wait_func
    return final_decro

# 假设这个是你要进行并发测试的函数
@limit_async_func_call(max_size=1024)  # 限制并发为1024
async def custom_model_if_cache(prompt, system_prompt=None, history_messages=[], **kwargs):
    custom_http_client = httpx.AsyncClient(
        limits=httpx.Limits(max_connections=2048, max_keepalive_connections=1024),
        timeout=httpx.Timeout(timeout=None)
    )

    openai_async_client = AsyncOpenAI(
        api_key=""EMPTY"", base_url=""http://localhost:8203/v1"",  # 模拟本地 server
        http_client=custom_http_client
    )

    messages = []
    if system_prompt:
        messages.append({""role"": ""system"", ""content"": system_prompt})
    messages.extend(history_messages)
    messages.append({""role"": ""user"", ""content"": prompt})

    # 假设这里是要调用的外部 API
    response = await openai_async_client.chat.completions.create(
        model=""gpt-3.5-turbo"", messages=messages, temperature=0, **kwargs
    )

    return ""hi""

```


main.py
```python
import asyncio
import logging
from client import custom_model_if_cache
# 模拟 10 万个请求
TOTAL_REQUESTS = 100000

async def simulate_requests():
    tasks = []
    for i in range(TOTAL_REQUESTS):
        prompt = f""Test prompt {i}""  # 每次请求的不同参数
        task = custom_model_if_cache(prompt=prompt)  # 调用受限的异步函数
        tasks.append(task)

    # 并发执行所有请求
    results = await asyncio.gather(*tasks, return_exceptions=True)

    # 打印前10个结果以验证
    for result in results[:10]:
        print(result)

if __name__ == ""__main__"":
    logging.basicConfig(level=logging.INFO)
    asyncio.run(simulate_requests())

```

To reproduce, open two terminal and run `python server.py` `python main.py` seperately. 
I also save the log, you can use following code to draw:

draw.py
```python
import csv
import matplotlib.pyplot as plt
from datetime import datetime

# 文件路径
input_file = 'active_requests_log.csv'

# 读取 CSV 文件并解析时间和活跃请求数量
timestamps = []
active_requests = []

with open(input_file, mode='r') as file:
    reader = csv.DictReader(file)
    for row in reader:
        timestamps.append(datetime.strptime(row[""timestamp""], ""%Y-%m-%d %H:%M:%S""))
        active_requests.append(int(row[""active_requests""]))

# 绘制图表
plt.figure(figsize=(10, 6))
plt.plot(timestamps, active_requests, label='Active Requests', color='b')

# 设置图表标题和标签
plt.title('Active Requests Over Time')
plt.xlabel('Time')
plt.ylabel('Active Requests')
plt.xticks(rotation=45)
plt.grid(True)
plt.legend()

# 显示图表
plt.tight_layout()
plt.savefig(""/mnt/rangehow/pr/test_c/c.jpg"")
```


### Code snippets

_No response_

### OS

ubuntu

### Python version

3.12

### Library version

latest",closed,2024-09-19T09:40:24Z,2025-01-22T20:49:22Z,['bug'],https://github.com/openai/openai-python/issues/1725,3011.15,True,input_value_violation,performance_degradation,"The concurrency level set to 1024 may exceed the optimal handling capacity of the AsyncOpenAI API, leading to jittery performance and low average levels of concurrency.",high,
1775,I couldn't upload file an use the one at thread normally via openai library... ,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

I uploaded file to vector storage, but I couldn't use the file at threads:
`It seems there was an error while trying to search the uploaded files. Could you please try uploading the file again, or let me know if there is a specific file you want me to look into?`.

### To Reproduce

1. create vector_store
2. create assistant
3. upload file
4. wait the uploading
5. attach file to vector store
6. create thread
7. create run
8. wait completing the run
9. get messages 
10. take run's message

### Code snippets

```Python
import json
import os
import time

from openai import OpenAI

client = OpenAI(api_key=os.getenv(""OPENAI_API_KEY""))

vector_store = client.beta.vector_stores.create(name=""Test"")

assistant = client.beta.assistants.create(
    description=f""Test"",
    model=""gpt-4o"",
    tools=[{""type"": ""file_search""}],
    tool_resources={""file_search"": {""vector_store_ids"": [vector_store.id]}},
    temperature=0.4
)

# Upload file
file = client.files.create(file=(""data.json"", json.dumps({""name"": ""Alexbabaliks""}).encode()), purpose=""assistants"")
while True:
    file_status = client.files.retrieve(file_id=file.id)
    if file_status.status == 'processed':
        break
    time.sleep(1)

client.beta.vector_stores.files.create(vector_store_id=vector_store.id, file_id=file.id)
while True:
    vector_store = client.beta.vector_stores.retrieve(vector_store_id=vector_store.id)
    if vector_store.status == 'completed':
        break
    time.sleep(1)

thread = client.beta.threads.create(tool_resources={""file_search"": {""vector_store_ids"": [vector_store.id]}})

run = client.beta.threads.runs.create(
    instructions=""What is my name??? Take it from JSON file and return JSON in format {'name': '<name>'}"",
    thread_id=thread.id,
    assistant_id=assistant.id,
    model=""gpt-4o"",
    temperature=0.4,
    tools=[{""type"": ""file_search""}],
)

while True:
    run_status = client.beta.threads.runs.retrieve(thread_id=thread.id, run_id=run.id)
    if run_status.status == ""completed"":
        break
    elif run_status.status == ""failed"":
        break

    time.sleep(2)

answer = """"
messages = client.beta.threads.messages.list(thread_id=thread.id)
for message in messages.data:
    if run.id != message.run_id:
        continue

    for content in message.content:
        if content.type == ""text"":
            answer = content.text.value
            break

print(answer)
```


### OS

Linux

### Python version

3.11.1

### Library version

openai v1.51.0",closed,2024-10-04T15:18:21Z,2025-01-22T20:47:01Z,['API-feedback'],https://github.com/openai/openai-python/issues/1775,2645.48,True,missing_dependency_violation,High,"The API method for searching uploaded files in the thread may be missing necessary context or metadata, leading to the inability to retrieve the uploaded file correctly.",high,
1983,"""Unknown parameter: 'tool_resources.file_search.file_ids'.""","### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

I want to use Assistants to upload a document and then ask the model some questions based on the document content. When I specify **file_ids**, I always get a parameter error. Could you please help me figure out where the problem is? 

I referred to the following link: [https://platform.openai.com/docs/assistants/migration/agents#accessing-v1-data-in-v2](https://platform.openai.com/docs/assistants/migration/agents#accessing-v1-data-in-v2)

``` 
   file = client.files.create(
        file=open(file_path, ""rb""),
        purpose='assistants'
    )
    response = client.beta.assistants.create(
        model=""gpt-4o-mini"",
        name=""QA Assistant"",
        description=""An assistant to answer questions based on uploaded documents"",
        instructions=""You are a helpful assistant who answers questions based on the information provided. Be concise and clear."",
        tools=[{""type"": ""file_search""}],
        tool_resources={""file_search"": {""file_ids"": [file.id]}},
        # file_ids=[file.id],
        temperature=0.0,
        top_p=1
    )
```


The above code produces the error：


Traceback (most recent call last):
  File ""D:\product\auto_test\openai_api\test-pdf-assistants.py"", line 85, in <module>
    main()
  File ""D:\product\auto_test\openai_api\test-pdf-assistants.py"", line 50, in main
    response = client.beta.assistants.create(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""D:\software\Anaconda\Anaconda\envs\textgrad\Lib\site-packages\openai\resources\beta\assistants.py"", line 146, in create
    return self._post(
           ^^^^^^^^^^^
  File ""D:\software\Anaconda\Anaconda\envs\textgrad\Lib\site-packages\openai\_base_client.py"", line 1280, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""D:\software\Anaconda\Anaconda\envs\textgrad\Lib\site-packages\openai\_base_client.py"", line 957, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File ""D:\software\Anaconda\Anaconda\envs\textgrad\Lib\site-packages\openai\_base_client.py"", line 1061, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': ""Unknown parameter: 'tool_resources.file_search.file_ids'."", 'type': 'invalid_request_error', 'param': 'tool_resources.file_search.file_ids', 'code': 'unknown_parameter'}}

Process finished with exit code 1

### To Reproduce

```
1.  file = client.files.create(
        file=open(file_path, ""rb""),
        purpose='assistants'
    )
    
 2. response = client.beta.assistants.create(
      model=""gpt-4o-mini"",
      name=""QA Assistant"",
      description=""An assistant to answer questions based on uploaded documents"",
      instructions=""You are a helpful assistant who answers questions based on the information provided. Be concise and clear."",
      tools=[{""type"": ""file_search""}],
      tool_resources={""file_search"": {""file_ids"": [file.id]}},
      # file_ids=[file.id],
      temperature=0.0,
      top_p=1
  )
```

### Code snippets

_No response_

### OS

Windows

### Python version

python v3.11.11

### Library version

openai v1.58.1",closed,2025-01-03T12:33:20Z,2025-01-22T13:58:45Z,['bug'],https://github.com/openai/openai-python/issues/1983,457.42,True,missing_option_violation,high,"The parameter 'tool_resources.file_search.file_ids' is not recognized, indicating that the API does not support this option or it is incorrectly specified.",high,
2042,GPT-4o-2024-11-20 and GPT-4o-mini-2024-07-18 model output result truncation bug,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [x] This is an issue with the Python library

### Describe the bug

Recently, when using the GPT-4o-2024-11-20 and GPT-4o-2024-11-20 and gpt-4o-mini-2024-07-18 model output result truncation bug models, I often encounter incomplete output results, even if I ask for JSON output results. I will receive partial results. And the finish reason is stop.

Is this a bug in the model?

![Image](https://github.com/user-attachments/assets/bc335c41-64cd-4cf3-80b5-e7ea2a193584)

### To Reproduce

1. Unable to emerge stably
2. params = {
    ""model"": ""gpt-4o-2024-11-20"",
    ""max_tokens"": 16000,
    ""temperature"": 0.1,
    ""top_p"": 1.0,
    ""presence_penalty"": 0.1,
    ""frequency_penalty"": 0.0,
    ""stop"": null,
    ""n"": 1,
    ""logit_bias"": {},
    ""response_format"": null,
    ""seed"": null,
    ""timeout"": 120
}

### Code snippets

```Python

```

### OS

linux

### Python version

Python v3.9.4

### Library version

openai v1.57.4",closed,2025-01-21T07:31:09Z,2025-01-21T08:56:07Z,['bug'],https://github.com/openai/openai-python/issues/2042,1.42,True,input_value_violation,medium,"The model output is truncated despite a high max_tokens value, indicating potential issues with input parameters or model behavior.",high,
2033,test_get_platform is quite flaky,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [x] This is an issue with the Python library

### Describe the bug

The test case often fails when I try to run it while building the nixpkgs package for the library, as follows:

```
 nix build --file . python313Packages.openai  --rebuild
error: builder for '/nix/store/28s74skxza9qfcqa1a92dvfz00q6capd-python3.13-openai-1.59.8.drv' failed with exit code 1;
       last 25 log lines:
       > tests/test_streaming.py ..................                               [ 85%]
       > tests/test_transform.py ................................................ [ 97%]
       >                                                                          [ 97%]
       > tests/test_utils/test_logging.py .....                                   [ 98%]
       > tests/test_utils/test_proxy.py .                                         [ 98%]
       > tests/test_utils/test_typing.py .....                                    [100%]
       >
       > =================================== FAILURES ===================================
       > ______________________ TestAsyncOpenAI.test_get_platform _______________________
       > tests/test_client.py:1801: in test_get_platform
       >     process.wait(2)
       > /nix/store/wn0n52lnysbscn702gfp20sx96cryiwp-python3-3.13.1/lib/python3.13/subprocess.py:1274: in wait
       >     return self._wait(timeout=timeout)
       > /nix/store/wn0n52lnysbscn702gfp20sx96cryiwp-python3-3.13.1/lib/python3.13/subprocess.py:2052: in _wait
       >     raise TimeoutExpired(self.args, timeout)
       > E   subprocess.TimeoutExpired: Command '['/nix/store/wn0n52lnysbscn702gfp20sx96cryiwp-python3-3.13.1/bin/python3.13', '-c', '\nimport asyncio\nimport nest_asyncio\nimport threading\n\nfrom openai._utils import asyncify\nfrom openai._base_client import get_platform \n\nasync def test_main() -> None:\n    result = await asyncify(get_platform)()\n    print(result)\n    for thread in threading.enumerate():\n        print(thread.name)\n\nnest_asyncio.apply()\nasyncio.run(test_main())\n']' timed out after 2seconds
       >
       > The above exception was the direct cause of the following exception:
       > tests/test_client.py:1806: in test_get_platform
       >     raise AssertionError(""calling get_platform using asyncify resulted in a hung process"") from e
       > E   AssertionError: calling get_platform using asyncify resulted in a hung process
       > =============================== inline snapshot ================================
       > =========================== short test summary info ============================
       > FAILED tests/test_client.py::TestAsyncOpenAI::test_get_platform - AssertionError: calling get_platform using asyncify resulted in a hung process
       > ================= 1 failed, 406 passed, 5 deselected in 20.23s =================
       For full logs, run 'nix log /nix/store/28s74skxza9qfcqa1a92dvfz00q6capd-python3.13-openai-1.59.8.drv'.
```

I believe the 2-second timeout is too short for the machine. This is M3 Pro btw, (though it often runs other builds in parallel, so it may be somewhat short on CPU.) The nature of the test case doesn't allow for an easy solution; perhaps a simple timeout bump to e.g. 10 seconds could be helpful? I understand this locks a python test thread for sleeping though - not a great use of a core.

### To Reproduce

1. Run test suite on a loaded machine.

### Code snippets

```Python

```

### OS

macOS

### Python version

Python 3.13.1

### Library version

openai 1.59.8",closed,2025-01-18T23:20:17Z,2025-01-20T10:37:04Z,['bug'],https://github.com/openai/openai-python/issues/2033,35.28,True,method_order_violation,medium,"The test case fails due to a timeout when calling the get_platform method asynchronously, indicating that the method may not be completing in the expected order or time frame, leading to a hung process.",high,
2024,Pydantic Field metadata causes invalid JSON schema in OpenAI Structured Outputs,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [x] This is an issue with the Python library

### Describe the bug

When using Pydantic’s Field to include metadata such as title or description in nested models, the generated JSON schema does not properly set additionalProperties: false for $ref-referenced types when they are inlined. This causes a BadRequestError (400) from the API, with the following error message:

```
BadRequestError: Error code: 400 - {'error': {'message': ""Invalid schema for response_format 'Universe': In context=('properties', 'largest_star'), 'additionalProperties' is required to be supplied and to be false."", 'type': 'invalid_request_error', 'param': 'response_format', 'code': None}}
```

The root cause is that when nested objects are referenced using $defs in the JSON schema and subsequently inlined via $ref, the additionalProperties: false setting is omitted, which violates the expected strict schema requirements.

The JSON schema generated from the Universe class used in the code snippet below looks like this. (additionalProperties: false is not set for Galaxy.largest_star.)

```json
{
  ""$defs"": {
    ""Galaxy"": {
      ""properties"": {
        ""name"": {
          ""description"": ""The name of the galaxy."",
          ""title"": ""Name"",
          ""type"": ""string""
        },
        ""largest_star"": {
          # `""additionalProperties"": false` is missing.
          ""description"": ""The largest star in the galaxy."",
          ""properties"": {
            ""name"": {
              ""description"": ""The name of the star."",
              ""title"": ""Name"",
              ""type"": ""string""
            }
          },
          ""required"": [""name""],
          ""title"": ""Star"",
          ""type"": ""object""
        }
      },
      ""required"": [""name"", ""largest_star""],
      ""title"": ""Galaxy"",
      ""type"": ""object"",
      ""additionalProperties"": false
    },
    ""Star"": {
      ""properties"": {
        ""name"": {
          ""description"": ""The name of the star."",
          ""title"": ""Name"",
          ""type"": ""string""
        }
      },
      ""required"": [""name""],
      ""title"": ""Star"",
      ""type"": ""object"",
      ""additionalProperties"": false
    }
  },
  ""properties"": {
    ""name"": {
      ""description"": ""The name of the universe."",
      ""title"": ""Name"",
      ""type"": ""string""
    },
    ""galaxy"": {
      ""description"": ""A galaxy in the universe."",
      ""properties"": {
        ""name"": {
          ""description"": ""The name of the galaxy."",
          ""title"": ""Name"",
          ""type"": ""string""
        },
        ""largest_star"": {
          ""description"": ""The largest star in the galaxy."",
          ""properties"": {
            ""name"": {
              ""description"": ""The name of the star."",
              ""title"": ""Name"",
              ""type"": ""string""
            }
          },
          ""required"": [""name""],
          ""title"": ""Star"",
          ""type"": ""object""
        }
      },
      ""required"": [""name"", ""largest_star""],
      ""title"": ""Galaxy"",
      ""type"": ""object"",
      ""additionalProperties"": false
    }
  },
  ""required"": [""name"", ""galaxy""],
  ""title"": ""Universe"",
  ""type"": ""object"",
  ""additionalProperties"": false
}
```

This issue occurs when using the AzureOpenAI client and is likely reproducible with the standard OpenAI client as well, as they both share the underlying schema handling mechanism.

### To Reproduce

1. Initialize an AzureOpenAI client (or an OpenAI client).
1. Create a nested Pydantic model structure with Field metadata (title and description).
1. Pass the top-level model as the response_format argument.
1. Observe the 400 error indicating the missing additionalProperties: false for nested objects.

### Code snippets

```Python
from typing import Annotated

from openai import AzureOpenAI
from pydantic import BaseModel, Field


class Star(BaseModel):
    name: Annotated[str, Field(description=""The name of the star."")]


class Galaxy(BaseModel):
    name: Annotated[str, Field(description=""The name of the galaxy."")]
    largest_star: Annotated[Star, Field(description=""The largest star in the galaxy."")]


class Universe(BaseModel):
    name: Annotated[str, Field(description=""The name of the universe."")]
    galaxy: Annotated[Galaxy, Field(description=""A galaxy in the universe."")]


client = AzureOpenAI(azure_endpoint=""endpoint"", api_key=""api-key"", api_version=""api-version"")

prompt = ""Create a fictional universe for a science fiction novel.""
completion = client.beta.chat.completions.parse(
    messages=[
        {""role"": ""user"", ""content"": prompt},
    ],
    model=""gpt-4o"",
    response_format=Universe,
)
```

### OS

macOS

### Python version

Python v3.12.8

### Library version

openai v1.59.7",closed,2025-01-16T23:30:54Z,2025-01-17T11:40:48Z,['bug'],https://github.com/openai/openai-python/issues/2024,12.16,True,missing_dependency_violation,high,"The generated JSON schema omits 'additionalProperties: false' for nested objects, leading to a BadRequestError from the API.",high,
2020,Update Realtime API code documentation,"### Confirm this is a feature request for the Python library and not the underlying OpenAI API.

- [x] This is a feature request for the Python library

### Describe the feature or improvement you're requesting

Please Update Realtime API code documentation example as the default model is 1-10-2024, please let the default is the newest model. I surprised  that I have been charged a lot and I didn't realize that I am using the old version which costs higher, so please make sure you update the code examples to use the newest version

### Additional context

_No response_",closed,2025-01-15T10:13:03Z,2025-01-15T14:06:26Z,['documentation'],https://github.com/openai/openai-python/issues/2020,3.89,False,,,,,
2018,openai.ChatCompletion.create is creating issue,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [x] This is an issue with the Python library

### Describe the bug

I was trying to verify if my openai API key is running properly, but I encountered this error several times, I downgraded to o
penai==0.28, upgraded to latest again, but it is not being resolved, thus, it is not even using my API key. 
Please suggest what to do, or if I am doing it wrong.

Error:

python setup.py
PS E:\gnews> python setup.py
Traceback (most recent call last):
  File ""E:\gnews\setup.py"", line 5, in <module>
    response = openai.ChatCompletion.create(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""E:\gnews\openai\lib\_old_api.py"", line 39, in __call__
    raise APIRemovedInV1(symbol=self._symbol)
openai.lib._old_api.APIRemovedInV1:

You tried to access openai.ChatCompletion, but this is no longer supported in openai
>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.    
You can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0
 interface.                                                                         
Alternatively, you can pin your installation to the old version, e.g. `pip install o
penai==0.28`                                                                        
A detailed migration guide is available here: https://github.com/openai/openai-pytho
n/discussions/742                                                                   


### To Reproduce

import openai

openai.api_key = ""sk-proj-myapikey""  
response = openai.ChatCompletion.create(
    model=""gpt-3.5-turbo"",
    messages=[
        {""role"": ""system"", ""content"": ""You are a helpful assistant.""},
        {""role"": ""user"", ""content"": ""Hello, how can I use the OpenAI API?""}
    ]
)

print(response.choices[0].message[""content""])


### Code snippets

```Python

```

### OS

windows

### Python version

python 3.12.4

### Library version

openai 0.28 & openai 1.0.0",closed,2025-01-14T13:43:30Z,2025-01-14T13:45:58Z,['question'],https://github.com/openai/openai-python/issues/2018,0.04,True,missing_dependency_violation,High,"The user is attempting to access a deprecated API method (openai.ChatCompletion.create) that is no longer supported in the version of the library they are using (>=1.0.0). This results in an inability to utilize the API key effectively, leading to incorrect system behavior.",high,
2005,Incorrect type hint for `chunking_strategy` params on `File` class methods,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

If I pass in a `StaticFileChunkingStrategyParam` to `OpenAI.beta.vector_stores.create` I get:

```
openai.BadRequestError: Error code: 400 - {'error': {'message': ""Missing required parameter: 'chunking_strategy.type'."", 'type': 'invalid_request_error', 'param': 'chunking_strategy.type', 'code': 'missing_required_parameter'}}
```

### To Reproduce

Run this repro:
```py
import tempfile

from openai import OpenAI
from openai.types.beta.static_file_chunking_strategy_object import (
    StaticFileChunkingStrategyObject,
)
from openai.types.beta.static_file_chunking_strategy_param import (
    StaticFileChunkingStrategyParam,
)

client = OpenAI()
with tempfile.NamedTemporaryFile(suffix="".txt"", mode=""w+b"") as temp_file:
    temp_file.write(b""foo bar"")
    temp_file.flush()
    temp_file.seek(0)
    file = client.files.create(file=temp_file.file, purpose=""assistants"")

vector_store = client.beta.vector_stores.create(name=""foo"")
client.beta.vector_stores.files.create(
    vector_store_id=vector_store.id,
    file_id=file.id,
    chunking_strategy=StaticFileChunkingStrategyParam(
        max_chunk_size_tokens=250,
        chunk_overlap_tokens=10,
    ),
)
```

Based on type hints I'd expect this to work, but I get the above api error.

It works instead with:
```py
from openai.types.beta.static_file_chunking_strategy_object import (
    StaticFileChunkingStrategyObject,
)
...
client.beta.vector_stores.files.create(
    vector_store_id=vector_store.id,
    file_id=file.id,
    chunking_strategy=StaticFileChunkingStrategyObject(
        type=""static"",
        static=StaticFileChunkingStrategyParam(
            max_chunk_size_tokens=250,
            chunk_overlap_tokens=10,
        ),
    ),
)
```

i.e. looks the type hint should be changed to `FileChunkingStrategy`. Note there are many such methods with this param in `Files` (e.g. create_and_poll, upload, ...). I haven't tested these, but this change may apply to them too.

### Code snippets

_No response_

### OS

macos 14.3.1

### Python version

Python 3.10.14

### Library version

openai==1.59.6",closed,2025-01-10T09:37:40Z,2025-01-14T13:13:08Z,['bug'],https://github.com/openai/openai-python/issues/2005,99.59,True,input_type_violation,high,"The method expects a specific type for the 'chunking_strategy' parameter, but a different type is provided, leading to a BadRequestError.",high,
1994,"@RobertCraigie Sorry, there are still some issue","              @RobertCraigie Sorry, there are still some issue

```python
client = OpenAI()

class EventHandler(AssistantEventHandler):
    def __init__(self) -> None:
        super().__init__()

    @override
    def on_event(self, event: AssistantStreamEvent) -> None:
        if isinstance(event, ThreadRunStepCompleted):
            if event.data.type == 'tool_calls':
                for tool_call in event.data.step_details.tool_calls:
                    # type(tool_call) is <class 'dict'>
                    if tool_call.type == 'code_interpreter':
                        pass
                    elif tool_call.type == 'file_search':
                        pass

    @override
    def _emit_sse_event(self, event: AssistantStreamEvent) -> None:
        if event.event == ""thread.run.step.delta"":
            print(f'{type(event)} {type(event.data)} {type(event.data.delta)} {str(event.data.delta)}')
        super()._emit_sse_event(event)


client = OpenAI()

assistant = client.beta.assistants.create(
    name=""Math Tutor"",
    instructions=""You are a personal math tutor. Write and run code to answer math questions."",
    tools=[{""type"": ""code_interpreter""}],
    model=""gpt-4o-mini"",
)

with client.beta.threads.create_and_run_stream(
    assistant_id=assistant.id,
    model=""gpt-4o-mini"",
    instructions=""Use browser tool first to answer the user's question"",
    thread={""messages"": [{""role"": ""user"", ""content"": ""Who is Tom""}]},
    tool_choice=""required"",
    event_handler=EventHandler()
) as stream:
    stream.until_done()
```

Output:
```
<class 'openai.types.beta.assistant_stream_event.ThreadRunStepDelta'> <class 'openai.types.beta.threads.runs.run_step_delta_event.RunStepDeltaEvent'> <class 'openai.types.beta.threads.runs.run_step_delta.RunStepDelta'> 
RunStepDelta(step_details=ToolCallDeltaObject(type='tool_calls', tool_calls=[CodeInterpreterToolCallDelta(index=0, type='browser', id='call_6GeU3til5JdXMxUCRmjLZK0L', code_interpreter=None, browser={})]))
/lib/python3.12/site-packages/pydantic/main.py:308: UserWarning: Pydantic serializer warnings:
  Expected `Union[RunStepDeltaMessageDelta, ToolCallDeltaObject]` but got `ToolCallDeltaObject` - serialized value may not be as expected
  Expected `Union[CodeInterpreterToolCallDelta, FileSearchToolCallDelta, FunctionToolCallDelta]` but got `CodeInterpreterToolCallDelta` - serialized value may not be as expected
  return self.__pydantic_serializer__.to_python(
Traceback (most recent call last):
  File ""demo.py"", line 51, in <module>
    stream.until_done()
  File ""/lib/python3.12/site-packages/openai/lib/streaming/_assistants.py"", line 102, in until_done
    consume_sync_iterator(self)
  File ""/lib/python3.12/site-packages/openai/_utils/_streams.py"", line 6, in consume_sync_iterator
    for _ in iterator:
  File ""/lib/python3.12/site-packages/openai/lib/streaming/_assistants.py"", line 69, in __iter__
    for item in self._iterator:
  File ""/lib/python3.12/site-packages/openai/lib/streaming/_assistants.py"", line 406, in __stream__
    self._emit_sse_event(event)
  File ""demo.py"", line 31, in _emit_sse_event
    super()._emit_sse_event(event)
  File ""/lib/python3.12/site-packages/openai/lib/streaming/_assistants.py"", line 256, in _emit_sse_event
    self.on_event(event)
  File ""demo.py"", line 22, in on_event
    if tool_call.type == 'code_interpreter':
       ^^^^^^^^^^^^^^
AttributeError: 'dict' object has no attribute 'type'
```

New environment:
```bash
> python -m pip list | grep -E 'openai|pydantic'              
openai                        1.36.0
pydantic                      2.8.2
pydantic_core                 2.20.1

> python --version
Python 3.12.2

> uname -a
Darwin Kernel Version 23.3.0
```

_Originally posted by @kunerzzz in https://github.com/openai/openai-python/issues/1574#issuecomment-2245119644_
            ",closed,2025-01-08T07:24:04Z,2025-01-13T19:12:45Z,[],https://github.com/openai/openai-python/issues/1994,131.81,True,input_type_violation,High,"The code attempts to access the 'type' attribute of a 'dict' object, which leads to an AttributeError, indicating a type mismatch in the expected input structure.",high,
1131,Slower than expected performance after upgrading,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

I'm not absolutely certain if this is an issue with the Python library, but after upgrading from v0.28.0 to v1.10.0, we noticed a significant increase in latency (by about a factor of 4x) when requesting embeddings via an Azure OpenAI ada v2 deployment. This was confirmed in the Azure portal, where latency was indeed about 4x higher immediately after we deployed our service using the upgraded package. After downgrading back to v0.28.0, the issue resolved itself.

### To Reproduce

1. Create an AzureOpenAI client
2. Request embeddings with the client.
For reference, we send about 2-3k embedding requests per 5 minutes

### Code snippets

This is how we query Azure with v0.28.0:
```Python
import openai
import os

texts = [""this"", ""is"", ""a"", ""test""]
embedding_args = {
                ""api_type"": AZURE_API_TYPE,
                ""api_version"": AZURE_API_VERSION,
                ""api_key"": os.getenv(""AZURE_OPENAI_API_KEY""),
                ""api_base"": os.getenv(""AZURE_OPENAI_API_BASE""),
                ""deployment_id"": os.getenv(""AZURE_OPENAI_DEPLOYMENT_ID""),
                ""input"": texts
            }
res = openai.Embedding.create(**embedding_args)
```

With v1.10.0:
```python
from openai import AzureOpenAI
import os

texts = [""this"", ""is"", ""a"", ""test""]
openai_client = AzureOpenAI(
                api_key=os.getenv(""AZURE_OPENAI_API_KEY""),
                api_version=AZURE_API_VERSION,
                azure_endpoint=os.getenv(""AZURE_OPENAI_API_BASE"")
            )
create_args = {
                        ""model"": os.getenv(""AZURE_OPENAI_DEPLOYMENT_ID""),
                        ""input"": texts
                    }
res = openai_client.embeddings.create(**create_args)
```

### OS

debian:bullseye-slim

### Python version

Python v3.11.7

### Library version

v1.10.0",closed,2024-02-07T07:31:32Z,2025-01-13T14:37:41Z,['bug'],https://github.com/openai/openai-python/issues/1131,8191.1,True,performance_degradation,Medium,Significant increase in latency (4x) when using the upgraded library version.,high,
2009,openai.ChatCompletion.create() Fails with _old_api.py in Fresh Environments,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

I am encountering an APIRemovedInV1 error referencing _old_api.py whenever I attempt to use openai.ChatCompletion.create() in completely fresh environments. This occurs even though I am using openai version 1.59.6 and correctly calling ChatCompletion.create().

This issue persists across multiple systems, including:

    Windows (local machine).
    Ubuntu (Hyper-V virtual machine).
    Fresh Python virtual environments.
    Clean Docker containers.

Please advise if this could be an edge case related to my account, the library itself, or something overlooked in my environment. I’d also appreciate any insights into additional debugging steps.

### To Reproduce

This issue occurs consistently with the following steps:

pip install --upgrade pip
pip install openai

import openai

openai.api_key = ""API KEY""

response = openai.ChatCompletion.create(
    model=""gpt-3.5-turbo"",
    messages=[
        {""role"": ""system"", ""content"": ""You are a helpful assistant.""},
        {""role"": ""user"", ""content"": ""Hello!""}
    ]
)
print(response[""choices""][0][""message""][""content""])

python test.py

### Code snippets

```Python
import openai

openai.api_key = ""[API KEY]""

response = openai.ChatCompletion.create(
    model=""gpt-3.5-turbo"",
    messages=[
        {""role"": ""system"", ""content"": ""You are a helpful assistant.""},
        {""role"": ""user"", ""content"": ""Hello!""}
    ]
)
print(response[""choices""][0][""message""][""content""])
```


### OS

Windows 11 Pro & Ubuntu 22.04

### Python version

Python 3.13 installed.

### Library version

openai v1.59.6",closed,2025-01-12T05:57:03Z,2025-01-12T19:39:06Z,['bug'],https://github.com/openai/openai-python/issues/2009,13.7,True,missing_dependency_violation,high,"The error indicates a missing dependency on the correct API version, suggesting that the library is attempting to access an endpoint or functionality that has been removed in the current version.",high,
1970,404 link on README,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

In the real-time API documentation, there is a link which is no longer active.

### To Reproduce

1. Go to https://github.com/openai/openai-python/tree/main
2. Search for ""A full event reference can be found [here](https://github.com/openai/openai-python/blob/main/platform.openai.com/docs/api-reference/realtime-client-events) and a guide can be found [here](https://platform.openai.com/docs/guides/realtime)."" This is under the ""Realtime API beta""
3. This takes you to this webpage - https://github.com/openai/openai-python/blob/main/platform.openai.com/docs/api-reference/realtime-client-events

This leads to a page with 404 error - 
![Screenshot 2024-12-24 at 8 21 54 PM](https://github.com/user-attachments/assets/df464ef3-026d-4c58-a1a7-6055f8458f51)


### Code snippets

_No response_

### OS

macOS

### Python version

Python v3.11

### Library version

open v1.0.1",closed,2024-12-24T14:53:28Z,2025-01-10T11:15:59Z,['bug'],https://github.com/openai/openai-python/issues/1970,404.38,False,,,,,
2006,The helper function `pydantic_function_tool` has been removed? ,"### Confirm this is a feature request for the Python library and not the underlying OpenAI API.

- [X] This is a feature request for the Python library

### Describe the feature or improvement you're requesting

In older openai python sdk version i recall a helper function called `pydantic_function_tool`, which could have been imported with:
`from openai import pydantic_function_tool`
os
`from openai.lib._tools import pydantic_function_tool`

It was super useful to bind tool calls and structured outputs, providing it a BaseModel.
In some older versions like 1.46.0 i can still find it.
However i tried recently and i cannot find it, even doing `pip install openai==1.46.0`

Is there a similar function or has it been changes/deprecated?


### Additional context

_No response_",closed,2025-01-10T09:45:15Z,2025-01-10T11:19:08Z,[],https://github.com/openai/openai-python/issues/2006,1.56,False,,,,,
1978,Free API key?,"### Confirm this is a feature request for the Python library and not the underlying OpenAI API.

- [X] This is a feature request for the Python library

### Describe the feature or improvement you're requesting

Can we have free API key based on IP address? Maybe limit it to 10 requests per hour or something.

### Additional context

Too much hassle to register and make an API key for how little I am using the API
```[tasklist]
### Tasks
```
",closed,2024-12-31T19:55:06Z,2025-01-06T16:45:34Z,[],https://github.com/openai/openai-python/issues/1978,140.84,False,,,,low,
1868,Descrepancy results AsyncAzureOpenAI.azure_deployment and client.chat.completions.create(model),"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

When running the AzureOpenAI service with azure_deployment gpt-4o 2024-05-13 I get very different completion results when I enter model='gpt-4' and model='gpt-4o'.
I would expect the model parameter not to matter in this case, as there is only one model to choose from the given deployment.

### To Reproduce

Run and observe stream:

```
async with AsyncAzureOpenAI(
        api_key=openai_api_key,
        azure_deployment=deployment_model,
        azure_endpoint=openai_azure_endpoint,
        api_version=""2023-12-01-preview""
    ) as client:
        openai_stream = await client.chat.completions.create(
            model=model_name,
            messages=messages,
            temperature=0.2,
            max_tokens=1200,
            top_p=0.45,
            frequency_penalty=0,
            presence_penalty=0,
            stop=None,
            stream=True
        )
```

twice:
 - azure_deployment='gpt-4o-0513', model_name='gpt-4'
 - azure_deployment='gpt-4o-0513', model_name='gpt-4o'

The first deployment will give markdown heavy response, structured with bold titles etc., while the second response will give solely paragraphs.

### Code snippets

_No response_

### OS

python:3.12-slim

### Python version

python v.3.12

### Library version

openai v.1.51.2",closed,2024-11-14T14:25:15Z,2025-01-07T08:13:34Z,['question'],https://github.com/openai/openai-python/issues/1868,1289.81,True,missing_dependency_violation,high,"The discrepancy in results based on the model parameter suggests a missing dependency between the model selection and the expected output format, leading to inconsistent behavior.",high,
1972,An issue with the API,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

Hi there,
I am trying to transcribe an audio file to text. I am sure the audio file, my KEY are both valid. But the API always returns a fixed text: ""The quick brown fox jumped over the lazy dogs.""

I don't know what I am wrong with the API 1.58.1, because the same audio file is proceeding successfully with version 0.28.
Thanks a lot for your help. 

### To Reproduce

choose an audio file and transcript it.

### Code snippets

```Python
from pathlib import Path
from openai import OpenAI

# Initialize the OpenAI client with your API key
openai = OpenAI(api_key=""valid key"")

# Define the path to the audio file
speech_file_path = Path(""sonia.m4a"")


def main() -> None:
    # Open the audio file in binary mode
    with open(speech_file_path, ""rb"") as audio_file:
        # Transcribe the audio using the Whisper API
        transcription = openai.audio.transcriptions.create(
                file=audio_file,  # Binary file object
                model=""whisper-1"",  # Model ID
                language=""en"",  # ISO 639-1 code for English (adjust if using another language)
                response_format=""json""  # Output format as plain text
        )

    # Print the transcription
    print(transcription.text)


if __name__ == ""__main__"":
    main()
```


### OS

Windows 11

### Python version

v3.12

### Library version

1.58.1",closed,2024-12-25T10:42:31Z,2025-01-06T17:39:03Z,['question'],https://github.com/openai/openai-python/issues/1972,294.94,True,input_value_violation,incorrect_functionality,"The API is returning a fixed text response instead of transcribing the audio file, indicating a potential issue with input handling or processing logic.",high,
1977,Api usage for different language,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

When i give a text and say ; grab product name and prices from this text, it works great for English texts.
But,
When i give Turkish text, it is messing up, prices are not true and product names are false.

### To Reproduce

def find_price_of_product(alt_tag, all_visible_text):
    prompt = f""""""
        Alt etiketi ve görünen metni analiz ederek, ürün adını, fiyatını ve görüntü bağlantısını bul. Yalnızca şunları yap:
        1. Ürün adı alt etiketi ile uyumlu olmalı veya alt etikette belirtilen bir kelime grubundan doğrudan ilişkili olmalıdır.
        2. Ürün fiyatı, TL cinsinden açıkça belirtilmelidir ve fiyat yalnızca rakamlar ve ""TL"" içermelidir (örnek: ""1.999 TL"").
        3. Görüntü URL'si yalnızca `.jpg` uzantısına sahip olmalıdır. `.svg`, `.gif`, `.png` veya başka uzantılar içeren URL'leri dikkate alma.
        4. Yanlış veya eksik veri içeren sonuçlar döndürme. Tüm veriler tam olmalıdır.
        5. Sadece şu formatta bir satır döndür: ürün_adı,ürün_fiyatı

        Alt etiketi: {alt_tag}
        Sayfadaki görünen metin: {all_visible_text}
        """"""

    # Call GPT API
    response = openai.ChatCompletion.create(
        model=""gpt-3.5-turbo"",  # Use 'gpt-3.5-turbo' if 'gpt-4' isn't available
        messages=[
            {""role"": ""system"", ""content"": ""Sen Türkçe bilen bir yardımcısın""},
            {""role"": ""user"", ""content"": prompt}
        ]
    )

    # Extracting the response text
    result = response['choices'][0]['message']['content'].strip()

    return result

### Code snippets

```Python
def find_prices_of_products(alt_tags, all_visible_text):
        results = []

        for alt in alt_tags:
            time.sleep(1)
            prompt = f""""""
            Given the following alt tag description and the visible text on the page, find the price associated with the product described by the alt tag. Ensure the price is clearly related to the product in the alt tag and exclude irrelevant numbers or information.

            Alt tag: {alt}

            Visible text on page: {all_visible_text}

            Return the results as a list in the following format:
            product_name,product_price
            """"""

            # Call GPT API
            response = openai.ChatCompletion.create(
                model=""gpt-3.5-turbo"",  # Use 'gpt-3.5-turbo' if 'gpt-4' isn't available
                messages=[
                    {""role"": ""system"", ""content"": ""You are a helpful assistant.""},
                    {""role"": ""user"", ""content"": prompt}
                ]
            )

            # Extracting the response text
            result = response['choices'][0]['message']['content'].strip()
            results.append(result)
```


### OS

macOs

### Python version

python 3.12

### Library version

gpt-3.5-turbo",closed,2024-12-28T13:59:29Z,2025-01-06T17:29:54Z,['bug'],https://github.com/openai/openai-python/issues/1977,219.51,True,input_value_violation,incorrect_functionality,"The API fails to correctly process Turkish text, leading to inaccurate extraction of product names and prices.",high,
1936,TypeError: issubclass() arg 1 must be a class,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

When calling `OpenAI.beta.chat.completions.parse` in a highly concurrent environment, and providing a `class` as the `response_format`, I get the following error:

```
    response = self.client.beta.chat.completions.parse(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.11/site-packages/openai/resources/beta/chat/completions.py"", line 156, in parse
    return self._post(
           ^^^^^^^^^^^
  File ""/local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.11/site-packages/openai/_base_client.py"", line 1280, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.11/site-packages/openai/_base_client.py"", line 957, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File ""/local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.11/site-packages/openai/_base_client.py"", line 1063, in _request
    return self._process_response(
           ^^^^^^^^^^^^^^^^^^^^^^^
  File ""/local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.11/site-packages/openai/_base_client.py"", line 1162, in _process_response
    return api_response.parse()
           ^^^^^^^^^^^^^^^^^^^^
  File ""/local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.11/site-packages/openai/_response.py"", line 319, in parse
    parsed = self._options.post_parser(parsed)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.11/site-packages/openai/resources/beta/chat/completions.py"", line 150, in parser
    return _parse_chat_completion(
           ^^^^^^^^^^^^^^^^^^^^^^^
  File ""/local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.11/site-packages/openai/lib/_parsing/_completions.py"", line 122, in parse_chat_completion
    construct_type_unchecked(
  File ""/local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.11/site-packages/openai/_models.py"", line 445, in construct_type_unchecked
    return cast(_T, construct_type(value=value, type_=type_))
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.11/site-packages/openai/_models.py"", line 519, in construct_type
    return type_.construct(**value)  # type: ignore[arg-type]
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.11/site-packages/openai/_models.py"", line 230, in construct
    fields_values[name] = _construct_field(value=values[key], field=field, key=key)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.11/site-packages/openai/_models.py"", line 394, in _construct_field
    return construct_type(value=value, type_=type_)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.11/site-packages/openai/_models.py"", line 513, in construct_type
    if not is_literal_type(type_) and (issubclass(origin, BaseModel) or issubclass(origin, GenericModel)):
                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""<frozen abc>"", line 123, in __subclasscheck__
TypeError: issubclass() arg 1 must be a class
```

This is being run in a multi-threaded environment. When I run with a single thread, I don't see this issue. I'm trying to churn through a bunch of data, so I'm attempting to use about 100 threads to make these API requests in parallel. If I reduce the count to 1 the problem goes away.

To work around this, I believe I can stop using the beta `parse` method with the provided `response_format`, 

### To Reproduce

1. Call `OpenAI.beta.chat.completions.parse` with 100 threads simultaneously

### Code snippets

```Python
class Bar(Enum):
    C = ""C""

class Qux(Enum):
    D = ""D""

class Foo(BaseModel):
    a: Bar
    b: Qux

def main(): # Called by 100 threads concurrently
    OpenAI(api_key=""..."").beta.chat.completions.parse(
        model=""gpt-4o-mini"",
        seed=0,
        temperature=0,
        messages=[
            {""role"": ""system"", ""content"": ""...""},
            {""role"": ""user"", ""content"": ""...""},
        ],
        response_format=Foo,
    )
```


### OS

Ubuntu 22.04.4 LTS

### Python version

Python v3.11.10

### Library version

openai v1.57.2",closed,2024-12-10T19:12:35Z,2025-01-06T17:14:19Z,['bug'],https://github.com/openai/openai-python/issues/1936,646.03,True,input_type_violation,High,"The provided 'response_format' argument is expected to be a class, but the input does not meet this requirement, leading to a TypeError.",high,
1833,Public helper for recombining ChatCompletionChunks into ChatCompletion,"### Confirm this is a feature request for the Python library and not the underlying OpenAI API.

- [X] This is a feature request for the Python library

### Describe the feature or improvement you're requesting

After streaming a chat completion response it is often necessary to recombine the streamed chunks into a message. Two examples are

- in the https://github.com/pydantic/logfire observability platform, when a streamed response has ended the final Assistant message could be displayed nicely in the UI.
    - issue: https://github.com/pydantic/logfire/issues/542
- in https://github.com/jackmpcollins/magentic , parallel tool calls are streamed to call these during the generation, and inserting the outputs back into `messages` requires also creating an Assistant message from the streamed chunks.

Currently the internal class `ChatCompletionStreamState` makes this easy, but it is private which indicates it should not be relied on. Would it be possible to make this or similar functionality a supported part of the public API?

The current feature set of `ChatCompletionStreamState` is ideal:
- get a `ChatCompletion` at any point during the stream (`current_completion_snapshot`). This allows logging a partial stream response in case of error, including if max_tokens was reached.
- parse the chunks into correct pydantic BaseModels for the tools/response_format (`get_final_completion()`)

Example usage of the existing class

```python
import openai
from openai.lib.streaming.chat._completions import ChatCompletionStreamState

state = ChatCompletionStreamState(
    input_tools=openai.NOT_GIVEN,
    response_format=openai.NOT_GIVEN,
)

response = client.chat.completions.create(...)
for chunk in response:
    state.handle_chunk(chunk)

print(state.current_completion_snapshot)
print(state.get_final_completion())
```

### Additional context

_No response_",closed,2024-10-30T03:51:24Z,2025-01-06T05:53:07Z,['enhancement'],https://github.com/openai/openai-python/issues/1833,1634.03,False,,,,,
969,Interrupt streaming for chat completions using the API like ChatGPT,"### Confirm this is a feature request for the Python library and not the underlying OpenAI API.

- [X] This is a feature request for the Python library

### Describe the feature or improvement you're requesting

Hello, I saw this thread on the forums [here](https://community.openai.com/t/how-to-abort-create-chat-completion-streaming-i-use-nodejs-typescript/377319/7) to allow for the ability to stop or interrupt the generation using the chat assistants API. There does not seem like there is anything in the docs to allow to work. Is it possible this can be implemented in the future to save tokens and generations on streaming responses?

### Additional context

_No response_",closed,2023-12-13T10:41:22Z,2023-12-15T02:07:58Z,[],https://github.com/openai/openai-python/issues/969,39.44,False,,,,,
1812,"Set ""$. messages [0] (SystemMessage). content (ArrayOfContentParts)"" as an empty list, and no 400 is returned","
### Describe the bug

It is mentioned in the official openapi document (https://storage.googleapis.com/stainless-sdk-openapi-specs/openai-f9320ebf347140052c7f8b0bc5c7db24f5e367c368c8cb34c3606af4e2b6591b.yml)
<img width=""1154"" alt=""image"" src=""https://github.com/user-attachments/assets/4f756d0e-9437-4a99-bdde-e9144dd19c4b"">




### To Reproduce

1.I made the following request with empty list
2.response said 200 but not 400 

I think this is inconsistent with the description in the document

### Code snippets

```Python
completion = client.chat.completions.create(
    model=""gpt-4o-mini"",
    messages=[
        {
            ""content"": [],
            ""role"": ""system""
        },
        {
            ""content"": ""hello"",
            ""role"": ""user""
        }
    ])
```


### OS

macOS

### Python version

Python v3.12.4

### Library version

openai v1.50.2",closed,2024-10-23T09:35:35Z,2024-12-27T09:02:57Z,['bug'],https://github.com/openai/openai-python/issues/1812,1559.46,True,input_value_violation,incorrect_functionality,"The API accepts an empty list for 'content' in a SystemMessage, which contradicts the expected input as per the official documentation.",high,
1968,"AuthenticationError: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-qVL45***************************************D1Vi. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}","### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

when I configure the proxy settings using the os package in Python to set environment variables, everything works as expected, and I can successfully make requests to the OpenAI API. So I am sure my API key is correct. 

However, when i create an llm instance and try to get response I receive the following error. 

code:
```python
response = await llm._achat_completion(messages=[""hello""])
```
report:
```
AuthenticationError: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-qVL45***************************************D1Vi. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
```

traceback:
```
---------------------------------------------------------------------------
AuthenticationError                       Traceback (most recent call last)
Cell In[13], [line 1](vscode-notebook-cell:?execution_count=13&line=1)
----> [1](vscode-notebook-cell:?execution_count=13&line=1) response = await llm._achat_completion(messages=[""hello""])

File [d:\anaconda3\envs\zgb2\Lib\site-packages\metagpt\provider\openai_api.py:148](file:///D:/anaconda3/envs/zgb2/Lib/site-packages/metagpt/provider/openai_api.py:148), in OpenAILLM._achat_completion(self, messages, timeout)
    [146](file:///D:/anaconda3/envs/zgb2/Lib/site-packages/metagpt/provider/openai_api.py:146) async def _achat_completion(self, messages: list[dict], timeout=USE_CONFIG_TIMEOUT) -> ChatCompletion:
    [147](file:///D:/anaconda3/envs/zgb2/Lib/site-packages/metagpt/provider/openai_api.py:147)     kwargs = self._cons_kwargs(messages, timeout=self.get_timeout(timeout))
--> [148](file:///D:/anaconda3/envs/zgb2/Lib/site-packages/metagpt/provider/openai_api.py:148)     rsp: ChatCompletion = await self.aclient.chat.completions.create(**kwargs)
    [149](file:///D:/anaconda3/envs/zgb2/Lib/site-packages/metagpt/provider/openai_api.py:149)     self._update_costs(rsp.usage)
    [150](file:///D:/anaconda3/envs/zgb2/Lib/site-packages/metagpt/provider/openai_api.py:150)     return rsp

File [d:\anaconda3\envs\zgb2\Lib\site-packages\openai\resources\chat\completions.py:1295](file:///D:/anaconda3/envs/zgb2/Lib/site-packages/openai/resources/chat/completions.py:1295), in AsyncCompletions.create(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_tokens, n, parallel_tool_calls, presence_penalty, response_format, seed, service_tier, stop, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)
   [1261](file:///D:/anaconda3/envs/zgb2/Lib/site-packages/openai/resources/chat/completions.py:1261) @required_args([""messages"", ""model""], [""messages"", ""model"", ""stream""])
   [1262](file:///D:/anaconda3/envs/zgb2/Lib/site-packages/openai/resources/chat/completions.py:1262) async def create(
   [1263](file:///D:/anaconda3/envs/zgb2/Lib/site-packages/openai/resources/chat/completions.py:1263)     self,
   (...)
   [1293](file:///D:/anaconda3/envs/zgb2/Lib/site-packages/openai/resources/chat/completions.py:1293)     timeout: float | httpx.Timeout | None | NotGiven = NOT_GIVEN,
   [1294](file:///D:/anaconda3/envs/zgb2/Lib/site-packages/openai/resources/chat/completions.py:1294) ) -> ChatCompletion | AsyncStream[ChatCompletionChunk]:
-> [1295](file:///D:/anaconda3/envs/zgb2/Lib/site-packages/openai/resources/chat/completions.py:1295)     return await self._post(
   [1296](file:///D:/anaconda3/envs/zgb2/Lib/site-packages/openai/resources/chat/completions.py:1296)         ""/chat/completions"",
   [1297](file:///D:/anaconda3/envs/zgb2/Lib/site-packages/openai/resources/chat/completions.py:1297)         body=await async_maybe_transform(
   [1298](file:///D:/anaconda3/envs/zgb2/Lib/site-packages/openai/resources/chat/completions.py:1298)             {
   [1299](file:///D:/anaconda3/envs/zgb2/Lib/site-packages/openai/resources/chat/completions.py:1299)                 ""messages"": messages,
   [1300](file:///D:/anaconda3/envs/zgb2/Lib/site-packages/openai/resources/chat/completions.py:1300)                 ""model"": model,
   [1301](file:///D:/anaconda3/envs/zgb2/Lib/site-packages/openai/resources/chat/completions.py:1301)                 ""frequency_penalty"": frequency_penalty,
   [1302](file:///D:/anaconda3/envs/zgb2/Lib/site-packages/openai/resources/chat/completions.py:1302)                 ""function_call"": function_call,
   [1303](file:///D:/anaconda3/envs/zgb2/Lib/site-packages/openai/resources/chat/completions.py:1303)                 ""functions"": functions,
   [1304](file:///D:/anaconda3/envs/zgb2/Lib/site-packages/openai/resources/chat/completions.py:1304)                 ""logit_bias"": logit_bias,
   [1305](file:///D:/anaconda3/envs/zgb2/Lib/site-packages/openai/resources/chat/completions.py:1305)                 ""logprobs"": logprobs,
   [1306](file:///D:/anaconda3/envs/zgb2/Lib/site-packages/openai/resources/chat/completions.py:1306)                 ""max_tokens"": max_tokens,
   [1307](file:///D:/anaconda3/envs/zgb2/Lib/site-packages/openai/resources/chat/completions.py:1307)                 ""n"": n,
   [1308](file:///D:/anaconda3/envs/zgb2/Lib/site-packages/openai/resources/chat/completions.py:1308)                 ""parallel_tool_calls"": parallel_tool_calls,
   [1309](file:///D:/anaconda3/envs/zgb2/Lib/site-packages/openai/resources/chat/completions.py:1309)                 ""presence_penalty"": presence_penalty,
   [1310](file:///D:/anaconda3/envs/zgb2/Lib/site-packages/openai/resources/chat/completions.py:1310)                 ""response_format"": response_format,
   [1311](file:///D:/anaconda3/envs/zgb2/Lib/site-packages/openai/resources/chat/completions.py:1311)                 ""seed"": seed,
   [1312](file:///D:/anaconda3/envs/zgb2/Lib/site-packages/openai/resources/chat/completions.py:1312)                 ""service_tier"": service_tier,
   [1313](file:///D:/anaconda3/envs/zgb2/Lib/site-packages/openai/resources/chat/completions.py:1313)                 ""stop"": stop,
   [1314](file:///D:/anaconda3/envs/zgb2/Lib/site-packages/openai/resources/chat/completions.py:1314)                 ""stream"": stream,
   [1315](file:///D:/anaconda3/envs/zgb2/Lib/site-packages/openai/resources/chat/completions.py:1315)                 ""stream_options"": stream_options,
   [1316](file:///D:/anaconda3/envs/zgb2/Lib/site-packages/openai/resources/chat/completions.py:1316)                 ""temperature"": temperature,
   [1317](file:///D:/anaconda3/envs/zgb2/Lib/site-packages/openai/resources/chat/completions.py:1317)                 ""tool_choice"": tool_choice,
   [1318](file:///D:/anaconda3/envs/zgb2/Lib/site-packages/openai/resources/chat/completions.py:1318)                 ""tools"": tools,
   [1319](file:///D:/anaconda3/envs/zgb2/Lib/site-packages/openai/resources/chat/completions.py:1319)                 ""top_logprobs"": top_logprobs,
   [1320](file:///D:/anaconda3/envs/zgb2/Lib/site-packages/openai/resources/chat/completions.py:1320)                 ""top_p"": top_p,
   [1321](file:///D:/anaconda3/envs/zgb2/Lib/site-packages/openai/resources/chat/completions.py:1321)                 ""user"": user,
   [1322](file:///D:/anaconda3/envs/zgb2/Lib/site-packages/openai/resources/chat/completions.py:1322)             },
   [1323](file:///D:/anaconda3/envs/zgb2/Lib/site-packages/openai/resources/chat/completions.py:1323)             completion_create_params.CompletionCreateParams,
   [1324](file:///D:/anaconda3/envs/zgb2/Lib/site-packages/openai/resources/chat/completions.py:1324)         ),
   [1325](file:///D:/anaconda3/envs/zgb2/Lib/site-packages/openai/resources/chat/completions.py:1325)         options=make_request_options(
   [1326](file:///D:/anaconda3/envs/zgb2/Lib/site-packages/openai/resources/chat/completions.py:1326)             extra_headers=extra_headers, extra_query=extra_query, extra_body=extra_body, timeout=timeout
   [1327](file:///D:/anaconda3/envs/zgb2/Lib/site-packages/openai/resources/chat/completions.py:1327)         ),
   [1328](file:///D:/anaconda3/envs/zgb2/Lib/site-packages/openai/resources/chat/completions.py:1328)         cast_to=ChatCompletion,
   [1329](file:///D:/anaconda3/envs/zgb2/Lib/site-packages/openai/resources/chat/completions.py:1329)         stream=stream or False,
   [1330](file:///D:/anaconda3/envs/zgb2/Lib/site-packages/openai/resources/chat/completions.py:1330)         stream_cls=AsyncStream[ChatCompletionChunk],
   [1331](file:///D:/anaconda3/envs/zgb2/Lib/site-packages/openai/resources/chat/completions.py:1331)     )

File [d:\anaconda3\envs\zgb2\Lib\site-packages\openai\_base_client.py:1836](file:///D:/anaconda3/envs/zgb2/Lib/site-packages/openai/_base_client.py:1836), in AsyncAPIClient.post(self, path, cast_to, body, files, options, stream, stream_cls)
   [1822](file:///D:/anaconda3/envs/zgb2/Lib/site-packages/openai/_base_client.py:1822) async def post(
   [1823](file:///D:/anaconda3/envs/zgb2/Lib/site-packages/openai/_base_client.py:1823)     self,
   [1824](file:///D:/anaconda3/envs/zgb2/Lib/site-packages/openai/_base_client.py:1824)     path: str,
   (...)
   [1831](file:///D:/anaconda3/envs/zgb2/Lib/site-packages/openai/_base_client.py:1831)     stream_cls: type[_AsyncStreamT] | None = None,
   [1832](file:///D:/anaconda3/envs/zgb2/Lib/site-packages/openai/_base_client.py:1832) ) -> ResponseT | _AsyncStreamT:
   [1833](file:///D:/anaconda3/envs/zgb2/Lib/site-packages/openai/_base_client.py:1833)     opts = FinalRequestOptions.construct(
   [1834](file:///D:/anaconda3/envs/zgb2/Lib/site-packages/openai/_base_client.py:1834)         method=""post"", url=path, json_data=body, files=await async_to_httpx_files(files), **options
   [1835](file:///D:/anaconda3/envs/zgb2/Lib/site-packages/openai/_base_client.py:1835)     )
-> [1836](file:///D:/anaconda3/envs/zgb2/Lib/site-packages/openai/_base_client.py:1836)     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)

File [d:\anaconda3\envs\zgb2\Lib\site-packages\openai\_base_client.py:1524](file:///D:/anaconda3/envs/zgb2/Lib/site-packages/openai/_base_client.py:1524), in AsyncAPIClient.request(self, cast_to, options, stream, stream_cls, remaining_retries)
   [1515](file:///D:/anaconda3/envs/zgb2/Lib/site-packages/openai/_base_client.py:1515) async def request(
   [1516](file:///D:/anaconda3/envs/zgb2/Lib/site-packages/openai/_base_client.py:1516)     self,
   [1517](file:///D:/anaconda3/envs/zgb2/Lib/site-packages/openai/_base_client.py:1517)     cast_to: Type[ResponseT],
   (...)
   [1522](file:///D:/anaconda3/envs/zgb2/Lib/site-packages/openai/_base_client.py:1522)     remaining_retries: Optional[int] = None,
   [1523](file:///D:/anaconda3/envs/zgb2/Lib/site-packages/openai/_base_client.py:1523) ) -> ResponseT | _AsyncStreamT:
-> [1524](file:///D:/anaconda3/envs/zgb2/Lib/site-packages/openai/_base_client.py:1524)     return await self._request(
   [1525](file:///D:/anaconda3/envs/zgb2/Lib/site-packages/openai/_base_client.py:1525)         cast_to=cast_to,
   [1526](file:///D:/anaconda3/envs/zgb2/Lib/site-packages/openai/_base_client.py:1526)         options=options,
   [1527](file:///D:/anaconda3/envs/zgb2/Lib/site-packages/openai/_base_client.py:1527)         stream=stream,
   [1528](file:///D:/anaconda3/envs/zgb2/Lib/site-packages/openai/_base_client.py:1528)         stream_cls=stream_cls,
   [1529](file:///D:/anaconda3/envs/zgb2/Lib/site-packages/openai/_base_client.py:1529)         remaining_retries=remaining_retries,
   [1530](file:///D:/anaconda3/envs/zgb2/Lib/site-packages/openai/_base_client.py:1530)     )

File [d:\anaconda3\envs\zgb2\Lib\site-packages\openai\_base_client.py:1625](file:///D:/anaconda3/envs/zgb2/Lib/site-packages/openai/_base_client.py:1625), in AsyncAPIClient._request(self, cast_to, options, stream, stream_cls, remaining_retries)
   [1622](file:///D:/anaconda3/envs/zgb2/Lib/site-packages/openai/_base_client.py:1622)         await err.response.aread()
   [1624](file:///D:/anaconda3/envs/zgb2/Lib/site-packages/openai/_base_client.py:1624)     log.debug(""Re-raising status error"")
-> [1625](file:///D:/anaconda3/envs/zgb2/Lib/site-packages/openai/_base_client.py:1625)     raise self._make_status_error_from_response(err.response) from None
   [1627](file:///D:/anaconda3/envs/zgb2/Lib/site-packages/openai/_base_client.py:1627) return await self._process_response(
   [1628](file:///D:/anaconda3/envs/zgb2/Lib/site-packages/openai/_base_client.py:1628)     cast_to=cast_to,
   [1629](file:///D:/anaconda3/envs/zgb2/Lib/site-packages/openai/_base_client.py:1629)     options=options,
   (...)
   [1633](file:///D:/anaconda3/envs/zgb2/Lib/site-packages/openai/_base_client.py:1633)     retries_taken=options.get_max_retries(self.max_retries) - retries,
   [1634](file:///D:/anaconda3/envs/zgb2/Lib/site-packages/openai/_base_client.py:1634) )

AuthenticationError: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-qVL45***************************************D1Vi. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
```


I would greatly appreciate it if I could receive any response or assistance from you!

### To Reproduce

...

### Code snippets

```Python
...
```


### OS

windows11

### Python version

Python v3.11.11

### Library version

openai v1.39.0",closed,2024-12-21T03:49:36Z,2024-12-26T08:06:55Z,['bug'],https://github.com/openai/openai-python/issues/1968,124.29,True,input_value_violation,High,"The API key provided is incorrect, leading to an AuthenticationError. This indicates that the input value (API key) does not meet the expected criteria for valid keys.",high,
1209,"AuthenticationError: Error code: 401 - {'error': {'message': 'Incorrect API key provided: 3*********************************5. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}","### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

API key does not work with Openai version ```openai==1.13.3```
But the same API key works with previous versions such as  ```openai==0.27.0```


### To Reproduce

use openai==1.13.3 and use this code
```
chat_completion = client.chat.completions.create(
    messages=[
        {
            ""role"": ""user"",
            ""content"": final_prompt
        }
    ],
    model=""model"",
)
print(chat_completion)
```
which will return the **Incorrect API key provided** error
BUT the same API key works with ```openai==0.27.0```
with this code
```
completion = openai.ChatCompletion.create(
          engine=""model"",
          temperature=0.1,
          messages=[{""role"": ""user"", ""content"": final_prompt}]
       )
```

### Code snippets

_No response_

### OS

Ubuntu

### Python version

Python3.9

### Library version

openai==1.13.3",closed,2024-03-04T11:15:41Z,2024-03-05T04:30:27Z,['bug'],https://github.com/openai/openai-python/issues/1209,17.25,True,missing_dependency_violation,high,"The API key is not recognized in the newer version of the library, indicating a potential missing dependency or compatibility issue between the library version and the API key.",high,
1938,passing `include` keyword argument to `beta.threads.runs.stream` function leads to openai.BadRequestError,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

passing `include` keyword argument to `beta.threads.runs.stream` function leads to 

> openai.BadRequestError: Error code: 400 - {'error': {'message': ""Unknown parameter: 'include'."", 'type': 'invalid_request_error', 'param': 'include', 'code': 'unknown_parameter'}}

### To Reproduce

```python
client.beta.threads.runs.stream(
                thread_id=thread.id,
                assistant_id=assistant.id,
                include=[""step_details.tool_calls[*].file_search.results[*].content""],
)
```

### Code snippets

_No response_

### OS

macOS

### Python version

Python v3.11.5

### Library version

openai v1.57.0",closed,2024-12-12T00:05:43Z,2024-12-16T12:24:51Z,['bug'],https://github.com/openai/openai-python/issues/1938,108.32,True,missing_option_violation,high,"The 'include' keyword argument is not recognized by the API, indicating that the client did not follow the API's contract regarding acceptable parameters.",high,
1838,"""openai migrate"" Error: Failed to download Grit CLI","### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

When running the `openai migrate` command, it attempts to download the Grit CLI from `https://github.com/getgrit/gritql/releases/latest/download/marzano-x86_64-unknown-linux-gnu.tar.gz`, but the link appears to be broken or outdated and it response `Error: Failed to download Grit CLI from https://github.com/getgrit/gritql/releases/latest/download/marzano-x86_64-unknown-linux-gnu.tar.gz`

### To Reproduce

1. Install the latest version of openai via pip: pip install openai --upgrade
2. Observe the command attempting to download the file marzano-x86_64-unknown-linux-gnu.tar.gz.

I think that the current Grit releases on GitHub seem to have changed the prefix from marzano- to grit- in the file names, which is likely causing the download to fail.

### Code snippets

_No response_

### OS

Ubuntu 20.04 LTS

### Python version

Python v3.11.4

### Library version

openai 1.53.0",closed,2024-10-31T14:51:21Z,2024-12-16T12:14:17Z,['bug'],https://github.com/openai/openai-python/issues/1838,1101.38,True,missing_dependency_violation,high,"The command fails to download the Grit CLI due to an outdated or broken link, indicating a missing dependency on the correct file naming convention.",high,
1944,AI Search Engine. (AISE),"### Confirm this is a feature request for the Python library and the underlying OpenAI API. 

- [X] This is a feature request for the Python library and OpenAI API

Confirm 

### Describe the feature or improvement you're requesting

Replace Google and try to use startpage or ecosia. 


",closed,2024-12-12T12:57:30Z,2024-12-12T13:05:14Z,[],https://github.com/openai/openai-python/issues/1944,0.13,False,,,,,
987,"APIRemovedInV1: You tried to access openai.ChatCompletion, but this is no longer supported","### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

I'm trying to create a simple BOT however getting the below error.
APIRemovedInV1: You tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API. You can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. Alternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28` A detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742

Validations:
Environment Check done
Upgraded OpenAI Python Library
Restarted Kernel
Validated API key




### To Reproduce

Code attached

### Code snippets

```Python
# Streamlit UI
st.title(""Vitamin Quiz Bot"")

# Initialize session state
if 'iteration' not in st.session_state:
    st.session_state.iteration = 0
    st.session_state.quiz_active = False

# Main loop for the quiz
user_input = st.text_input(""You (Iteration {}):"".format(st.session_state.iteration))

if user_input.lower() == ""lets begin the quiz"":
    st.session_state.quiz_active = True

if st.session_state.quiz_active:
    # Generate a question
    chat_history = [
        {""role"": ""system"", ""content"": ""You are a helpful assistant.""},
        {""role"": ""user"", ""content"": ""Generate a question about vitamins and minerals for a 3rd-grade quiz.""},
    ]
    response = openai.ChatCompletion.create(
        model=""gpt-3.5-turbo"",
        messages=chat_history,
        max_tokens=50,
        temperature=0.7,
    )
    question = response['choices'][0]['message']['content'].strip()

    # Display the question
    st.text(""Bot: {}"".format(question))

    # Get user's response
    user_response = st.text_input(""You:"")

    # Evaluate the response
    if st.button(""Submit""):
        st.text(""You said: {}"".format(user_response))
```


### OS

windows

### Python version

Python 3.12.1

### Library version

openai 1.5.0",closed,2023-12-18T18:11:49Z,2023-12-19T03:12:15Z,['bug'],https://github.com/openai/openai-python/issues/987,9.01,True,missing_option_violation,high,"The code attempts to access an API method (openai.ChatCompletion) that has been removed in the current library version, indicating a failure to follow the updated API contract.",high,
1894,Azure Endpoint URL is not Trailing-Slash Agnostic,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

In src/openai/lib/azure.py, there are four cases where the endpoint URL is simply concatenated like:

`f""{azure_endpoint}/openai/deployments/{azure_deployment}""`
or
`f""{azure_endpoint}/openai""`

If `azure_endpoint` has a trailing forward slash (which many comments in the code suggest is allowed, as well as [an example in the repository](https://github.com/openai/openai-python/blob/83f4774156dc3e29c7fe6be9ffd681df68534509/examples/azure.py#L29)), it will have `//` in the URL, which is wrong.

To make it agnostic, it should be:

`f""{azure_endpoint.rstrip(""/"")}/openai/deployments/{azure_deployment}""`
and
`f""{azure_endpoint.rstrip(""/"")}/openai""`

This would be sufficient to fix it: #1893 https://github.com/openai/openai-python/pull/1893/commits/673a496582ccf5d960e26a846cd6df81e837a9f9

### To Reproduce

1. Initialize an AsyncAzureOpenAI client.
2. Await on client.chat.completions.with_raw_response.create(**model_args).
3. The error code will be 404 Resource not found because the URL is not properly formatted.

### Code snippets

_No response_

### OS

Windows 10

### Python version

Python 3.12.4

### Library version

openai v1.6.1",closed,2024-11-26T00:25:11Z,2024-12-10T11:53:06Z,['bug'],https://github.com/openai/openai-python/issues/1894,347.47,True,input_value_violation,high,"The endpoint URL concatenation does not handle trailing slashes correctly, leading to malformed URLs and potential 404 errors.",high,
1928,Json_schema structured output type fails when extra_body is provided ,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

Trying to use the client.beta.chat.completions.parse function works until i try to provide an extra body argument, in this case Azure AI Search index.
Tested with versions 1.53.0, 1.54.3 and 1.57.0 

### To Reproduce

1. Run the completion without extra body
2. Remove comment for extra_body(give a valid azure search endpoint and index)

Error:
BadRequestError: Error code: 400 - {'error': {'requestid': '', 'code': 400, 'message': ""Validation error at #/response_format/type: Input should be 'text' or 'json_object'""}}    


### Code snippets

```Python
class Answer(BaseModel):
    answer: str
extra_body = {
                 ""data_sources"": [
                    {
                        ""type"": ""azure_search"",
                        ""parameters"": {
                            ""in_scope"": True,
                            ""strictness"": int(4),
                            ""top_n_documents"": int(10),
                            ""endpoint"": ""<azure search endppoint>"",
                            ""index_name"": ""<index-name>"",
                            ""authentication"": {
                                ""type"": ""system_assigned_managed_identity""
                            },
                            ""embedding_dependency"": {
                                ""type"": ""deployment_name"", 
                                ""deployment_name"": ""text-embedding-3-large"",
                                ""dimensions"": 3072,
                            },
                            ""query_type"": ""vector_semantic_hybrid"",
                            ""semantic_configuration"": ""my-semantic-config"",
                        }
                    }
                ]
            }
message_text = [ {""role"": ""user"", ""content"": ""when was google founded?""}]

structured_completion = openai_client.beta.chat.completions.parse(  
    model=""gpt-4o"",  
    messages=message_text,  
    max_tokens=800,  
    temperature=0.3, 
    # extra_body=extra_body,
    response_format=Answer
)  

response = structured_completion.choices[0].message.parsed
print(response)
```


### OS

macOS

### Python version

Python 3.12.7

### Library version

1.57.0 ",closed,2024-12-09T13:16:19Z,2024-12-09T18:16:49Z,"['Azure', 'API-feedback']",https://github.com/openai/openai-python/issues/1928,5.01,True,input_value_violation,high,"The API method fails when an extra_body is provided, indicating that the input value does not conform to the expected response_format type.",high,
1790,Limits argument can never get passed down?,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

It seems as though when initializing an openAI client like so:


since you cannot pass in the limits argument as a parameter, it will always default to the DEFAULT value here:

https://github.com/openai/openai-python/blob/aa6818997468b753546d55365d8142e2bb1c6640/src/openai/_base_client.py#L1127

openai.AsyncClient(())

### To Reproduce

1. Initialize an openai client like so: openai.AsyncClient(())
2. Try to pass in 'limits' argument or modify the 'http_client' to have custom Limits. 
3. This will not actually be propagated 

### Code snippets

```Python
openai.AsyncClient(())
```


### OS

linux

### Python version

3.9.12 

### Library version

1.50.0",closed,2024-10-10T22:15:23Z,2024-12-09T10:51:37Z,['bug'],https://github.com/openai/openai-python/issues/1790,1428.6,True,missing_option_violation,medium,"The 'limits' argument cannot be passed to the AsyncClient, leading to reliance on default values.",high,
1926,openai.ChatCompletion.create still triggering APIRemovedInV1 error despite using openai>=1.57.0,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

Despite installing the latest OpenAI Python library (1.57.0), I am encountering the error:
You tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0.

I would greatly appreciate any insights into resolving this issue. Is this a library bug or something specific to my setup? Thank you for your support!

### To Reproduce

- Verified the Python version (Python 3.12.5) on both Windows and Linux environments.
- Used pip uninstall openai and pip install openai to ensure the latest version is installed.
- Replaced all code references to legacy APIs with the correct openai.ChatCompletion.create syntax.
- Tested API connectivity via curl, which worked successfully and returned the list of available models.

Despite these steps, running the Python script still produces the error indicating the legacy API is being called.
When I test the connection, the traceback includes:

openai.lib._old_api.APIRemovedInV1: You tried to access openai.ChatCompletion, but this is no longer supported...


### Code snippets

```Python
import openai

openai.api_key = ""sk-xxx""  # Replace with actual key during testing

try:
    response = openai.ChatCompletion.create(
        model=""gpt-4"",
        messages=[{""role"": ""user"", ""content"": ""Test connection""}]
    )
    print(response[""choices""][0][""message""][""content""])
except Exception as e:
    print(f""Error: {e}"")

--------------------

Expected Outcome: Successful API connection with a basic response from GPT.

--------------------

Output / Error Traceback:

Traceback (most recent call last):
  File ""test_openai_connections.py"", line 8, in <module>
    response = openai.ChatCompletion.create(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/user/venv/lib/python3.12/site-packages/openai/lib/_old_api.py"", line 39, in __call__
    raise APIRemovedInV1(symbol=self._symbol)
openai.lib._old_api.APIRemovedInV1: You tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0.
```


### OS

Windows 11 and Linux (Debian VM via VirtualBox) --> Virtual environment (`venv`) used on Linux VM.

### Python version

Python 3.12.5

### Library version

openai==1.57.0",closed,2024-12-06T23:44:33Z,2024-12-09T10:44:23Z,['bug'],https://github.com/openai/openai-python/issues/1926,59.0,False,,,,low,
1632,`client.beta.chat.completions.parse` error 400: Missing required parameter: 'response_format.json_schema',"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

I literally just copied the structured output example from inside [the official blog](https://openai.com/index/introducing-structured-outputs-in-the-api/) and it won't run:

```python
from dotenv import load_dotenv
from openai import OpenAI
from pydantic import BaseModel

load_dotenv()


class Step(BaseModel):
    explanation: str
    output: str


class MathResponse(BaseModel):
    steps: list[Step]
    final_answer: str


client = OpenAI()

completion = client.beta.chat.completions.parse(
    model=""gpt-4o-2024-08-06"",
    messages=[
        {""role"": ""system"", ""content"": ""You are a helpful math tutor.""},
        {""role"": ""user"", ""content"": ""solve 8x + 31 = 2""},
    ],
    response_format=MathResponse,
)

message = completion.choices[0].message
if message.parsed:
    print(message.parsed.steps)
    print(message.parsed.final_answer)
else:
    print(message.refusal)
```

output:

```bash
raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': ""Missing required parameter: 'response_format.json_schema'. (request id: ) (request id: 20240810152859212775791fSLFCpem)"", 'type': 'invalid_request_error', 'param': 'response_format.json_schema', 'code': 'missing_required_parameter'}}
```

I'm so confused now🤔. Maybe something going wrong inside `beta.chat.completions.parse`.

### To Reproduce

1. Copy the example from blog
2. run

### Code snippets

_No response_

### OS

macOS

### Python version

Python v3.12

### Library version

openai v1.40.1",closed,2024-08-10T07:34:48Z,2024-08-10T07:35:53Z,['bug'],https://github.com/openai/openai-python/issues/1632,0.02,True,missing_dependency_violation,high,"The API call is missing the required parameter 'response_format.json_schema', which is necessary for the structured output to be processed correctly.",high,
1765,the latest openai version has problem with httpx library when compile to app.,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

I found this error when update the openai to new version.

  Referenced from: <198A9232-F82E-3E0A-92CB-D131D45DBB4F> /xx/_ssl.cpython-312-darwin.so
  Expected in:     <284AF566-569A-311D-8493-DC6928BBD628> /xx/libcrypto.3.dylib
Failed to execute script 'main' due to unhandled exception: dlopen(/xxx/Frameworks/lib-dynload/_ssl.cpython-312-darwin.so, 0x0002): **Symbol not found: _X509_STORE_get1_objects**

File ""openai/__init__.py"", line 8, in <module>
  File ""openai/types/__init__.py"", line 5, in <module>
  File ""openai/types/batch.py"", line 7, in <module>
  File ""openai/_models.py"", line 26, in <module>
  File ""openai/_types.py"", line 21, in <module>
  File ""httpx/__init__.py"", line 2, in <module>


### To Reproduce

1. write a sample code and import the latest openai.
2. compile file with pyinstaller on macOS.
3. run the execute file and found this issue.

### Code snippets

```Python
the problem looks lead to httpx library which calling by openai.
Not sure if it need the specific version or latest version of the library?
```


### OS

macOS

### Python version

Python 3.12

### Library version

openai 1.50.2",closed,2024-10-01T08:14:07Z,2024-11-05T07:38:58Z,['bug'],https://github.com/openai/openai-python/issues/1765,839.41,False,,,,low,
1903,TypeError: Client.__init__() got an unexpected keyword argument 'proxies',"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

`SyncHttpxClientWrapper` has hard coded proxies but it no longer exists in httpx 0.28

### To Reproduce

1. `pip install openai`
2. the bug appears when calling `openai.OpenAI(**client_params, **sync_specific)`

### Code snippets

```Python
Traceback (most recent call last):
  File ""/Users/greg/Documents/Work/browser-use/browser-use/examples/try.py"", line 45, in <module>
    llm = get_llm(args.provider)
          ^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/greg/Documents/Work/browser-use/browser-use/examples/try.py"", line 28, in get_llm
    return ChatOpenAI(model='gpt-4o', temperature=0.0)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/greg/Documents/Work/browser-use/browser-use/.venv/lib/python3.11/site-packages/langchain_core/load/serializable.py"", line 125, in __init__
    super().__init__(*args, **kwargs)
  File ""/Users/greg/Documents/Work/browser-use/browser-use/.venv/lib/python3.11/site-packages/pydantic/main.py"", line 214, in __init__
    validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/greg/Documents/Work/browser-use/browser-use/.venv/lib/python3.11/site-packages/langchain_openai/chat_models/base.py"", line 551, in validate_environment
    self.root_client = openai.OpenAI(**client_params, **sync_specific)  # type: ignore[arg-type]
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/greg/Documents/Work/browser-use/browser-use/.venv/lib/python3.11/site-packages/openai/_client.py"", line 123, in __init__
    super().__init__(
  File ""/Users/greg/Documents/Work/browser-use/browser-use/.venv/lib/python3.11/site-packages/openai/_base_client.py"", line 857, in __init__
    self._client = http_client or SyncHttpxClientWrapper(
                                  ^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/greg/Documents/Work/browser-use/browser-use/.venv/lib/python3.11/site-packages/openai/_base_client.py"", line 755, in __init__
    super().__init__(**kwargs)
TypeError: Client.__init__() got an unexpected keyword argument 'proxies'
```


### OS

macOS

### Python version

Python v3.11.4

### Library version

openai 1.55.2",closed,2024-11-28T15:50:44Z,2024-11-28T16:32:23Z,['bug'],https://github.com/openai/openai-python/issues/1903,0.69,True,input_type_violation,high,"The client initialization is receiving an unexpected keyword argument 'proxies', indicating a type mismatch in the expected parameters.",high,
1925,httpx version not pinned - upgrade to httpx==28.0.0 breaks AzureOpenAI class,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

The AzureOpenAI class makes use of the httpx package, which has very loose version restrictions in the openapi-python package.

`dependencies = [
    ""httpx>=0.23.0, <1"", ... ]`

The httpx package version was bumped to `0.28.0` which breaks the AzureOpenAI class.
Downgrading the httpx version to `~0.27.0` solves the issue.
Even though this is a httpx package issue, I would suggest to pin the version to avoid breaking changes for any major releases in subdependencies.

### To Reproduce

1. Use `openai==1.43.0` and `httpx==0.28.0` (also happens with higher versions of the openapi-python package)
2. Instantiate an object from the AzureOpenAI() class.

### Code snippets

```Python
client = AzureOpenAI(

File ""/usr/local/lib/python3.11/site-packages/openai/lib/azure.py"", line 205, in __init__
    super().__init__(

File ""/usr/local/lib/python3.11/site-packages/openai/_client.py"", line 123, in __init__
    super().__init__(

File ""/usr/local/lib/python3.11/site-packages/openai/_base_client.py"", line 844, in __init__
    self._client = http_client or SyncHttpxClientWrapper(

File ""/usr/local/lib/python3.11/site-packages/openai/_base_client.py"", line 742, in __init__
    super().__init__(**kwargs)

TypeError: Client.__init__() got an unexpected keyword argument 'proxies'
```


### OS

Debian Linux

### Python version

Python v3.11.1

### Library version

openai v1.43.0",closed,2024-12-05T16:21:30Z,2024-12-05T16:50:57Z,['bug'],https://github.com/openai/openai-python/issues/1925,0.49,True,missing_dependency_violation,high,"The AzureOpenAI class relies on a specific version of the httpx package, and the lack of version pinning leads to compatibility issues with newer versions, resulting in runtime errors.",high,
1902,Use of proxies kwarg in httpx==0.28.0,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

httpx version 0.28.0 removes the deprecated proxies argument; this argument is hardcoded in the library at the moment.

### To Reproduce

1. Update to httpx version 0.28.0

### Code snippets

```
  File ""/var/lang/lib/python3.10/site-packages/openai/_client.py"", line 337, in __init__
    super().__init__(
  File ""/var/lang/lib/python3.10/site-packages/openai/_base_client.py"", line 1438, in __init__
    self._client = http_client or AsyncHttpxClientWrapper(
  File ""/var/lang/lib/python3.10/site-packages/openai/_base_client.py"", line 1335, in __init__
    super().__init__(**kwargs)
TypeError: AsyncClient.__init__() got an unexpected keyword argument 'proxies'
```

### OS

macOS

### Python version

Python 3.10.12

### Library version

openai-1.55.2",closed,2024-11-28T15:37:43Z,2024-11-28T17:02:10Z,['bug'],https://github.com/openai/openai-python/issues/1902,1.41,True,input_type_violation,high,"The 'proxies' keyword argument is being passed to AsyncClient, which does not accept it in httpx version 0.28.0, leading to a TypeError.",high,
1123,Mypy typing error when treating a ChatCompletionSystemMessageParam as dict,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

We have a list like:

```
messages: list[ChatCompletionMessageParam]
```

We then convert a message to a dict:

```
dict(messages[0])
```

Mypy does not like that:


Argument 1 to ""dict"" has incompatible type ""ChatCompletionSystemMessageParam | ChatCompletionUserMessageParam | ChatCompletionAssistantMessageParam | ChatCompletionToolMessageParam | ChatCompletionFunctionMessageParam""; expected ""SupportsKeysAndGetItem[str, str]""  [arg-type]


I'm not sure if this is a bug with mypy or with the SDK, to be honest. Or maybe there's even a better approach in our code. I see that your classes extend TypedDict, so I'd think that should be considered as SupportsKeysAndGetItem, but I'm probably missing something.



### To Reproduce

See code above. You can also see our CI failing here: https://github.com/Azure-Samples/azure-search-openai-demo/actions/runs/7786990746/job/21233073297?pr=1233


### Code snippets

_No response_

### OS

MacOS

### Python version

3.11

### Library version

1.10.0",closed,2024-02-05T17:30:19Z,2024-12-04T17:00:22Z,['enhancement'],https://github.com/openai/openai-python/issues/1123,7271.5,True,input_type_violation,high,"The type of input provided to the 'dict' function does not match the expected type, leading to a typing error in Mypy.",high,
1736,"Even for Pydantic V1, getting the error ""warnings is only supported in Pydantic v2"" | Error while use streaming in Assistant","### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

There seems to be a issue in this PR. I'm using pydantic V1 in my code and I'm getting the error ""warnings is only supported in Pydantic v2"", which is defined in`_models.py`.

Note: We are not planning to change the pydantic version to V2 as it will be a breaking change.

PR where the bug started: https://github.com/openai/openai-python/pull/1722/files

Traceback:
OpenAi error:: Traceback (most recent call last):
  File ""/usr/local/lib/python3.10/site-packages/openai/lib/streaming/_assistants.py"", line 942, in accumulate_event
    block = current_message_snapshot.content[content_delta.index]
IndexError: list index out of range

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/app/app/openai/assistants/gpt/assistant_workflow.py"", line 142, in handle_incoming_message
    response = await gpt_assistant_wrapper.call_assistant_and_stream(
  File ""/app/app/openai/v2/api/assistants/assistant_wrapper.py"", line 212, in call_assistant_and_stream
    await stream.until_done()
  File ""/usr/local/lib/python3.10/site-packages/openai/lib/streaming/_assistants.py"", line 534, in until_done
    await consume_async_iterator(self)
  File ""/usr/local/lib/python3.10/site-packages/openai/_utils/_streams.py"", line 11, in consume_async_iterator
    async for _ in iterator:
  File ""/usr/local/lib/python3.10/site-packages/openai/lib/streaming/_assistants.py"", line 501, in __aiter__
    async for item in self._iterator:
  File ""/usr/local/lib/python3.10/site-packages/openai/lib/streaming/_assistants.py"", line 838, in __stream__
    await self._emit_sse_event(event)
  File ""/usr/local/lib/python3.10/site-packages/openai/lib/streaming/_assistants.py"", line 690, in _emit_sse_event
    self.__current_message_snapshot, new_content = accumulate_event(
  File ""/usr/local/lib/python3.10/site-packages/openai/lib/streaming/_assistants.py"", line 951, in accumulate_event
    value=content_delta.model_dump(exclude_unset=True, warnings=False),
  File ""/usr/local/lib/python3.10/site-packages/openai/_models.py"", line 304, in model_dump
    raise ValueError(""warnings is only supported in Pydantic v2"")
ValueError: warnings is only supported in Pydantic v2

### To Reproduce

Error is occuring while using openAi Assistant stream.
Pydantic version ""^1.10.7""
Error is in function `stream.until_done()`


### Code snippets

```Python
async with client.beta.threads.runs.stream(
            thread_id=thread_id,
            assistant_id=assistant_id,
            event_handler=main_event_handler,
        ) as stream:

            await stream.until_done()

Error happens in stream.until_done()
```


### OS

macOs

### Python version

Python v3.10.6

### Library version

openAi v1.47.2",closed,2024-09-23T10:41:02Z,2024-09-23T10:48:50Z,['bug'],https://github.com/openai/openai-python/issues/1736,0.13,True,missing_dependency_violation,high,"The code is attempting to use a feature (warnings) that is only supported in Pydantic v2, while the user is using Pydantic v1, indicating a missing dependency on the correct version of the library.",high,
1915,"httpx 0.28.0 removed proxies in httpx.Client, result in error","### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

httpx 0.28.0 removed proxies in httpx.Client, result in error:
```
  File "".../test.py"", line 3, in <module>
    client = OpenAI(
        api_key=""API_KEY"",
        base_url=""https://some_url/v1"",
    )
  File "".../lib/python3.13/site-packages/openai/_client.py"", line 123, in __init__
    super().__init__(
    ~~~~~~~~~~~~~~~~^
        version=__version__,
        ^^^^^^^^^^^^^^^^^^^^
    ...<6 lines>...
        _strict_response_validation=_strict_response_validation,
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "".../lib/python3.13/site-packages/openai/_base_client.py"", line 857, in __init__
    self._client = http_client or SyncHttpxClientWrapper(
                                  ~~~~~~~~~~~~~~~~~~~~~~^
        base_url=base_url,
        ^^^^^^^^^^^^^^^^^^
    ...<5 lines>...
        follow_redirects=True,
        ^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "".../lib/python3.13/site-packages/openai/_base_client.py"", line 755, in __init__
    super().__init__(**kwargs)
    ~~~~~~~~~~~~~~~~^^^^^^^^^^
TypeError: Client.__init__() got an unexpected keyword argument 'proxies'
```

### To Reproduce

With a python file with the following content can reproduce the error:
```
from openai import OpenAI

client = OpenAI(
    api_key=""API_KEY"",
    base_url=""https://some_url/v1"",
)
```

Output of `pip freeze`: (note httpx is 0.28.0)
```
annotated-types==0.7.0
anyio==4.6.2.post1
blinker==1.9.0
build==1.2.2.post1
certifi==2024.8.30
cffi==1.17.1
charset-normalizer==3.4.0
click==8.1.7
cryptography==43.0.3
distro==1.9.0
Flask==3.1.0
h11==0.14.0
httpcore==1.0.7
httpx==0.28.0
idna==3.10
itsdangerous==2.2.0
Jinja2==3.1.4
jiter==0.8.0
MarkupSafe==3.0.2
openai==1.54.4
packaging==24.2
pip-tools==7.4.1
pycparser==2.22
pydantic==2.10.2
pydantic_core==2.27.1
pyproject_hooks==1.2.0
python-dateutil==2.9.0.post0
python-dotenv==1.0.1
requests==2.32.3
setuptools==75.1.0
six==1.16.0
sniffio==1.3.1
tqdm==4.67.1
typing_extensions==4.12.2
urllib3==2.2.3
volcengine-python-sdk==1.0.109
Werkzeug==3.1.3
wheel==0.44.0
```


### Code snippets

_No response_

### OS

Linux Ubuntu 22.04.3 LTS

### Python version

Python 3.13.0

### Library version

openai==1.54.4",closed,2024-12-02T09:39:45Z,2024-12-02T09:42:27Z,['bug'],https://github.com/openai/openai-python/issues/1915,0.04,True,input_type_violation,high,"The OpenAI client initialization fails due to an unexpected keyword argument 'proxies' after the removal of proxies in httpx 0.28.0, indicating a type mismatch in expected parameters.",high,
1911,Request to Reactivate SoundCloud Link Analysis in ChatGPT,"### Confirm this is a feature request for the Python library and not the underlying OpenAI API.

- [X] This is a feature request for the Python library

### Describe the feature or improvement you're requesting

### Summary  
The ability to analyze SoundCloud links directly in ChatGPT was a critical feature for my creative workflow as a musician. Unfortunately, this functionality seems to have been disabled.

### Steps to Reproduce  
1. Provide a SoundCloud link to ChatGPT.  
2. Observe that it no longer analyzes or provides feedback on the audio.  

### Expected Behavior  
ChatGPT should be able to analyze the link, providing suggestions for dynamics, frequencies, and transitions.

### Current Behavior  
The feature is no longer functional, which impacts my ability to refine my music effectively.

### Context  
I relied heavily on this feature to improve my musical projects and compositions. It helped me gain valuable feedback that I cannot replicate manually. Its removal has disrupted my workflow significantly.

### Request  
Please consider reactivating this feature or providing an alternative method to analyze audio directly in ChatGPT.

Thank you for considering my request!

### Additional context

_No response_",closed,2024-11-29T20:26:42Z,2024-12-01T17:59:15Z,[],https://github.com/openai/openai-python/issues/1911,45.54,False,,,,low,
1909,Test Issue Creation,This is a test issue created via the API for demonstration purposes.,closed,2024-11-29T02:10:46Z,2024-11-29T02:11:10Z,[],https://github.com/openai/openai-python/issues/1909,0.01,False,,,,,
1908,New https release is breaking openai sdk while intitiating Async client,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

Recently, my codebase started to fail while initiating AsyncOpenAi. While debugging, I noticed AsyncOpenAI is using a version of `httpx` that accept keyword `proxies` but in the latest version (httpx 0.28.0), this keyword argument is removed from `__init__`.

To fix it, I manually limited installed httpx to use `0.27.2` which solves the problem but OpenAI SDK should have applied already that limitation on httpx allowed versions.

### To Reproduce

1. Install openai v1.30.1 with latest version of `httpx` (0.28)
2. Create a new instance of AsyncOpenAI
3. You should see the error at the instance creation.

### Code snippets

_No response_

### OS

Ubuntu

### Python version

3.9

### Library version

openai v1.30.1",closed,2024-11-28T20:16:02Z,2024-11-28T20:40:56Z,['bug'],https://github.com/openai/openai-python/issues/1908,0.41,True,missing_dependency_violation,high,"The OpenAI SDK does not specify a compatible version of httpx, leading to a failure when using an incompatible version.",high,
1647,"openai migrate - ""ERROR (code: 200) - Too many params for text: expected maximum 1"" ","### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

```ERROR (code: 200) - Too many params for text: expected maximum 1``` when running ```openai migrate```

### To Reproduce

openai migrate

### Code snippets

_No response_

### OS

macOS

### Python version

Python 3.12.5

### Library version

1.40.6",closed,2024-08-13T16:01:39Z,2024-08-14T11:33:49Z,['bug'],https://github.com/openai/openai-python/issues/1647,19.54,True,input_value_violation,medium,"The error indicates that too many parameters were provided for the 'text' method, exceeding the expected maximum of one parameter.",high,
1890,Primero,"### Confirm this is a feature request for the Python library and not the underlying OpenAI API.

- [X] This is a feature request for the Python library

### Describe the feature or improvement you're requesting

Abrir

### Additional context

Una meta
```[tasklist]
### Tasks
- [ ] https://github.com/openai/openai-python/pull/1889
```

```[tasklist]
### Tasks
- [ ] https://github.com/openai/openai-python/pull/1888
- [ ] https://github.com/openai/openai-python/pull/636
- [ ] https://github.com/openai/openai-python/pull/1889
- [ ] https://github.com/openai/openai-python/issues/1384
- [ ] https://github.com/openai/openai-python/issues/1869
```

```[tasklist]
### Tasks
- [ ] https://github.com/openai/openai-python/issues/1869
- [ ] https://github.com/openai/openai-python/pull/1888
- [ ] https://github.com/openai/openai-python/pull/1889
- [ ] https://github.com/openai/openai-python/issues/1884
- [ ] https://github.com/openai/openai-python/pull/1886
- [ ] https://github.com/openai/openai-python/pull/636
- [ ] https://github.com/openai/openai-python/issues/873
```
",closed,2024-11-24T09:39:27Z,2024-11-24T09:39:50Z,[],https://github.com/openai/openai-python/issues/1890,0.01,True,missing_dependency_violation,medium,"The issue lacks clear context or dependencies related to the feature request, which may lead to misunderstandings about the intended functionality.",medium,
1384,"Streaming wwwith ""chat.completions.create"" endpoint produces a httpx.RemoteProtocolError","### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

base_url:  links fastchat with the openai API
LLM for inference: ""mistralai/Mixtral-8x7B-Instruct-v0.1""

When I try to stream chunks of generated text I get this error:
```
Traceback (most recent call last):
  File ""/home/philip/miniconda3/lib/python3.12/site-packages/httpx/_transports/default.py"", line 69, in map_httpcore_exceptions
    yield
  File ""/home/philip/miniconda3/lib/python3.12/site-packages/httpx/_transports/default.py"", line 113, in __iter__
    for part in self._httpcore_stream:
  File ""/home/philip/miniconda3/lib/python3.12/site-packages/httpcore/_sync/connection_pool.py"", line 367, in __iter__
    raise exc from None
  File ""/home/philip/miniconda3/lib/python3.12/site-packages/httpcore/_sync/connection_pool.py"", line 363, in __iter__
    for part in self._stream:
  File ""/home/philip/miniconda3/lib/python3.12/site-packages/httpcore/_sync/http11.py"", line 349, in __iter__
    raise exc
  File ""/home/philip/miniconda3/lib/python3.12/site-packages/httpcore/_sync/http11.py"", line 341, in __iter__
    for chunk in self._connection._receive_response_body(**kwargs):
  File ""/home/philip/miniconda3/lib/python3.12/site-packages/httpcore/_sync/http11.py"", line 210, in _receive_response_body
    event = self._receive_event(timeout=timeout)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/philip/miniconda3/lib/python3.12/site-packages/httpcore/_sync/http11.py"", line 220, in _receive_event
    with map_exceptions({h11.RemoteProtocolError: RemoteProtocolError}):
  File ""/home/philip/miniconda3/lib/python3.12/contextlib.py"", line 158, in __exit__
    self.gen.throw(value)
  File ""/home/philip/miniconda3/lib/python3.12/site-packages/httpcore/_exceptions.py"", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.RemoteProtocolError: peer closed connection without sending complete message body (incomplete chunked read)

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File ""/home/philip/opena.py"", line 19, in <module>
    for chunk in stream:
  File ""/home/philip/miniconda3/lib/python3.12/site-packages/openai/_streaming.py"", line 46, in __iter__
    for item in self._iterator:
  File ""/home/philip/miniconda3/lib/python3.12/site-packages/openai/_streaming.py"", line 58, in __stream__
    for sse in iterator:
  File ""/home/philip/miniconda3/lib/python3.12/site-packages/openai/_streaming.py"", line 50, in _iter_events
    yield from self._decoder.iter_bytes(self.response.iter_bytes())
  File ""/home/philip/miniconda3/lib/python3.12/site-packages/openai/_streaming.py"", line 280, in iter_bytes
    for chunk in self._iter_chunks(iterator):
  File ""/home/philip/miniconda3/lib/python3.12/site-packages/openai/_streaming.py"", line 291, in _iter_chunks
    for chunk in iterator:
  File ""/home/philip/miniconda3/lib/python3.12/site-packages/httpx/_models.py"", line 829, in iter_bytes
    for raw_bytes in self.iter_raw():
  File ""/home/philip/miniconda3/lib/python3.12/site-packages/httpx/_models.py"", line 883, in iter_raw
    for raw_stream_bytes in self.stream:
  File ""/home/philip/miniconda3/lib/python3.12/site-packages/httpx/_client.py"", line 126, in __iter__
    for chunk in self._stream:
  File ""/home/philip/miniconda3/lib/python3.12/site-packages/httpx/_transports/default.py"", line 112, in __iter__
    with map_httpcore_exceptions():
  File ""/home/philip/miniconda3/lib/python3.12/contextlib.py"", line 158, in __exit__
    self.gen.throw(value)
  File ""/home/philip/miniconda3/lib/python3.12/site-packages/httpx/_transports/default.py"", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: peer closed connection without sending complete message body (incomplete chunked read)
```
**Note** when I dont stream `stream = False` I don't get any errors and I get my inference.

### To Reproduce

Simply run the code below, using the LLM I used, as well as python and openai versions and so on.

### Code snippets

```Python
from openai import OpenAI

client = OpenAI(
    # This is the default and can be omitted
    api_key=""myKey"",
    base_url=""https://base_url.io/v1""
)

stream = client.chat.completions.create(
    messages=[
        {
            ""role"": ""user"",
            ""content"": ""Say this is a test"",
        }
    ],
    model=""mistralai/Mixtral-8x7B-Instruct-v0.1"",
    stream=True
)
for chunk in stream:
    if chunk.choices[0].delta.content is not None:
        print(chunk.choices[0].delta.content, end="""")
```


### OS

linux

### Python version

Python 3.11.8

### Library version

openai v.1.24.0",closed,2024-04-30T02:29:50Z,2024-05-13T01:43:30Z,['bug'],https://github.com/openai/openai-python/issues/1384,311.23,True,input_value_violation,performance_degradation,"The streaming functionality is producing a RemoteProtocolError due to incomplete message body, indicating potential issues with input values or streaming configuration.",high,
873,peer closed connection without sending complete message body occurred when use different session to send msg at same time,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

background:

1) I used llama2 as backend for inferenceing
2) use chanlit to start a frontend for user.
And I created two users, then I logined with user in different browser.
If one session was inferencing now, and I send msg in another session, it will raise error as above.

1) And if I changed ""openai_api_base"" to openai api endpoint, everything works.
2) if I changed ""openai_api_base"" to my local api endpoint(for example llama2 service endpoint), it will raise error.

From above 3 and 4 seems like my local endpoint is  borken right? But I've tried use concurrent to send multiple requests at a same time, everything works. (my endpoints based on llama.cpp)

So I suspect if there is some differences between openai official endpoint with local endpoint, especially when use openai library to send multiple completions request to local endpoint.

Many thx

### To Reproduce

1. start a local llama2 service which based on llama.cpp and llama-cpp-python
2. start a frontend service based on chainlit
3. open two webpage in different browsers
4. to start to chat on frontend at same time, the later one will interrupt former one.

```

2023-11-24 00:19:39 - Your app is available at http://localhost:8001
2023-11-24 00:19:47 - HTTP Request: POST http://localhost:8000/v1/chat/completions ""HTTP/1.1 200 OK""
2023-11-24 00:19:49 - Error in LangchainTracer.on_llm_error callback: IndexError('list index out of range')
2023-11-24 00:19:49 - peer closed connection without sending complete message body (incomplete chunked read)
Traceback (most recent call last):
  File ""/Users/vince/miniconda3/envs/python310env/lib/python3.10/site-packages/httpcore/_exceptions.py"", line 10, in map_exceptions
    yield
  File ""/Users/vince/miniconda3/envs/python310env/lib/python3.10/site-packages/httpcore/_async/http11.py"", line 209, in _receive_event
    event = self._h11_state.next_event()
  File ""/Users/vince/miniconda3/envs/python310env/lib/python3.10/site-packages/h11/_connection.py"", line 469, in next_event
    event = self._extract_next_receive_event()
  File ""/Users/vince/miniconda3/envs/python310env/lib/python3.10/site-packages/h11/_connection.py"", line 419, in _extract_next_receive_event
    event = self._reader.read_eof()  # type: ignore[attr-defined]
  File ""/Users/vince/miniconda3/envs/python310env/lib/python3.10/site-packages/h11/_readers.py"", line 204, in read_eof
    raise RemoteProtocolError(
h11._util.RemoteProtocolError: peer closed connection without sending complete message body (incomplete chunked read)

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File ""/Users/vince/miniconda3/envs/python310env/lib/python3.10/site-packages/httpx/_transports/default.py"", line 66, in map_httpcore_exceptions
    yield
  File ""/Users/vince/miniconda3/envs/python310env/lib/python3.10/site-packages/httpx/_transports/default.py"", line 249, in __aiter__
    async for part in self._httpcore_stream:
  File ""/Users/vince/miniconda3/envs/python310env/lib/python3.10/site-packages/httpcore/_async/connection_pool.py"", line 361, in __aiter__
    async for part in self._stream:
  File ""/Users/vince/miniconda3/envs/python310env/lib/python3.10/site-packages/httpcore/_async/http11.py"", line 337, in __aiter__
    raise exc
  File ""/Users/vince/miniconda3/envs/python310env/lib/python3.10/site-packages/httpcore/_async/http11.py"", line 329, in __aiter__
    async for chunk in self._connection._receive_response_body(**kwargs):
  File ""/Users/vince/miniconda3/envs/python310env/lib/python3.10/site-packages/httpcore/_async/http11.py"", line 198, in _receive_response_body
    event = await self._receive_event(timeout=timeout)
  File ""/Users/vince/miniconda3/envs/python310env/lib/python3.10/site-packages/httpcore/_async/http11.py"", line 208, in _receive_event
    with map_exceptions({h11.RemoteProtocolError: RemoteProtocolError}):
  File ""/Users/vince/miniconda3/envs/python310env/lib/python3.10/contextlib.py"", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File ""/Users/vince/miniconda3/envs/python310env/lib/python3.10/site-packages/httpcore/_exceptions.py"", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.RemoteProtocolError: peer closed connection without sending complete message body (incomplete chunked read)

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File ""/Users/vince/miniconda3/envs/python310env/lib/python3.10/site-packages/chainlit/utils.py"", line 39, in wrapper
    return await user_function(**params_values)
  File ""/Users/vince/Downloads/myworkspace/llm/llm/chainlit_test/profile/app.py"", line 92, in on_message
    async for chunk in runnable.astream(
  File ""/Users/vince/miniconda3/envs/python310env/lib/python3.10/site-packages/langchain/schema/runnable/base.py"", line 1802, in astream
    async for chunk in self.atransform(input_aiter(), config, **kwargs):
  File ""/Users/vince/miniconda3/envs/python310env/lib/python3.10/site-packages/langchain/schema/runnable/base.py"", line 1788, in atransform
    async for chunk in self._atransform_stream_with_config(
  File ""/Users/vince/miniconda3/envs/python310env/lib/python3.10/site-packages/langchain/schema/runnable/base.py"", line 1158, in _atransform_stream_with_config
    async for chunk in iterator:
  File ""/Users/vince/miniconda3/envs/python310env/lib/python3.10/site-packages/langchain/schema/runnable/base.py"", line 1761, in _atransform
    async for output in final_pipeline:
  File ""/Users/vince/miniconda3/envs/python310env/lib/python3.10/site-packages/langchain/schema/output_parser.py"", line 343, in atransform
    async for chunk in self._atransform_stream_with_config(
  File ""/Users/vince/miniconda3/envs/python310env/lib/python3.10/site-packages/langchain/schema/runnable/base.py"", line 1158, in _atransform_stream_with_config
    async for chunk in iterator:
  File ""/Users/vince/miniconda3/envs/python310env/lib/python3.10/site-packages/langchain/schema/output_parser.py"", line 321, in _atransform
    async for chunk in input:
  File ""/Users/vince/miniconda3/envs/python310env/lib/python3.10/site-packages/langchain/utils/aiter.py"", line 97, in tee_peer
    item = await iterator.__anext__()
  File ""/Users/vince/miniconda3/envs/python310env/lib/python3.10/site-packages/langchain/schema/runnable/base.py"", line 683, in atransform
    async for output in self.astream(final, config, **kwargs):
  File ""/Users/vince/miniconda3/envs/python310env/lib/python3.10/site-packages/langchain/chat_models/base.py"", line 273, in astream
    raise e
  File ""/Users/vince/miniconda3/envs/python310env/lib/python3.10/site-packages/langchain/chat_models/base.py"", line 262, in astream
    async for chunk in self._astream(
  File ""/Users/vince/miniconda3/envs/python310env/lib/python3.10/site-packages/langchain/chat_models/openai.py"", line 469, in _astream
    async for chunk in await acompletion_with_retry(
  File ""/Users/vince/miniconda3/envs/python310env/lib/python3.10/site-packages/openai/_streaming.py"", line 94, in __aiter__
    async for item in self._iterator:
  File ""/Users/vince/miniconda3/envs/python310env/lib/python3.10/site-packages/openai/_streaming.py"", line 123, in __stream__
    async for sse in iterator:
  File ""/Users/vince/miniconda3/envs/python310env/lib/python3.10/site-packages/openai/_streaming.py"", line 98, in _iter_events
    async for sse in self._decoder.aiter(self.response.aiter_lines()):
  File ""/Users/vince/miniconda3/envs/python310env/lib/python3.10/site-packages/openai/_streaming.py"", line 190, in aiter
    async for line in iterator:
  File ""/Users/vince/miniconda3/envs/python310env/lib/python3.10/site-packages/httpx/_models.py"", line 966, in aiter_lines
    async for text in self.aiter_text():
  File ""/Users/vince/miniconda3/envs/python310env/lib/python3.10/site-packages/httpx/_models.py"", line 953, in aiter_text
    async for byte_content in self.aiter_bytes():
  File ""/Users/vince/miniconda3/envs/python310env/lib/python3.10/site-packages/httpx/_models.py"", line 932, in aiter_bytes
    async for raw_bytes in self.aiter_raw():
  File ""/Users/vince/miniconda3/envs/python310env/lib/python3.10/site-packages/httpx/_models.py"", line 990, in aiter_raw
    async for raw_stream_bytes in self.stream:
  File ""/Users/vince/miniconda3/envs/python310env/lib/python3.10/site-packages/httpx/_client.py"", line 146, in __aiter__
    async for chunk in self._stream:
  File ""/Users/vince/miniconda3/envs/python310env/lib/python3.10/site-packages/httpx/_transports/default.py"", line 248, in __aiter__
    with map_httpcore_exceptions():
  File ""/Users/vince/miniconda3/envs/python310env/lib/python3.10/contextlib.py"", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File ""/Users/vince/miniconda3/envs/python310env/lib/python3.10/site-packages/httpx/_transports/default.py"", line 83, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: peer closed connection without sending complete message body (incomplete chunked read)
2023-11-24 00:19:49 - HTTP Request: POST http://localhost:8000/v1/chat/completions ""HTTP/1.1 200 OK""
```

### Code snippets

```Python
import json
from typing import Optional

import chainlit as cl
from langchain.chat_models import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from langchain.schema import StrOutputParser
from langchain.schema.runnable import Runnable
from langchain.schema.runnable.config import RunnableConfig
import platform

url = 'xxxx'  # local service endpoint
url = 'openai endpoint'


@cl.set_chat_profiles
async def chat_profile(current_user: cl.AppUser):
    if ""admin_user"" not in current_user.tags:
        # Default to 3.5 when not admin
        return [
            cl.ChatProfile(
                name=""GPT-3.5"",
                markdown_description=""The underlying LLM model is **GPT-3.5**."",
                icon=""https://picsum.photos/200"",
            )
        ]

    return [
        cl.ChatProfile(
            name=""GPT-3.5"",
            markdown_description=""The underlying LLM model is **GPT-3.5**."",
            icon=""https://picsum.photos/200"",
        ),
        cl.ChatProfile(
            name=""GPT-4"",
            markdown_description=""The underlying LLM model is **GPT-4**."",
            icon=""https://picsum.photos/250"",
        ),
    ]


@cl.password_auth_callback
def auth_callback(username: str, password: str) -> Optional[cl.AppUser]:
    auth = None
    with open('authentication.json', 'r') as f:
        auth = json.load(f)
    if (username, password) == (""admin"", ""admin""):
        return cl.AppUser(
            username=""admin"", role=""ADMIN"", provider=""credentials"", tags=[""admin_user""]
        )
    elif username == auth.get(username, None):
        return cl.AppUser(
            username=username, role=""USER"", provider=""credentials"", tags=[""regular_user""]
        )
    else:
        return None


@cl.on_chat_start
async def on_chat_start():
    app_user = cl.user_session.get(""user"")
    chat_profile = cl.user_session.get(""chat_profile"")
    await cl.Message(
        content=f""starting chat with {app_user.username} using the {chat_profile} chat profile""
    ).send()

    model = ChatOpenAI(temperature=0.0, openai_api_base=url, openai_api_key=""xxx"", max_tokens=2048, streaming=True)
    prompt = ChatPromptTemplate.from_messages(
        [
            (
                ""system"",
                # ""You're a very knowledgeable historian who provides accurate and eloquent answers to historical questions."",
                ""You're a Linux expert who provides accurate and eloquent answers to Linux system related questions. Only answer the question about Linux""
            ),
            (""human"", ""{question}""),
        ]
    )
    runnable = prompt | model | StrOutputParser()
    cl.user_session.set(app_user.username, runnable)


@cl.on_message
async def on_message(message: cl.Message):
    runnable = cl.user_session.get(message.author)  # type: Runnable
    print(message.author, message.content)
    msg = cl.Message(content="""")

    async for chunk in runnable.astream(
            {""question"": message.content},
            config=RunnableConfig(callbacks=[cl.LangchainCallbackHandler()]),
    ):
        await msg.stream_token(chunk)
```


### OS

macos

### Python version

python 3.10

### Library version

openai 1.3.5",closed,2023-11-23T16:39:31Z,2023-11-23T16:56:29Z,[],https://github.com/openai/openai-python/issues/873,0.28,True,missing_dependency_violation,medium,"The local endpoint may not handle concurrent requests properly, leading to incomplete message body errors when multiple sessions interact simultaneously.",high,
1884,pip Install openai in android cli,"### Confirm this is a feature request for the Python library and not the underlying OpenAI API.

- [X] This is a feature request for the Python library

### Describe the feature or improvement you're requesting

When I install via `pip install openai`, I encounter `can't find Rust compiler`. Since rust does not support the android host temporarily, is there an alternative solution?
``` bash
(ENV) msmnile_gvmq:/data/local/tmp $ pip install openai
Collecting openai
  Using cached openai-1.55.0-py3-none-any.whl.metadata (24 kB)
Requirement already satisfied: anyio<5,>=3.5.0 in ./ENV/lib/python3.12/site-packages (from openai) (4.6.2.post1)
Collecting distro<2,>=1.7.0 (from openai)
  Using cached distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)
Requirement already satisfied: httpx<1,>=0.23.0 in ./ENV/lib/python3.12/site-packages (from openai) (0.27.2)
Collecting jiter<1,>=0.4.0 (from openai)
  Using cached jiter-0.7.1.tar.gz (162 kB)
  Installing build dependencies ... error
  error: subprocess-exited-with-error

  × pip subprocess to install build dependencies did not run successfully.
  │ exit code: 1
  ╰─> [50 lines of output]
      Collecting maturin<2,>=1
        Using cached maturin-1.7.4.tar.gz (191 kB)
        Installing build dependencies: started
        Installing build dependencies: finished with status 'done'
        Getting requirements to build wheel: started
        Getting requirements to build wheel: finished with status 'done'
        Preparing metadata (pyproject.toml): started
        Preparing metadata (pyproject.toml): finished with status 'done'
      Building wheels for collected packages: maturin
        Building wheel for maturin (pyproject.toml): started
        Building wheel for maturin (pyproject.toml): finished with status 'error'
        error: subprocess-exited-with-error

        × Building wheel for maturin (pyproject.toml) did not run successfully.
        │ exit code: 1
        ╰─> [28 lines of output]
            running bdist_wheel
            running build
            running build_py
            creating build/lib.linux-armv8l-cpython-312/maturin
            copying maturin/__init__.py -> build/lib.linux-armv8l-cpython-312/maturin
            copying maturin/__main__.py -> build/lib.linux-armv8l-cpython-312/maturin
            running egg_info
            writing maturin.egg-info/PKG-INFO
            writing dependency_links to maturin.egg-info/dependency_links.txt
            writing requirements to maturin.egg-info/requires.txt
            writing top-level names to maturin.egg-info/top_level.txt
            reading manifest file 'maturin.egg-info/SOURCES.txt'
            reading manifest template 'MANIFEST.in'
            warning: no files found matching '*.json' under directory 'src/python_interpreter'
            writing manifest file 'maturin.egg-info/SOURCES.txt'
            running build_ext
            running build_rust
            **error: can't find Rust compiler**

            If you are using an outdated pip version, it is possible a prebuilt wheel is available for this package but pip is not able to install from it. Installing from the wheel would avoid the need for a Rust compiler.

            To update pip, run:

                pip install --upgrade pip

            and then retry package installation.

            If you did intend to build this package from source, try installing a Rust compiler from your system package manager and ensure it is on the PATH during installation. Alternatively, rustup (available at https://rustup.rs) is the recommended way to download and update the Rust compiler toolchain.
            [end of output]

        note: This error originates from a subprocess, and is likely not a problem with pip.
        ERROR: Failed building wheel for maturin
      Failed to build maturin
      ERROR: ERROR: Failed to build installable wheels for some pyproject.toml based projects (maturin)
      [end of output]

  note: This error originates from a subprocess, and is likely not a problem with pip.
error: subprocess-exited-with-error

× pip subprocess to install build dependencies did not run successfully.
│ exit code: 1
╰─> See above for output.

note: This error originates from a subprocess, and is likely not a problem with pip.
```

### Additional context

_No response_",closed,2024-11-22T03:32:23Z,2024-11-22T12:27:49Z,[],https://github.com/openai/openai-python/issues/1884,8.92,True,missing_dependency_violation,High,"The installation process fails due to the absence of a Rust compiler, which is a required dependency for building the 'maturin' package necessary for the 'openai' library.",high,
1793,Invalid API key environment variables lead to confusing encoding errors,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

Referring to [this issue](https://github.com/langchain-ai/langchain/issues/27365)

When the API key is accidentally set with invalid characters, an error occurs within the HTTP client when openai library attempts to set headers.  

While eventually one might deduce the problem from the stack trace, having some sort of upstream ""check"" and more informative error about the API key would seem to help users understand the error. This caused us to spend multiple days trying to find the source of the problem assuming the error was in the message content instead of in the API key.

### To Reproduce

1. Set the API key to a value with non-ASCII characters
2. Execute a chat completion with the client
3. Receive encoding error `UnicodeEncodeError: 'ascii' codec can't encode characters in position 7-34: ordinal not in range(128)`

### Code snippets

```Python
from openai import OpenAI

#######################
#### CONFIGURATION ####
#######################

configs = dict(
    api_key = 'здравейздравейздравейздравей',
)

client = OpenAI(**configs)

######################
###### RUN TEST ######
######################

message = 'Hello!'
completion = client.chat.completions.create(
    model='gpt-4-turbo-preview',
    messages=[
        {
            'role': 'user',
            'content': message
        }
    ]    
)

print(completion.choices[0].message.content)
```


### OS

any

### Python version

3.11

### Library version

openai v1.44.0",closed,2024-10-16T01:23:57Z,2024-11-22T12:22:47Z,['bug'],https://github.com/openai/openai-python/issues/1793,898.98,True,input_value_violation,performance_degradation,Invalid API key with non-ASCII characters leads to encoding errors during header setting.,high,
1882,"Pydantic-V1 ""warnings"" error","### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

I am getting this Error:
```
File ""/Users/konnoryoung/Documents/projects/contract-ai-poc/.errors/lib/python3.9/site-packages/openai/lib/streaming/_assistants.py"", line 943, in accumulate_event
block = current_message_snapshot.content[content_delta.index]
IndexError: list index out of range
```

Which comes from the try/except block in the ""accumulate_event"" function:

```
try:
    block = current_message_snapshot.content[content_delta.index]
except IndexError:
    current_message_snapshot.content.insert(
    content_delta.index,
    cast(
        MessageContent,
        construct_type(
            # mypy doesn't allow Content for some reason
            type_=cast(Any, MessageContent),
            value=model_dump(content_delta, exclude_unset=True, warnings=False),
            ),
        ),
    )
    new_content.append(content_delta)
```

That wouldn't stop my code from running except that it raises another exception:

```
File ""/Users/konnoryoung/Documents/projects/contract-ai-poc/.errors/lib/python3.9/site-packages/openai/_models.py"", line 313, in model_dump
raise ValueError(""warnings is only supported in Pydantic v2"")
ValueError: warnings is only supported in Pydantic v2
```

Which comes from the if statement:

```
if warnings != True:
    raise ValueError(""warnings is only supported in Pydantic v2"")
```

In the `if not PYDANTIC_V2` version of the `model_dump` method of the `BaseModel` class in the _model.py file.

### To Reproduce

1- Pydantic < 2 (I have v1.9.0)
2- Create an assistant
3- Create a thread
4- Attempt to run an Async Stream (my code is below)



### Code snippets

```Python
from __future__ import annotations

import asyncio

import openai

client = openai.AsyncOpenAI()

assistant_id= """""" GetID OR Create an Assistant """"""
thread_id= """"""  GetID OR Create a Thread """"""
async def main() -> None:
    async with client.beta.threads.runs.stream(
        thread_id=thread_id,
        assistant_id=assistant_id,
        ) as stream:
            async for event in stream:
                print()

asyncio.run(main())
```


### OS

masOS

### Python version

Python 3.9.6

### Library version

openai v1.55.0",closed,2024-11-21T18:18:32Z,2024-11-22T11:24:23Z,['bug'],https://github.com/openai/openai-python/issues/1882,17.1,True,input_value_violation,high,"The code attempts to access an index in a list that is out of range, leading to an IndexError.",high,
1883,Rock,"### Confirm this is a feature request for the Python library and not the underlying OpenAI API.

- [X] This is a feature request for the Python library

### Describe the feature or improvement you're requesting

We use [Rye](https://rye.astral.sh/) to manage dependencies because it will automatically provision a Python environment with the expected Python version. To set it up, run:

```sh
$ ./scripts/bootstrap
```

Or [install Rye manually](https://rye.astral.sh/guide/installation/) and run:

```sh
$ rye sync --all-features
```

You can then run scripts using `rye run python script.py` or by activating the virtual environment:

```sh
$ rye shell
# or manually activate - https://docs.python.org/3/library/venv.html#how-venvs-work
$ source .venv/bin/activate

# now you can omit the `rye run` prefix
$ python script.py
```

### Without Rye

Alternatively if you don't want to install `Rye`, you can stick with the standard `pip` setup by ensuring you have the Python version specified in `.python-version`, create a virtual environment however you desire and then install dependencies using this command:

```sh
$ pip install -r requirements-dev.lock
```

## Modifying/Adding code

Most of the SDK is generated code. Modifications to code will be persisted between generations, but may
result in merge conflicts between manual patches and changes from the generator. The generator will never
modify the contents of the `src/openai/lib/` and `examples/` directories.

## Adding and running examples

All files in the `examples/` directory are not modified by the generator and can be freely edited or added to.

```py
# add an example to examples/<your-example>.py

#!/usr/bin/env -S rye run python
…
```

```sh
$ chmod +x examples/<your-example>.py
# run the example against your api
$ ./examples/<your-example>.py
```

## Using the repository from source

If you’d like to use the repository from source, you can either install from git or link to a cloned repository:

To install via git:

```sh
$ pip install git+ssh://git@github.com/openai/openai-python.git
```

Alternatively, you can build from source and install the wheel file:

Building this package will create two files in the `dist/` directory, a `.tar.gz` containing the source files and a `.whl` that can be used to install the package efficiently.

To create a distributable version of the library, all you have to do is run this command:

```sh
$ rye build
# or
$ python -m build
```

Then to install:

```sh
$ pip install ./path-to-wheel-file.whl
```

## Running tests

Most tests require you to [set up a mock server](https://github.com/stoplightio/prism) against the OpenAPI spec to run the tests.

```sh
# you will need npm installed
$ npx prism mock path/to/your/openapi.yml
```

```sh
$ ./scripts/test
```

## Linting and formatting

This repository uses [ruff](https://github.com/astral-sh/ruff) and
[black](https://github.com/psf/black) to format the code in the repository.

To lint:

```sh
$ ./scripts/lint
```

To format and fix all ruff issues automatically:

```sh
$ ./scripts/format
```




### Additional context


## Publishing and releases

Changes made to this repository via the automated release PR pipeline should publish to PyPI automatically. If
the changes aren't made through the automated pipeline, you may want to make releases manually.

### Publish with a GitHub workflow

You can release to package managers by using [the `Publish PyPI` GitHub action](https://www.github.com/openai/openai-python/actions/workflows/publish-pypi.yml). This requires a setup organization or repository secret to be set up.

### Publish manually

If you need to manually release a package, you can run the `bin/publish-pypi` script with a `PYPI_TOKEN` set on
the environement.",closed,2024-11-22T01:56:07Z,2024-11-22T07:50:36Z,[],https://github.com/openai/openai-python/issues/1883,5.91,False,,,,,
1881,make prompt caching available through api for prompts with less than 1024 token,"### Confirm this is a feature request for the Python library and not the underlying OpenAI API.

- [X] This is a feature request for the Python library

### Describe the feature or improvement you're requesting

we could make great use of this awesome feature but our prompts are only around 700 tokens big

### Additional context

_No response_",closed,2024-11-21T09:28:47Z,2024-11-21T09:29:41Z,[],https://github.com/openai/openai-python/issues/1881,0.01,False,,,,,
1827,AnyIO worker threads not terminating when using asyncify with get_platform,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

When using the OpenAI Python library (version 1.52.2), I encountered an issue where AnyIO worker threads are not terminating properly after asynchronous requests. This leads to tests hanging and not exiting when using `pytest`. The problem occurs because the library utilizes `asyncify(get_platform)` to call the synchronous `get_platform` function in an asynchronous context, causing AnyIO to create non-daemon worker threads that remain alive after the request completes.

### To Reproduce

Use the OpenAI Python library in an asynchronous environment.
Make an asynchronous API request that triggers the `_request` method.
Run tests using `pytest`.
Observe that after the tests complete, the process hangs and does not exit due to lingering AnyIO worker threads.

### Code snippets

The issue arises in the _request method where asyncify is used:

```Python
async def _request(
    self,
    cast_to: Type[ResponseT],
    options: FinalRequestOptions,
    *,
    stream: bool,
    stream_cls: type[_AsyncStreamT] | None,
    retries_taken: int,
) -> ResponseT | _AsyncStreamT:
    if self._platform is None:
        # `get_platform` can make blocking IO calls so we
        # execute it earlier while we are in an async context
        self._platform = await asyncify(get_platform)()
    # ... rest of the method ...
```
Expected behavior

The AnyIO worker threads should terminate after completing their tasks, allowing the program or tests to exit cleanly without hanging.

### OS

macOS

### Python version

Python 3.12.4

### Library version

1.52.2",closed,2024-10-27T13:30:41Z,2024-11-18T12:42:40Z,['bug'],https://github.com/openai/openai-python/issues/1827,527.2,True,method_order_violation,medium,"The use of asyncify with get_platform in an asynchronous context leads to lingering AnyIO worker threads, causing tests to hang and not exit properly.",high,
254,Ability to batch prompts in Chat Completion like for the Completion endpoint.,"### Describe the feature or improvement you're requesting

In the Completion endpoint we can batch input prompts in one API request. It would be nice to have the same feature for the Chat Completion. 

### Additional context

_No response_",closed,2023-03-02T14:17:25Z,2023-03-03T16:29:02Z,['enhancement'],https://github.com/openai/openai-python/issues/254,26.19,False,,,,low,
1866,Does logit_bias work for audio_preview?,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

https://platform.openai.com/docs/api-reference/chat/create#logit_bias

### To Reproduce

When I pass logit_bias into the text-only models I get substantially different results than when I pass it into gpt-4o-audio-preview. It doesn't reject the logit_bias which makes me believe it is valid, but I'm unusure it's having an effect. Could you confirm? Thanks! 

### Code snippets

_No response_

### OS

macOS

### Python version

Python V2.11.4

### Library version

openai v1.01",closed,2024-11-13T16:07:19Z,2024-11-14T11:31:14Z,['API-feedback'],https://github.com/openai/openai-python/issues/1866,19.4,True,missing_option_violation,medium,"The logit_bias parameter may not be supported or function as intended in the gpt-4o-audio-preview model, leading to inconsistent behavior.",high,
1864,ModuleNotFoundError: No module named 'jiter.jiter' when Importing openai Package,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

Description
I'm encountering a ModuleNotFoundError when attempting to import the openai Python library. The error indicates that the jiter.jiter module is missing, preventing the import from succeeding.

Error Message
plaintext
Copy code
ModuleNotFoundError: No module named 'jiter.jiter'

### To Reproduce

Steps to Reproduce
Set Up Virtual Environment:

bash
Copy code
python -m venv myenv_py311
myenv_py311\Scripts\activate  # On Windows
Install openai:

bash
Copy code
pip install openai==1.54.3
Also tried openai==1.53.0 with the same result.
Install jiter from GitHub:

bash
Copy code
pip install git+https://github.com/openai/jiter.git
Attempt to Import openai:

python
Copy code
import openai
Result: Raises ModuleNotFoundError: No module named 'jiter.jiter'
Additional Steps Taken:

Uninstalled External jiter:
bash
Copy code
pip uninstall jiter
Reinstalled openai:
bash
Copy code
pip uninstall openai
pip install openai
Verified Package Structure:
Checked the jiter package directory; jiter.py is present after installing from GitHub.
Created a Fresh Virtual Environment:
Issue persists even in a new environment.

### Code snippets

```Python
(myenv) D:\Resume Project>python -c ""import openai""
Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
    import openai
  File ""D:\Resume Project\myenv\Lib\site-packages\openai\__init__.py"", line 11, in <module>
    from ._client import Client, OpenAI, Stream, Timeout, Transport, AsyncClient, AsyncOpenAI, AsyncStream, RequestOptions
  File ""D:\Resume Project\myenv\Lib\site-packages\openai\_client.py"", line 11, in <module>
    from . import resources, _exceptions
  File ""D:\Resume Project\myenv\Lib\site-packages\openai\resources\__init__.py"", line 3, in <module>
    from .beta import (
    ...<6 lines>...
    )
  File ""D:\Resume Project\myenv\Lib\site-packages\openai\resources\beta\__init__.py"", line 3, in <module>
    from .beta import (
    ...<6 lines>...
    )
  File ""D:\Resume Project\myenv\Lib\site-packages\openai\resources\beta\beta.py"", line 14, in <module>
    from .chat.chat import Chat, AsyncChat
  File ""D:\Resume Project\myenv\Lib\site-packages\openai\resources\beta\chat\__init__.py"", line 3, in <module>
    from .chat import Chat, AsyncChat
  File ""D:\Resume Project\myenv\Lib\site-packages\openai\resources\beta\chat\chat.py"", line 6, in <module>
    from .completions import Completions, AsyncCompletions
  File ""D:\Resume Project\myenv\Lib\site-packages\openai\resources\beta\chat\completions.py"", line 27, in <module>
    from ....lib.streaming.chat import ChatCompletionStreamManager, AsyncChatCompletionStreamManager
  File ""D:\Resume Project\myenv\Lib\site-packages\openai\lib\streaming\chat\__init__.py"", line 21, in <module>
    from ._completions import (
    ...<4 lines>...
    )
  File ""D:\Resume Project\myenv\Lib\site-packages\openai\lib\streaming\chat\_completions.py"", line 8, in <module>
    from jiter import from_json
  File ""D:\Resume Project\myenv\Lib\site-packages\jiter\__init__.py"", line 1, in <module>
    from .jiter import *
ModuleNotFoundError: No module named 'jiter.jiter'
```


### OS

Windows 11

### Python version

Python 3.13

### Library version

both OpenAI 1.54. 3 AND 1.53.0",closed,2024-11-12T12:08:03Z,2024-11-12T13:27:18Z,['question'],https://github.com/openai/openai-python/issues/1864,1.32,True,missing_dependency_violation,high,"The openai library is dependent on the jiter module, which is not found, leading to import failure.",high,
1863,limit parameter in messages.list does not work.,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

When I try to fetch a limited number of messages from a thread, I receive all the messages instead.

### To Reproduce

1. Fetch a thread
2. Retrieve the messages of the thread with limit parameter
3. Iterate over retrieved message objects

### Code snippets

```Python
import openai
import os

client = openai.Client(api_key=os.getenv(""OPENAI_API_KEY""))

thread = client.beta.threads.retrieve(thread_id=""<thread_id>"")

messages  = client.beta.threads.messages.list(thread_id=thread.id, limit=3)

for i,message in enumerate(messages):
    print(message.role)
    print(message.created_at)
```


### OS

Ubuntu 22.04

### Python version

Python v3.10.12

### Library version

openai v1.54.3",closed,2024-11-12T09:41:43Z,2024-11-12T09:55:49Z,[],https://github.com/openai/openai-python/issues/1863,0.23,True,incorrect_functionality,high,"The limit parameter in the messages.list method does not function as expected, returning all messages instead of the specified limit.",high,
1825,Missing import for asyncio in README example code,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

Hi there,

I noticed a small typo in the example code provided in the README file. The code snippet uses *asyncio.run(main())* but is missing the import statement for *asyncio*. This could cause confusion for users running the example as it will result in a *NameError: name 'asyncio' is not defined*.

### To Reproduce

Here's the updated code with the necessary import:

```python
from openai import AsyncOpenAI
import asyncio  # Add this import

client = AsyncOpenAI()

async def main():
    stream = await client.chat.completions.create(
        model=""gpt-4"",
        messages=[{""role"": ""user"", ""content"": ""Say this is a test""}],
        stream=True,
    )
    async for chunk in stream:
        print(chunk.choices[0].delta.content or """", end="""")

asyncio.run(main())
```

### Code snippets

_No response_

### OS

-

### Python version

-

### Library version

-",closed,2024-10-26T18:39:56Z,2024-11-11T10:18:00Z,['bug'],https://github.com/openai/openai-python/issues/1825,375.63,True,missing_dependency_violation,high,"The example code in the README is missing an import statement for 'asyncio', leading to a NameError when users attempt to run the code.",high,
1082,"When debug logging is enabled, api-key header is also printed","### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

When debug logging is enabled, `FinalRequestOptions` is entirely dumped because of the following code:

https://github.com/openai/openai-python/blob/3d61ed42aba652b547029095a7eb269ad4e1e957/src/openai/_base_client.py#L436-L437

This includes the `api-key` header that would be great to avoid logging

### To Reproduce

1. Enable debug logging
2. Get a AsyncAzureOpenAI client and execute any kind of request against the service
3. See the api-key getting logged as part of the headers

### Code snippets

_No response_

### OS

WSL

### Python version

Python v3.9.18

### Library version

openai v1.7.2",closed,2024-01-17T12:58:06Z,2024-11-06T21:35:08Z,['bug'],https://github.com/openai/openai-python/issues/1082,7064.62,True,missing_dependency_violation,high,The logging of sensitive information such as the api-key header indicates a missing dependency on security practices that should prevent sensitive data from being logged.,high,
1784,Latest version fails to install on Python 3.7,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

Due to dependency conflicts, the latest versions of the library fail to install on Python 3.7.

### To Reproduce

1. Using Python 3.7, attempt to install the `openai` package:

``` shell
$ py -3.7-32 -m pip install openai
```

This fails due to `jiter` and `typing_extensions`.

Please either drop support for 3.7 officially (3.8 works) or loosen the dependency constraints.

### Code snippets

_No response_

### OS

Windows 11 26100.1882

### Python version

3.7.9

### Library version

1.51.x",closed,2024-10-09T06:14:59Z,2024-11-05T02:40:12Z,['bug'],https://github.com/openai/openai-python/issues/1784,644.42,True,missing_dependency_violation,high,"The library fails to install on Python 3.7 due to dependency conflicts, indicating a missing or incompatible dependency for this Python version.",high,
1788,Repeated Error: openai.Completion Not Supported in OpenAI Python >=1.0.0 Despite Multiple Fix Attempts (Python Versions and API Key Methods),"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

I am encountering a recurring error when trying to access openai.Completion. The error message suggests that this is no longer supported in versions of the OpenAI Python library greater than 1.0.0. Despite following the official migration guide and pinning to earlier versions (e.g., openai==0.28), the issue remains unresolved. I have tried various solutions such as:

Downgrading Python from 3.12 to 3.10, and even 3.9
Adjusting API key access methods (environment variables, hardcoding, etc.)
Attempting multiple different virtual environments
Following the migration instructions from the OpenAI repository and discussions, such as using openai migrate.
No matter what I try, I keep encountering the following error:

vbnet
Copy code
You tried to access openai.Completion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.

You can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface.

Alternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`
Despite using the suggested approach to pin the version (pip install openai==0.28), the issue remains unresolved. Additionally, following the migration guide (https://github.com/openai/openai-python/discussions/742) didn't yield any success.

### To Reproduce

Steps to reproduce the behavior:

1. Set up a Python virtual environment with Python 3.10.
2. Install openai using pip install openai==0.27.0 or any version before 1.0.0.
3. Attempt to use openai.Completion.create() in any script.
4. The above error consistently occurs.

### Code snippets

```Python
Code snippets
Here’s the code I used to trigger the error:

python
Copy code
import openai
openai.api_key = 'sk-xxxxxxx'

response = openai.Completion.create(
    engine=""davinci"",
    prompt=""Tell me a joke."",
    max_tokens=50
)
print(response.choices[0].text)
Error output:
vbnet
Copy code
You tried to access openai.Completion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.

You can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface.

Alternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`

What I Tried:
Downgrading the Python version from 3.12 to 3.10.
Using different versions of the OpenAI library (0.27.0, 0.28.0, and 0.29.0).
Setting API keys via environment variables and directly in the code.
Trying the migration tool (openai migrate) but to no avail.
Ensuring all dependencies (like google-api-python-client and pdfplumber) are up to date and properly installed.
```


### OS

macOS Monterey

### Python version

Python 3.10.15 (also tried Python 3.12.5)

### Library version

openai v1.0.1 (Issue also persists with version 0.27.0 and 0.28.0)",closed,2024-10-10T14:08:18Z,2024-10-27T15:48:08Z,[],https://github.com/openai/openai-python/issues/1788,409.66,True,missing_option_violation,high,"The user is attempting to access openai.Completion in a version of the library that no longer supports it, indicating a failure to follow the updated API usage guidelines after version 1.0.0.",high,
1826,AnyIO worker threads not terminating when using asyncify with get_platform,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

When using the OpenAI Python library (version 1.52.2), I encountered an issue where AnyIO worker threads are not terminating properly after asynchronous requests. This leads to tests hanging and not exiting when using `pytest`. The problem occurs because the library utilizes `asyncify(get_platform)` to call the synchronous `get_platform` function in an asynchronous context, causing AnyIO to create non-daemon worker threads that remain alive after the request completes.



### To Reproduce

1. Use the OpenAI Python library in an asynchronous environment.
2. Make an asynchronous API request that triggers the `_request` method.
3. Run tests using `pytest`.
4. Observe that after the tests complete, the process hangs and does not exit due to lingering AnyIO worker threads.

### Code snippets

The issue arises in the _request method where asyncify is used:

# In openai/_openai_client.py
```Python



async def _request(
    self,
    cast_to: Type[ResponseT],
    options: FinalRequestOptions,
    *,
    stream: bool,
    stream_cls: type[_AsyncStreamT] | None,
    retries_taken: int,
) -> ResponseT | _AsyncStreamT:
    if self._platform is None:
        # `get_platform` can make blocking IO calls so we
        # execute it earlier while we are in an async context
        self._platform = await asyncify(get_platform)()
    # ... rest of the method ...
```

Expected behavior

The AnyIO worker threads should terminate after completing their tasks, allowing the program or tests to exit cleanly without hanging.
```


### OS

macOS

### Python version

Python 3.12.4

### Library version

1.52.2",closed,2024-10-27T13:25:31Z,2024-10-27T13:27:16Z,['bug'],https://github.com/openai/openai-python/issues/1826,0.03,True,method_order_violation,hang,"The use of asyncify with get_platform leads to lingering AnyIO worker threads, causing the system to hang after asynchronous requests.",high,
1820,Error note that json_schema is not available on gpt-4o-mini,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [ ] This is an issue with the Python library

### Describe the bug

I used AsyncOpenAI Client to execute the tasks that required structed output (model as ""gpt-4o-mini""). The first few runs of the same code returned normal results, but without any changes to the code, Suddenly I get a 400 error ""response_format value as json_schema is enabled only for api versions 2024-08-01-preview and later"".

### To Reproduce

1. Instantiate one AsyncOpenAI:
```python
aclient = AsyncOpenAI()
```
2. Define the data model as follows:
```python
class Term(BaseModel):
    term: str
    definition: str
class Terms(BaseModel):
    terms: List[Term]
```
4. Use the following code to make the request:
```python
reference_text = ""Machine learning is a very important subject.""
system_promt = ""You are an expert in the field of machine learning.Please extract the terms and definitions from the reference text according to the appropriate format.""
user_prompt = ""Given the following reference text:\n{}""

async def aquery(self, reference_text: str):
    response = await aclient.beta.chat.completions.parse(
        model=""gpt-4o-mini"",
        messages=[{""role"": ""system"", ""content"": system_prompt},
                  {""role"": ""user"", ""content"": user_prompt.format(reference_text)}],
        temperature=0.1,
        response_format=Terms
    )
    return json.loads(response.choices[0].message.content)
```

### Code snippets

```Python
openai.BadRequestError: Error code: 400 - {'error': {'message': 'response_format value as json_schema is enabled only for api versions 2024-08-01-preview and later', 'type': '', 'param': '', 'code': 'BadRequest'}}
```


### OS

Ubuntu 20.04.6 LTS

### Python version

Python 3.10.15

### Library version

openai v1.51.1",closed,2024-10-25T06:22:25Z,2024-10-25T06:53:52Z,['bug'],https://github.com/openai/openai-python/issues/1820,0.52,True,missing_dependency_violation,high,"The request to use 'response_format' as json_schema is not supported by the specified model 'gpt-4o-mini', indicating a missing dependency on the API version required for this feature.",high,
1819,Responses are in binary?,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

Hey, I'm using `vcrpy` to cache responses from OpenAI models to run tests easier and faster, but recently I've noticed responses now are in binary for some reason.

Here are couple of questions regarding this, was there any big change that caused this change to happen? Whats the reason behind this change and how to convert this binary back into `JSON` format to continue testing OpenAI SDK?

Before
```yaml
  response:
    body:
      string: ""{\n  \""id\"": \""chatcmpl-AJ4U6CKcRZJQ9uWFy7Rz0drsowtgH\"",\n  \""object\"":
        \""chat.completion\"",\n  \""created\"": 1729108834,\n  \""model\"": \""gpt-4-0613\"",\n
        \ \""choices\"": [\n    {\n      \""index\"": 0,\n      \""message\"": {\n        \""role\"":
        \""assistant\"",\n        \""content\"": \""Hello! How can I assist you today?\"",\n
        \       \""refusal\"": null\n      },\n      \""logprobs\"": null,\n      \""finish_reason\"":
        \""stop\""\n    }\n  ],\n  \""usage\"": {\n    \""prompt_tokens\"": 8,\n    \""completion_tokens\"":
        9,\n    \""total_tokens\"": 17,\n    \""prompt_tokens_details\"": {\n      \""cached_tokens\"":
        0\n    },\n    \""completion_tokens_details\"": {\n      \""reasoning_tokens\"":
        0\n    }\n  },\n  \""system_fingerprint\"": null\n}\n""
```

After

```yaml
  response:
    body:
      string: !!binary |
        g0QBAMRCne5apuvPdgOsAqPAQcUvU8JTN4Ee1H63cIMtkLnPF0nnUa+vgQYeWHbN29u6+EfNqN5d
        CoAGliTI9FVuxt6FB9fz6c3r3bDzNH27ycxMNyqjo2lujyJ/NKNAAMR6GJkcqW+mGHgDjlVw5XJR
        ZEmi0qp22uVqo9wUPhizjRxJUM/nYZ3DarlaD8vtsNwEC/s8MFFGEl8CAJazFUDP349Iohys4mQM
        k6TiTwCl7CKSIJVlgyxXcU6BQqF7nZQEnUfO8QbOeQqjYlwA9AVzLpCzVfM9cOHJ+mnlSCIunCM1
        1t6kHPd8yjrTIV7iRm+Dv0u1xkmCspw9+eSvBfAT9bgIRIJ8ymOf/+U8iuKMJNoxDGiXC3LQKfyJ
        QEF0VGkFfPpfRzojM3cGkFGmH1m0Uh6L1mZraWCAq0Hc8wExaYLNoOL/F3/dQdyLUp8OTuG+6/9U
        y5abUb2qWiTWwgAD
```

### To Reproduce

Package I'm working on is open-source, so you can clone from here

https://github.com/withmartian/adapters

Delete current cassette files and then rerun tests

New cassette file will have binary in their response body field

### Code snippets

```Python
# Example:
# From https://github.com/withmartian/adapters

@pytest.mark.parametrize(""model_path"", MODEL_PATHS)
@pytest.mark.vcr
def test_sync(vcr, model_path: str):
    adapter = AdapterFactory.get_adapter_by_path(model_path)

    assert adapter is not None

    adapter_response = adapter.execute_sync(SIMPLE_CONVERSATION_USER_ONLY)

    cassette_response = get_response_content_from_vcr(vcr, adapter)

    assert adapter_response.choices[0].message.content == cassette_response
```


### OS

macOs

### Python version

Python v3.11.10

### Library version

openai v1.52.2",closed,2024-10-24T20:56:04Z,2024-10-25T05:24:51Z,['bug'],https://github.com/openai/openai-python/issues/1819,8.48,True,input_type_violation,high,"The API is returning responses in an unexpected binary format instead of the expected JSON format, leading to incorrect functionality and potential issues in processing the responses.",high,
1080,logprobs parameter support for Chat Completion API and GPT-4 model ,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

May you please share a documentation regarding the correct usage of logprobs parameter in Chat Completion API as well as support for the following models.

1. gpt-4-0613
2. gpt-4-1106-Preview

Currently, when using the parameter with the above models the following errors are returned by the Chat Completion API. openai version is 1.7.2

gpt-4-06-13
BadRequestError: Error code: 400 - {'error': {'message': 'Unrecognized request argument supplied: logprobs', 'type': 'invalid_request_error', 'param': None, 'code': None}}

gpt-4-1106
BadRequestError: Error code: 400 - {'error': {'message': ""This model does not support the 'logprobs' parameter."", 'type': 'invalid_request_error', 'param': 'logprobs', 'code': None}}


### To Reproduce

Please see code snippet below.

### Code snippets

```Python
from openai import AzureOpenAI
 
client = AzureOpenAI(
  azure_endpoint = 'API_ENDPOINT', 
  api_key='API_KEY',  
  api_version='2023-05-15',
  azure_deployment = 'gpt-4-1106'
)

response = client.chat.completions.create(
    model='gpt-4',
    messages=[
        {....}
    ],
    logprobs=True,
    top_logprobs=2
)
response
```


### OS

Windows11

### Python version

Python v3.11.2

### Library version

openai v1.7.2",closed,2024-01-17T01:19:40Z,2024-03-03T01:08:57Z,['bug'],https://github.com/openai/openai-python/issues/1080,1103.82,True,input_value_violation,high,"The 'logprobs' parameter is not recognized by the specified models, leading to BadRequestError responses.",high,
1676,Completions.parse() got an unexpected keyword argument 'stream',"### Confirm this is a feature request for the Python library and not the underlying OpenAI API.

- [X] This is a feature request for the Python library

### Describe the feature or improvement you're requesting

The new client.beta.chat.completions.parse() does not yet support streaming. 

I want to do something like:

```python
  response = client.beta.chat.completions.parse(
      model=MODEL_NAME,
      messages=[
          {""role"": ""system"", ""content"": prompt},
          {""role"": ""user"", ""content"": content},
      ],
      temperature=0,
      top_p=0.4,
      stream=True,
      response_format=DocumentStructure,
  )
```

But this will throw the following error:

TypeError: Completions.parse() got an unexpected keyword argument 'stream'

### Additional context

_No response_",closed,2024-08-26T12:35:34Z,2024-09-03T18:04:49Z,['question'],https://github.com/openai/openai-python/issues/1676,197.49,True,missing_option_violation,medium,"The API method completions.parse() does not support the 'stream' argument, which indicates a failure to follow the expected options provided by the API contract.",high,
1809,Create Message and create Run Assistant take too long time,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [ ] This is an issue with the Python library

### Describe the bug

From the API I had constructed a number of calls to the Wizards I had created. However, as of today, when I run the following command:
`client.beta.threads.messages.create(thread_id=thread.id, role=""user"", content=user_message)`
and
`client.beta.threads.runs.create(thread_id=thread.id,assistant_id=assistant_id)`
Upon examination of the library during debugging, a significant loop was identified. However, due to my limited understanding of the library, I am unable to discern its nature.
The prolonged time required to create a message and a Run, coupled with the status of 'expired' assigned to the Run upon reaching the execution point of a calling function, has resulted in the unavailability of the implementation, which is already operational in a production environment.

Thanks a lot for the hard work maintaining this code!

### To Reproduce

```python
#Create any thread
thread = client.beta.threads.create()
client.beta.threads.messages.create(thread_id=thread.id, role=""user"", content=user_message) # Here the code takes more than 1 minute
client.beta.threads.runs.create(thread_id=thread.id,assistant_id=assistant_id,) # Here the code take more than 9 minutes
```

### Code snippets

```Python
def get_response(client, thread):
    return client.beta.threads.messages.list(thread_id=thread.id, order=""asc"")


def process(client, assistant_id, question):
    time1 = time.time()
    print(""init"")
    thread, run = create_thread_and_run(client, assistant_id, question)
    print(f""Thread and run created in {time.time() - time1} seconds"")
    run = wait_on_run(client, run, thread)
    if (
        run.status == ""requires_action""
        and run.required_action.submit_tool_outputs.tool_calls[0].function.name == ""GetTopHeadlinesNews""
    ):
        parameters = json_loads(run.required_action.submit_tool_outputs.tool_calls[0].function.arguments)
        response = GetTopHeadlinesNews(parameters)
        tool_outputs = [
            {
                ""tool_call_id"": run.required_action.submit_tool_outputs.tool_calls[0].id,
                ""output"": json_dumps(response),
            }
        ]
        timeStart = time.time()
        run = client.beta.threads.runs.submit_tool_outputs(
            thread_id=thread.id,
            run_id=run.id,
            tool_outputs=tool_outputs,
        )
        print(f""Time taken to submit tool outputs: {time.time()-timeStart}"")
        run = wait_on_run(run, thread)
        return get_response(thread)


# Function calling
def GetTopHeadlinesNews(arguments):
    url = ""https://newsapi.org/v2/top-headlines""
    parameters = arguments
    response = requests.get(url, params=parameters, headers={""X-Api-Key"": api_key_news})
    data = response.json()
    return data


# Waiting in a loop
def wait_on_run(client, run, thread):
    timeStart = time.time()
    print(""Waiting for run to complete"")
    while run.status == ""queued"" or run.status == ""in_progress"":
        run = client.beta.threads.runs.retrieve(
            thread_id=thread.id,
            run_id=run.id,
        )
    print(f""Time taken to wait on run: {time.time()-timeStart}"")
    print(f""Run status: {run.status}"")
    return run


def create_thread_and_run(client, assistant_id, user_input):
    timeStart = time.time()
    thread = client.beta.threads.create()
    print(f""Time taken to create thread: {time.time()-timeStart}"")
    run = submit_message(client, assistant_id, thread, user_input)
    return thread, run


def submit_message(client, assistant_id, thread, user_message):
    timeStart = time.time()
    print(f""Submitting message..."")
    client.beta.threads.messages.create(thread_id=thread.id, role=""user"", content=user_message)
    print(f""Time taken to submit message: {time.time()-timeStart}"")
    print(f""Creating run..."")
    return client.beta.threads.runs.create(
        thread_id=thread.id,
        assistant_id=assistant_id,
    )
```


### OS

Windows

### Python version

Python v3.12.7

### Library version

openai v1.52.0",closed,2024-10-22T16:13:49Z,2024-10-22T16:35:52Z,['API-feedback'],https://github.com/openai/openai-python/issues/1809,0.37,True,performance_degradation,Medium,"The API calls to create a message and a run are taking excessively long, indicating a potential performance issue.",high,
1808,Audio generation from audio,"### Confirm this is a feature request for the Python library and not the underlying OpenAI API.

- [X] This is a feature request for the Python library

### Describe the feature or improvement you're requesting

OpenAI recently introduced a new feature for generating audio from audio, and according to their documentation, you can use the new feature in Python by adding a parameter called modalities. However, when I try to use the feature, I receive the following error:

TypeError: Completions.create() got an unexpected keyword argument 'modalities'

It seems that the modalities parameter might not be recognized or properly implemented in the version I'm using. Could you please assist in resolving this issue and confirm if there are additional steps or updates required to use this feature?

### Additional context

_No response_",closed,2024-10-21T15:56:09Z,2024-10-21T17:17:38Z,[],https://github.com/openai/openai-python/issues/1808,1.36,True,input_type_violation,unknown,"The 'modalities' parameter is not recognized, indicating a potential type mismatch or missing implementation in the API.",high,
1804,AsyncOpenAI does not do any retries when Exception occurs,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

Hi, please refer to https://github.com/openai/openai-python/pull/1803 for details.

### To Reproduce

(see that PR)

### Code snippets

_No response_

### OS

unrelated

### Python version

unrelated

### Library version

latest",closed,2024-10-20T03:08:11Z,2024-10-21T07:12:48Z,['bug'],https://github.com/openai/openai-python/issues/1804,28.08,True,missing_dependency_violation,medium,"The library does not implement retry logic for handling exceptions, which may lead to unhandled errors and degraded performance.",high,
1776,"with multiprocessing, pickle issue with  client.beta.chat.completions.parse output","### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

We use multiprocessing to handle the calls. It was working with client.chat.completions.create. Currently, we are trying client.beta.chat.completions.parse with response_format, i got error:

Traceback (most recent call last):
  File ""/anaconda/lib/python3.10/multiprocessing/queues.py"", line 244, in _feed
    obj = _ForkingPickler.dumps(obj)
  File ""/anaconda/lib/python3.10/multiprocessing/reduction.py"", line 51, in dumps
    cls(buf, protocol).dump(obj)
_pickle.PicklingError: Can't pickle <class 'openai.types.chat.parsed_chat_completion.ParsedChatCompletion[CalendarEvent]'>: attribute lookup ParsedChatCompletion[CalendarEvent] on openai.types.chat.parsed_chat_completion failed

anyone can help ?

Thanks

### To Reproduce
```
from pydantic import BaseModel
from . import get_token_provider
import openai
import asyncio
import pickle

class CalendarEvent(BaseModel):
    name: str
    date: str
    participants: list[str]

def original_test():

    MODEL = ""gpt-4o-2024-08-06""
    ENDPOINT = ""gpt-4o-2024-08-06-endpoint""
    token_provider = get_token_provider()

    client = openai.AsyncAzureOpenAI(
    azure_ad_token_provider=token_provider,
    azure_endpoint=ENDPOINT,
    api_version=""2024-08-01-preview"",
    )

    func = client.beta.chat.completions.parse(
        model=MODEL,
        messages=[
            {""role"": ""system"", ""content"": ""Extract the event information.""},
            {""role"": ""user"", ""content"": ""Alice and Bob are going to a science fair on Friday.""},
        ],
        response_format=CalendarEvent,
        )

    completion = asyncio.run(func)
    pickle.dumps(completion)
    

if __name__ == ""__main__"":
    original_test()
```
### Code snippets

_No response_

### OS

linux

### Python version

py3.10

### Library version

openai 1.51.0",closed,2024-10-05T15:16:49Z,2024-10-18T13:48:06Z,['bug'],https://github.com/openai/openai-python/issues/1776,310.52,True,input_type_violation,high,"The response_format argument provided to the client.beta.chat.completions.parse method is of an incompatible type (CalendarEvent) that cannot be pickled, leading to a PicklingError.",high,
1794,Structured Outputs via function calling - Descriptions for important keys,"### Confirm this is a feature request for the Python library and not the underlying OpenAI API.

- [X] This is a feature request for the Python library

### Describe the feature or improvement you're requesting


I like the new SDK helper to parse the model's output:

```python
from pydantic import BaseModel
from openai import OpenAI

client = OpenAI()

class ResearchPaperExtraction(BaseModel):
    title: str
    authors: list[str]
    abstract: str
    keywords: list[str] # How do I add a description for this or any of the parameters? 
    # How does the model know exactly how to use this? will it infer on its own or we specify this in the system message? 

completion = client.beta.chat.completions.parse(
    model=""gpt-4o-2024-08-06"",
    messages=[
        {""role"": ""system"", ""content"": ""You are an expert at structured data extraction. You will be given unstructured text from a research paper and should convert it into the given structure.""},
        {""role"": ""user"", ""content"": ""...""}
    ],
    response_format=ResearchPaperExtraction,
)

research_paper = completion.choices[0].message.parsed
```

But how do I get the granularity of using manual schema with the SDK objects?
Mainly adding detailed description for the parameters

```json
{
    ""name"": ""get_weather"",
    ""description"": ""Fetches the weather in the given location"",
    ""strict"": true,
    ""parameters"": {
        ""type"": ""object"",
        ""properties"": {
            ""location"": {
                ""type"": ""string"",
                ""description"": ""The location to get the weather for""
            },
            ""unit"": {
                ""type"": ""string"",
                ""description"": ""The unit to return the temperature in"",
                ""enum"": [""F"", ""C""]
            }
        },
        ""additionalProperties"": false,
        ""required"": [""location"", ""unit""]
    }
}
```

_Snippets taken from the official OpenAI Documentation._

### Additional context

Please improve the documentation. How can I submit requests to improve it? 
Also I get no response from the OpenAI Developer Forum, thus I raise an issue here (@RobertCraigie is a good guy :)",closed,2024-10-17T06:51:03Z,2024-10-17T12:24:53Z,[],https://github.com/openai/openai-python/issues/1794,5.56,False,,,,,
816,"Error ""cannot import name 'TypeAliasType' from 'typing_extensions'"" when importing openai just after installation","### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

---------------------------------------------------------------------------
```
ImportError                               Traceback (most recent call last)
Cell In[38], line 1
----> 1 import openai
      2 import os
      4 opai.api_key = os.getenv(""OPENAI_API_KEY"")

File D:\Anaconda\envs\py38\lib\site-packages\openai\__init__.py:8
      5 import os as _os
      6 from typing_extensions import override
----> 8 from . import types
      9 from ._types import NoneType, Transport, ProxiesTypes
     10 from ._utils import file_from_path

File D:\Anaconda\envs\py38\lib\site-packages\openai\types\__init__.py:5
      1 # File generated from our OpenAPI spec by Stainless.
      3 from __future__ import annotations
----> 5 from .edit import Edit as Edit
      6 from .image import Image as Image
      7 from .model import Model as Model

File D:\Anaconda\envs\py38\lib\site-packages\openai\types\edit.py:6
      3 from typing import List
      4 from typing_extensions import Literal
----> 6 from .._models import BaseModel
      7 from .completion_usage import CompletionUsage
      9 __all__ = [""Edit"", ""Choice""]

File D:\Anaconda\envs\py38\lib\site-packages\openai\_models.py:20
     18 import pydantic
     19 import pydantic.generics
---> 20 from pydantic.fields import FieldInfo
     22 from ._types import (
     23     Body,
     24     IncEx,
   (...)
     31     HttpxRequestFiles,
     32 )
     33 from ._utils import (
     34     is_list,
     35     is_given,
   (...)
     39     strip_not_given,
     40 )

File D:\Anaconda\envs\py38\lib\site-packages\pydantic\fields.py:24
     21 from pydantic_core import PydanticUndefined
     22 from typing_extensions import Literal, Unpack
---> 24 from . import types
     25 from ._internal import _decorators, _fields, _generics, _internal_dataclass, _repr, _typing_extra, _utils
     26 from .config import JsonDict

File D:\Anaconda\envs\py38\lib\site-packages\pydantic\types.py:33
     31 from annotated_types import BaseMetadata, MaxLen, MinLen
     32 from pydantic_core import CoreSchema, PydanticCustomError, core_schema
---> 33 from typing_extensions import Annotated, Literal, Protocol, TypeAlias, TypeAliasType, deprecated
     35 from ._internal import (
     36     _core_utils,
     37     _fields,
   (...)
     41     _validators,
     42 )
     43 from ._migration import getattr_migration

ImportError: cannot import name 'TypeAliasType' from 'typing_extensions' (D:\Anaconda\envs\py38\lib\site-packages\typing_extensions.py)
```


### To Reproduce

import openai

then error happens
the typing_extensions version is 4.8.0(latest)

### Code snippets

_No response_

### OS

windows

### Python version

Python 3.8.16

### Library version

openai v 1.2.4",closed,2023-11-14T05:53:51Z,2023-12-23T23:26:27Z,['bug'],https://github.com/openai/openai-python/issues/816,953.54,True,input_type_violation,high,"The error indicates that the library is attempting to import a type 'TypeAliasType' from 'typing_extensions', which is not available in the installed version (4.8.0). This suggests a mismatch between the expected and actual types, leading to an ImportError.",high,
492,Make Decorators a built-in part of the function call functionality,"### Describe the feature or improvement you're requesting

It would be really nice if we could have built-in decorators that abstract away a lot of the unnecessary function-calling boilerplate that causes the code to be more confusing and difficult to use than necessary. 

I felt like it took much more time to wrap my head around the value and utility of function calls in the recent API update than was necessary in part because of the boilerplate. Additionally, having a bunch of text describing functions that will eventually no longer be needed or might need to be changed syntactically or for other reasons in future updates seems like it would take a lot of time and effort to refactor in the long term. 

So I'd propose that built-in decorators for function calling are at least considered as a future option when granular control over the function call isn't immediately required.

### Additional context

I think [this project](https://github.com/unconv/gpt-pydecorator) does a great job of creating the functionality that I'm referring to here.",closed,2023-06-18T18:53:36Z,2024-03-03T01:02:49Z,['enhancement'],https://github.com/openai/openai-python/issues/492,6198.15,False,,,,,
1469,Stream options are not available for azure openAI?,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

I was following this article on stream option.https://cookbook.openai.com/examples/how_to_stream_completions.

I am using:

""2023-05-15"",  ""gpt-35-turbo""


code:
```

response = open_ai_client.chat.completions.create(
    model=OpenAIConstants.generator,
    messages=[
        {'role': 'user', 'content': ""What's 1+1? Answer in one word.""}
    ],
    temperature=0,
    stream=True,
    stream_options={""include_usage"": True}, # retrieving token usage for stream response
)

for chunk in response:
    print(f""choices: {chunk.choices}\nusage: {chunk.usage}"")
    print(""****************"")

```

```
Traceback (most recent call last):
  File ""C:\Users\nandurisai.venkatara\projects\knowledge-base\Archive\sample.py"", line 15, in <module>
    response = open_ai_client.chat.completions.create(
  File ""C:\Users\nandurisai.venkatara\projects\knowledge-base\.venv\lib\site-packages\openai\_utils\_utils.py"", line 277, in wrapper
    return func(*args, **kwargs)
  File ""C:\Users\nandurisai.venkatara\projects\knowledge-base\.venv\lib\site-packages\openai\resources\chat\completions.py"", line 590, in create
    return self._post(
  File ""C:\Users\nandurisai.venkatara\projects\knowledge-base\.venv\lib\site-packages\openai\_base_client.py"", line 1240, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File ""C:\Users\nandurisai.venkatara\projects\knowledge-base\.venv\lib\site-packages\openai\_base_client.py"", line 921, in request
    return self._request(
  File ""C:\Users\nandurisai.venkatara\projects\knowledge-base\.venv\lib\site-packages\openai\_base_client.py"", line 1020, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': 'Unrecognized request argument supplied: stream_options', 'type': 'invalid_request_error', 'param': None, 'code': None}}
```

### To Reproduce

I was following this article on stream option.https://cookbook.openai.com/examples/how_to_stream_completions.

I am using:

""2023-05-15"",  ""gpt-35-turbo""


code:
```

response = open_ai_client.chat.completions.create(
    model=OpenAIConstants.generator,
    messages=[
        {'role': 'user', 'content': ""What's 1+1? Answer in one word.""}
    ],
    temperature=0,
    stream=True,
    stream_options={""include_usage"": True}, # retrieving token usage for stream response
)

for chunk in response:
    print(f""choices: {chunk.choices}\nusage: {chunk.usage}"")
    print(""****************"")

```

```
Traceback (most recent call last):
  File ""C:\Users\nandurisai.venkatara\projects\knowledge-base\Archive\sample.py"", line 15, in <module>
    response = open_ai_client.chat.completions.create(
  File ""C:\Users\nandurisai.venkatara\projects\knowledge-base\.venv\lib\site-packages\openai\_utils\_utils.py"", line 277, in wrapper
    return func(*args, **kwargs)
  File ""C:\Users\nandurisai.venkatara\projects\knowledge-base\.venv\lib\site-packages\openai\resources\chat\completions.py"", line 590, in create
    return self._post(
  File ""C:\Users\nandurisai.venkatara\projects\knowledge-base\.venv\lib\site-packages\openai\_base_client.py"", line 1240, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File ""C:\Users\nandurisai.venkatara\projects\knowledge-base\.venv\lib\site-packages\openai\_base_client.py"", line 921, in request
    return self._request(
  File ""C:\Users\nandurisai.venkatara\projects\knowledge-base\.venv\lib\site-packages\openai\_base_client.py"", line 1020, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': 'Unrecognized request argument supplied: stream_options', 'type': 'invalid_request_error', 'param': None, 'code': None}}
```

### Code snippets

_No response_

### OS

Win

### Python version

3.10

### Library version

1.30.1",closed,2024-06-05T17:37:17Z,2024-07-06T20:41:14Z,['bug'],https://github.com/openai/openai-python/issues/1469,747.07,True,missing_option_violation,high,"The API call includes an unrecognized argument 'stream_options', indicating that the client is not following the correct usage as specified by the API contract.",high,
1777,`openai.AsyncOpenAI` not safe when shared across `async` tests,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

When sharing `AsyncOpenAI` across `async` tests, it's possible to hit `openai.APIConnectionError: Connection error.`

### To Reproduce

https://colab.research.google.com/drive/1icWDkN2iYPl3mxCEwA0-kE7GSSOeM9E8?usp=sharing

### Code snippets

```Python
import pytest
from openai import AsyncOpenAI

client = AsyncOpenAI()


@pytest.mark.asyncio
async def test_completion1() -> None:
    await client.chat.completions.create(
        model=""gpt-4o-mini"",
        messages=[{
            ""content"": (
                ""Here is a question, the correct answer to the question, and a proposed""
                "" answer to the question. Please tell me if the proposed answer is""
                "" correct, given the correct answer. ONLY SAY 'YES' OR 'NO'. No other""
                "" output is permitted.\n\nQuestion: What is 25 * 10? \n\nCorrect""
                "" answer: 250 \n\nProposed answer: 250""
            ),
            ""role"": ""user"",
        }],
    )


@pytest.mark.asyncio
async def test_completion2() -> None:
    await client.chat.completions.create(
        model=""gpt-4o-mini"",
        messages=[{
            ""content"": (
                ""Here is a question, the correct answer to the question, and a proposed""
                "" answer to the question. Please tell me if the proposed answer is""
                "" correct, given the correct answer. ONLY SAY 'YES' OR 'NO'. No other""
                "" output is permitted.\n\nQuestion: What is 25 * 10? \n\nCorrect""
                "" answer: 250 \n\nProposed answer: 250""
            ),
            ""role"": ""user"",
        }],
    )
```


### OS

linux

### Python version

3.10.12

### Library version

1.51.0",closed,2024-10-06T01:57:11Z,2024-10-09T12:18:44Z,[],https://github.com/openai/openai-python/issues/1777,82.36,True,missing_dependency_violation,medium,The sharing of AsyncOpenAI client across async tests may lead to connection errors due to missing context or initialization dependencies between tests.,high,
1091,openai_object,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

I am 
from openai import openai_object

it reports the error:
ImportError: cannot import name 'openai_object' from 'openai'

my openai version is 1.8.0
I wonder if there exist a version problem

### To Reproduce

1.from openai import openai_object
2.ImportError: cannot import name 'openai_object' from 'openai'

### Code snippets

_No response_

### OS

Linux

### Python version

Python v3.8

### Library version

openai v1.8.0",closed,2024-01-22T09:23:19Z,2024-01-24T02:21:06Z,['bug'],https://github.com/openai/openai-python/issues/1091,40.96,True,missing_dependency_violation,unknown,"The user is attempting to import 'openai_object', which may not exist in the specified version of the library, indicating a potential missing dependency or version mismatch.",high,
1738,examples/embeddings/Visualize_in_3d.ipynb is empty,"### Confirm this is a feature request for the Python library and not the underlying OpenAI API.

- [X] This is a feature request for the Python library

### Describe the feature or improvement you're requesting

Actually all the examples of embeddings are empty, could anybody share some example codes?

### Additional context

_No response_",closed,2024-09-24T03:18:08Z,2024-10-03T14:14:28Z,['documentation'],https://github.com/openai/openai-python/issues/1738,226.94,True,missing_dependency_violation,medium,"The issue indicates that all examples of embeddings are empty, suggesting a lack of necessary example codes that are required for proper usage of the API.",high,
1770,No Healthy Upstream,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

Facing the following issue:
![image](https://github.com/user-attachments/assets/841e2d52-589f-4b2e-8f82-3fb1815d55c6)


### To Reproduce

Azure OpenAI Studio

### Code snippets

```Python
import os
import requests
import base64

# Configuration
API_KEY = ""YOUR_API_KEY""
IMAGE_PATH = ""YOUR_IMAGE_PATH""
encoded_image = base64.b64encode(open(IMAGE_PATH, 'rb').read()).decode('ascii')
headers = {
    ""Content-Type"": ""application/json"",
    ""api-key"": API_KEY,
}

# Payload for the request
payload = {
  ""messages"": [
    {
      ""role"": ""system"",
      ""content"": [
        {
          ""type"": ""text"",
          ""text"": ""You are an AI assistant that helps people find information.""
        }
      ]
    },
    {
      ""role"": ""user"",
      ""content"": [
        {
          ""type"": ""text"",
          ""text"": ""Hi""
        }
      ]
    },
    {
      ""role"": ""user"",
      ""content"": [
        {
          ""type"": ""text"",
          ""text"": ""Pretty strange""
        }
      ]
    }
  ],
  ""temperature"": 0.7,
  ""top_p"": 0.95,
  ""max_tokens"": 800
}

ENDPOINT = ""https://abc-openai-gpt4o.openai.azure.com/openai/deployments/gpt4o/chat/completions?api-version=2024-02-15-preview""

# Send request
try:
    response = requests.post(ENDPOINT, headers=headers, json=payload)
    response.raise_for_status()  # Will raise an HTTPError if the HTTP request returned an unsuccessful status code
except requests.RequestException as e:
    raise SystemExit(f""Failed to make the request. Error: {e}"")

# Handle the response as needed (e.g., print or process)
print(response.json())
```


### OS

Linux

### Python version

Python v3.11

### Library version

openai v1.42.0",closed,2024-10-02T00:51:30Z,2024-10-02T20:49:09Z,[],https://github.com/openai/openai-python/issues/1770,19.96,True,input_type_violation,high,"The payload structure for 'messages' contains lists where objects are expected, leading to potential misinterpretation by the API.",high,
1759,Get info about model,"### Confirm this is a feature request for the Python library and not the underlying OpenAI API.

- [X] This is a feature request for the Python library

### Describe the feature or improvement you're requesting

I can list models, like this:

```python
import openai

for model in openai.models.list():
    print(model.id)

```

but i cant get info about model type. I dont get info about data types, that models acepts.

### Additional context

_No response_",closed,2024-09-28T19:31:54Z,2024-09-30T21:25:04Z,[],https://github.com/openai/openai-python/issues/1759,49.89,True,missing_dependency_violation,medium,"The request lacks information about the data types that models accept, which is a necessary dependency for understanding model capabilities.",high,
1760,ds-store,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

.DS_Store file in project root should not be there.

Add to .gitignore.

### To Reproduce

n/a

### Code snippets

_No response_

### OS

macOS

### Python version

Python v.12

### Library version

v1.50.2",closed,2024-09-29T03:09:56Z,2024-09-30T14:20:10Z,['bug'],https://github.com/openai/openai-python/issues/1760,35.17,False,,,,,
1174,Stream as True returns Nothing due to a bug in _streaming.py SSEDecoder decode function,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

decode function returns ""None"" when input is valid, Which leads to a 'None' output for streaming
![image](https://github.com/openai/openai-python/assets/4355494/e4fbad61-b930-4ec5-a05f-d6146e7898c1)


### To Reproduce

1. set stream as True in client.chat.completions.create
2. when waiting for response iterator, 'None' returned instead of Completion Chunks
3. 

### Code snippets

```Python
Below function always return None when input argument 'line' is not None.

def decode(self, line: str) -> ServerSentEvent | None:
        # See: https://html.spec.whatwg.org/multipage/server-sent-events.html#event-stream-interpretation  # noqa: E501

        if not line:
            if not self._event and not self._data and not self._last_event_id and self._retry is None:
                return None

            sse = ServerSentEvent(
                event=self._event,
                data=""\n"".join(self._data),
                id=self._last_event_id,
                retry=self._retry,
            )

            # NOTE: as per the SSE spec, do not reset last_event_id.
            self._event = None
            self._data = []
            self._retry = None

            return sse

        if line.startswith("":""):
            return None

        fieldname, _, value = line.partition("":"")

        if value.startswith("" ""):
            value = value[1:]

        if fieldname == ""event"":
            self._event = value
        elif fieldname == ""data"":
            self._data.append(value)
        elif fieldname == ""id"":
            if ""\0"" in value:
                pass
            else:
                self._last_event_id = value
        elif fieldname == ""retry"":
            try:
                self._retry = int(value)
            except (TypeError, ValueError):
                pass
        else:
            pass  # Field is ignored.

        return None
```


### OS

Linux

### Python version

python 3.9

### Library version

openai 1.13.2",closed,2024-02-21T21:13:17Z,2024-02-22T18:06:03Z,['bug'],https://github.com/openai/openai-python/issues/1174,20.88,True,input_value_violation,incorrect_functionality,"The decode function returns None when valid input is provided, leading to a lack of expected output for streaming.",high,
1633,The return type hint of `client.audio.transcriptions.create()` is incorrect.,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

The return type hint of `client.audio.transcriptions.create()` is `Transcription`, and this type has a `text` attribute.

However, when the `response_format` is equal to `text`, `srt`, or `vtt`, its return value is str.
In this case, calling the text attribute will result in an error.

### To Reproduce

```py
import openai
from dotenv import load_dotenv

load_dotenv()

file_path = ""example.wav""

client = openai.OpenAI()
with open(file_path, ""rb"") as file:
    transcript = client.audio.transcriptions.create(
        model=""whisper-1"", file=file, response_format=""srt""
    )

    print(transcript)
    print(type(transcript))
    print(transcript.text)
    print(type(transcript.text))
```

### OS

Windows 10

### Python version

Python v3.12.4

### Library version

openai v1.40.3",closed,2024-08-10T14:59:25Z,2024-09-27T23:04:24Z,['bug'],https://github.com/openai/openai-python/issues/1633,1160.08,True,input_type_violation,high,"The return type hint of the method does not match the actual return type when the response_format is 'text', 'srt', or 'vtt', leading to potential runtime errors when accessing the 'text' attribute.",high,
1739, AttributeError: module 'openai' has no attribute 'Completions' when using the latest version and APIRemovedInV1,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

I'm encountering an AttributeError when trying to use the openai.Completions attribute in my Python code. I'm using the latest version of the OpenAI Python library (version 1.47.1).
Here is the brief description of code:
import openai
input = ""I love to travel""
response = sentiment_analysis(input)
print(input, ""The sentiment is"" , response)
When I run this code, I get the following error:
APIRemovedInV1: 

You tried to access openai.Completion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.

You can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. 

Alternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`

A detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742



### To Reproduce

The code should successfully create a completion using the openai.Completions.create function.
Please let me know if there are any known issues or changes related to the openai.Completions attribute in the latest version of the library.
I've verified that I'm using the correct API key and that my internet connection is stable. I've also tried reinstalling the OpenAI library to ensure I'm using the latest version.
Any suggestions or workarounds would be greatly appreciated.

### Code snippets

```Python
import openai
def sentiment_analysis(text):
  messages = [{""Role"":""system"", ""content"":""""""You are trained to analyze an detect the sentiment of the given text.
                                             if you are unsure of answer you can say""Not sure"" and recommend users to review manually.""""""},
              {""Role"":""user"", ""content"":""""""Analyze the following text and determine if the sentiment is: positive or negative.
                                           return answer in single word as either positive or negative: {text}""""""}]
  response = openai.Completion.create(
      engine=""text-davinci-003"", 
      messages=messages,
      max_tokens=1,
      n=1,
      stop=None,
      temperature=0)
  sentiment = response.choices[0].message.content.strip().lower()
  return sentiment
input = ""I love to travel""
response = sentiment_analysis(input)
print(input, ""The sentiment is"" , response)
```


### OS

windows

### Python version

3.12.6

### Library version

1.47.1",closed,2024-09-24T10:26:22Z,2024-09-24T10:28:13Z,['question'],https://github.com/openai/openai-python/issues/1739,0.03,True,missing_option_violation,high,"The user is attempting to access an API method (openai.Completions) that has been removed in the latest version of the library, indicating a failure to adapt to the updated API contract.",high,
1685,array + enum in function calling,"### Confirm this is a feature request for the Python library and not the underlying OpenAI API.

- [X] This is a feature request for the Python library

### Describe the feature or improvement you're requesting

Is it currently possible to get function calling with an array of inputs for an argument and a restriction via enums:
something like:

'parameters': {'type': 'object',
    'properties': {'lab_values': {'type': 'array',
      'enum': ['Asparate Aminotransferase (AST)',
       'Barbiturate Screen',
       'Benzodiazepine Screen',
]}},
    'required': ['lab_values'],
    'additionalProperties': False}}}]

?



### Additional context

_No response_",closed,2024-09-04T13:39:48Z,2024-09-24T09:32:54Z,[],https://github.com/openai/openai-python/issues/1685,475.88,True,input_type_violation,medium,"The proposed feature involves using an array with enum restrictions, which may not align with the expected input types for the function, potentially leading to type mismatches.",high,
376,Pricing in the api,"### Describe the feature or improvement you're requesting

Might not be a critical request, but we have several users using our service (which uses openai api) and we'd like to be able to be able to do further cost analysis on their usage, so we want to calculate the query cost for each query and store it.

I know I can use the tokens count and multiply by the price in the pricing page, but this pricing has changed in the past and likely will change in the future so it would be awesome not to have this hardcoded on our side and rather use it from openai source.

I think the simplest approach would be to have this returned directly in the OpenAIObject returned from Completion/ChatCompletion, etc calls.


### Additional context

_No response_",closed,2023-04-04T12:56:48Z,2023-09-07T15:43:45Z,['API-feedback'],https://github.com/openai/openai-python/issues/376,3746.78,True,missing_option_violation,medium,"The request suggests a feature to include pricing information directly in API responses, which is not currently supported. This could lead to suboptimal behavior if users rely on hardcoded pricing.",high,
1724,AzureOpenAI chat completion endpoint does not recognize max_completion_tokens,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

With the o-1 release, `max_tokens` was deprecated in favor of `max_completion_tokens`, and while that argument works with the OpenAI client, it doesn't seem to be working with the AzureOpenAI client. I see the following error:
```
BadRequestError: Error code: 400 - {'error': {'message': 'Unrecognized request argument supplied: max_completion_tokens', 'type': 'invalid_request_error', 'param': None, 'code': None}}
```

### To Reproduce

```Python
import os
from openai import AzureOpenAI

client = AzureOpenAI(
  azure_endpoint = os.getenv(""AZURE_OPENAI_ENDPOINT""), 
  api_key=os.getenv(""AZURE_OPENAI_API_KEY""),  
  api_version=""2024-08-01-preview""
)

m = {'role': 'user', 'content': ""Hello!""}
client.chat.completions.create(messages=[m], model='gpt-4o', max_completion_tokens=100)
```

### Code snippets

_No response_

### OS

linux

### Python version

3.11.9

### Library version

1.46.0",closed,2024-09-19T04:13:40Z,2024-09-19T12:52:15Z,"['question', 'Azure']",https://github.com/openai/openai-python/issues/1724,8.64,True,input_value_violation,unknown,"The AzureOpenAI client does not recognize the 'max_completion_tokens' parameter, which suggests a potential mismatch in accepted input values after the deprecation of 'max_tokens'.",high,
1732,1.46.1 audio transcription's temperature param default is not the API's default,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

The API documentation for audio transcriptions says:
```
temperature number Optional Defaults to 0

The sampling temperature, between 0 and 1. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. If set to 0, the model will use [log probability](https://en.wikipedia.org/wiki/Log_probability) to automatically increase the temperature until certain thresholds are hit.
```

But the behavior between explicitly setting temperature=0 and not passing the param is different, and produces different outputs

### To Reproduce

```py
client.audio.transcriptions.create(
        model=""whisper-1"",
        file=track,
        response_format=""verbose_json"",
    )
```

vs

```py
client.audio.transcriptions.create(
        model=""whisper-1"",
        file=track,
        response_format=""verbose_json"",
        temperature=0,
    )
```

### Code snippets

_No response_

### OS

linux

### Python version

3.12

### Library version

1.46.1 ",closed,2024-09-19T17:06:01Z,2024-09-19T17:22:18Z,['API-feedback'],https://github.com/openai/openai-python/issues/1732,0.27,True,missing_option_violation,medium,"The default behavior of the temperature parameter in the API does not match the expected default as per the documentation, leading to inconsistent outputs when the parameter is omitted.",high,
1054,Invalid port error,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

Hi, When I run the following code, one error raises, any thoughts?
```
from openai import OpenAI
client = OpenAI(api_key=""XXXXX"")
```
Raising error:
```
Traceback (most recent call last):                                                                                                    
  File ""/home/tiger/.local/lib/python3.9/site-packages/httpx/_urlparse.py"", line 339, in normalize_port
    port_as_int = int(port)                                                                                                           
ValueError: invalid literal for int() with base 10: ':1'                                                                              
                                                                                                                                      
During handling of the above exception, another exception occurred: 

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/tiger/.local/lib/python3.9/site-packages/openai/_client.py"", line 106, in __init__
    super().__init__(
  File ""/home/tiger/.local/lib/python3.9/site-packages/openai/_base_client.py"", line 758, in __init__
    self._client = http_client or SyncHttpxClientWrapper(
  File ""/home/tiger/.local/lib/python3.9/site-packages/httpx/_client.py"", line 682, in __init__
    self._mounts: typing.Dict[URLPattern, typing.Optional[BaseTransport]] = {
  File ""/home/tiger/.local/lib/python3.9/site-packages/httpx/_client.py"", line 683, in <dictcomp>
    URLPattern(key): None
  File ""/home/tiger/.local/lib/python3.9/site-packages/httpx/_utils.py"", line 397, in __init__
    url = URL(pattern)
  File ""/home/tiger/.local/lib/python3.9/site-packages/httpx/_urls.py"", line 113, in __init__
    self._uri_reference = urlparse(url, **kwargs)
  File ""/home/tiger/.local/lib/python3.9/site-packages/httpx/_urlparse.py"", line 246, in urlparse
    parsed_port: typing.Optional[int] = normalize_port(port, scheme)
  File ""/home/tiger/.local/lib/python3.9/site-packages/httpx/_urlparse.py"", line 341, in normalize_port
    raise InvalidURL(""Invalid port"")
httpx.InvalidURL: Invalid port
```

### To Reproduce

from openai import OpenAI
client = OpenAI(api_key=""XXXXX"")

### Code snippets

_No response_

### OS

unbuntu

### Python version

python 3.9

### Library version

openai 1.6.0",closed,2024-01-08T10:00:05Z,2024-03-03T01:06:50Z,[],https://github.com/openai/openai-python/issues/1054,1311.11,True,input_value_violation,high,"The provided port value is invalid, causing an error during URL parsing.",high,
1721,APIRemovedInV1 Error When Using openai.ChatCompletion.create in Clean Environments (v1.45.0+ and Docker),"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

I am encountering an `APIRemovedInV1` error when using the `openai.ChatCompletion.create()` method in both local and Docker environments, despite using the latest versions of the OpenAI Python library (v1.45.0 and v1.46.0).

The error persists even after following the official migration guide and ensuring that the correct API is being used. I’ve tested this in multiple clean environments, including an isolated Docker container, but the issue still occurs.

The error message is:

```
APIRemovedInV1: You tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0
```

Expected Behavior:
I expect `openai.ChatCompletion.create()` to work correctly without triggering the `APIRemovedInV1` error, as per the latest API documentation.

**Environment:**
Python version: 3.10
OpenAI Python library version: 1.45.0 and 1.46.0 (tried both)
OS: Ubuntu (both local and Docker environments)
Additional Context:
I've purged all old versions of the OpenAI library and even rebuilt environments without caching (including Docker).
The issue persists even in fresh environments with no previous configurations.
I followed the official migration guide, and the method `openai.ChatCompletion.create()` is being used as described in the documentation.


### To Reproduce

Install openai via pip (pip install openai==1.45.0 or pip install openai==1.46.0).
Attempt to use openai.ChatCompletion.create() with the code snippet below:

### Code snippets

```Python
import openai
openai.api_key = 'my api key from .env file'

response = openai.ChatCompletion.create(
    model=""gpt-3.5-turbo"",
    messages=[
        {""role"": ""system"", ""content"": ""You are a helpful assistant.""},
        {""role"": ""user"", ""content"": ""Hello, ChatGPT!""}
    ]
)
print(response['choices'][0]['message']['content'])
```


### OS

Windows with WSL / Ubuntu

### Python version

Python 3.12.12

### Library version

openai 1.45.0 & openai 1.46.0",closed,2024-09-18T12:23:43Z,2024-09-18T14:37:49Z,['question'],https://github.com/openai/openai-python/issues/1721,2.23,True,missing_dependency_violation,high,"The method openai.ChatCompletion.create() is being called, but the API has been removed in versions >=1.0.0, indicating a missing dependency on the correct API version.",high,
1712,Медленно отвечает новая демо версия OpenAI,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

1. Медленно даёт ответ 15-20 сек.
2. Не даёт ответ в 20% приблизительно.
3. Не дописывает текст до конца и зависает в 20%.
4. ChatGPT 4o так же начал зависать и не давать ответ, хотя раньше такое было реже.
5. Я предоставил o1-preview возможность выбора и это было его единственной задачей, выбирать слова, слов было 10 которые я так же указал, он не мог справиться с этой задачей и зависал или вообще не выдавал результат, ему нужно было просто писать слова по одному и ждать любой моей ответ и потом снова писать одно из 10 указанных слов.
6. Не предоставляя информацию о лимите по количеству запросов я исчерпал его на 7 дней, если бы знал то умнее бы распоряжался данной возможностью, тем не менее я надеюсь что полная версия выйдет намного лучше прошлой и демо.

### To Reproduce

1. Написать 10 слов и указать что бы он выбирал в произвольном порядке.
2. Указать что бы писал по одному слову, дожидаясь моего ответа.
3. После ответа повторять процедуру, пока слова не закончатся. 

### Code snippets

_No response_

### OS

Windows

### Python version

Python v3.11.4

### Library version

openai v1.0.1.",closed,2024-09-13T23:38:34Z,2024-09-16T15:31:08Z,['question'],https://github.com/openai/openai-python/issues/1712,63.88,True,performance_degradation,Medium,"The API exhibits slow response times and hangs during operation, indicating performance issues.",high,
413,error_data is 'Not Found',"### Describe the bug

when I run this script,it make mistake, AttributeError: 'str' object has no attribute 'get'
then I debug this script and find that error_data is 'Not Found'

![image](https://user-images.githubusercontent.com/98522540/233877518-a316639a-334a-4194-834c-39942be9dc2c.png)

![image](https://user-images.githubusercontent.com/98522540/233878008-928eb1b5-6c39-4e57-b724-a65a87825906.png)

![image](https://user-images.githubusercontent.com/98522540/233877128-16bd024d-b243-434f-8680-e52c8900d8cb.png)


### To Reproduce

1. update 'api_requestor.py' file,add lines in request_raw function: 
```python       
proxy = {
            'http': '127.0.0.1:7890',
            'https': '127.0.0.1:7890'
        }
# 7890 is my proxy port
```
3. update 'proxies=_thread_context.session.proxies' to proxies=proxy
4. run it in main program:
```python
import openai

openai.api_key = ""my openai key""
openai.api_base = ""https://api.openai-proxy.com""

# list models
models = openai.Model.list()

# print the first model's id
print(models.data[0].id)

# create a completion
completion = openai.Completion.create(model=""ada"", prompt=""Hello world"")

# print the completion
print(completion.choices[0].text)
```


### Code snippets

_No response_

### OS

windows

### Python version

Python v3.10.0

### Library version

openai-poython v0.27.4",closed,2023-04-24T01:25:17Z,2023-11-10T04:31:48Z,['bug'],https://github.com/openai/openai-python/issues/413,4803.11,True,input_type_violation,High,"The error indicates that a string object is being treated as a dictionary, suggesting that the input type for the API call does not match the expected type.",high,
822,add list method for  threads object,"### Confirm this is a feature request for the Python library and not the underlying OpenAI API.

- [X] This is a feature request for the Python library

### Describe the feature or improvement you're requesting

It seems that there's an undocumented GET endpoint for the threads in the underlying OpenAI API that can list all the treads. It's convenient for developers to see all the threads since this can't be done in the Assistants page in the playgound site. 

### Additional context

_No response_",closed,2023-11-15T03:20:26Z,2023-11-16T03:29:47Z,[],https://github.com/openai/openai-python/issues/822,24.16,False,,,,,
975,client.chat.completions.create() not taking logprobs and top_logprobs as arguments,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

I just updated to 1.4.0, and the `client.chat.completions.create()` in python library is not taking `logprobs` or `top_logprobs` as arguments but these arguments are already enabled if I access them through http request.

### To Reproduce

To reproduce:
```
from openai import OpenAI
client = OpenAI()

completion = client.chat.completions.create(
  model=""gpt-3.5-turbo"",
  messages=[
    {""role"": ""system"", ""content"": ""You are a helpful assistant.""},
    {""role"": ""user"", ""content"": ""Hello!""}
  ],
 logprobs=True,
)
```
Then it will return:
`TypeError: Completions.create() got an unexpected keyword argument 'logprobs'`

But you can use:
```
import requests

url = ""https://api.openai.com/v1/chat/completions""
headers = {
    ""Content-Type"": ""application/json"",
    ""Authorization"": f""Bearer {OPENAI_API_KEY}""
}
data = {
    ""model"": ""gpt-3.5-turbo"",
    ""messages"": [
        {
            ""role"": ""system"",
            ""content"": ""You are a helpful assistant.""
        },
        {
            ""role"": ""user"",
            ""content"": ""Hello!""
        }
    ],
    'logprobs': True,
}

response = requests.post(url, headers=headers, json=data)
```
And you will get a response with logprobs.

### Code snippets

_No response_

### OS

macOS

### Python version

Python v3.11.5

### Library version

openai v1.4.0",closed,2023-12-15T19:06:13Z,2023-12-19T03:35:50Z,['bug'],https://github.com/openai/openai-python/issues/975,80.49,True,missing_option_violation,medium,"The Python library does not support 'logprobs' and 'top_logprobs' arguments in the 'client.chat.completions.create()' method, while these options are available through the HTTP API.",high,
1704,Bug caused by incorrect usage of Pydantic and Typing Extensions,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

You define the `JSONSchema` class in the path `openai.types.shared_params.response_format_json_schema.py`  as follow
```python
class JSONSchema(TypedDict, total=False):
    name: Required[str]
    """"""The name of the response format.

    Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length
    of 64.
    """"""

    description: str
    """"""
    A description of what the response format is for, used by the model to determine
    how to respond in the format.
    """"""

    schema: Dict[str, object]
    """"""The schema for the response format, described as a JSON Schema object.""""""

    strict: Optional[bool]
    """"""Whether to enable strict schema adherence when generating the output.

    If set to true, the model will always follow the exact schema defined in the
    `schema` field. Only a subset of JSON Schema is supported when `strict` is
    `true`. To learn more, read the
    [Structured Outputs guide](https://platform.openai.com/docs/guides/structured-outputs).
    """"""
```
Then, you used it in each class where you defined the format of the response body, as follows. 
```python
ResponseFormat: TypeAlias = Union[ResponseFormatText, ResponseFormatJSONObject, ResponseFormatJSONSchema]

class CompletionCreateParamsBase(TypedDict, total=False):
       .
       .
       .
      response_format: ResponseFormat
```
However, the field named 'schema' is a built-in field in Pydantic. When you convert this model to Pydantic, it triggers the following error.
`NameError: Field name ""schema"" shadows a BaseModel attribute; use a different field name with ""alias='schema'`
This bug exists in version 1.40 and later versions.


### To Reproduce

1.
from openai.types.chat.completion_create_params import CompletionCreateParamsNonStreaming
from pydantic.v1 import create_model_from_typedict
2. ref = create_model_from_typedict(CompletionCreateParamsNonStreaming)
3.  Then error occurred

### Code snippets

_No response_

### OS

any

### Python version

any

### Library version

openai v1.40.0",closed,2024-09-12T11:15:46Z,2024-09-12T11:34:22Z,['question'],https://github.com/openai/openai-python/issues/1704,0.31,True,input_type_violation,high,"The field name 'schema' in the JSONSchema class conflicts with a built-in Pydantic field, causing a NameError when used in a Pydantic model.",high,
1700,Include usage information in LengthFinishReasonError,"### Confirm this is a feature request for the Python library and not the underlying OpenAI API.

- [X] This is a feature request for the Python library

### Describe the feature or improvement you're requesting

**Situation**
When calling `AsyncOpenAI(...).beta.chat.completions.parse(..., response_format=SomePydanticModel)`, the OpenAI library raises `LengthFinishReasonError` when `finish_reason == ""length""` and raises `ContentFilterFinishReasonError` when `finish_reason == ""content_filter""`, without providing any information as to what the response contained.

**Complication**
Because there is no way to retrieve any information about the response, I cannot programmatically save information about the context. For example, I cannot access and track information from the `usage` object in the chat completion response.

**Desired behavior**
As a library user, I always want to know details about responses from LLM calls that costs tokens for me. More specifically, I want to inspect `usage` to know how many tokens I ""wasted"" calling the LLM, for instance when `max_tokens` was set to a too low value for the LLM to generate a complete structured output. This enables me to track and control costs.

I see two potential solutions:
1. Stop raising exceptions for these scenarios and always return a chat completion object. I believe this is the behavior in the non-beta version of the chat completion call
2. Return the response as an attribute in the exception object so that it can be used by the calling programmer

**Version used**
- 1.44.1 (latest on PyPI at the time of writing)

**Code location**
- File and line: [`openai.lib._parsing._completions.py` on lines 71-75](https://github.com/openai/openai-python/blob/main/src/openai/lib/_parsing/_completions.py#L71)
- Function: `parse_chat_completion`

### Additional context

_No response_",closed,2024-09-10T08:23:57Z,2024-09-10T15:47:19Z,['enhancement'],https://github.com/openai/openai-python/issues/1700,7.39,True,missing_dependency_violation,high,"The API does not provide necessary usage information in the case of exceptions, leading to a lack of context for the user regarding token usage and costs.",high,
1699,Add Memory to API,"### Confirm this is a feature request for the Python library and not the underlying OpenAI API.

- [X] This is a feature request for the Python library

### Describe the feature or improvement you're requesting

The long term memory feature in ChatGPT is a great feature that i'd like to make available to my users via API. Before I invest time developing a less capable solution I want to check with the devs if this is on the API roadmap?

### Additional context

_No response_",closed,2024-09-10T04:18:44Z,2024-09-10T08:44:23Z,[],https://github.com/openai/openai-python/issues/1699,4.43,False,,,,,
864,TTS streaming does not work,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

When following the documentation on how to use `client.audio.speech.create()`, the returned response has a method called `stream_to_file(file_path)` which explains that when used, it should stream the content of the audio file as it's being created. This does not seem to work. I've used a rather large text input that generates a 3.5 minute sound file and the file is only created once the whole request is completed.

### To Reproduce

Utilize the following code and replace the text input with a decently large amount of text.
```python
from pathlib import Path
from openai import OpenAI
client = OpenAI()

speech_file_path = Path(__file__).parent / ""speech.mp3""
response = client.audio.speech.create(
    model=""tts-1"",
    voice=""alloy"",
    input=""""""
        <Decently large bit of text here>
    """"""
)

response.stream_to_file(speech_file_path)
```

Notice that when the script is run that the `speech.mp3` file is only ever created _after_ the request is fully completed.

### Code snippets

_No response_

### OS

macOS

### Python version

Python 3.11.6

### Library version

openai v1.2.4",closed,2023-11-22T20:37:23Z,2024-03-03T00:55:17Z,['bug'],https://github.com/openai/openai-python/issues/864,2428.3,True,method_order_violation,performance_degradation,"The streaming functionality does not operate as expected, leading to delayed file creation until the entire request is completed.",high,
1686,The content of the api_key or default_headers of an AsyncOpenAI instance can be modified surreptitiously? ,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

Deploying a local LLM. Create an instance of AsyncOpenAI called `aclient`, then call `aclient.chat.completions.create` and return an asynchronous iterator. When processing the chunks yielded by the iterator, it appears that the `api_key` in the `aclient` instance is continuously changing.


### To Reproduce

Set the N in user_id to increment from 1 for each request, i.e. 'user_id_1', 'user_id_2', ... 'user_id_30' ... When the number of concurrent requests is greater than a certain number, the id(aclient) changes every time the for loop is entered, and user_id_from_header and user_id_from_api_key change, and the member variables of different AsyncOpenAI instances (aclient) are mixed together: the same aclient instance, e.g. user_id_from_header='user_id_20', user_id_from_api_key='user_id_15',

### Code snippets

```Python
user_id = <user_id_N>
aclient = AsyncOpenAI(api_key= user_id, default_headers= {'user_id': user_id})
stream =  await aclient.chat.completions.create(...)
async for chunk in stream:
     do_something(...)
     logger.info(f'user_id_from_header={aclient.default_headers['user_id']}, user_id_from_api_key={aclient.api_key}, id={id(aclient)}')
```


### OS

CentOS

### Python version

Python 3.11.4

### Library version

openai v1.43.0",closed,2024-09-04T14:14:01Z,2024-09-05T05:50:50Z,['bug'],https://github.com/openai/openai-python/issues/1686,15.61,True,missing_dependency_violation,high,"The api_key and default_headers are being modified unexpectedly, leading to mixed state across AsyncOpenAI instances, which undermines the integrity of the API's expected behavior.",high,
1622,Unable to access the OpenAI API with genuine key,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

As per the recommendations I used openai.Completion.create with openai version 0.28.0 as per this link https://github.com/openai/openai-python/discussions/742 and used openai.ChatCompletion.create with the most latest version 1.40.1 

In both the cases I get error

Error in both versions 1.40.1 0.28.0

An error occurred: 

You tried to access openai.Completion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.

You can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. 

Alternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`

A detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742


### To Reproduce

pip install --upgrade openai

for 1.40.1

pip install openai==0.28

response = openai.ChatCompletion.create(
            model=""gpt-3.5-turbo"",  # Use a model like ""gpt-3.5-turbo"" or ""gpt-4""
            messages=[
                {""role"": ""user"", ""content"": prompt}
            ],
            max_tokens=1024,
            temperature=0.5
        )

For 0.28.0

response = openai.Completion.create(
            model=""gpt-3.5-turbo"",  # Use a model like ""gpt-3.5-turbo"" or ""gpt-4""
            messages=[
                {""role"": ""user"", ""content"": prompt}
            ],
            max_tokens=1024,
            temperature=0.5
        )


Error

You tried to access openai.Completion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.

You can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. 

Alternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`

A detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742


### Code snippets

```Python
As explained above
```


### OS

Windows

### Python version

3.11.7

### Library version

1.40.1",closed,2024-08-08T06:51:32Z,2024-08-08T08:36:57Z,['question'],https://github.com/openai/openai-python/issues/1622,1.76,True,missing_option_violation,high,"The user is attempting to use an outdated method (openai.Completion) that is no longer supported in the current library version (1.40.1), indicating a failure to follow the updated API contract.",high,
1669,Run Lifecycle Documentation Broken Link in README,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

The error involves a broken link in the README file that was supposed to direct users to the ""Run Lifecycle Documentation."" The broken link prevents users from accessing essential information about the project's execution stages, leading to potential confusion or errors in using the project. Fixing it is important for proper guidance.

### To Reproduce

- Navigate to the Repository: Open the GitHub 
- Locate the README File: Scroll down the main repository page to find the README file
- Find the Polling Helpers Section:
- Within the ""Polling Helpers"" section, look for a link labeled ""Run Lifecycle Documentation.""
- Click on this link.
- Observe the Error:
-  ""404 Not Found"" on https://platform.openai.com/docs/assistants/how-it-works/run-lifecycle 

### Code snippets

_No response_

### OS

All

### Python version

All

### Library version

All",closed,2024-08-21T23:21:27Z,2024-09-03T18:52:18Z,"['bug', 'documentation']",https://github.com/openai/openai-python/issues/1669,307.51,False,,,,,
1678,Indentation error in _utils/_utils.py,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

It looks like there's a bug in the _utils/_utils.py file:
```
            for variant in variants:
                matches = all((param in given_params for param in variant))
                if matches:
                    break
            else:  # no break
                if len(variants) > 1:
                    variations = human_join(
                        [""("" + human_join([quote(arg) for arg in variant], final=""and"") + "")"" for variant in variants]
                    )
                    msg = f""Missing required arguments; Expected either {variations} arguments to be given""
```
That `if matches:` and `else:  # no break` aren't aligned in the wrapper function. 

### To Reproduce

Look at the code in the wrapper() function of _utils/_utils.py

### Code snippets

_No response_

### OS

macOS

### Python version

Python 3.11

### Library version

openai v1.42.0",closed,2024-08-26T20:42:12Z,2024-08-26T21:23:12Z,['question'],https://github.com/openai/openai-python/issues/1678,0.68,False,,,,,
1616,replace jiter dependency with build-in pydantic function,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

https://pypi.org/project/jiter/
```
This is a standalone version of the JSON parser used in pydantic-core
The recommendation is to only use this package directly if you do not use pydantic.
```

The reason of this request is some OS (Gentoo in my case) do provide both ```pydantic``` and ```pydantic-core``` libraries, but ```jiter```

openai-python uses ```pydantic``` already, so it should be possible to re-use build-in function (perhaps, via ```pydantic-core```)

### To Reproduce

1. inspect this line: https://github.com/openai/openai-python/blob/main/pyproject.toml#L19

### Code snippets

```Python
https://github.com/openai/openai-python/blob/main/src/openai/lib/streaming/chat/_completions.py
 jiter import from_json
```


### OS

any

### Python version

any

### Library version

1.40.0",closed,2024-08-08T01:15:10Z,2024-08-14T15:12:20Z,[],https://github.com/openai/openai-python/issues/1616,157.95,True,missing_dependency_violation,medium,"The request to replace the jiter dependency with a built-in pydantic function indicates a missing dependency on the pydantic library, which may lead to compatibility issues or incorrect functionality.",high,
1672,file attatched to thread cannot be searched,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

i have a assistant with tool file search

and create a new thread , ask question ""who is berryhoneffaaa""  (berryhoneffaaa is person only me know in my private knowlege database) ,  assisitant anwered  ""I couldn't find any information on ""berryhoneffaaa"" in the files provided. If you have any additional context or details about Berryhoneffaaa, please provide them so I can better assist you.""  . this is what i excepted

and then i upload my private knowlege file and attatached the file to the thread , and ask ""who is berryhoneffaaa"" , it **still** **answer ""i dont known""**

![image](https://github.com/user-attachments/assets/947c43f7-235c-4771-86d7-8f13e6a32c0f)



**if i upload file before a ask the question, the assistant file search will search the uploaded file and  get the right answer**



### To Reproduce

see the Describe

### Code snippets

```Python
openai playground have this problem too
```


### OS

macOS

### Python version

python3.10

### Library version

openai 1.40.8",closed,2024-08-23T12:57:40Z,2024-08-23T14:28:33Z,['API-feedback'],https://github.com/openai/openai-python/issues/1672,1.51,True,missing_dependency_violation,high,"The assistant fails to utilize the uploaded file for context when responding to the query, indicating a missing dependency between the file upload and the query processing.",high,
1670,ChatCompletionSystemMessageParam.name is not optional but the description says it should be,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

The class param [`ChatCompletionSystemMessageParam.name`](https://github.com/openai/openai-python/blob/05fa732c024b55ddda1f5f8b107ce78233acd3ce/src/openai/types/chat/chat_completion_system_message_param.py#L20) is described as being optional, in fact, the API works without issues if we don't pass this param. But the type is not marked as optional. (Same issue with [`ChatCompletionUserMessageParam.name`](https://github.com/openai/openai-python/blob/05fa732c024b55ddda1f5f8b107ce78233acd3ce/src/openai/types/chat/chat_completion_user_message_param.py#L20) and [`ChatCompletionAssistantMessageParam.name`](https://github.com/openai/openai-python/blob/05fa732c024b55ddda1f5f8b107ce78233acd3ce/src/openai/types/chat/chat_completion_assistant_message_param.py#L47)).

Instead of being declared as

```py
class ChatCompletionSystemMessageParam(TypedDict, total=False):
    content: Required[Union[str, Iterable[ChatCompletionContentPartTextParam]]]
    """"""The contents of the system message.""""""

    role: Required[Literal[""system""]]
    """"""The role of the messages author, in this case `system`.""""""

    name: str
    """"""An optional name for the participant.

    Provides the model information to differentiate between participants of the same
    role.
    """"""
```

It should be declared as:

```py
class ChatCompletionSystemMessageParam(TypedDict, total=False):
    content: Required[Union[str, Iterable[ChatCompletionContentPartTextParam]]]
    """"""The contents of the system message.""""""

    role: Required[Literal[""system""]]
    """"""The role of the messages author, in this case `system`.""""""

    name:  Optional[str]
    """"""An optional name for the participant.

    Provides the model information to differentiate between participants of the same
    role.
    """"""
```


### To Reproduce

Given this test file:

```
# test.py
import openai

client = openai.OpenAI()

response = client.beta.chat.completions.parse(
    model=""gpt-4o-mini"",
    messages=[
        {""role"": ""system"", ""content"": ""You are an assistant.""},  # <-  mypy claims this line has an error
        {
            ""role"": ""user"",
            ""content"": [
                {""type"": ""text"", ""text"": ""What's up?""},
            ],
        },
    ],
)
```

Run:

``` bash
mypy .
```

Output:
```py
test.py:8: error: Type of TypedDict is ambiguous, none of (""ChatCompletionSystemMessageParam"", ""ChatCompletionUserMessageParam"", ""ChatCompletionAssistantMessageParam"") matches cleanly  [misc]
```

### Code snippets

_No response_

### OS

ubuntu

### Python version

Python 3.12.2

### Library version

openai 1.42.0",closed,2024-08-22T13:56:45Z,2024-08-22T14:03:33Z,[],https://github.com/openai/openai-python/issues/1670,0.11,True,input_type_violation,high,"The API documentation states that the 'name' parameter is optional, but its type is not marked as such, leading to type ambiguity and potential runtime errors.",high,
1614,Support pydantic dataclasses in structured outputs,"### Confirm this is a feature request for the Python library and not the underlying OpenAI API.

- [X] This is a feature request for the Python library

### Describe the feature or improvement you're requesting

It would be great to be able to use data schemas defined as [pydantic dataclasses](https://docs.pydantic.dev/latest/concepts/dataclasses/) in structured outputs. E.g.
```python

from pydantic.dataclasses import dataclass
from openai import OpenAI

client = OpenAI()

@dataclass
class CalendarEvent:
    name: str
    date: str
    participants: list[str]

completion = client.beta.chat.completions.parse(
    model=""gpt-4o-2024-08-06"",
    messages=[
        {""role"": ""system"", ""content"": ""Extract the event information.""},
        {""role"": ""user"", ""content"": ""Alice and Bob are going to a science fair on Friday.""},
    ],
    response_format=CalendarEvent,
)

event = completion.choices[0].message.parsed
```

Pydantic dataclasses can be easily transformed into JSON schema via the [model_json_schema](https://docs.pydantic.dev/latest/api/json_schema/#pydantic.json_schema.model_json_schema) function.

### Additional context

_No response_",closed,2024-08-07T19:03:42Z,2024-08-20T22:54:05Z,['enhancement'],https://github.com/openai/openai-python/issues/1614,315.84,False,,,,,
1454,Incompatibility between openai and fastapi,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

Using openai python library with fastapi throws a pydantic error. python '3.12.3', fastapi '0.111.0', openai '1.30.4' (although the same happens with python 3.8 and 3.10)

Simply try and define an endpoint that uses CompletionCreateParams. See stack trace in code section

### To Reproduce

```
$ python3.12 -m venv 3.12venv
$ source 3.12venv/bin/activate
$ pip install fastapi
$ pip install openai
$ python
>>> from fastapi import FastAPI
>>> from openai.types.chat import CompletionCreateParams
>>> app = FastAPI()
>>> @app.post(""/v1/chat/completions"")
... def chat_completions(r: CompletionCreateParams):
...  pass
```


### Code snippets

```Python
Traceback (most recent call last):
  File ""/Users/LarryMartell/dispatcher/3.12venv/lib/python3.12/site-packages/pydantic/type_adapter.py"", line 210, in __init__
    core_schema = _getattr_no_parents(type, '__pydantic_core_schema__')
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/LarryMartell/dispatcher/3.12venv/lib/python3.12/site-packages/pydantic/type_adapter.py"", line 98, in _getattr_no_parents
    raise AttributeError(attribute)
AttributeError: __pydantic_core_schema__

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/Users/LarryMartell/dispatcher/3.12venv/lib/python3.12/site-packages/fastapi/routing.py"", line 944, in decorator
    self.add_api_route(
  File ""/Users/LarryMartell/dispatcher/3.12venv/lib/python3.12/site-packages/fastapi/routing.py"", line 883, in add_api_route
    route = route_class(
            ^^^^^^^^^^^^
  File ""/Users/LarryMartell/dispatcher/3.12venv/lib/python3.12/site-packages/fastapi/routing.py"", line 513, in __init__
    self.dependant = get_dependant(path=self.path_format, call=self.endpoint)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/LarryMartell/dispatcher/3.12venv/lib/python3.12/site-packages/fastapi/dependencies/utils.py"", line 261, in get_dependant
    type_annotation, depends, param_field = analyze_param(
                                            ^^^^^^^^^^^^^^
  File ""/Users/LarryMartell/dispatcher/3.12venv/lib/python3.12/site-packages/fastapi/dependencies/utils.py"", line 444, in analyze_param
    field = create_response_field(
            ^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/LarryMartell/dispatcher/3.12venv/lib/python3.12/site-packages/fastapi/utils.py"", line 99, in create_response_field
    return ModelField(**kwargs)  # type: ignore[arg-type]
           ^^^^^^^^^^^^^^^^^^^^
  File ""<string>"", line 6, in __init__
  File ""/Users/LarryMartell/dispatcher/3.12venv/lib/python3.12/site-packages/fastapi/_compat.py"", line 109, in __post_init__
    self._type_adapter: TypeAdapter[Any] = TypeAdapter(
                                           ^^^^^^^^^^^^
  File ""/Users/LarryMartell/dispatcher/3.12venv/lib/python3.12/site-packages/pydantic/type_adapter.py"", line 212, in __init__
    core_schema = _get_schema(type, config_wrapper, parent_depth=_parent_depth + 1)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/LarryMartell/dispatcher/3.12venv/lib/python3.12/site-packages/pydantic/type_adapter.py"", line 81, in _get_schema
    schema = gen.generate_schema(type_)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/LarryMartell/dispatcher/3.12venv/lib/python3.12/site-packages/pydantic/_internal/_generate_schema.py"", line 502, in generate_schema
    schema = self._generate_schema_inner(obj)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/LarryMartell/dispatcher/3.12venv/lib/python3.12/site-packages/pydantic/_internal/_generate_schema.py"", line 737, in _generate_schema_inner
    return self._annotated_schema(obj)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/LarryMartell/dispatcher/3.12venv/lib/python3.12/site-packages/pydantic/_internal/_generate_schema.py"", line 1757, in _annotated_schema
    schema = self._apply_annotations(source_type, annotations)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/LarryMartell/dispatcher/3.12venv/lib/python3.12/site-packages/pydantic/_internal/_generate_schema.py"", line 1825, in _apply_annotations
    schema = get_inner_schema(source_type)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/LarryMartell/dispatcher/3.12venv/lib/python3.12/site-packages/pydantic/_internal/_schema_generation_shared.py"", line 82, in __call__
    schema = self._handler(source_type)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/LarryMartell/dispatcher/3.12venv/lib/python3.12/site-packages/pydantic/_internal/_generate_schema.py"", line 1907, in new_handler
    schema = metadata_get_schema(source, get_inner_schema)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/LarryMartell/dispatcher/3.12venv/lib/python3.12/site-packages/pydantic/_internal/_generate_schema.py"", line 1903, in <lambda>
    lambda source, handler: handler(source)
                            ^^^^^^^^^^^^^^^
  File ""/Users/LarryMartell/dispatcher/3.12venv/lib/python3.12/site-packages/pydantic/_internal/_schema_generation_shared.py"", line 82, in __call__
    schema = self._handler(source_type)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/LarryMartell/dispatcher/3.12venv/lib/python3.12/site-packages/pydantic/_internal/_generate_schema.py"", line 1806, in inner_handler
    schema = self._generate_schema_inner(obj)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/LarryMartell/dispatcher/3.12venv/lib/python3.12/site-packages/pydantic/_internal/_generate_schema.py"", line 758, in _generate_schema_inner
    return self.match_type(obj)
           ^^^^^^^^^^^^^^^^^^^^
  File ""/Users/LarryMartell/dispatcher/3.12venv/lib/python3.12/site-packages/pydantic/_internal/_generate_schema.py"", line 840, in match_type
    return self._match_generic_type(obj, origin)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/LarryMartell/dispatcher/3.12venv/lib/python3.12/site-packages/pydantic/_internal/_generate_schema.py"", line 864, in _match_generic_type
    return self._union_schema(obj)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/LarryMartell/dispatcher/3.12venv/lib/python3.12/site-packages/pydantic/_internal/_generate_schema.py"", line 1152, in _union_schema
    choices.append(self.generate_schema(arg))
                   ^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/LarryMartell/dispatcher/3.12venv/lib/python3.12/site-packages/pydantic/_internal/_generate_schema.py"", line 502, in generate_schema
    schema = self._generate_schema_inner(obj)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/LarryMartell/dispatcher/3.12venv/lib/python3.12/site-packages/pydantic/_internal/_generate_schema.py"", line 758, in _generate_schema_inner
    return self.match_type(obj)
           ^^^^^^^^^^^^^^^^^^^^
  File ""/Users/LarryMartell/dispatcher/3.12venv/lib/python3.12/site-packages/pydantic/_internal/_generate_schema.py"", line 806, in match_type
    return self._typed_dict_schema(obj, None)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/LarryMartell/dispatcher/3.12venv/lib/python3.12/site-packages/pydantic/_internal/_generate_schema.py"", line 1254, in _typed_dict_schema
    for field_name, annotation in get_type_hints_infer_globalns(
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/LarryMartell/dispatcher/3.12venv/lib/python3.12/site-packages/pydantic/_internal/_fields.py"", line 56, in get_type_hints_infer_globalns
    return get_type_hints(obj, globalns=globalns, localns=localns, include_extras=include_extras)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/typing.py"", line 2244, in get_type_hints
    value = _eval_type(value, base_globals, base_locals)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/typing.py"", line 414, in _eval_type
    return t._evaluate(globalns, localns, recursive_guard)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/typing.py"", line 924, in _evaluate
    eval(self.__forward_code__, globalns, localns),
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""<string>"", line 1, in <module>
TypeError: 'pydantic_core._pydantic_core.PydanticUndefinedType' object is not subscriptable
```
```


### OS

macOS 14.4.1

### Python version

3.12.3

### Library version

1.30.4",closed,2024-05-28T23:14:40Z,2024-08-19T16:23:15Z,[],https://github.com/openai/openai-python/issues/1454,1985.14,True,input_type_violation,High,"The CompletionCreateParams type is not compatible with FastAPI's expected input types, leading to a Pydantic error during endpoint definition.",high,
1660,Request for Endpoint to List All Threads,"### Confirm this is a feature request for the Python library and not the underlying OpenAI API.

- [X] This is a feature request for the Python library

### Describe the feature or improvement you're requesting

Hey mate, hope all is well.

I would like to request the addition of an endpoint that allows for the retrieval of all threads. If there are concerns about potential security risks, such as team members or others within the same organization gaining access to all threads, I suggest implementing an additional layer of security. For example, you could restrict the listing of threads to those generated under a specific project and token bearer.

Additionally, I have noticed that this request has been reiterated multiple times since January but has not yet been addressed. Given the ongoing interest and potential impact of this feature, I believe it would be highly beneficial for many users.

This enhancement would significantly improve the usability and flexibility of the API, particularly for teams working on collaborative projects.

Thank you for considering this request. I look forward to your feedback.

Cheers

### Additional context

_No response_",closed,2024-08-18T12:26:19Z,2024-08-19T15:13:47Z,[],https://github.com/openai/openai-python/issues/1660,26.79,False,,,,,
1658,Errors after migrating openai ,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [ ] This is an issue with the Python library

### Describe the bug

[{
	""resource"": ""/home/dmtarmey/agent-zero/main.py"",
	""owner"": ""_generated_diagnostic_collection_name_#1"",
	""code"": {
		""value"": ""reportUndefinedVariable"",
		""target"": {
			""$mid"": 1,
			""path"": ""/microsoft/pyright/blob/main/docs/configuration.md"",
			""scheme"": ""https"",
			""authority"": ""github.com"",
			""fragment"": ""reportUndefinedVariable""
		}
	},
	""severity"": 8,
	""message"": ""\""get_openaiAI\"" is not defined"",
	""source"": ""Pylance"",
	""startLineNumber"": 9,
	""startColumn"": 10,
	""endLineNumber"": 9,
	""endColumn"": 22
}]

### To Reproduce

python main.py ggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggg

### Code snippets

```Python
import os
from dotenv import load_dotenv
from openai import OpenAI

# Load environment variables from .env file
load_dotenv()

# Initialize the DeepAI client with the API key
client = get_openaiAI(api_key=os.environ.get(""API_KEY_OPENAI""))

def get_openai_response(prompt):
    """"""Fetch a response from the DeepAI API based on the input prompt.""""""
    try:
        response = client.chat.completions.create(
            model=""gpt-3.5-turbo"",
            messages=[{""role"": ""user"", ""content"": prompt}]
        )
        return response.choices[0].message.content  # Return the response content
    except Exception as e:
        print(f""Error occurred: {e}"")
        return ""An error occurred. Please try again later.""

def main():
    prompt = ""Tell me a joke.""  # Your desired prompt here
    response = get_openai_response(prompt)  # Get response
    print(""Response:"", response)

if __name__ == ""__main__"":
    main()
```


### OS

linux tuxido 

### Python version

python3.11

### Library version

 openai 1.41.0",closed,2024-08-16T22:01:23Z,2024-08-17T18:06:00Z,['question'],https://github.com/openai/openai-python/issues/1658,20.08,True,input_type_violation,unknown,"The function 'get_openaiAI' is not defined, leading to a potential type mismatch when attempting to call it.",high,
1618,Too permissive permissions in file,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

Extracting files from a malicious tar archive without validating that the destination file path is within the destination directory can cause files outside the destination directory to be overwritten, due to the possible presence of directory traversal elements (..) in archive paths.

Affected file: https://github.com/openai/openai-python/blob/631a2a7156299351874f37c4769308a104ce19ed/src/openai/cli/_tools/migrate.py#L151-151

### To Reproduce

Bug present in file - https://github.com/openai/openai-python/blob/631a2a7156299351874f37c4769308a104ce19ed/src/openai/cli/_tools/migrate.py#L151-151

### Code snippets

```Python
Indicated here - https://github.com/openai/openai-python/blob/631a2a7156299351874f37c4769308a104ce19ed/src/openai/cli/_tools/migrate.py#L151-151
```


### OS

macOS

### Python version

3.11.4

### Library version

1.40.1",closed,2024-08-08T05:33:10Z,2024-08-17T18:10:08Z,[],https://github.com/openai/openai-python/issues/1618,228.62,True,missing_dependency_violation,high,"The lack of validation for file paths in the extraction process can lead to overwriting files outside the intended directory, indicating a missing dependency on proper path validation before file operations.",high,
1620,Arbitrary file write during tarfile extraction,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

Extracting files from a malicious tar archive without validating that the destination file path is within the destination directory can cause files outside the destination directory to be overwritten, due to the possible presence of directory traversal elements (..) in archive paths.

### To Reproduce

Issue present in this line - https://github.com/openai/openai-python/blob/631a2a7156299351874f37c4769308a104ce19ed/src/openai/cli/_tools/migrate.py#L144

### Code snippets

```Python
https://github.com/openai/openai-python/blob/631a2a7156299351874f37c4769308a104ce19ed/src/openai/cli/_tools/migrate.py#L144
```


### OS

macOS

### Python version

3.11.4

### Library version

1.40.1",closed,2024-08-08T05:40:48Z,2024-08-17T18:08:17Z,[],https://github.com/openai/openai-python/issues/1620,228.46,True,missing_dependency_violation,high,"The code does not validate the destination file path during tarfile extraction, allowing for potential overwriting of files outside the intended directory due to directory traversal vulnerabilities.",high,
1625,"openai.APIError: The server had an error processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if you keep seeing this error","### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

When I use langchain `astream_events` to call openai stream api, I will get this api error some times.

```
    async for item in self._iterator:
  File ""C:\Users\jimmy\AppData\Local\Programs\Python\Python311\Lib\site-packages\openai\_streaming.py"", line 174, in __stream__
    raise APIError(
openai.APIError: The server had an error processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if you keep seeing this error.
```

### To Reproduce

1. Use langchain agent `astream_events`. [link](https://python.langchain.com/v0.1/docs/modules/agents/how_to/streaming/)
2. Use openai be the llm model.

### Code snippets

_No response_

### OS

windows11

### Python version

Python3.11.8

### Library version

openai 1.35.13",closed,2024-08-09T01:21:03Z,2024-08-14T02:45:21Z,['bug'],https://github.com/openai/openai-python/issues/1625,121.41,True,input_value_violation,medium,The API error indicates potential issues with input values or configurations that lead to server errors during processing.,high,
1639,ChatCompletionStreamManager object does not support the asynchronous context manager protocol,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

The docs [here](https://github.com/openai/openai-python/blob/main/helpers.md#chat-completions-events) say that the following should be possible

```
import openai
import asyncio

async def test_streaming():
    client = openai.OpenAI()

    async with client.beta.chat.completions.stream(
        model='gpt-4o-2024-08-06',
        messages=[
            {""role"": ""system"", ""content"": ""You are a helpful assistant.""},
            {""role"": ""user"", ""content"": ""Tell me a joke.""},
        ],
    ) as stream:
        async for event in stream:
            if event.type == 'content.delta':
                print(event.delta, flush=True, end='')
            elif event.type == 'content.done':
                print(""\nContent generation complete."")
                break

# Run the streaming test
asyncio.run(test_streaming())
```

However, this gives 

`TypeError: 'ChatCompletionStreamManager' object does not support the asynchronous context manager protocol`

When I run without async it works fine ie

```
import openai

def test_streaming():
    client = openai.OpenAI()

    with client.beta.chat.completions.stream(
        model='gpt-4o-2024-08-06',
        messages=[
            {""role"": ""system"", ""content"": ""You are a helpful assistant.""},
            {""role"": ""user"", ""content"": ""Tell me a joke.""},
        ],
    ) as stream:
        for event in stream:
            if event.type == 'content.delta':
                print(event.delta, flush=True, end='')
            elif event.type == 'content.done':
                print(""\nContent generation complete."")
                break

# Run the streaming test
test_streaming()

```

### To Reproduce

Run the above code snippet which is the beta async chat_completion (and should handle the new pydantic parsing)

### Code snippets

```Python

```


### OS

macOS

### Python version

Python 3.11-3.12

### Library version

1.40.4",closed,2024-08-12T18:30:51Z,2024-08-12T19:01:20Z,['documentation'],https://github.com/openai/openai-python/issues/1639,0.51,True,missing_dependency_violation,unknown,"The ChatCompletionStreamManager object does not implement the asynchronous context manager protocol, indicating a missing implementation that is required for async usage.",high,
1635,client.chat.completions.create not working with base64 images,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

No idea why, but when using the lib to give gtp-4o-mini an image in base64 it's giving me the following error:

```
openai.BadRequestError: Error code: 400 - {'error': {'message': ""Invalid type for 'messages[0].content[1].image_url': expected an object, but got a string instead."", 'type': 'invalid_request_error', 'param': 'messages[0].content[1].image_url', 'code': 'invalid_type'}}
```

It's essentialy the same code provided in https://platform.openai.com/docs/guides/vision/uploading-base-64-encoded-images which works fine for me.

### To Reproduce

Just run the code snippet, I guess.

### Code snippets

```Python
client = OpenAI(
  project='',
  api_key=""""
)

response = client.chat.completions.create(
        model=""gpt-4o-mini"",
        messages=[
            {
                ""role"": ""user"",
                ""content"": [
                    {""type"": ""text"", ""text"": ""Describe this image in a few words.""},
                    {
                        ""type"": ""image_url"",
                        ""image_url"": f""data:image/jpeg;base64,{base64_image}""
                    },
                ],
            }
        ],
        max_tokens=100
    )
```


### OS

macOS

### Python version

Python 3.10.14

### Library version

1.40.3",closed,2024-08-11T18:24:35Z,2024-08-11T18:47:09Z,[],https://github.com/openai/openai-python/issues/1635,0.38,True,input_type_violation,High,"The API expects an object for 'messages[0].content[1].image_url', but a string (base64 image) was provided instead.",high,
1626,"openai.InternalServerError: upstream connect error or disconnect/reset before headers. reset reason: connection failure, transport failure reason: delayed connect error: 111","### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

I am doing a hundreds of requests by hour, 95% works perfectly.
Sometimes it crashes with this error: 
```openai.InternalServerError: upstream connect error or disconnect/reset before headers. reset reason: connection failure, transport failure reason: delayed connect error: 111```

The thing is that the same code works most of the time and fails sometime, making the debugging really hard, I failed to find the pattern that make theses requests fail, it can arrive anytime.

It can pass with big requests, with a lot of requests at the same time. It can crash with tiny request and with few requests in the same time.
So the intuition that they are too much request or too big request seems wrong.

I have this Traceback:
```
return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File ""/usr/local/lib/python3.11/site-packages/openai/resources/chat/completions.py"", line 650, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File ""/usr/local/lib/python3.11/site-packages/openai/_base_client.py"", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File ""/usr/local/lib/python3.11/site-packages/openai/_base_client.py"", line 936, in request\n    return self._request(\n           ^^^^^^^^^^^^^^\n  File ""/usr/local/lib/python3.11/site-packages/openai/_base_client.py"", line 1025, in _request\n    return self._retry_request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File ""/usr/local/lib/python3.11/site-packages/openai/_base_client.py"", line 1074, in _retry_request\n    return self._request(\n           ^^^^^^^^^^^^^^\n  File ""/usr/local/lib/python3.11/site-packages/openai/_base_client.py"", line 1025, in _request\n    return self._retry_request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File ""/usr/local/lib/python3.11/site-packages/openai/_base_client.py"", line 1074, in _retry_request\n    return self._request(\n           ^^^^^^^^^^^^^^\n  File ""/usr/local/lib/python3.11/site-packages/openai/_base_client.py"", line 1040, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.InternalServerError: upstream connect error or disconnect/reset before headers. reset reason: connection failure, transport failure reason: delayed connect error: 111\n
```

### To Reproduce

from langchain_openai import ChatOpenAI

llm = ChatOpenAI(
    model=""gpt-4o"",
    temperature=0,
    max_tokens=None,
    timeout=None,
    max_retries=2,
)



### Code snippets

_No response_

### OS

Linux

### Python version

python v3.11

### Library version

openai v1.35.10",closed,2024-08-09T08:46:38Z,2024-08-09T12:58:23Z,[],https://github.com/openai/openai-python/issues/1626,4.2,False,,,,low,
1617,BUG: version `1.40.x`: NameError for `CompletionCreateParamsNonStreaming`,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

Error stack:
```
File ~/miniconda3/lib/python3.9/site-packages/pydantic/v1/validators.py:738, in find_validators(type_, config)
    736     return
    737 if is_typeddict(type_):
--> 738     yield make_typeddict_validator(type_, config)
    739     return
    741 class_ = get_class(type_)

File ~/miniconda3/lib/python3.9/site-packages/pydantic/v1/validators.py:624, in make_typeddict_validator(typeddict_cls, config)
    619 def make_typeddict_validator(
    620     typeddict_cls: Type['TypedDict'], config: Type['BaseConfig']  # type: ignore[valid-type]
    621 ) -> Callable[[Any], Dict[str, Any]]:
    622     from .annotated_types import create_model_from_typeddict
--> 624     TypedDictModel = create_model_from_typeddict(
    625         typeddict_cls,
    626         __config__=config,
    627         __module__=typeddict_cls.__module__,
    628     )
    629     typeddict_cls.__pydantic_model__ = TypedDictModel  # type: ignore[attr-defined]
    631     def typeddict_validator(values: 'TypedDict') -> Dict[str, Any]:  # type: ignore[valid-type]

File ~/miniconda3/lib/python3.9/site-packages/pydantic/v1/annotated_types.py:55, in create_model_from_typeddict(typeddict_cls, **kwargs)
     49 required_keys: FrozenSet[str] = typeddict_cls.__required_keys__  # type: ignore[attr-defined]
     50 field_definitions = {
     51     field_name: (field_type, Required if field_name in required_keys else None)
     52     for field_name, field_type in typeddict_cls.__annotations__.items()
     53 }
---> 55 return create_model(typeddict_cls.__name__, **kwargs, **field_definitions)

File ~/miniconda3/lib/python3.9/site-packages/pydantic/v1/main.py:1024, in create_model(__model_name, __config__, __base__, __module__, __validators__, __cls_kwargs__, __slots__, **field_definitions)
   1022     ns['__orig_bases__'] = __base__
   1023 namespace.update(ns)
-> 1024 return meta(__model_name, resolved_bases, namespace, **kwds)

File ~/miniconda3/lib/python3.9/site-packages/pydantic/v1/main.py:197, in ModelMetaclass.__new__(mcs, name, bases, namespace, **kwargs)
    189     if (
    190         is_untouched(value)
    191         and ann_type != PyObject
   (...)
    194         )
    195     ):
    196         continue
--> 197     fields[ann_name] = ModelField.infer(
    198         name=ann_name,
    199         value=value,
    200         annotation=ann_type,
    201         class_validators=vg.get_validators(ann_name),
    202         config=config,
    203     )
    204 elif ann_name not in namespace and config.underscore_attrs_are_private:
    205     private_attributes[ann_name] = PrivateAttr()

File ~/miniconda3/lib/python3.9/site-packages/pydantic/v1/fields.py:504, in ModelField.infer(cls, name, value, annotation, class_validators, config)
    501     required = False
    502 annotation = get_annotation_from_field_info(annotation, field_info, name, config.validate_assignment)
--> 504 return cls(
    505     name=name,
    506     type_=annotation,
    507     alias=field_info.alias,
    508     class_validators=class_validators,
    509     default=value,
    510     default_factory=field_info.default_factory,
    511     required=required,
    512     model_config=config,
    513     field_info=field_info,
    514 )

File ~/miniconda3/lib/python3.9/site-packages/pydantic/v1/fields.py:434, in ModelField.__init__(self, name, type_, class_validators, model_config, default, default_factory, required, final, alias, field_info)
    432 self.shape: int = SHAPE_SINGLETON
    433 self.model_config.prepare_field(self)
--> 434 self.prepare()

File ~/miniconda3/lib/python3.9/site-packages/pydantic/v1/fields.py:555, in ModelField.prepare(self)
    553 if self.default is Undefined and self.default_factory is None:
    554     self.default = None
--> 555 self.populate_validators()

File ~/miniconda3/lib/python3.9/site-packages/pydantic/v1/fields.py:829, in ModelField.populate_validators(self)
    825 if not self.sub_fields or self.shape == SHAPE_GENERIC:
    826     get_validators = getattr(self.type_, '__get_validators__', None)
    827     v_funcs = (
    828         *[v.func for v in class_validators_ if v.each_item and v.pre],
--> 829         *(get_validators() if get_validators else list(find_validators(self.type_, self.model_config))),
    830         *[v.func for v in class_validators_ if v.each_item and not v.pre],
    831     )
    832     self.validators = prep_validators(v_funcs)
    834 self.pre_validators = []

File ~/miniconda3/lib/python3.9/site-packages/pydantic/v1/validators.py:738, in find_validators(type_, config)
    736     return
    737 if is_typeddict(type_):
--> 738     yield make_typeddict_validator(type_, config)
    739     return
    741 class_ = get_class(type_)

File ~/miniconda3/lib/python3.9/site-packages/pydantic/v1/validators.py:624, in make_typeddict_validator(typeddict_cls, config)
    619 def make_typeddict_validator(
    620     typeddict_cls: Type['TypedDict'], config: Type['BaseConfig']  # type: ignore[valid-type]
    621 ) -> Callable[[Any], Dict[str, Any]]:
    622     from .annotated_types import create_model_from_typeddict
--> 624     TypedDictModel = create_model_from_typeddict(
    625         typeddict_cls,
    626         __config__=config,
    627         __module__=typeddict_cls.__module__,
    628     )
    629     typeddict_cls.__pydantic_model__ = TypedDictModel  # type: ignore[attr-defined]
    631     def typeddict_validator(values: 'TypedDict') -> Dict[str, Any]:  # type: ignore[valid-type]

File ~/miniconda3/lib/python3.9/site-packages/pydantic/v1/annotated_types.py:55, in create_model_from_typeddict(typeddict_cls, **kwargs)
     49 required_keys: FrozenSet[str] = typeddict_cls.__required_keys__  # type: ignore[attr-defined]
     50 field_definitions = {
     51     field_name: (field_type, Required if field_name in required_keys else None)
     52     for field_name, field_type in typeddict_cls.__annotations__.items()
     53 }
---> 55 return create_model(typeddict_cls.__name__, **kwargs, **field_definitions)

File ~/miniconda3/lib/python3.9/site-packages/pydantic/v1/main.py:1024, in create_model(__model_name, __config__, __base__, __module__, __validators__, __cls_kwargs__, __slots__, **field_definitions)
   1022     ns['__orig_bases__'] = __base__
   1023 namespace.update(ns)
-> 1024 return meta(__model_name, resolved_bases, namespace, **kwds)

File ~/miniconda3/lib/python3.9/site-packages/pydantic/v1/main.py:186, in ModelMetaclass.__new__(mcs, name, bases, namespace, **kwargs)
    184     class_vars.add(ann_name)
    185 elif is_valid_field(ann_name):
--> 186     validate_field_name(bases, ann_name)
    187     value = namespace.get(ann_name, Undefined)
    188     allowed_types = get_args(ann_type) if is_union(get_origin(ann_type)) else (ann_type,)

File ~/miniconda3/lib/python3.9/site-packages/pydantic/v1/utils.py:167, in validate_field_name(bases, field_name)
    165 for base in bases:
    166     if getattr(base, field_name, None):
--> 167         raise NameError(
    168             f'Field name ""{field_name}"" shadows a BaseModel attribute; '
    169             f'use a different field name with ""alias=\'{field_name}\'"".'
    170         )

NameError: Field name ""schema"" shadows a BaseModel attribute; use a different field name with ""alias='schema'"".
```

Before v1.40.1, everything works fine.

### To Reproduce

Mini code:
```
from openai.types.chat.completion_create_params
from openai.types.chat.completion_create_params import CompletionCreateParamsNonStreaming
from pydantic.v1 import create_model_from_typeddict
create_model_from_typeddict(CompletionCreateParamsNonStreaming)
```

### Code snippets

_No response_

### OS

I think all the os should have this issue

### Python version

Python 3.9

### Library version

openai v1.40.1",closed,2024-08-08T03:01:14Z,2024-08-08T10:08:28Z,[],https://github.com/openai/openai-python/issues/1617,7.12,True,input_type_violation,High,"The field name 'schema' in CompletionCreateParamsNonStreaming shadows a BaseModel attribute, causing a NameError when creating a model from the TypedDict.",high,
1595,be explicit about parallel_tool_call option for streams,"### Confirm this is a feature request for the Python library and not the underlying OpenAI API.

- [X] This is a feature request for the Python library

### Describe the feature or improvement you're requesting

For runs created by opening a stream, parallel_tool_call options could only be set to false using the extra_body argument, because the runs.stream function does not accept parallel_tool_call as an argument.
Because multiple tool outputs can only be submitted in a single request, managing and syncing the state of each tool_calls can add unnecessary complication in some use_cases. In that case, it requires testing to find out that turning off the parallel_tool_call option is also possible in streams, since documentation for parallel_tool_call is only provided for non-stream approach.

I've checked that parallel_tool_call option works as expected if passed in as extra_body. Although this works fine for me, I think such option should be explicitly set as keyword argument.

### Additional context

I would like to try and make a PR for this if my request turns out to be valid. Thanks",closed,2024-08-05T02:41:58Z,2024-08-05T14:24:49Z,[],https://github.com/openai/openai-python/issues/1595,11.71,False,,,,low,
676,v1.0 drops embeddings_util.py breaking semantic text search,"### Describe the bug

The previous version of the OpenAI Python library contained `embeddings_utils.py` which provided functions like `cosine_similarity` which are used for semantic text search with embeddings. Without this functionality existing code including OpenAI's cookbook example: https://cookbook.openai.com/examples/semantic_text_search_using_embeddings will fail due to this dependency.

Are there plans to add this support back-in or should we just create our own cosine_similarity function based on the one that was present in `embeddings_utils`:

```python
def cosine_similarity(a, b):
    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))
```

### To Reproduce

Cookbook example cannot be converted to use v1.0 without removing the dependency on `embeddings_utils.py` https://cookbook.openai.com/examples/semantic_text_search_using_embeddings

### Code snippets

```Python
from openai.embeddings_utils import get_embedding, cosine_similarity

# search through the reviews for a specific product
def search_reviews(df, product_description, n=3, pprint=True):
    product_embedding = get_embedding(
        product_description,
        engine=""text-embedding-ada-002""
    )
    df[""similarity""] = df.embedding.apply(lambda x: cosine_similarity(x, product_embedding))

    results = (
        df.sort_values(""similarity"", ascending=False)
        .head(n)
        .combined.str.replace(""Title: "", """")
        .str.replace(""; Content:"", "": "")
    )
    if pprint:
        for r in results:
            print(r[:200])
            print()
    return results


results = search_reviews(df, ""delicious beans"", n=3)
```


### OS

Windows

### Python version

Python v3.10.11

### Library version

openai-python==1.0.0rc2 ",closed,2023-11-06T13:09:56Z,2023-11-06T13:55:17Z,[],https://github.com/openai/openai-python/issues/676,0.76,True,missing_dependency_violation,high,"The removal of `embeddings_utils.py` causes existing code that relies on its functions, such as `cosine_similarity`, to fail, indicating a missing dependency.",high,
1583,`_strict_response_validation` deprecation and Azure content filters,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

If `_strict_response_validation` may be deprecated, will the fixed behavior be set to `False`?

The Azure content filter annotation requires `object` attribute set to `""""`. It relies on `_strict_response_validation=False`:

https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/content-filter?tabs=warning%2Cuser-prompt%2Cpython-new#annotations-and-sample-responses

### To Reproduce

1. Enable content filters in Azure OpenAI Service.
2. Stream chat completions with client:

```python
client = AzureOpenAI(
    azure_endpoint=""<azure-endpoint>"",
    azure_deployment=""gpt-4-turbo"",
    api_key=""<api-key>"",
    api_version=""2024-02-01"",
    _strict_response_validation=True # This will make strict validations and the request will fail.
)
```

Error:

```
object: input should be 'chat.completion.chunk' [type=literal_error, input_value='', input_type=str]
```

### Code snippets

_No response_

### OS

Ubuntu 22.04

### Python version

v3.10.0

### Library version

v1.37.1",closed,2024-07-29T09:20:01Z,2024-07-29T09:43:18Z,['question'],https://github.com/openai/openai-python/issues/1583,0.39,True,input_value_violation,high,"The input value for the 'object' attribute is empty, which is unacceptable for the Azure content filter annotation.",high,
1580,add parallel_tool_calls to Runs.create_and_poll,"### Confirm this is a feature request for the Python library and not the underlying OpenAI API.

- [X] This is a feature request for the Python library

### Describe the feature or improvement you're requesting

Currently client.beta.threads.runs.create_and_poll doesn't accept parallel_tool_calls as an argument, and throws:
`TypeError: Runs.create_and_poll() got an unexpected keyword argument 'parallel_tool_calls'`

It can be included and passed to `self.create` here:
https://github.com/openai/openai-python/blob/195c05a64d39c87b2dfdf1eca2d339597f1fce03/src/openai/resources/beta/threads/runs/runs.py#L866C12-L889C10



### Additional context

_No response_",closed,2024-07-26T21:49:42Z,2024-07-29T08:55:09Z,[],https://github.com/openai/openai-python/issues/1580,59.09,False,,,,low,
1334,AttributeError: 'ThreadRunCreated' object has no attribute 'type',"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

From the docs at https://github.com/openai/openai-python/blob/main/helpers.md:


```python
from typing_extensions import override
from openai import AssistantEventHandler, OpenAI
from openai.types.beta.threads import Text, TextDelta
from openai.types.beta.threads.runs import ToolCall, ToolCallDelta

client = OpenAI()  # NOTE: this is an edit to the original code: `client = openai.OpenAI()`

class EventHandler(AssistantEventHandler):
  @override
  def on_text_created(self, text: Text) -> None:
    print(f""\nassistant > "", end="""", flush=True)

  @override
  def on_text_delta(self, delta: TextDelta, snapshot: Text):
    print(delta.value, end="""", flush=True)

  def on_tool_call_created(self, tool_call: ToolCall):
    print(f""\nassistant > {tool_call.type}\n"", flush=True)

  def on_tool_call_delta(self, delta: ToolCallDelta, snapshot: ToolCall):
    if delta.type == 'code_interpreter':
      if delta.code_interpreter.input:
        print(delta.code_interpreter.input, end="""", flush=True)
      if delta.code_interpreter.outputs:
        print(f""\n\noutput >"", flush=True)
        for output in delta.code_interpreter.outputs:
          if output.type == ""logs"":
            print(f""\n{output.logs}"", flush=True)

with client.beta.threads.runs.stream(
  thread_id=thread.id,            # using real thread ID
  assistant_id=assistant.id    # using real assistant ID
) as stream:
    for event in stream:
        if event.type == ""thread.message.delta"" and event.data.delta.content:
            print(event.data.delta.content[0].text)
```

throws the following error:

```
  File ""/workspaces/ChatGPT-Arc/tmp/sandbox.py"", line 108, in main
    stream_run(client, thread, assistant)
  File ""/workspaces/ChatGPT-Arc/tmp/sandbox.py"", line 80, in stream_run
    if event.type == ""thread.message.delta"" and event.data.delta.content:
  File ""/usr/local/lib/python3.9/site-packages/pydantic/main.py"", line 792, in __getattr__
    raise AttributeError(f'{type(self).__name__!r} object has no attribute {item!r}')
AttributeError: 'ThreadRunCreated' object has no attribute 'type'
```

It appears that the docs are out-of-date.

Using ` event.data.object` instead of `event.type` works for me:

```python
        for event in stream:
            if event.data.object == ""thread.run.step.delta"" and event.data.delta.content:
                print(event.data.delta.content[0].text)
```

Note also that  `client = openai.OpenAI()` should be `client = OpenAI()` in the example code.

### To Reproduce

See above

### Code snippets

_No response_

### OS

macOS

### Python version

3.9.19

### Library version

1.21.2",closed,2024-04-18T04:08:56Z,2024-04-18T21:35:59Z,['bug'],https://github.com/openai/openai-python/issues/1334,17.45,True,input_type_violation,high,"The code attempts to access an attribute 'type' on an object 'ThreadRunCreated' which does not exist, indicating a mismatch between expected and actual object structure.",high,
1577,Embeddings,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

Error:

You tried to access openai.Embedding, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.

You can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface.

Alternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`

A detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742 

### To Reproduce

*

### Code snippets

```Python
def get_embeddings(self, chunks):
        embeddings = []
        for chunk in chunks:
            try:
          
                response = openai.Embedding.create(
                    input=chunk  
                    model=""text-embedding-ada-002""
                )
                embeddings.append(response['data'][0]['embedding'])
            except Exception as e:
                print(f""Error: {e}"")
        return embeddings
```


### OS

Windows

### Python version

Python 3.11.5

### Library version

openai 1.37.0",closed,2024-07-23T15:59:58Z,2024-07-24T09:56:58Z,['question'],https://github.com/openai/openai-python/issues/1577,17.95,False,,,,low,
1564,Adding support for customized file name,"### Confirm this is a feature request for the Python library and not the underlying OpenAI API.

- [X] This is a feature request for the Python library

### Describe the feature or improvement you're requesting

On my server I save the file with a different name, and I would like to customize the name of the created file by passing a parameter to the client.files.create method as in the example below. Is this viable?
```py
attachment = client.files.create(file=open(""randomfilename.pdf"", ""rb""), purpose=purpose, filename=""MyCustomFilename.pdf"")
```

### Additional context

_No response_",closed,2024-07-19T15:32:51Z,2024-07-22T10:46:25Z,['question'],https://github.com/openai/openai-python/issues/1564,67.23,False,,,,,
1035,`openai api audio.transcriptions.create` not working when response format is not json,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

If you specify `--response-format` to a format other than json (like `srt` or `vtt`), the `openai api audio.transcriptions.create` command will fail with error

### To Reproduce

1. Install openai python library version v1.6.1
2. Prepare an audio file, such as a recording saying ""hello""
3. Run `OPENAI_API_KEY=<YOUR_API_KEY> openai api audio.transcriptions.create -f 1.m4a --response-format vtt`
4. You can see the error message

```
Traceback (most recent call last):
  File ""/usr/local/bin/openai"", line 8, in <module>
    sys.exit(main())
             ^^^^^^
  File ""/usr/local/lib/python3.11/site-packages/openai/cli/_cli.py"", line 129, in main
    _main()
  File ""/usr/local/lib/python3.11/site-packages/openai/cli/_cli.py"", line 209, in _main
    parsed.func(
  File ""/usr/local/lib/python3.11/site-packages/openai/cli/_api/audio.py"", line 78, in transcribe
    print_model(model)
  File ""/usr/local/lib/python3.11/site-packages/openai/cli/_utils.py"", line 36, in print_model
    sys.stdout.write(model_json(model, indent=2) + ""\n"")
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.11/site-packages/openai/_compat.py"", line 125, in model_json
    return model.model_dump_json(indent=indent)
           ^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'str' object has no attribute 'model_dump_json'
```

I can also reproduce this using `python:3.12` Docker image.

### Code snippets

_No response_

### OS

macOS

### Python version

Python 3.11.6

### Library version

v1.6.1",closed,2024-01-01T21:30:48Z,2024-07-22T10:39:56Z,"['bug', 'good first issue', 'CLI']",https://github.com/openai/openai-python/issues/1035,4861.15,True,missing_option_violation,high,"The API fails when an unsupported response format is specified, indicating that the client did not follow the acceptable options for the response format.",high,
1419,Any Examples of GPT-4o？,"### Confirm this is a feature request for the Python library and not the underlying OpenAI API.

- [X] This is a feature request for the Python library

### Describe the feature or improvement you're requesting

There are currently no multi-modal calling examples for 4o in this lib or documentation.

### Additional context

_No response_",closed,2024-05-13T20:01:22Z,2024-07-22T10:38:59Z,[],https://github.com/openai/openai-python/issues/1419,1670.63,False,,,,,
456,stream completions did not work on async api,"### Describe the bug

According to [How_to_stream_completions](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_stream_completions.ipynb)
```
response = openai.ChatCompletion.create(
    model='gpt-3.5-turbo',
    messages=[
        {'role': 'user', 'content': ""What's 1+1? Answer in one word.""}
    ],
    temperature=0,
    stream=True  
)

for chunk in response:
    print(chunk)
```
we can stream a chat completion via `ChatCompletion.create`
but when I use `ChatCompletion.acreate` ,  it failed.
```
TypeError: 'async_generator' object is not iterable
```

### To Reproduce

```
response = openai.ChatCompletion.acreate(
    model='gpt-3.5-turbo',
    messages=[
        {'role': 'user', 'content': ""What's 1+1? Answer in one word.""}
    ],
    temperature=0,
    stream=True  
)

for chunk in response:
    print(chunk)
```

### Code snippets

```Python
response = openai.ChatCompletion.acreate(
    model='gpt-3.5-turbo',
    messages=[
        {'role': 'user', 'content': ""What's 1+1? Answer in one word.""}
    ],
    temperature=0,
    stream=True  
)

for chunk in response:
    print(chunk)
```
```


### OS

macOS

### Python version

Python 3.11

### Library version

0.27.2",closed,2023-05-19T10:39:31Z,2023-05-19T10:54:01Z,['bug'],https://github.com/openai/openai-python/issues/456,0.24,True,method_order_violation,medium,"The use of 'ChatCompletion.acreate' with 'stream=True' is incorrect as it returns an async generator, which cannot be iterated in the same way as a synchronous response.",high,
1190,Provide a callback whenever retry is triggered,"### Confirm this is a feature request for the Python library and not the underlying OpenAI API.

- [X] This is a feature request for the Python library

### Describe the feature or improvement you're requesting

Is it possible to provide a callback whenever a [retry](https://github.com/openai/openai-python/blob/a7115b5f33acd27326e5f78e19beb0d73bd3268e/src/openai/_base_client.py#L1466) is triggered internally, so that we can know when and how the requests failed?

### Additional context

We want to give our users some insights if some OpenAI requests fail with rate limit error",closed,2024-02-24T00:30:24Z,2024-07-18T04:37:32Z,['enhancement'],https://github.com/openai/openai-python/issues/1190,3484.12,False,,,,,
1556,"Just updated the README.md file to make it easier for people to understand like novices like myself, it explains what things are, what they do.","### Confirm this is a feature request for the Python library and not the underlying OpenAI API.

- [X] This is a feature request for the Python library

### Describe the feature or improvement you're requesting

Enhanced README.md with Teen-Friendly Explanations.

N.B if needed i can complete the readme and have all the internal hyperlinks working to connect to the Quick Definitions section. And to have these links in blue for people to learn.  

Summary
This pull request updates the README.md file, specifically enhancing the Pagination section with additional explanations tailored for younger audiences (around 15 years old). The changes aim to make the documentation more accessible and easier to understand for newcomers to the OpenAI API and programming concepts.
Changes Made

Added emoji-prefixed explanations after each code block in the Pagination section.
Introduced relatable analogies (e.g., books, relay races, encyclopedias) to explain complex concepts.
Maintained the original code examples and structure while improving readability.
Ensured explanations cover both synchronous and asynchronous pagination methods.

Benefits

Improves accessibility of the documentation for younger developers or those new to API concepts.
Provides clear, relatable explanations without sacrificing technical accuracy.
Enhances overall user experience by making complex topics more approachable.

### Additional context

This is 90% finished, but wanted to share before i finished if this is not needed, but love helping out and making the README easier to understand by new users who know nothing (Just like myself!)
[README.md](https://github.com/user-attachments/files/16257841/README.md)

```[tasklist]
### Tasks
```
",closed,2024-07-17T00:38:29Z,2024-07-17T09:47:16Z,[],https://github.com/openai/openai-python/issues/1556,9.15,False,,,,,
1558,`ModuleNotFoundError: No module named 'typing_extensions'` when installed with poetry,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

Poetry incorrectly handles the dependencies of the package. As far as I understand, issues like this most often arise due to non-PEP-compliant dependency specifications. I may be wrong about this and it could be an issue with poetry itself. Thanks.

### To Reproduce

Create empty venv with Python 3.9, create basic pyproject.toml, add `openai` as a dependency using **poetry**, try to import `OpenAI` from `openai`:

```
➜  openai-breaking ls   
➜  openai-breaking python3.9 -m venv venv
➜  openai-breaking source venv/bin/activate
(venv) ➜  openai-breaking poetry init

This command will guide you through creating your pyproject.toml config.

Package name [openai-breaking]:  
Version [0.1.0]:  
Description []:  
Author [Danila Mikhaltsov <dmikhaltsov@ispras.ru>, n to skip]:  
License []:  
Compatible Python versions [^3.9]:  

Would you like to define your main dependencies interactively? (yes/no) [yes] no
Would you like to define your development dependencies interactively? (yes/no) [yes] no
Generated file

[tool.poetry]
name = ""openai-breaking""
version = ""0.1.0""
description = """"
authors = [""Danila Mikhaltsov <dmikhaltsov@ispras.ru>""]
readme = ""README.md""
packages = [{include = ""openai_breaking""}]

[tool.poetry.dependencies]
python = ""^3.9""


[build-system]
requires = [""poetry-core""]
build-backend = ""poetry.core.masonry.api""


Do you confirm generation? (yes/no) [yes] 
(venv) ➜  openai-breaking poetry add openai 
Using version ^1.35.14 for openai

Updating dependencies
Resolving dependencies... (0.1s)

Writing lock file

Package operations: 1 install, 0 updates, 0 removals

  • Installing openai (1.35.14)
(venv) ➜  openai-breaking python 
Python 3.9.17 (main, Jun  6 2023, 20:11:21) 
[GCC 11.3.0] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> from openai import OpenAI
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/dmikhaltsov_/code/tmp/openai-breaking/venv/lib/python3.9/site-packages/openai/__init__.py"", line 6, in <module>
    from typing_extensions import override
ModuleNotFoundError: No module named 'typing_extensions'
>>> 
```

However, when installing with pip, all is fine:

```
➜  openai-breaking python3.9 -m venv venv
➜  openai-breaking source venv/bin/activate
(venv) ➜  openai-breaking pip install openai
Collecting openai
  Downloading openai-1.35.14-py3-none-any.whl (328 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 328.5/328.5 kB 4.2 MB/s eta 0:00:00
Collecting sniffio
  Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)
Collecting anyio<5,>=3.5.0
  Using cached anyio-4.4.0-py3-none-any.whl (86 kB)
Collecting httpx<1,>=0.23.0
  Using cached httpx-0.27.0-py3-none-any.whl (75 kB)
Collecting distro<2,>=1.7.0
  Downloading distro-1.9.0-py3-none-any.whl (20 kB)
Collecting typing-extensions<5,>=4.7
  Using cached typing_extensions-4.12.2-py3-none-any.whl (37 kB)
Collecting tqdm>4
  Using cached tqdm-4.66.4-py3-none-any.whl (78 kB)
Collecting pydantic<3,>=1.9.0
  Downloading pydantic-2.8.2-py3-none-any.whl (423 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 423.9/423.9 kB 15.7 MB/s eta 0:00:00
Collecting idna>=2.8
  Using cached idna-3.7-py3-none-any.whl (66 kB)
Collecting exceptiongroup>=1.0.2
  Downloading exceptiongroup-1.2.2-py3-none-any.whl (16 kB)
Collecting httpcore==1.*
  Using cached httpcore-1.0.5-py3-none-any.whl (77 kB)
Collecting certifi
  Downloading certifi-2024.7.4-py3-none-any.whl (162 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 163.0/163.0 kB 31.4 MB/s eta 0:00:00
Collecting h11<0.15,>=0.13
  Using cached h11-0.14.0-py3-none-any.whl (58 kB)
Collecting annotated-types>=0.4.0
  Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)
Collecting pydantic-core==2.20.1
  Downloading pydantic_core-2.20.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.1/2.1 MB 51.6 MB/s eta 0:00:00
Installing collected packages: typing-extensions, tqdm, sniffio, idna, h11, exceptiongroup, distro, certifi, annotated-types, pydantic-core, httpcore, anyio, pydantic, httpx, openai
Successfully installed annotated-types-0.7.0 anyio-4.4.0 certifi-2024.7.4 distro-1.9.0 exceptiongroup-1.2.2 h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 idna-3.7 openai-1.35.14 pydantic-2.8.2 pydantic-core-2.20.1 sniffio-1.3.1 tqdm-4.66.4 typing-extensions-4.12.2

[notice] A new release of pip is available: 23.0.1 -> 24.1.2
[notice] To update, run: pip install --upgrade pip
(venv) ➜  openai-breaking python
Python 3.9.17 (main, Jun  6 2023, 20:11:21) 
[GCC 11.3.0] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> from openai import OpenAI
>>> 
```

### Code snippets

_No response_

### OS

Linux

### Python version

3.9.17

### Library version

1.35.14",closed,2024-07-17T09:09:21Z,2024-07-17T09:53:45Z,['bug'],https://github.com/openai/openai-python/issues/1558,0.74,True,missing_dependency_violation,high,"The 'openai' package is missing a dependency on 'typing_extensions', which is required for its functionality, leading to a ModuleNotFoundError when using poetry for installation.",high,
1551,Validate Function Specifications ,"### Confirm this is a feature request for the Python library and not the underlying OpenAI API.

- [X] This is a feature request for the Python library

### Describe the feature or improvement you're requesting

It would be nice to have a warning or error message when incorrect function specification is passed while using the Function Calling.

Example:

Right Usage:
```python
tools = [
    {
        ""type"": ""function"",
        ""function"": {
            ""name"": ""get_current_weather"",
            ""description"": ""Get the current weather in a given location"",
            ""parameters"": {
                ""type"": ""object"",
                ""properties"": {
                    ""location"": {
                        ""type"": ""string"",
                        ""description"": ""The city and state, e.g. San Francisco, CA"",
                    },
                    ""unit"": {""type"": ""string"", ""enum"": [""celsius"", ""fahrenheit""]},
                },
                ""required"": [""location""],
            },
        },
    }
]
```
    
Wrong Usage:
```python
tools = [
    {
        ""type"": ""function"",
        ""function"": {
            ""name"": ""get_current_weather"",
            ""description"": ""Get the current weather in a given location"",
            ""input_schema"": {
                ""type"": ""object"",
                ""properties"": {
                    ""location"": {
                        ""type"": ""string"",
                        ""description"": ""The city and state, e.g. San Francisco, CA"",
                    },
                    ""unit"": {""type"": ""string"", ""enum"": [""celsius"", ""fahrenheit""]},
                },
                ""required"": [""location""],
            },
        },
    }
]
 ```

While in this case the model generalizes and passes the correct arguments, for a different function, the model passes empty arguments.

Example:
```python
tools = [
    {
        ""type"": ""function"",
        ""function"": {
            ""name"": ""sematic_search"",
            ""description"": ""Use this function to retrieve a list of the most relevant and semantically similar answers to the user's question."",
            ""input_schema"": {
                ""type"": ""object"",
                ""properties"": {
                    ""user_question"": {
                        ""type"": ""string"",
                        ""description"": ""The user_question for which sematically similar answers need to be retrieved. This parameter is mandatory and should be a meaningful question or phrase."",
                    }
                },
                ""required"": [""user_question""],
            },
            ""output_schema"": {
                ""type"": ""string"",
                ""description"": ""The retrieved semantically similar answers to user question"",
            },
        },
    }
]
```

I understand that LLMs are non deterministic in nature, but we can programmatically make consuming LLMs more deterministic. 

This kind of functionality exists already when using the JSON mode, even the prompt is scrutinized here.
`'messages' must contain the word 'json' in some form, to use 'response_format' of type 'json_object'`

Thus, it is even more critical to verify the function specification to ensure some kind of predictable behavior and helpful error messages. 

### Additional context

_No response_",closed,2024-07-16T10:57:03Z,2024-07-16T11:07:06Z,[],https://github.com/openai/openai-python/issues/1551,0.17,True,input_type_violation,high,Incorrect function specification passed (input_schema instead of parameters) leading to potential misinterpretation of function arguments.,high,
1545,Something went wrong during completion 2. Reason: Message text is empty,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

Something went wrong during completion 2. Reason: Message text is empty

### To Reproduce

chatgpt_telegram_bot  | 2024-07-14 21:40:09,080 - DEBUG - httpcore.http11 - response_closed.complete
chatgpt_telegram_bot  | 2024-07-14 21:40:09,080 - DEBUG - openai_utils - Pre-processed answer: 
chatgpt_telegram_bot  | 2024-07-14 21:40:09,080 - DEBUG - openai_utils - Post-processed answer: 
chatgpt_telegram_bot  | 2024-07-14 21:40:09,081 - ERROR - openai_utils - Message text is empty
chatgpt_telegram_bot  | 2024-07-14 21:40:09,081 - ERROR - openai_utils - Exception: Message text is empty
chatgpt_telegram_bot  | 2024-07-14 21:40:09,082 - ERROR - __main__ - Something went wrong during completion 2. Reason: Message text is empty
chatgpt_telegram_bot  | 2024-07-14 21:40:09,082 - DEBUG - telegram.ext.ExtBot - Passing request through rate limiter of type <class 'telegram.ext._aioratelimiter.AIORateLimiter'> with rate_limit_args None
chatgpt_telegram_bot  | 2024-07-14 21:40:09,083 - DEBUG - telegram.ext.ExtBot - Calling Bot API endpoint `sendMessage` with parameters `{'chat_id': 5212252839, 'text': 'Something went wrong during completion 2. Reason: Message text is empty'}`

### Code snippets

```Python
class ChatGPT:
    def __init__(self, model=""gpt-4-1106-preview""):
        assert model in {
            ""text-davinci-003"", ""gpt-3.5-turbo-16k"", ""gpt-3.5-turbo"", ""gpt-4"", ""gpt-4-1106-preview"", 
            ""gpt-4-vision-preview"", ""gpt-4-turbo-2024-04-09"", ""gpt-4o""
        }, f""Unknown model: {model}""
        self.model = model
        self.logger = logging.getLogger(__name__)
        self.headers = {
            ""Authorization"": f""Bearer {config.openai_api_key}"",
            ""Content-Type"": ""application/json"",
        }
        self.client = global_client

    async def send_message(self, message, dialog_messages=[], chat_mode=""assistant""):
        if chat_mode not in config.chat_modes.keys():
            raise ValueError(f""Chat mode {chat_mode} is not supported"")

        n_dialog_messages_before = len(dialog_messages)
        answer = None
        n_input_tokens, n_output_tokens = 0, 0
        n_first_dialog_messages_removed = 0
        while answer is None:
            try:
                if self.model in {""gpt-3.5-turbo-16k"", ""gpt-3.5-turbo"", ""gpt-4"", ""gpt-4-1106-preview"", ""gpt-4-vision-preview"", ""gpt-4-turbo-2024-04-09"", ""gpt-4o""}:
                    messages = self._generate_prompt_messages(message, dialog_messages, chat_mode)
                    self.logger.debug(f""Generated messages: {messages}"")
                    validate_payload({
                        ""model"": self.model,
                        ""messages"": messages,
                        **OPENAI_COMPLETION_OPTIONS
                    })
                    r = await self.client.chat.completions.create(
                        model=self.model,
                        messages=messages,
                        **OPENAI_COMPLETION_OPTIONS
                    )
                    self.logger.debug(f""OpenAI API response: {r}"")
                    if not r.choices or not r.choices[0].message or not r.choices[0].message.content:
                        self.logger.error(""Received empty message content from OpenAI API."")
                        raise ValueError(""Received empty message content from OpenAI API."")
                    answer = r.choices[0].message.content
                elif self.model == ""text-davinci-003"":
                    prompt = self._generate_prompt(message, dialog_messages, chat_mode)
                    self.logger.debug(f""Generated prompt: {prompt}"")
                    validate_payload({
                        ""model"": self.model,
                        ""prompt"": prompt,
                        **OPENAI_COMPLETION_OPTIONS
                    })
                    r = await self.client.completions.create(
                        model=self.model,
                        prompt=prompt,
                        **OPENAI_COMPLETION_OPTIONS
                    )
                    self.logger.debug(f""OpenAI API response: {r}"")
                    if not r.choices or not r.choices[0].text:
                        self.logger.error(""Received empty message content from OpenAI API."")
                        raise ValueError(""Received empty message content from OpenAI API."")
                    answer = r.choices[0].text
                else:
                    raise ValueError(f""Unknown model: {self.model}"")

                answer = self._postprocess_answer(answer)
                if not answer.strip():
                    self.logger.error(""Post-processed answer is empty"")
                    raise ValueError(""Post-processed answer is empty"")
                n_input_tokens, n_output_tokens = r.usage.prompt_tokens, r.usage.completion_tokens
            except Exception as e:
                self.logger.error(f""Exception: {str(e)}"")
                if len(dialog_messages) == 0:
                    raise ValueError(""Dialog messages is reduced to zero, but still has too many tokens to make completion"") from e

                dialog_messages = dialog_messages[1:]

        n_first_dialog_messages_removed = n_dialog_messages_before - len(dialog_messages)

        return answer, (n_input_tokens, n_output_tokens), n_first_dialog_messages_removed

    async def send_message_stream(self, message, dialog_messages=[], chat_mode=""assistant""):
        if chat_mode not in config.chat_modes.keys():
            raise ValueError(f""Chat mode {chat_mode} is not supported"")

        n_dialog_messages_before = len(dialog_messages)
        answer = None
        n_input_tokens, n_output_tokens, n_first_dialog_messages_removed = 0, 0, 0
        while answer is None:
            try:
                if self.model in {""gpt-3.5-turbo-16k"", ""gpt-3.5-turbo"", ""gpt-4"", ""gpt-4-1106-preview"", ""gpt-4-turbo-2024-04-09"", ""gpt-4o""}:
                    messages = self._generate_prompt_messages(message, dialog_messages, chat_mode)
                    self.logger.debug(f""Generated messages: {messages}"")
                    
                    r_gen = await self.client.chat.completions.create(
                        model=self.model,
                        messages=messages,
                        stream=True,
                        **OPENAI_COMPLETION_OPTIONS
                    )

                    answer = """"
                    async for r_item in r_gen:
                        delta = r_item.choices[0].delta

                        if ""content"" in delta:
                            answer += delta.content
                            n_input_tokens, n_output_tokens = self._count_tokens_from_messages(messages, answer, model=self.model)
                            n_first_dialog_messages_removed = 0

                            yield ""not_finished"", answer, (n_input_tokens, n_output_tokens), n_first_dialog_messages_removed
                                
                elif self.model == ""text-davinci-003"":
                    prompt = self._generate_prompt(message, dialog_messages, chat_mode)
                    self.logger.debug(f""Generated prompt: {prompt}"")
                    r_gen = self.client.completions.create(
                        model=self.model,
                        prompt=prompt,
                        stream=True,
                        **OPENAI_COMPLETION_OPTIONS
                    )

                    answer = """"
                    async for r_item in r_gen:
                        if not r_item.choices or not r_item.choices[0].text:
                            self.logger.error(""Received empty message content from OpenAI API stream."")
                            raise ValueError(""Received empty message content from OpenAI API stream."")
                        answer += r_item.choices[0].text
                        n_input_tokens, n_output_tokens = self._count_tokens_from_prompt(prompt, answer, model=self.model)
                        n_first_dialog_messages_removed = n_dialog_messages_before - len(dialog_messages)
                        yield ""not_finished"", answer, (n_input_tokens, n_output_tokens), n_first_dialog_messages_removed

                answer = self._postprocess_answer(answer)
                if not answer.strip():
                    self.logger.error(""Message text is empty"")
                    raise ValueError(""Message text is empty"")

            except Exception as e:
                self.logger.error(f""Exception: {str(e)}"")
                if len(dialog_messages) == 0:
                    raise e

                dialog_messages = dialog_messages[1:]
                n_first_dialog_messages_removed = n_dialog_messages_before - len(dialog_messages)

        yield ""finished"", answer, (n_input_tokens, n_output_tokens), n_first_dialog_messages_removed

    async def send_vision_message(
        self,
        message,
        dialog_messages=[],
        chat_mode=""assistant"",
        image_buffer: BytesIO = None,
    ):
        n_dialog_messages_before = len(dialog_messages)
        answer = None
        n_input_tokens, n_output_tokens = 0, 0
        n_first_dialog_messages_removed = 0
        while answer is None:
            try:
                if self.model == ""gpt-4-vision-preview"":
                    messages = self._generate_prompt_messages(
                        message, dialog_messages, chat_mode, image_buffer
                    )
                    self.logger.debug(f""Generated messages: {messages}"")
                    r = await self.client.chat.completions.create(
                        model=self.model,
                        messages=messages,
                        **OPENAI_COMPLETION_OPTIONS
                    )
                    self.logger.debug(f""OpenAI API response: {r}"")
                    if not r.choices or not r.choices[0].message or not r.choices[0].message.content:
                        self.logger.error(""Received empty message content from OpenAI API."")
                        raise ValueError(""Received empty message content from OpenAI API."")
                    answer = r.choices[0].message.content
                else:
                    raise ValueError(f""Unsupported model: {self.model}"")

                answer = self._postprocess_answer(answer)
                if not answer.strip():
                    self.logger.error(""Message text is empty"")
                    raise ValueError(""Message text is empty"")
                n_input_tokens, n_output_tokens = (
                    r.usage.prompt_tokens,
                    r.usage.completion_tokens,
                )
            except Exception as e:
                self.logger.error(f""Exception: {str(e)}"")
                if len(dialog_messages) == 0:
                    raise ValueError(
                        ""Dialog messages is reduced to zero, but still has too many tokens to make completion""
                    ) from e

                dialog_messages = dialog_messages[1:]

            n_first_dialog_messages_removed = n_dialog_messages_before - len(dialog_messages)

        return (
            answer,
            (n_input_tokens, n_output_tokens),
            n_first_dialog_messages_removed,
        )

    async def send_vision_message_stream(
        self,
        message,
        dialog_messages=[],
        chat_mode=""assistant"",
        image_buffer: BytesIO = None,
    ):
        n_dialog_messages_before = len(dialog_messages)
        answer = None
        n_input_tokens, n_output_tokens = 0, 0
        n_first_dialog_messages_removed = 0
        while answer is None:
            try:
                if self.model == ""gpt-4-vision-preview"":
                    messages = self._generate_prompt_messages(
                        message, dialog_messages, chat_mode, image_buffer
                    )
                    self.logger.debug(f""Generated messages: {messages}"")
                    
                    r_gen = await self.client.chat.completions.create(
                        model=self.model,
                        messages=messages,
                        stream=True,
                        **OPENAI_COMPLETION_OPTIONS,
                    )

                    answer = """"
                    async for r_item in r_gen:
                        delta = r_item.choices[0].delta
                        if ""content"" in delta:
                            answer += delta.content
                            (
                                n_input_tokens,
                                n_output_tokens,
                            ) = self._count_tokens_from_messages(
                                messages, answer, model=self.model
                            )
                            n_first_dialog_messages_removed = (
                                n_dialog_messages_before - len(dialog_messages)
                            )
                            yield ""not_finished"", answer, (
                                n_input_tokens,
                                n_output_tokens,
                            ), n_first_dialog_messages_removed

                answer = self._postprocess_answer(answer)
                if not answer.strip():
                    self.logger.error(""Message text is empty"")
                    raise ValueError(""Message text is empty"")

            except Exception as e:
                self.logger.error(f""Exception: {str(e)}"")
                if len(dialog_messages) == 0:
                    raise e
                dialog_messages = dialog_messages[1:]

        yield ""finished"", answer, (
            n_input_tokens,
            n_output_tokens,
        ), n_first_dialog_messages_removed

    def _generate_prompt(self, message, dialog_messages, chat_mode):
        prompt = config.chat_modes[chat_mode][""prompt_start""]
        prompt += ""\n\n""

        if len(dialog_messages) > 0:
            prompt += ""Chat:\n""
            for dialog_message in dialog_messages:
                prompt += f""User: {dialog_message['user']}\n""
                prompt += f""Assistant: {dialog_message['bot']}\n""

        prompt += f""User: {message}\n""
        prompt += ""Assistant: ""

        return prompt

    def _encode_image(self, image_buffer: BytesIO) -> bytes:
        return base64.b64encode(image_buffer.read()).decode(""utf-8"")

    def _generate_prompt_messages(self, message, dialog_messages, chat_mode, image_buffer: BytesIO = None):
        prompt = config.chat_modes[chat_mode][""prompt_start""]

        messages = [{""role"": ""system"", ""content"": prompt}]

        for dialog_message in dialog_messages:
            messages.append({""role"": ""user"", ""content"": dialog_message[""user""]})
            messages.append({""role"": ""assistant"", ""content"": dialog_message[""bot""]})

        if image_buffer is not None:
            messages.append(
                {
                    ""role"": ""user"", 
                    ""content"": [
                        {
                            ""type"": ""text"",
                            ""text"": message,
                        },
                        {
                            ""type"": ""image"",
                            ""image"": self._encode_image(image_buffer),
                        }
                    ]
                }
                
            )
        else:
            messages.append({""role"": ""user"", ""content"": message})

        return messages

    def _postprocess_answer(self, answer):
        self.logger.debug(f""Pre-processed answer: {answer}"")
        answer = answer.strip()
        self.logger.debug(f""Post-processed answer: {answer}"")
        return answer

    def _count_tokens_from_messages(self, messages, answer, model=""gpt-4-1106-preview""):
        encoding = tiktoken.encoding_for_model(model)

        tokens_per_message = 3
        tokens_per_name = 1

        if model.startswith(""gpt-3""):
            tokens_per_message = 4
            tokens_per_name = -1
        elif model.startswith(""gpt-4""):
            tokens_per_message = 3
            tokens_per_name = 1 
        else:
            raise ValueError(f""Unknown model: {model}"")

        n_input_tokens = 0
        for message in messages:
            n_input_tokens += tokens_per_message
            if isinstance(message[""content""], list):
                for sub_message in message[""content""]:
                    if ""type"" in sub_message:
                        if sub_message[""type""] == ""text"":
                            n_input_tokens += len(encoding.encode(sub_message[""text""]))
                        elif sub_message[""type""] == ""image_url"":
                            pass
            else:
                if ""type"" in message:
                    if message[""type""] == ""text"":
                        n_input_tokens += len(encoding.encode(message[""text""]))
                    elif message[""type""] == ""image_url"":
                        pass

        n_input_tokens += 2

        n_output_tokens = 1 + len(encoding.encode(answer))

        return n_input_tokens, n_output_tokens

    def _count_tokens_from_prompt(self, prompt, answer, model=""text-davinci-003""):
        encoding = tiktoken.encoding_for_model(model)

        n_input_tokens = len(encoding.encode(prompt)) + 1
        n_output_tokens = len(encoding.encode(answer))

        return n_input_tokens, n_output_tokens
    
async def transcribe_audio(audio_file) -> str:
    r = await global_client.audio.transcriptions.create(
        model=""whisper-1"",
        file=audio_file
    )
    return r.text or """"


async def generate_images(prompt, model=""dall-e-2"", n_images=4, size=""1024x1024"", quality=""standard""):
    if model==""dalle-2"":
        model=""dall-e-2""
        quality=""standard""

    if model==""dalle-3"":
        model=""dall-e-3""
        n_images=1

    response = await global_client.images.generate(
        model=model,
        prompt=prompt,
        n=n_images,
        size=size,
        quality=quality
    )

    image_urls = [item.url for item in response.data]
    return image_urls


async def is_content_acceptable(prompt):
    r = await global_client.moderations.create(input=prompt)
    return not all(r.results[0].categories.values())
```


### OS

Debian 12.0

### Python version

Python 3.8

### Library version

openAI 1.35",closed,2024-07-14T21:48:53Z,2024-07-15T09:46:53Z,[],https://github.com/openai/openai-python/issues/1545,11.97,True,input_value_violation,High,"The API received an empty message text, which is unacceptable input for generating a response.",high,
952,Make the OpenAI API model names and info available under an enum,"### Confirm this is a feature request for the Python library and not the underlying OpenAI API.

- [X] This is a feature request for the Python library

### Describe the feature or improvement you're requesting

It would greatly help to have the names and several fields of interest (like max tokens) of the available OpenAI models under some sort of enum.

This would greatly help with the model validation and accessing the fields of interest. For now, if we want a list of available models, we need to declare it ourselves (see code below).

```PY
from dataclasses import dataclass


@dataclass
class ChatGPTModel:
    name: str
    max_tokens: int


CHAT_GPT_MODELS = {
    ""gpt-3.5-turbo"": ChatGPTModel(name=""gpt-3.5-turbo"", max_tokens=4000),
    ""gpt-4"": ChatGPTModel(name=""gpt-4"", max_tokens=8000),
}

```


### Additional context

If needed, I can add this change myself (however some guidance would be necessary).",closed,2023-12-08T12:00:32Z,2024-07-12T13:14:05Z,['enhancement'],https://github.com/openai/openai-python/issues/952,5209.23,False,,,,,
1246,Memory leak,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

I am using the AsyncAzureOpenAI class to instantiate a client and using a stream call to client.chat.completions.create. Even after performing close() on both client and response within a try-finally block, I am still encountering a memory leak that eventually leads to server crash.
I tried the solution outlined in https://github.com/openai/openai-python/issues/1181, where the pydantic package was upgraded to 2.6.3, but this hasn't resolved my issue.
I noticed using the gc library that memory usage increases after each call to this service. Our service is used for centralized management of AzureOpenAI accounts, hence a client is instantiated for every incoming request. Given the concurrent nature of this service, I'm wondering if client.with_options can support concurrent usage. Do you have any good solutions to address this memory leak issue?

### To Reproduce

Several calls in a row, for example, to embeddings that are wrapped with asynс.


### Code snippets

```Python
class LlmStreamApiHandler(tornado.web.RequestHandler):
    executor = ThreadPoolExecutor(200)

    def __init__(self, *args, **kwargs):
        super(LlmStreamApiHandler, self).__init__(*args, **kwargs)
        self.set_header('Content-Type', 'text/event-stream')
        self.set_header('Access-Control-Allow-Origin', ""*"")
        self.set_header(""Access-Control-Allow-Headers"", ""*"")
        self.set_header(""Access-Control-Allow-Methods"", ""*"")

    def on_finish(self):
        return super().on_finish()


    async def post(self):

        try:
            result = await self.process(...)
        except Exception as e:
            ...

        self.write(json.dumps(result) + ""\n"")
        await self.flush()


    async def process(self, ...)
        client = openai.AsyncAzureOpenAI(
            api_version=api_version,
            api_key=api_key,
            azure_endpoint=azure_endpoint,
            http_client=httpx.AsyncClient(
                proxies=config.api_proxy,
            ),
            max_retries=0
        )
        response_text = False
        try:

            response_text = await client.chat.completions.create(**prompt)
            async for chunk in response_text:
                chunk = chunk.model_dump()
                if chunk['choices'] == [] and chunk['id'] == """" and chunk['model'] == """" and chunk['object'] == """":
                    continue
                chunk_message = chunk['choices'][0]['delta']
                current_text = chunk_message.get('content', '')
                if bool(chunk_message) and current_text:
                    ...
                elif chunk['choices'][0][""finish_reason""] == ""stop"":
                    break

                elif current_text == '' and chunk_message.get('role', '') == ""assistant"":
                    ...
                elif chunk['choices'][0][""finish_reason""] == ""content_filter"":
                    ...
                else:
                    continue
                self.write(json.dumps(json_data) + ""\n"")
                await self.flush()
        except Exception as e:
            ...
            raise ...
        finally:
            if response_text:
                await response_text.close()
            await client.close()
        return ...
```


### OS

CentOS

### Python version

Python 3.8

### Library version

openai v1.12.0",closed,2024-03-19T09:22:16Z,2024-07-12T13:01:16Z,['bug'],https://github.com/openai/openai-python/issues/1246,2763.65,True,memory_out_of_bound,High,"The memory leak issue indicates that the system is exceeding its allocated memory, leading to server crashes.",high,
1490,"Python code defaults to 'base64' encoding_format in the embeddings endpoint, but docs say it should be 'float'","### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

Hi, your docs say that the default `encoding_format` in your [embeddings/create endpoint is `float`](https://platform.openai.com/docs/api-reference/embeddings/create#embeddings-create-encoding_format), however your [python package code applies a default of 'base64' when a value isn't supplied](https://github.com/openai/openai-python/blob/main/src/openai/resources/embeddings.py#L94-L95).

Can you switch this to 'float' so the python package is conformant with your docs? 

### To Reproduce

Run an OpenAI embeddings query with the python client:
```
import openai
client = openai.OpenAI(...)
response = client.embeddings.create(
   model=<model>,
   input=""I am a string to embed!"",
   # notably, NOT specifying encoding_format here
) 
 ```
 Uses base64 encoding instead of float, oddly.

### Code snippets

```Python
NA
```


### OS

macOS

### Python version

3.12.3

### Library version

1.35.0",closed,2024-06-19T21:04:42Z,2024-07-12T13:00:13Z,[],https://github.com/openai/openai-python/issues/1490,543.93,True,missing_option_violation,high,"The Python library defaults to 'base64' for encoding_format instead of the documented 'float', leading to a mismatch between implementation and documentation.",high,
751,ImportError: cannot import name 'override' from 'typing_extensions' with openai==1.2.0,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

ImportError                               Traceback (most recent call last)
File <command-2965749824381288>, line 4
      2 from langchain.embeddings import OpenAIEmbeddings, AzureOpenAIEmbeddings
      3 #import openai
----> 4 from openai import AzureOpenAI 
      5 def utils_embedding(input_chunk, OPENAI_API_KEY, deployment_str = ""xxxxx, api_base = ""xxxx""):
      7     OPENAI_API_KEY = XXX)

File /local_disk0/.ephemeral_nfs/envs/pythonEnv-853491b7-ec2b-411d-9c68-bdb8f2c9309a/lib/python3.10/site-packages/openai/__init__.py:6
      3 from __future__ import annotations
      5 import os as _os
----> 6 from typing_extensions import override
      8 from . import types
      9 from ._types import NoneType, Transport, ProxiesTypes

ImportError: cannot import name 'override' from 'typing_extensions' (/databricks/python/lib/python3.10/site-packages/typing_extensions.py)

### To Reproduce

import openai

### Code snippets

_No response_

### OS

windows

### Python version

python v3.10.12

### Library version

openai v1.2.0",closed,2023-11-09T14:13:07Z,2023-11-10T12:34:53Z,[],https://github.com/openai/openai-python/issues/751,22.36,True,input_type_violation,high,"The code attempts to import a name 'override' from 'typing_extensions' that does not exist, indicating a type mismatch or incorrect usage of the library.",high,
1432,Does it support multi key polling？,"### Confirm this is a feature request for the Python library and not the underlying OpenAI API.

- [X] This is a feature request for the Python library

### Describe the feature or improvement you're requesting

Does it support multi key polling？

### Additional context

Does it support multi key polling？",closed,2024-05-21T08:40:17Z,2024-07-12T00:32:12Z,[],https://github.com/openai/openai-python/issues/1432,1239.87,True,missing_option_violation,unknown,"The request for multi key polling lacks clarity on the acceptable options or configurations, which could lead to suboptimal implementation.",medium,
1540,Bug: ImportError: cannot import name ‘ThreadMessage’ from ‘openai.types.beta.threads’,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

After updating openai via conda i got the above error message:

ImportError: cannot import name ‘ThreadMessage’ from ‘openai.types.beta.threads’

It is also reported here:

https://community.openai.com/t/importerror-cannot-import-name-threadmessage-from-openai-types-beta-threads/728654

After trying to find the bug, i think it is related to a wrong import statement in:

\Lib\site-packages\openai\resources\beta\threads\messages\messages.py

were for example in the case of  message_list_params the module is imported from :

\Lib\site-packages\openai\types\beta\threads\message_list_params.py


There is however no module in:

\Lib\site-packages\openai\types\beta\threads\ThreadMessage.py

instead there is the module 
\Lib\site-packages\openai\types\beta\threads\thread_message.py

which contains the class ThreadMessage.

Either the import in \Lib\site-packages\openai\resources\beta\threads\messages\messages.py is changed to :

#from .....types.beta.threads import ThreadMessage, message_list_params, message_create_params, message_update_params
from .....types.beta.threads import message_list_params, message_create_params, message_update_params
from .....types.beta.threads.thread_message import ThreadMessage



or which would be more consistent, the import is changed to:

from .....types.beta.threads import thread_message, message_list_params, message_create_params, message_update_params


and every occurance of:

ThreadMessage


 in the code is changed to


thread_message.ThreadMessage

### To Reproduce

update to newest openai version via conda and use:

from openai import AzureOpenAI

### Code snippets

_No response_

### OS

windows

### Python version

3.10

### Library version

1.13.3",closed,2024-07-10T17:33:56Z,2024-07-10T18:11:22Z,['bug'],https://github.com/openai/openai-python/issues/1540,0.62,True,input_type_violation,high,"The import statement is attempting to import a class that does not exist under the expected module name, leading to an ImportError.",high,
1412,Parametr name mismatch.,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

Parametr name mismatch in method create_and_run_stream parametr `tool_resources` when call  post request body data named tool.
And result :
openai.BadRequestError: Error code: 400 - {'error': {'message': ""Unknown parameter: 'tool'."", 'type': 'invalid_request_error', 'param': 'tool', 'code': 'unknown_parameter'}}
![image](https://github.com/openai/openai-python/assets/40039970/c99b6a77-d749-4257-b3e4-f283d880512c)


### To Reproduce

Call method client.beta.threads.create_and_run() with filled tool_resources param.

### Code snippets

_No response_

### OS

Windows

### Python version

python v3.10

### Library version

openai v1.28.1",closed,2024-05-12T19:01:27Z,2024-07-10T09:57:30Z,['bug'],https://github.com/openai/openai-python/issues/1412,1406.93,True,input_value_violation,high,"The parameter name 'tool' does not match the expected parameter 'tool_resources', leading to a BadRequestError.",high,
1526,AzureOpenAI authentication issue: new token from azure_ad_token_provider not utilized after expiration,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

When the initial token expires during a series of retries due to multiple failures, the `azure_ad_token_provider` generates a new token as expected. However, this new token is not utilized by `AzureOpenAI` for the remaining retries.

The problem originates from the `openai/lib/azure.py` -> `_prepare_options` function, where the `azure_ad_token` is only set to `headers[""Authorization""]` during initialization and not when `azure_ad_token` is changed.
See `if headers.get(""Authorization"") is None` below:

```
    def _prepare_options(self, options: FinalRequestOptions) -> None:
        headers: dict[str, str | Omit] = {**options.headers} if is_given(options.headers) else {}
        options.headers = headers

        azure_ad_token = self._get_azure_ad_token()
        if azure_ad_token is not None:
            if headers.get(""Authorization"") is None:
                headers[""Authorization""] = f""Bearer {azure_ad_token}""
```



### To Reproduce

The `azure.identity.ClientSecretCredential.get_token` function was utilized as the `azure_ad_token_provider`, and the `AzureOpenAI.max_retries` was set to `1000`. Due to numerous failures and retries, the process exceeded the 60-minute validity period of the token. It is important to note that `Langchain` is being used, rather than directly interfacing with `AzureOpenAI`.

### Code snippets

_No response_

### OS

SuSE12

### Python version

Python 3.9.6

### Library version

openai v1.27.0",closed,2024-07-04T18:35:06Z,2024-07-09T18:04:44Z,['bug'],https://github.com/openai/openai-python/issues/1526,119.49,True,missing_dependency_violation,high,"The new token generated by azure_ad_token_provider is not utilized, leading to authentication failures during retries.",high,
1525,Send input below model and other parameters in json bodies of HTTP requests,"### Confirm this is a feature request for the Python library and not the underlying OpenAI API.

- [X] This is a feature request for the Python library

### Describe the feature or improvement you're requesting

When I send a request to an OpenAI compatible endpoint using the following code:
```python
import openai
client = openai.OpenAI(
	base_url=""https://base.tld/v1"",
	api_key=""xxx"",
)
embedding = client.embeddings.create(
	model=""e5-mistral-7b-instruct"",
	input=""San Francisco"",
)
print(embedding)
```
It sends an HTTP request with the following body, where the input always comes before the model.
```json
{""input"": ""San Francisco"", ""model"": ""e5-mistral-7b-instruct""}
```
I think it would be better to always put the `input` last, after the `model` so that implementers of the OpenAI API can branch earlier in processing a request based on `model`. This is only a problem for large (eg. >8KB) inputs, which do not fit into typical web server buffers anymore.
That is, the body should look like this instead:
```json
{""model"": ""e5-mistral-7b-instruct"", ""input"": ""San Francisco""}
```

At $WORK we offer an OpenAI compatible API and route based on the model parameter, so we have to buffer the whole `input` before the request can be forwarded to the correct upstream server hosting that model. This routing only works for inputs that don't exceed our buffer size, meaning we can only process smaller requests.
If the model came before input instead, it would be possible to parse the model and route based off it with only a small buffer size, even for inputs that far exceed the buffer size.

An added benefit of the proposed parameter order would be slightly improved latency, because the upstream can be decided earlier and the input is starting to get sent sooner.

I imagine we are not the only ones branching based on `model`, so this change would improve latency and allow for bigger inputs across the whole ecosystem.

I am honestly not sure if this can be changed in this repo since the code is generated. Maybe changing the order in the API spec:
https://github.com/openai/openai-openapi/blob/0df12d7b3ac997517fda38842f41d99a8e7f4e6a/openapi.yaml#L8873
would be the easiest way to fix it, although the OpenAPI spec is not really *wrong* since the API is oblivious to the order.

The issue exists for other endpoints such as `/chat/completions`, where messages comes before model. I encountered it while creating `/embeddings` for many inputs.


### Additional context

_No response_",closed,2024-07-04T13:38:23Z,2024-07-06T22:05:36Z,[],https://github.com/openai/openai-python/issues/1525,56.45,True,method_order_violation,medium,"The order of parameters in the API request body does not follow the optimal sequence for processing, which could lead to performance degradation due to increased buffering requirements.",high,
1529,There is no way to pass proxy to `AsyncOpenAI`,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

The parent of `AsyncOpenAI` which is `AsyncAPIClient` Accepts a parameter called `proxies`. However, there is no way to pass `proxies` when constructing `AsyncOpenAI`.  This makes it **tough** to use the library **correctly**.

**Clarification**
It's not ideal to pass an HTTP client at all. `AsyncAPIClient` constructs a special client called `AsyncHttpxClientWrapper` with certain defaults; therefore it's hard to make a **correct** client and pass it as http_client

### To Reproduce

1- Try to pass `proxies` param to `AsyncOpenAI`

2- It will fail as it does not accept `proxies` as a parameter

### Code snippets

```Python
AsyncClient(proxies="""")
# Traceback (most recent call last):
#   File ""<stdin>"", line 1, in <module>
# TypeError: AsyncOpenAI.__init__() got an unexpected keyword argument 'proxies'
```


### OS

Linux

### Python version

Pytho v3.12.3

### Library version

1.35.10",closed,2024-07-07T05:22:48Z,2024-07-07T16:49:00Z,[],https://github.com/openai/openai-python/issues/1529,11.44,True,missing_option_violation,medium,"The AsyncOpenAI class does not allow passing the 'proxies' parameter, which is expected based on the parent class AsyncAPIClient.",high,
1510,This old format doesn;t supported anymore,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

response = client.chat.completions.create(
    # model=""alibaba/Qwen1.5-110B-Chat"",
    model=""Qwen/Qwen2-7B-Instruct"",
    messages=[
        {
            ""role"": ""system"",
            ""content"": [{""type"": ""text"", ""text"": ""you are a dog""}],
            ""name"": """",
        },
        {
            ""role"": ""user"",
            ""content"": [{""type"": ""text"", ""text"": ""你好啊""}],
            ""name"": """",
        },
        {
            ""role"": ""assistant"",
            ""content"": [
                {
                    ""type"": ""text"",
                    ""text"": ""你好，我是一条狗，我叫汪昂"",
                }
            ],
            ""name"": """",
        },
        {
            ""role"": ""user"",
            ""content"": [{""type"": ""text"", ""text"": ""Hello""}],
            ""name"": """",
        },
    ],
    stream=True,
)

Am using openai to create, got request format error:

openai.BadRequestError: Error code: 400 - {'code': 50400, 'message': 'The request parameter is invalid, Please check again.', 'data': None, 'error': 'The request parameter is invalid, Please check again.'}

this format previous works OK> why?


### To Reproduce

gyiguy

### Code snippets

_No response_

### OS

macOS

### Python version

3.11

### Library version

openai latest",closed,2024-06-29T16:27:44Z,2024-07-06T21:48:50Z,['bug'],https://github.com/openai/openai-python/issues/1510,173.35,True,input_type_violation,high,"The 'content' field in the 'messages' array is expected to be a string, but it is provided as a list of dictionaries, which is an incorrect format.",high,
1527,AzureOpenAI AuthenticationError,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

Hi, 

I have a `.env` file with the necessary AzureOpenAI credentials. However, I am getting this error message AuthenticationError: 
Error code: 401 - {'statusCode': 401, 'message': 'Access denied due to missing subscription key. Make sure to include subscription key when making requests to an API.'}

When I try to execute the code from this microsoft learn source: https://learn.microsoft.com/en-us/azure/ai-services/openai/chatgpt-quickstart?tabs=command-line%2Cpython-new&pivots=programming-language-python


### To Reproduce

### Below is the full python code.

`import os
from openai import AzureOpenAI

client = AzureOpenAI(
  api_key = os.getenv(""AZURE_OPENAI_KEY""),  
  api_version = os.getenv(""AZURE_OPENAI_VERSION""),
  azure_endpoint = os.getenv(""AZURE_OPENAI_ENDPOINT"")
)

response = client.chat.completions.create(
    model=""gpt-35-turbo"", # model = ""deployment_name"".
    messages=[
        {""role"": ""system"", ""content"": ""Assistant is a large language model trained by OpenAI.""},
        {""role"": ""user"", ""content"": ""Who were the founders of Microsoft?""}
    ]
)

#print(response)
print(response.model_dump_json(indent=2))
print(response.choices[0].message.content)`

### Code snippets

_No response_

### OS

Windows

### Python version

Python v3.11.7

### Library version

openai v1.35.10",closed,2024-07-06T14:48:20Z,2024-07-06T20:43:33Z,[],https://github.com/openai/openai-python/issues/1527,5.92,True,missing_dependency_violation,high,"The error indicates a missing subscription key, which is a required dependency for making requests to the AzureOpenAI API.",high,
1513,"Assistant access doesn't work without OPENAI_API_KEY env variable, preventing from accessing assistants in different projects.","### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

Assistant access doesn't work without OPENAI_API_KEY env variable, preventing from accessing assistants in different projects.

Why is it needed?
I have two assistant that i need to access from slack bot
they are located in different projects

I can't rely on the environment variable - as the api_key needed is different.


### To Reproduce

1. Create an assistant
2. fill the details
3. run the code snippet

### Code snippets

```Python
import os
import openai
from retrying import retry


def should_reject(run_status):
    return run_status.status != ""completed""


@retry(retry_on_result=should_reject, stop_max_attempt_number=20, wait_fixed=3000)
def retrieve_run_status(thread_id, run_id):
    return openai.beta.threads.runs.retrieve(thread_id=thread_id, run_id=run_id)


def monitor_thread_status(thread_id, run_id):
    run_status = retrieve_run_status(thread_id, run_id)
    return run_status

def access_the_assistant(api_key):

    question = ""What can you help with?""
    project_id = 'proj_..........'
    assistant_id = 'asst_...........'

    client = openai.OpenAI(project=project_id, api_key=api_key)

    the_thread = client.beta.threads.create()
    thread_id = the_thread.id

    _ = client.beta.threads.messages.create(
        thread_id, role=""user"", content=question
    )
    run = client.beta.threads.runs.create(
        thread_id=thread_id, assistant_id=assistant_id
    )

    run_status = retrieve_run_status(thread_id, run.id)

    thread_messages = client.beta.threads.messages.list(
        thread_id,
    )
    return ( str (thread_messages.data) )

def main():
    #load the api key
    api_key = os.getenv(""OPENAI_API_KEY"")

    # if uncommented fix the issue - caches the api_key
    # ret = access_the_assistant(api_key)
    # print(ret)

    del os.environ['OPENAI_API_KEY']

    ret = access_the_assistant(api_key)
    print(ret)


if __name__ == ""__main__"":
    main()
```


### OS

Mac

### Python version

3.9

### Library version

1.35.7",closed,2024-07-01T11:54:11Z,2024-07-02T09:56:22Z,['bug'],https://github.com/openai/openai-python/issues/1513,22.04,True,missing_dependency_violation,high,"The code fails to provide the required OPENAI_API_KEY environment variable, which is necessary for accessing assistants across different projects.",high,
1515,"JSONDecodeError  caused by ""Unterminated string"" when streaming response from chat.completions.with_raw_response.create","### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

I am writing this bug report on behalf of a customer using an application which my team has developed using the `openai` package.  Unfortunately, I am unable to get specific repro data for this issue due to the customer's data being confidential, but it is suspected that the issue may be related to a previously closed issue #650 based on the conditions in which the problem is arising.  

In our specific case, our application is attempting to read the streaming response from the output of the `parse` method, and observing this unterminated string issue, e.g:

```
raw_response = await client.chat.completions.with_raw_response.create(**model_args)
response = raw_response.parse()
async for chunk in response:  # <-- Here is where unterminated string error occurs
   ...
```

I realize that without a specific repro with data this bug may not be actionable, but I wanted to at least ask if anyone here has any suggestions about how we might further diagnose the problem.  We got confirmation from our customer that they are using a 1.x version of the openai package, so I wanted to raise the possibility that there might be another issue somewhere else, possibly due to the transformation of the raw response to chat completion.

### To Reproduce

Repro data is unfortunately not available.

### Code snippets

_No response_

### OS

Linux

### Python version

Python 3.11

### Library version

openai v1.6.1",closed,2024-07-01T19:59:19Z,2024-07-01T20:17:03Z,['bug'],https://github.com/openai/openai-python/issues/1515,0.3,True,input_value_violation,medium,"The issue seems to arise from an unterminated string in the streaming response, indicating that the input values or the response format may not be adhering to the expected structure, potentially leading to performance degradation.",high,
1511,upload_and_poll a in memory file,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

upload_and_poll a in memory file

I get a streamlit file that way:

```python
uploaded_file = st.file_uploader(""Choisir un fichier pdf"") #https://docs.streamlit.io/develop/api-reference/widgets/st.file_uploader
if uploaded_file is not None:
    st.write(""Uploaded..."")
    # To read file as bytes:
    bytes_data = uploaded_file.getvalue()
```

When I upload it as a file with `vector_stores.file_batches.upload_and_poll()` using `[io.BytesIO(blob)]` as the stream array it makes an error:

**""Files with extensions [none] are not supported for retrieval.""**

It seems that OpenAI file storage recognize the file type via its extension. It should be an optional parameter.


### To Reproduce

see code above

### Code snippets

_No response_

### OS

linux

### Python version

python 3.12

### Library version

openai                    1.33.0",closed,2024-06-30T08:42:10Z,2024-07-01T10:47:18Z,['question'],https://github.com/openai/openai-python/issues/1511,26.09,True,input_value_violation,high,"The API does not accept files without extensions, leading to an error when attempting to upload an in-memory file.",high,
1446,Async completions.create method does I/O in the event loop,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

I am working on the Home Assistant OpenAI integration. Home Assistant has started warning when we see I/O being done in the event loop and it printed this warning:

```
2024-05-25 03:03:21.716 WARNING (MainThread) [homeassistant.util.loop] Detected blocking call to open inside the event loop by integration 'openai_conversation' at homeassistant/components/openai_conversation/conversation.py, line 170: result = await client.chat.completions.create( (offender: /home/vscode/.local/ha-venv/lib/python3.12/site-packages/distro/distro.py, line 1099: with open(self.os_release_file, encoding=""utf-8"") as release_file:)
```

It looks like `distro` is used to set the platform headers. When calling the Linux platform, it [opens the release file](https://github.com/python-distro/distro/blob/master/src/distro/distro.py#L1103). This makes `distro` not async-safe.


### To Reproduce

1. Call `await client.chat.completions.create`

### Code snippets

_No response_

### OS

macOS

### Python version

Python 3.12.3

### Library version

openai==1.3.8",closed,2024-05-25T03:13:27Z,2024-07-01T11:17:40Z,['bug'],https://github.com/openai/openai-python/issues/1446,896.07,True,method_order_violation,medium,"The async completions.create method performs blocking I/O operations within the event loop, which violates the expected asynchronous behavior of the method.",high,
1435,TypeError: TarFile.extractall() got an unexpected keyword argument 'filter' when running openai migrate,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

When running the openai migrate command to update my code to the latest version due to deprecated functions, I encountered a TypeError. The error message indicates that the TarFile.extractall() method received an unexpected keyword argument filter. The error seems to originate from the migrate.py file within the openai package, specifically from the line attempting to call archive.extractall with an unsupported filter argument.

### To Reproduce

1. Run this command  in your terminal:
`openai migrate`
2. Observe the error message:

`Retrieving Grit CLI metadata from https://api.keygen.sh/v1/accounts/custodian-dev/artifacts/marzano-macos-arm64
Traceback (most recent call last):
  File ""path/to/openai"", line 8, in <module>
    sys.exit(main())
             ^^^^^^
  File ""path/to/_cli.py"", line 129, in main
    _main()
  File ""path/to/_cli.py"", line 209, in _main
    parsed.func(
  File ""path/to/migrate.py"", line 53, in migrate
    grit_path = install()
                ^^^^^^^^^
  File ""path/to/migrate.py"", line 141, in install
    archive.extractall(unpacked_dir, filter=""data"")
TypeError: TarFile.extractall() got an unexpected keyword argument 'filter'
`

### Code snippets

_No response_

### OS

macos

### Python version

3.11.3

### Library version

1.30.1",closed,2024-05-22T19:48:25Z,2024-06-26T18:10:31Z,['bug'],https://github.com/openai/openai-python/issues/1435,838.37,True,input_type_violation,high,"The TarFile.extractall() method is being called with an unsupported keyword argument 'filter', indicating a type mismatch in the expected method signature.",high,
1160,"All API Errors raised during streaming raise the same generic message (""An error occurred during streaming"")","### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

All API Errors raised during streaming raise the same generic message (""An error occurred during streaming"").    
See [line 64 of _streaming.py](https://github.com/openai/openai-python/blob/main/src/openai/_streaming.py#L64).

The `data[""error""]` field is completely disregarded for streaming errors. Instead, it could be read to raise the correct type of error.
This is a major quality-of-life issue with custom inference servers that seek to follow the openai standard.

### To Reproduce

1. Spin up a custom inference server.
2. Have users access it via the `openai` Python library.
3. Introduce an arbitrary error in your inference server, and handle it correctly, by returning the error's code, param, and type inside of the streaming response.
4. Notice that the users will always get the generic ""An error occurred during streaming"" message.

### Code snippets

```Python
The relevant part of the codebase uses this code snippet:

if is_mapping(data) and data.get(""error""):
    raise APIError(
        message=""An error occurred during streaming"",
        request=self.response.request,
        body=data[""error""],
    )
```

A different part of the codebase uses this code snippet:
```
raise self._make_status_error_from_response(err.response) from None
```

This latter type of functionality should also be used while iterating over a stream.
```


### OS

Ubuntu 20.04.5 LTS

### Python version

Python v.3.11.6

### Library version

openai v1.11.1",closed,2024-02-15T21:12:32Z,2024-03-06T22:42:04Z,['bug'],https://github.com/openai/openai-python/issues/1160,481.49,True,incorrect_functionality,high,"The API raises a generic error message instead of providing specific error details, leading to incorrect system behavior and poor user experience.",high,
1335,Assistants API: inconsistency between API reference and a migration guide,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

Greetings fellows,

[A migration guide for assistant file upload](https://platform.openai.com/docs/assistants/migration/what-has-changed) suggests using the following format:

```
""attachments"": [
    {
      ""file_id"": ""file-123"",
      ""tools"": [
        { ""type"": ""file_search"" },
        { ""type"": ""code_interpreter"" }
      ]
    }
  ]
```

However, the API reference shows something else:
https://platform.openai.com/docs/api-reference/messages/createMessage#messages-createmessage-attachments

Whenever I am using the version explained in the migration guide, I get the following traceback:

```
Failed to run listener function (error: Error code: 400 - {'error': {'message': ""Missing required parameter: 'attachments[0].tools'."", 'type': 'invalid_request_error', 'param': 'attachments[0].tools', 'code': 'missing_required_parameter'}})
```

### To Reproduce

1. Try to migrate existing code that uses assistants API with file uploads to the new version
2. Get stuck. :(

### Code snippets

I think, this is the suspected piece in the Python library that differs:
`openai.types.beta.threads.message_create_params.Attachment`

```python
class Attachment(TypedDict, total=False):
    add_to: List[Literal[""file_search"", ""code_interpreter""]]

    file_id: str
    """"""The ID of the file to attach to the message.""""""
```

It expects `add_to` and not a `tools` list.

### OS

macOS

### Python version

Python v3.12.1

### Library version

openai v1.21.2",closed,2024-04-18T09:05:03Z,2024-04-18T22:29:53Z,['bug'],https://github.com/openai/openai-python/issues/1335,13.41,True,missing_option_violation,high,"The migration guide suggests using 'tools' in the attachments format, while the API reference requires 'add_to', leading to a mismatch in expected input.",high,
1492,Error while using parallel_tool_calls parameter with AzureOpenAI,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

Getting below error when calling the chat.completion.create function with parallel_function_calls=False parameter with AzureOpenAI client. 

openai.BadRequestError: Error code: 400 - {'error': {'message': ""Unknown parameter: 'parallel_tool_calls'."", 'type': 'invalid_request_error', 'param': 'parallel_tool_calls', 'code': 'unknown_parameter'}}

NOTE: This is only for the scenario where the Function Calling feature is being used. 

### To Reproduce

Just create a client with AzureOpenAI.
Call the client.chat.completion.create method and with that, also pass the parallel_function_calls=False (as it is True by default).
You will get the above error.

### Code snippets

_No response_

### OS

Any

### Python version

Any

### Library version

openai v1.34.0",closed,2024-06-20T11:55:09Z,2024-06-21T12:52:26Z,['bug'],https://github.com/openai/openai-python/issues/1492,24.95,True,missing_option_violation,high,"The parameter 'parallel_tool_calls' is not recognized by the API, indicating a failure to adhere to the expected options for the method.",high,
1485,assistant streaming slow,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

When I attempt to use the assistant function and perform streaming returns, I notice that the assistant and thread created with my most frequently used API key have a very slow response time, taking about 20 seconds to return content after establishing the connection. However, when I create a new assistant and thread with another API key, it only takes about 3-5 seconds to get a response. Why is this happening? Is it because my frequently used API key internally stores a large amount of assistant, thread, and file content?

### To Reproduce

just request with the different api key  and their assistant and thread

### Code snippets

_No response_

### OS

macOS

### Python version

Python 3.10

### Library version

openai             1.23.6",closed,2024-06-18T07:30:14Z,2024-06-21T02:14:51Z,['bug'],https://github.com/openai/openai-python/issues/1485,66.74,False,,,,low,
632,Missing HTTP/2 support because python requests only supports HTTP/1.1,"### Describe the bug

This official python library for OpenAI is relying on python requests which does not support HTTP2 which is much more efficient. 

I recommend exploring switching to [HTTPX library](https://www.python-httpx.org/async/).

#### OpenAI python lib / cli:

<img width=""1800"" alt=""image"" src=""https://github.com/openai/openai-python/assets/637225/04494767-c733-45d1-ad23-cbea670eeac9"">

<img width=""1800"" alt=""image"" src=""https://github.com/openai/openai-python/assets/637225/63e74e41-736f-4b87-a80e-799b3ed8bb68"">


#### Using custom script HTTPX (uses HTTP/2):

<img width=""1800"" alt=""image"" src=""https://github.com/openai/openai-python/assets/637225/e212185b-def9-47d0-92d7-9c3a5a5964aa"">

<img width=""1800"" alt=""image"" src=""https://github.com/openai/openai-python/assets/637225/4d8ec595-a36c-416c-9680-ad1db4fee7c0"">



### To Reproduce

1. pip install openai
2. Setup a proxy and intercept the traffic
3. `$ openai -v api chat_completions.create -m gpt-3.5-turbo -g user ""Hello world""`
4. Check it uses HTTP/1.1 as it's the only version requests supports

### Code snippets

HTTPX with HTTP/2 basic example:


```bash
$ python openai-httpx.py --user ""hey, how are you doing?"" --system ""Act as robot"" --model ""gpt-4""
```

```python
import httpx
import os
import argparse
import asyncio
import json
import sys

timeout = httpx.Timeout(10.0, read=None)

async def create_chat_completion(user, system, model):
    url = ""https://api.openai.com/v1/chat/completions""

    headers = {
        ""Authorization"": f""Bearer {os.getenv('OPENAI_API_KEY')}"",
        ""Content-Type"": ""application/json"",
        ""Accept"": ""text/event-stream"",
        ""Connection"": ""keep-alive""
    }

    data = {
        ""model"": model,
        ""messages"": [{""role"": ""system"", ""content"": system},
                     {""role"": ""user"", ""content"": user}],
        ""stream"": True
    }

    async with httpx.AsyncClient(http2=True) as client:
        async with client.stream('POST', url, headers=headers, json=data, timeout=timeout) as response:
            async for line in response.aiter_lines():
                if line.startswith(""data: ""):
                    json_str = line[len(""data: ""):]
                    if json_str != '[DONE]':
                        frame = json.loads(json_str)
                        delta_content = frame.get('choices', [{}])[0].get('delta', {}).get('content', '')
                        sys.stdout.write(delta_content)
                        sys.stdout.flush()

if __name__ == ""__main__"":
    parser = argparse.ArgumentParser()
    parser.add_argument(""--user"", required=True, help=""User message to OpenAI"")
    parser.add_argument(""--system"", required=True, help=""System message to OpenAI"")
    parser.add_argument(""--model"", required=True, help=""Model to use for completion"")
    args = parser.parse_args()

    asyncio.run(create_chat_completion(args.user, args.system, args.model))
```


### OS

macOS 14 23A344

### Python version

Python 3.11.4

### Library version

openai 0.27.8",closed,2023-09-30T21:12:58Z,2023-10-15T22:56:36Z,['bug'],https://github.com/openai/openai-python/issues/632,361.73,True,missing_option_violation,medium,"The OpenAI Python library does not support HTTP/2, which is a recommended option for improved efficiency. Users are advised to switch to HTTPX for HTTP/2 support, indicating a missing option in the API contract.",high,
1449,tool_resources parameter throwing error while creating Assistant v2 with AZURE client,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

I am using openai==1.28.1 and while creating assistant I am getting `BadRequestError: Error code: 400 - {'error': {'message': ""Unknown parameter: 'tool_resources'."", 'type': 'invalid_request_error', 'param': 'tool_resources', 'code': 'unknown_parameter'}} `. I am taking references from [Source](https://platform.openai.com/docs/assistants/migration/what-has-changed) [source](https://learn.microsoft.com/en-us/azure/ai-services/openai/whats-new#assistants-v2-preview) Below is the code which is throwing me error when I am trying tool_resource

### To Reproduce

Run below code with AZURE api key and endpoint, even if I change the below  code with `tool_choice={""code_interpreter"": {""file_ids"": [file.id]}}` it won' t work  it gives error `TypeError: Assistants.create() got an unexpected keyword argument 'tool_choice'`
[source](https://learn.microsoft.com/en-us/azure/ai-services/openai/whats-new#assistants-v2-preview) 

```
assistant = client.beta.assistants.create(
      instructions='''My Instruction''',
      model=""My model deployment name"",
      tools=[{""type"": ""code_interpreter""}],
      tool_resources={""code_interpreter"": {""file_ids"": [file.id]}}
    )
```

### Code snippets

_No response_

### OS

Linux

### Python version

3.10.1

### Library version

openai==1.28.1",closed,2024-05-27T20:15:59Z,2024-05-29T11:42:32Z,['bug'],https://github.com/openai/openai-python/issues/1449,39.44,True,missing_option_violation,high,"The parameter 'tool_resources' is not recognized by the API, indicating that the user did not follow the correct options provided by the API's contract.",high,
1484,AzureOpenAI doesn't support parameter chunking_strategy while creating vector store.,"### Confirm this is a feature request for the Python library and not the underlying OpenAI API.

- [x] This is a feature request for the Python library

### Describe the feature or improvement you're requesting

OpenAI recently added a new feature to their library, which says we can customize the chunking strategy used to split and store files in vector stores using the new parameter **""chunking_strategy""** to be used while creating a vector store.
https://platform.openai.com/docs/changelog (June 3rd, file search customizations)
![image](https://github.com/openai/openai-python/assets/97459596/b28ac955-ce32-44ac-bef5-185ebb19e45d)
If I try creating a new vector store mentioning the custom chunking strategy using the **OpenAI client**, I am able to successfully create one.
![image](https://github.com/openai/openai-python/assets/97459596/1cf032f3-94ca-45b3-a94e-78b11806994b)
But, if I try creating a new vector store mentioning the custom chunking strategy using the **AzureOpenAI client**, I get BadRequestError which says **unknown parameter: chunking_strategy**.
![image](https://github.com/openai/openai-python/assets/97459596/c96cc2a7-22e1-4bcc-9112-6da8a207039b)
![image](https://github.com/openai/openai-python/assets/97459596/d066f3e5-3936-43a9-8b65-319b632ff7f3)
I have tried upgrading the openai python library, but that doesn't solve the issue.
I request AzureOpenAI to update their module as per the changes done by openai library.",closed,2024-06-17T17:54:52Z,2024-06-18T17:00:50Z,[],https://github.com/openai/openai-python/issues/1484,23.1,True,missing_option_violation,high,"The AzureOpenAI client does not support the 'chunking_strategy' parameter, which is available in the OpenAI client, leading to a failure when attempting to use this option.",high,
965,Error with the openai library,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

I tried a simple chat requestion and throws this error.
I have installed pip 3 and python 3.11
I have updated the environment variables with my API KEY

import os
from openai import OpenAI
#from openai.types import FunctionDefinition, FunctionParameters

client = OpenAI(
    # This is the default and can be omittedpip uninstall openai
    api_key=os.environ.get(""OPENAI_API_KEY""),
)

chat_completion = client.chat.completions.create(
    model=""gpt-3.5-turbo"",
  messages = [
      { ""role"": ""user"", ""content"": ""Write a tagline for an ice cream shop."" }
  ]
)
print(chat_completion['choices'][0]['message']['content'])

**ERROR**

Traceback (most recent call last):
  File ""c:\Users\desmo\Desktop\chatAttempt1.py"", line 10, in <module>
    chat_completion = client.chat.completions.create(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\desmo\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\openai\_utils\_utils.py"", line 303, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\desmo\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\openai\resources\chat\completions.py"", line 598, in create
    return self._post(
           ^^^^^^^^^^^
  File ""C:\Users\desmo\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\openai\_base_client.py"", line 1086, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\desmo\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\openai\_base_client.py"", line 846, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File ""C:\Users\desmo\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\openai\_base_client.py"", line 884, in _request
    return self._retry_request(
           ^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\desmo\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\openai\_base_client.py"", line 956, in _retry_request
    return self._request(
           ^^^^^^^^^^^^^^
  File ""C:\Users\desmo\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\openai\_base_client.py"", line 884, in _request
    return self._retry_request(
           ^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\desmo\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\openai\_base_client.py"", line 956, in _retry_request
    return self._request(
           ^^^^^^^^^^^^^^
  File ""C:\Users\desmo\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\openai\_base_client.py"", line 898, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}


### To Reproduce

Just run the code it will show an error

### Code snippets

```Python
import os
from openai import OpenAI
#from openai.types import FunctionDefinition, FunctionParameters

client = OpenAI(
    # This is the default and can be omittedpip uninstall openai
    api_key=os.environ.get(""OPENAI_API_KEY""),
)

chat_completion = client.chat.completions.create(
    model=""gpt-3.5-turbo"",
  messages = [
      { ""role"": ""user"", ""content"": ""Write a tagline for an ice cream shop."" }
  ]
)
print(chat_completion['choices'][0]['message']['content'])
```


### OS

Windows 11

### Python version

python 3.11 

### Library version

  1.3.8",closed,2023-12-12T13:17:54Z,2023-12-15T03:21:00Z,[],https://github.com/openai/openai-python/issues/965,62.05,True,missing_dependency_violation,High,"The error indicates that the API call failed due to exceeding the quota, suggesting a missing dependency on the user's billing plan or quota status that was not checked before making the request.",high,
840,Missing module openai.error,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

I am using library prompt2model, and its demo shows that `import openai.error`

While then there is a  Error `ModuleNotFoundError: No module named 'openai.error'`

My question is whether openai python have deleted the library openai.error? Is there any alternative code here?


### To Reproduce

I think there is a general problem of Module missing. Maybe try as following

1.In terminal, ` pip install openai`
2. In any file, try `import openai.error`

Alternatively, when I did this in terminal
1. `pip install prompt2model`
2. `python prompt2model_demo.py`

### Code snippets

```Python
""""""Tools for accessing API-based models.""""""

from __future__ import annotations  # noqa FI58

import asyncio
import json
import logging
import time

import aiolimiter
import litellm.utils
import openai
import openai.error
import tiktoken
from aiohttp import ClientSession
from litellm import acompletion, completion
from tqdm.asyncio import tqdm_asyncio

# Note that litellm converts all API errors into openai errors,
# so openai errors are valid even when using other services.
API_ERRORS = (
    openai.error.APIError,
    openai.error.Timeout,
    openai.error.RateLimitError,
    openai.error.ServiceUnavailableError,
    openai.error.InvalidRequestError,
    json.decoder.JSONDecodeError,
    AssertionError,
)

ERROR_ERRORS_TO_MESSAGES = {
    openai.error.InvalidRequestError: ""API Invalid Request: Prompt was filtered"",
    openai.error.RateLimitError: ""API rate limit exceeded. Sleeping for 10 seconds."",
    openai.error.APIConnectionError: ""Error Communicating with API"",
    openai.error.Timeout: ""API Timeout Error: API Timeout"",
    openai.error.ServiceUnavailableError: ""API service unavailable error: {e}"",
    openai.error.APIError: ""API error: {e}"",
}
```


### OS

Mac OS

### Python version

Python v3.9

### Library version

openai v1.3.0",closed,2023-11-16T17:28:59Z,2023-11-16T20:31:15Z,['question'],https://github.com/openai/openai-python/issues/840,3.04,True,missing_dependency_violation,high,"The code attempts to import a module 'openai.error' that is not found, indicating a missing dependency in the library.",high,
1261,New Feature Proposal: Assistant API - Chaining streams for function execution ,"### Confirm this is a feature request for the Python library and not the underlying OpenAI API.

- [X] This is a feature request for the Python library

### Describe the feature or improvement you're requesting

Hi, 

I have developed a monkey patch to add the capacity for chaining streams which is very beneficial for the Assistant API function execution workflow. I think it could be integrated into the openai library. So, I guess you want to know the use case, right?

Imagine you are processing the assistant events in a loop (in my case I use the Async stream client but it's basically the almost same for the non-async streaming one):

```
async for chunk in assistant_stream_response:
    # Process chunk here
    
    
    # Process function calls
    if isinstance(chunk, ThreadRunRequiresAction):
        tool_outputs = # Execute the function and gather the outputs in this var     
    
        new_stream = await async_client.beta.threads.runs.submit_tool_outputs(
            thread_id=thread_id, # stored along the way
            run_id=chunk.data.id,
            tool_outputs=tool_outputs,
            stream=True
        )
        # we can chain the new_stream at the end of the current one to avoid writing another chunk processing loop
        assistant_stream_response.chain_stream(new_stream)


    yield result
```

With this, we can chain the tool submit stream response to the current one to avoid writing another chunk processing loop.
Tested & working.

It very beneficial, especially when you integrate the assistant API inside a project to avoid changing the existing workflow.
Here is the monkey patch:


```
#--------------------------------------MONKEY-PATCH-OPENAI--------------------------------------------------------------
import openai
from typing import Any, TypeVar, AsyncIterator, cast
from openai._utils import is_mapping
from openai._exceptions import APIError
from openai import AsyncOpenAI
import httpx

_T = TypeVar(""_T"")


def monkey_patch__init__(self, *, cast_to: type[_T], response: httpx.Response, client: AsyncOpenAI) -> None:
  self.response = response
  self._cast_to = cast_to
  self._client = client
  self._decoder = client._make_sse_decoder()
  self._iterator = self.__stream__()
  self._chained_stream = None # MOD HERE 

def chain_stream(self, stream): # NEW FUNCT HERE
  if self._chained_stream:
    self._chained_stream.chain_stream(stream)
  else:
    self._chained_stream = stream

async def monkey_patch__stream__(self) -> AsyncIterator[_T]:
  cast_to = cast(Any, self._cast_to)
  response = self.response
  process_data = self._client._process_response_data
  iterator = self._iter_events()

  async for sse in iterator:
    if sse.data.startswith(""[DONE]""):
      break

    if sse.event is None:
      data = sse.json()
      if is_mapping(data) and data.get(""error""):
        message = None
        error = data.get(""error"")
        if is_mapping(error):
          message = error.get(""message"")
        if not message or not isinstance(message, str):
          message = ""An error occurred during streaming""

        raise APIError(
          message=message,
          request=self.response.request,
          body=data[""error""],
        )

      yield process_data(data=data, cast_to=cast_to, response=response)

    else:
      data = sse.json()

      if sse.event == ""error"" and is_mapping(data) and data.get(""error""):
        message = None
        error = data.get(""error"")
        if is_mapping(error):
          message = error.get(""message"")
        if not message or not isinstance(message, str):
          message = ""An error occurred during streaming""

        raise APIError(
          message=message,
          request=self.response.request,
          body=data[""error""],
        )

      yield process_data(data={""data"": data, ""event"": sse.event}, cast_to=cast_to, response=response)

  async for _sse in iterator:
    ...

  if self._chained_stream: # MOD HERE
    async for chunk in self._chained_stream:
      yield chunk


openai.AsyncStream.__init__ = monkey_patch__init__
openai.AsyncStream.__stream__ = monkey_patch__stream__
openai.AsyncStream.chain_stream = chain_stream
#-----------------------------------------------------------------------------------------------------------------------
```

Best regards,
Paul Irolla

### Additional context

I have implemented this inside my personal fork of LiteLLM for integrating the assistant API into the existing workflow without changing a thousand of code lines.",closed,2024-03-22T09:16:19Z,2024-05-13T01:31:07Z,[],https://github.com/openai/openai-python/issues/1261,1240.25,False,,,,low,
1433,`ChatCompletionAssistantMessageParam` is incorrectly typed,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

The `ChatCompletionAssistantMessageParam` has the incorrect types for the `function_call`, `name`, and `tool_calls` fields. All of these fields should be marked `Optional[...]` but they are not.

While this doesn't break the type itself, downstream it prevents the use of `pydantic` with these types.

### To Reproduce

Run the code in the snippet below.

`pydantic` will try to validate `ChatCompletionAssistantMessageParam` and fail because `function_call` is None (as returned by the API). I would expect either (1) that the returned message does not include the `function_call` field like it excludes the `name` field or (2) for the `ChatCompletionAssistantMessageParam` type to allow for `function_call` to be `None` in it's type definition.

### Code snippets

```Python
from openai.types.chat import ChatCompletionAssistantMessageParam
from pydantic import BaseModel

class MyModel(BaseModel):
    history: list[ChatCompletionAssistantMessageParam]

history = [
    {
        ""content"": None,
        ""role"": ""assistant"",
        ""function_call"": None,
        ""tool_calls"": [
            {
                ""id"": ""id"",
                ""function"": {
                    ""arguments"": '{""location"":""Tokyo, Japan""}',
                    ""name"": ""GetCurrentWeather"",
                },
                ""type"": ""function"",
            }
        ],
    },
]
my_model = MyModel(history=history)
print(my_model)
```


### OS

14.2.1

### Python version

3.9.16

### Library version

1.30.1",closed,2024-05-21T11:47:00Z,2024-05-28T22:29:15Z,['bug'],https://github.com/openai/openai-python/issues/1433,178.7,True,input_type_violation,high,"The fields 'function_call', 'name', and 'tool_calls' in 'ChatCompletionAssistantMessageParam' are not marked as Optional, leading to type validation issues with pydantic when these fields are None.",high,
1143,AttributeError: 'OpenAI' object has no attribute 'chat',"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

pip install -U openai 

i met this error  i think `v1.12.0` some issue


### To Reproduce

OS : wsl-Ubuntu
python : 3.9.18
Package-version : 1.12.0

### Code snippets

```Python
#### Error
 bash
AttributeError: 'OpenAI' object has no attribute 'chat' 
```

``` python
Name: openai
Version: 1.12.0
Summary: The official Python library for the openai API
Home-page: 
Author: 
Author-email: OpenAI <support@openai.com>
```
```


### OS

wsl-Ubuntu

### Python version

3.9.18

### Library version

1.12.0",closed,2024-02-09T16:23:00Z,2024-02-11T04:51:33Z,['bug'],https://github.com/openai/openai-python/issues/1143,36.48,True,missing_dependency_violation,high,"The error indicates that the 'OpenAI' object is missing the 'chat' attribute, suggesting a potential missing dependency or misconfiguration in the library version being used.",high,
1473,AsyncAssistantStreamManager Error,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

async def requestGpt(response):
    print(""requestGpt called"")
    async with client.beta.threads.runs.stream(
        thread_id=thread.id,
        assistant_id=assistant.id,
        event_handler=EventHandler(response)
    ) as stream:
        await stream.until_done()

When I use this way to call a assistant streaming, the type of stream is AssistantStreamManager, not the AsyncAssistantStreamManager

### To Reproduce

async def requestGpt(response):
    print(""requestGpt called"")
    async with client.beta.threads.runs.stream(
        thread_id=thread.id,
        assistant_id=assistant.id,
        event_handler=EventHandler(response)
    ) as stream:
        await stream.until_done()

### Code snippets

_No response_

### OS

macOS

### Python version

Python 3.10

### Library version

openai             1.23.6",closed,2024-06-06T09:21:40Z,2024-06-11T02:58:38Z,['bug'],https://github.com/openai/openai-python/issues/1473,113.62,False,,,,low,
690,TypeError: ConnectionPool.__init__() got an unexpected keyword argument 'socket_options',"# Code

```
import os
from openai import OpenAI

client = OpenAI(
    api_key=settings.OPEN_AI_API_KEY,
    organization='org-xxxx'
)
client.models.list()

```

# Stack Trace

```
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
Cell In[3], line 6
      2 from openai import OpenAI
      4 # os.environ['OPENAI_API_KEY'] = settings.OPEN_AI_API_KEY
----> 6 client = OpenAI(
      7     api_key=settings.OPEN_AI_API_KEY,
      8     organization='org-xxxx'
      9 )
     10 client.models.list()
     12 # from django.conf import settings
     13 # from openai import OpenAI
     14 # client = OpenAI(organization = settings.OPEN_AI_API_KEY)
   (...)
     17 
     18 # client.models.list()

File /usr/local/lib/python3.11/site-packages/openai/_client.py:105, in OpenAI.__init__(self, api_key, organization, base_url, timeout, max_retries, default_headers, default_query, http_client, _strict_response_validation)
    102 if base_url is None:
    103     base_url = f""https://api.openai.com/v1""
--> 105 super().__init__(
    106     version=__version__,
    107     base_url=base_url,
    108     max_retries=max_retries,
    109     timeout=timeout,
    110     http_client=http_client,
    111     custom_headers=default_headers,
    112     custom_query=default_query,
    113     _strict_response_validation=_strict_response_validation,
    114 )
    116 self._default_stream_cls = Stream
    118 self.completions = resources.Completions(self)

File /usr/local/lib/python3.11/site-packages/openai/_base_client.py:738, in SyncAPIClient.__init__(self, version, base_url, max_retries, timeout, transport, proxies, limits, http_client, custom_headers, custom_query, _strict_response_validation)
    723         timeout = DEFAULT_TIMEOUT
    725 super().__init__(
    726     version=version,
    727     limits=limits,
   (...)
    736     _strict_response_validation=_strict_response_validation,
    737 )
--> 738 self._client = http_client or httpx.Client(
    739     base_url=base_url,
    740     # cast to a valid type because mypy doesn't understand our type narrowing
    741     timeout=cast(Timeout, timeout),
    742     proxies=proxies,
    743     transport=transport,
    744     limits=limits,
    745 )
    746 self._has_custom_http_client = bool(http_client)

File /usr/local/lib/python3.11/site-packages/httpx/_client.py:672, in Client.__init__(self, auth, params, headers, cookies, verify, cert, http1, http2, proxies, mounts, timeout, follow_redirects, limits, max_redirects, event_hooks, base_url, transport, app, trust_env, default_encoding)
    669 allow_env_proxies = trust_env and app is None and transport is None
    670 proxy_map = self._get_proxy_map(proxies, allow_env_proxies)
--> 672 self._transport = self._init_transport(
    673     verify=verify,
    674     cert=cert,
    675     http1=http1,
    676     http2=http2,
    677     limits=limits,
    678     transport=transport,
    679     app=app,
    680     trust_env=trust_env,
    681 )
    682 self._mounts: typing.Dict[URLPattern, typing.Optional[BaseTransport]] = {
    683     URLPattern(key): None
    684     if proxy is None
   (...)
    694     for key, proxy in proxy_map.items()
    695 }
    696 if mounts is not None:

File /usr/local/lib/python3.11/site-packages/httpx/_client.py:720, in Client._init_transport(self, verify, cert, http1, http2, limits, transport, app, trust_env)
    717 if app is not None:
    718     return WSGITransport(app=app)
--> 720 return HTTPTransport(
    721     verify=verify,
    722     cert=cert,
    723     http1=http1,
    724     http2=http2,
    725     limits=limits,
    726     trust_env=trust_env,
    727 )

File /usr/local/lib/python3.11/site-packages/httpx/_transports/default.py:136, in HTTPTransport.__init__(self, verify, cert, http1, http2, limits, trust_env, proxy, uds, local_address, retries, socket_options)
    133 ssl_context = create_ssl_context(verify=verify, cert=cert, trust_env=trust_env)
    135 if proxy is None:
--> 136     self._pool = httpcore.ConnectionPool(
    137         ssl_context=ssl_context,
    138         max_connections=limits.max_connections,
    139         max_keepalive_connections=limits.max_keepalive_connections,
    140         keepalive_expiry=limits.keepalive_expiry,
    141         http1=http1,
    142         http2=http2,
    143         uds=uds,
    144         local_address=local_address,
    145         retries=retries,
    146         socket_options=socket_options,
    147     )
    148 elif proxy.url.scheme in (""http"", ""https""):
    149     self._pool = httpcore.HTTPProxy(
    150         proxy_url=httpcore.URL(
    151             scheme=proxy.url.raw_scheme,
   (...)
    165         socket_options=socket_options,
    166     )

TypeError: ConnectionPool.__init__() got an unexpected keyword argument 'socket_options'
```",closed,2023-11-06T22:05:47Z,2023-11-06T22:08:58Z,[],https://github.com/openai/openai-python/issues/690,0.05,True,input_type_violation,unknown,"The API method received an unexpected keyword argument 'socket_options', indicating a mismatch in expected input types or parameters.",high,
1465,json_object response_format not behaving as expected on Azure gpt-3.5-turbo-0125,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

Using the OpenAI python library with an Azure OpenAI instance, I am trying ot generate a json response guaranteed to be in json format (as only including it in text promt sometimes yields inadequate results). 

For a request with the following parameters:
> 'model': 'gpt-3.5-turbo-0125', 'response_format': {'type': 'json_object'}

I am getting the following error:
> openai.BadRequestError: Error code: 400 - {'error': {'message': ""Invalid parameter: 'response_format' of type 'json_object' is not supported with this model."", 'type': 'invalid_request_error', 'param': 'response_format', 'code': None}}

Yet, my understnading is that (according to the [Azure documentation page](https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/json-mode?tabs=python)):
- json mode is supported by the model gpt-35-turbo (0125)
- and the [2024-02-01 API version](https://github.com/Azure/azure-rest-api-specs/blob/main/specification/cognitiveservices/data-plane/AzureOpenAI/inference/stable/2024-02-01/inference.json) which I use, supports json_object response_format

### To Reproduce

1. Set up an Azure OpenAI instance
2. from openai.lib.azure import AsyncAzureOpenAI
3. 

client = AsyncAzureOpenAI(
        api_key=<your Azure Open AI key>,
        api_version=""2024-02-01"",
        azure_endpoint=<your Azure Open AI endpoint>,
        azure_deployment=<your Azure Open AI deployment name>
    )

response = await client.chat.completions.create(
        model= ""gpt-3.5-turbo-0125"",
        messages=[
            {""role"": ""user"", ""content"": <Your prompt>},
        ],
        response_format={'type': 'json_object'}
)

print (response.choices[0].message.content)
    
  

### Code snippets

_No response_

### OS

masOS

### Python version

Python 3.11.2

### Library version

openai v1.30.4",closed,2024-06-04T18:46:09Z,2024-06-06T08:54:08Z,['bug'],https://github.com/openai/openai-python/issues/1465,38.13,True,input_type_violation,high,"The provided 'response_format' parameter of type 'json_object' is not supported by the specified model 'gpt-3.5-turbo-0125', indicating a type mismatch between the expected and provided input.",high,
1457,It takes forever to complete run for AZURE Assistant v2,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

I am working with AZURE Assistant api and using tool  code_interpreter. I am stuck while creating run on the thread to generate a response by calling the model and the tools. It goes forever. 

### To Reproduce

Run below code to reproduce:
 #Send file to assistant
```
file_data = client.files.create(
      file=open(""test.csv"",""rb""),
      purpose='assistants'
    ) 
``` 
#create Assistant
```
assistant = client.beta.assistants.create(
    instructions='''My system instruction.''',
    model=  ""gpt-35-turbo-0613"",
    tools=[{""type"": ""code_interpreter""}],
    temperature=0.4,
    tool_resources={""code_interpreter"":{""file_ids"":[file.id]}}
    )
```
#create thread
`thread = client.beta.threads.create()
`
#Add message to thread
```
message = client.beta.threads.messages.create(
                    thread_id=thread.id,
                    role=""user"",
                    content=""User question"",
                    
                )

```
# create a run (and it goes forever) this is the point I am stuck 
```
run = client.beta.threads.runs.create_and_poll(
  thread_id=thread.id,
  assistant_id=assistant.id,
  
)
```

### Code snippets

_No response_

### OS

Linux

### Python version

3.10.12

### Library version

openai- 1.28.1",closed,2024-05-29T16:42:52Z,2024-06-05T17:06:30Z,['bug'],https://github.com/openai/openai-python/issues/1457,168.39,True,method_order_violation,hang,"The run method is being called without ensuring that the thread and assistant are properly initialized and ready, leading to the system hanging indefinitely.",high,
1463,Hwo to get log probablity of input tokens?,"### Confirm this is a feature request for the Python library and not the underlying OpenAI API.

- [X] This is a feature request for the Python library

### Describe the feature or improvement you're requesting

For a complete context: 
`Q:Where is Beijing? A:china`
I want to get the log_probs of `china`, how can I do this?

### Additional context

_No response_",closed,2024-06-04T04:17:47Z,2024-06-04T05:09:36Z,[],https://github.com/openai/openai-python/issues/1463,0.86,True,missing_dependency_violation,medium,"The request lacks necessary context or details on how to retrieve log probabilities, which may lead to confusion or incorrect implementation.",high,
331,Error when trigger Azure OpenAI’s content management policy,"### Describe the bug

Azure OpenAI API will return in some cases
```
{""error"":{""message"":""The response was filtered due to the prompt triggering Azure OpenAI’s content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766"",""type"":null,""param"":""prompt"",""code"":""content_filter"",""status"":400}}
```
it will failed when using like this:
```
response = openai.Completion.create(
    engine=""text-davinci-003"",
    prompt=""something here"",
    temperature=0.3,
    max_tokens=250,
    top_p=1,
    frequency_penalty=0,
    presence_penalty=0,
    best_of=1,
    stop=None)
```


### To Reproduce

1. run the code snippets

### Code snippets

```Python
response = openai.Completion.create(
    engine=""text-davinci-003"",
    prompt=""SUBREDDIT: r/AskReddit TITLE: Cock blocked by a friend (Who's a girl). POST: So for the past week there's ""
         ""been this girl in one of my classes I've been talking to, she's pretty cute (dyed red hair, fair skin, ""
         ""a few freckles, not ginger), she loves star wars and I suspect she's a redditor. I was going to ask her for ""
         ""her number today, but a girl i met about a year ago came and sat right where the red head had been sitting, ""
         ""effectively cock-blocking me and driving the girl I was interested in away. Now it seems like the red head ""
         ""thinks I'm uninterested in her and has since found some other guy to talk to. Has anybody been in a similar ""
         ""scenario? Advice? \nTL;DR: Got cock blocked by a friend who's a girl."",
    temperature=0.3,
    max_tokens=250,
    top_p=1,
    frequency_penalty=0,
    presence_penalty=0,
    best_of=1,
    stop=None)
```


### OS

Windows

### Python version

Python v3.11

### Library version

openai v0.27.2",closed,2023-03-22T08:45:42Z,2023-11-10T03:26:07Z,"['bug', 'Azure']",https://github.com/openai/openai-python/issues/331,5586.67,True,input_value_violation,medium,"The prompt provided may contain content that triggers Azure OpenAI's content management policy, leading to a filtered response.",high,
1336,AttributeError When Using 'vector_stores' Method in OpenAI Python Client,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

After upgrading to the latest version of the OpenAI Python client (openai==1.21.2), I am encountering an AttributeError when attempting to use the new 'vector_stores' method as described in the documentation. The error suggests that the 'OpenAI' object does not have an attribute 'vector_stores'.

![image](https://github.com/openai/openai-python/assets/29830264/dd25dbe3-5b69-443a-b8f1-52db32ca7636)

![image](https://github.com/openai/openai-python/assets/29830264/cd0b735e-3dba-458c-b2f8-bebc8d42a8b4)

### To Reproduce

Install the latest version of the OpenAI Python client (openai==1.21.2).
Execute the following Python code:
```
from openai import OpenAI
client = OpenAI()

vector_store = client.vector_stores.create(
  name=""Support FAQ""
)
print(vector_store)
```

### Code snippets

```Python
from openai import OpenAI
client = OpenAI()

vector_store = client.vector_stores.create(
  name=""Support FAQ""
)
print(vector_store)
```


### OS

Ubuntu 20.04.6 LTS

### Python version

Python 3.8.10

### Library version

openai 1.21.2 ",closed,2024-04-18T12:11:48Z,2024-04-22T21:23:40Z,['bug'],https://github.com/openai/openai-python/issues/1336,105.2,True,input_type_violation,unknown,"The 'vector_stores' method is not recognized, indicating a potential mismatch in expected API structure or method availability.",high,
1416,Using Azure open AI Assistant throws TypeError: create() got an unexpected keyword argument 'file_ids',"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

I am working on Azure notebook and while creating Azure Open AI Assistant [source1](https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/code-interpreter?tabs=python), [source2](https://github.com/Azure/AI-in-a-Box/blob/main/gen-ai/Assistants/api-in-a-box/personal_finance/assistant-personal_finance.ipynb) I am getting `TypeError: create() got an unexpected keyword argument 'file_ids'`, I tried the solution mentioned [here](https://community.openai.com/t/typeerror-messages-create-got-an-unexpected-keyword-argument-file-ids/722104) even though it is not from AZURE but it gives me different error TypeError: Assistants.create() got an unexpected keyword argument 'attachments', The same[ code](https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/code-interpreter?tabs=python) works fine locally without any `TypeError: create() got an unexpected keyword argument 'file_ids'` is there an update for creating assistants api on AZURE notebook(or any other cloud based notebook it is not working on Databricks notebook  too , it was working on Databrick notebook few weeks ago) or it is a new bug related to this [bug](https://github.com/microsoft/autogen/issues/2580) ? I am using gpt-35-turbo-16k-0613 model, openai 1.28.1 version. I am not using `autogen` library.  

### To Reproduce




[details](https://learn.microsoft.com/en-us/answers/questions/1665693/using-azure-open-ai-assistant-throws-typeerror-cre
Run below code in Azure or Databricks notebook to reproduce error 
`from openai import AzureOpenAI
    
client = AzureOpenAI(
    api_key=os.getenv(""AZURE_OPENAI_API_KEY""),  
    api_version=""2024-02-15-preview"",
    azure_endpoint = os.getenv(""AZURE_OPENAI_ENDPOINT"")
    )

# Upload a file with an ""assistants"" purpose
file = client.files.create(
  file=open(""speech.py"", ""rb""),
  purpose='assistants'
)

# Create an assistant using the file ID
assistant = client.beta.assistants.create(
  instructions=""You are an AI assistant that can write code to help answer math questions."",
  model=""gpt-4-1106-preview"",
  tools=[{""type"": ""code_interpreter""}],
  file_ids=[file.id]
)`)

<img width=""635"" alt=""image"" src=""https://github.com/openai/openai-python/assets/60409314/920b2478-f4e1-4663-afef-e7b6f038810b"">
<img width=""649"" alt=""image"" src=""https://github.com/openai/openai-python/assets/60409314/883922db-e096-46ab-b97e-a3fcc174292e"">
<img width=""635"" alt=""image"" src=""https://github.com/openai/openai-python/assets/60409314/37105618-6b69-4ba8-91a1-40f630ec4695"">

### Code snippets

_No response_

### OS

Linux 22.05.09, 

### Python version

3.8.5

### Library version

openai- 1.28.1",closed,2024-05-13T16:26:30Z,2024-05-22T19:17:19Z,['bug'],https://github.com/openai/openai-python/issues/1416,218.85,True,input_type_violation,High,"The API method 'create()' is receiving an unexpected keyword argument 'file_ids', indicating a mismatch between the expected input type and the provided arguments.",high,
1448,AttributeError: 'NoneType' object has no attribute 'create',"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

test_prompt_results.py:54: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../services/chain.py:146: in dispatch2
    result = await chain.ainvoke(input_text)
../.venv/lib/python3.11/site-packages/langchain_core/runnables/base.py:2405: in ainvoke
    input = await step.ainvoke(
../.venv/lib/python3.11/site-packages/langchain_core/language_models/llms.py:299: in ainvoke
    llm_result = await self.agenerate_prompt(
../.venv/lib/python3.11/site-packages/langchain_core/language_models/llms.py:643: in agenerate_prompt
    return await self.agenerate(
../.venv/lib/python3.11/site-packages/langchain_core/language_models/llms.py:1018: in agenerate
    output = await self._agenerate_helper(
../.venv/lib/python3.11/site-packages/langchain_core/language_models/llms.py:882: in _agenerate_helper
    raise e
../.venv/lib/python3.11/site-packages/langchain_core/language_models/llms.py:866: in _agenerate_helper
    await self._agenerate(
../.venv/lib/python3.11/site-packages/langchain_community/llms/openai.py:1181: in _agenerate
    full_response = await acompletion_with_retry(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

llm = OpenAIChat(verbose=True, client=APIRemovedInV1Proxy, model_name='gpt-4o')
run_manager = <langchain_core.callbacks.manager.AsyncCallbackManagerForLLMRun object at 0x14f3e9c10>
kwargs = {'messages': [{'content': 'Human: Role: You are an advanced tender developer focused on generating winning tender resp...ertise, demonstrate the ability to cope with volume of works?\nHelpful Answer:  ', 'role': 'user'}], 'model': 'gpt-4o'}

    async def acompletion_with_retry(
        llm: Union[BaseOpenAI, OpenAIChat],
        run_manager: Optional[AsyncCallbackManagerForLLMRun] = None,
        **kwargs: Any,
    ) -> Any:
        """"""Use tenacity to retry the async completion call.""""""
        if is_openai_v1():
>           return await llm.async_client.create(**kwargs)
E           AttributeError: 'NoneType' object has no attribute 'create'

../.venv/lib/python3.11/site-packages/langchain_community/llms/openai.py:132: AttributeError

### To Reproduce

model = OpenAI(model_name=model_name, verbose=True)
chain = (
        {
            ""context"": get_context,
            ""extra_instructions"": get_instructions,
            ""question"": get_question,
        }
        | prompt
        | model
        | StrOutputParser()
)

result = await chain.ainvoke(input_text)

### Code snippets

```Python
The code worked fine until I updated to a more recent version.

openai==1.30.3

```


### OS

macOS

### Python version

3.11.4

### Library version

openai==1.30.3",closed,2024-05-25T22:08:54Z,2024-05-26T01:44:41Z,['bug'],https://github.com/openai/openai-python/issues/1448,3.6,True,missing_dependency_violation,high,"The error indicates that a required client object is not initialized, leading to an AttributeError when attempting to call the 'create' method.",high,
1438,Omit redundant information in chat completion message params,"### Confirm this is a feature request for the Python library and not the underlying OpenAI API.

- [X] This is a feature request for the Python library

### Describe the feature or improvement you're requesting

Currently, classes like `ChatCompletionSystemMessageParam` require a parameter of type Required[Literal['system']].

The class name already contains this required role, so this param is simply redundant information while not allowing any different values.

Suggestion: This parameter should be made optional with the required literal as default value for backwards compatibility or even completely omitted.

### Additional context

_No response_",closed,2024-05-23T10:40:46Z,2024-05-26T01:24:36Z,[],https://github.com/openai/openai-python/issues/1438,62.73,True,missing_option_violation,medium,"The API design requires a redundant parameter that could be omitted or made optional, leading to suboptimal usage.",high,
1431,"api-key missing from the request in case of async calls (AsyncOpenAI, AsyncAzureOpenAI)","### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

When trying to perform an await AsyncOpenAI.chat.completions.create call, it will results in _'statusCode': 401, 'message': 'Unauthorized. Access token is missing._


The found workaround it to add **_extra_headers_**

```
    response = await client.chat.completions.create(
        model=""deployment_name"",
        extra_headers={""api-key"": ""_my_api_key""},
        messages=[{""role"": ""user"", ""content"": prompt_text}],
    )
```


After debugging I've seen that `_prepare_options` for sync calls (OpenAI, AzureOpenAI) is implemented, and it actually sets the api-key in the headers, while it is not for async chat completions (AsyncOpenAI, AsyncAzureOpenAI).

### To Reproduce

Library version 1.30.1

```
   client = AsyncAzureOpenAI(
        azure_endpoint=""my_azure_endpoinf"",
        api_key=""my_api_key"",
        api_version=""my_api_version"",
        azure_deployment=""my_deployment"",
    )
    response = await client.chat.completions.create(
        model=""my_deployment"",
        # extra_headers={""api-key"": ""my_api_key""},  # will not work if this is commented out
        messages=[{""role"": ""user"", ""content"": prompt_text}],
    )
```


=>

> openai.AuthenticationError: Error code: 401 - {'statusCode': 401, 'message': 'Unauthorized. Access token is missing, invalid, audience is incorrect (https://cognitiveservices.azure.com), or have expired.'}

### Code snippets

_No response_

### OS

Windows

### Python version

3.11.2

### Library version

1.30.1",closed,2024-05-17T10:09:40Z,2024-05-26T01:22:05Z,['bug'],https://github.com/openai/openai-python/issues/1431,207.21,True,missing_dependency_violation,high,"The async API method call fails due to the absence of the required 'api-key' in the request headers, which is necessary for authentication.",high,
1434,Missing Authorization header in request,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

{
    ""error"": {
        ""message"": ""You didn't provide an API key. You need to provide your API key in an Authorization header using Bearer auth (i.e. Authorization: Bearer YOUR_KEY), or as the password field (with blank username) if you're accessing the API from your browser and are prompted for a username and password. You can obtain an API key from https://platform.openai.com/account/api-keys."",
        ""type"": ""invalid_request_error"",
        ""param"": null,
        ""code"": null
    }
}

### To Reproduce

Make API requests 

### Code snippets

```Python
import openai 
import os
import re

class OpenAI:
    def __init__(self):
        # os.environ[""OPENAI_API_KEY""] = OPENAI_API_KEY
        openai.api_key = openai.api_key = os.getenv(""OPENAI_API_KEY"")
      


    def gpt(self, prompt):
        response = openai.chat.completions.create(
            model=""gpt-3.5-turbo-0125"",
            # model=""gpt-4"",
            messages=[
                {""role"": ""system"", ""content"": ""You are a helpful assistant.""},
                {""role"": ""user"", ""content"": prompt},
                {""role"": ""assistant"", ""content"": ""Respond consisely, response should only include natural language.""},
            ],
                temperature = 0,
                top_p=0.5,
                seed=123
        )
        print(response)
        return response.choices[0].message.content
```


### OS

Windows 11

### Python version

Python v3.11.4

### Library version

openai v1.0.1",closed,2024-05-22T07:40:18Z,2024-05-26T01:14:44Z,['bug'],https://github.com/openai/openai-python/issues/1434,89.57,True,missing_dependency_violation,High,"The API request is missing the required Authorization header, which is necessary for authentication.",high,
1195,Multiple Async calls to the api fail catastrophically,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

For a resume-writing program with multiple levels of async calls, launching relatively small scale async processing causes the API to fail catastrophically.

Attempted the OpenAIClient and httpx.AsyncClient solutions which were suggested here and elsewhere:
https://github.com/openai/openai-python/issues/769

When called synchronously, code processes 50 resumes sequentially with no problem, and perhaps 3 or 4 'Timeout' failures in aggregate that are successfully completed using exponential backoff. The average completion time for each document is 50 seconds with a std of perhaps 10 seconds.

When the same 50 documents are run simultaneously using asyncio:
await asyncio.gather(*tasks) 

Several hundred - several thousand timeout errors occur in aggregate, and most of the time, the processes will fail catastrophically as None is returned by the OpenAI api, which then fails cascadingly throughout the system.

Average completion time rises to 240 seconds with an std of perhaps 30 seconds.

I've confirmed that unique clients are created for each document:
OpenAIClient object at 0x7f9a57762fb0
OpenAIClient object at 0x7f9a5764f430
OpenAIClient object at 0x7f9a57249870
... 



Running with a clean new environment updated today:
python==3.10.13
openai==1.12.0
httpx==0.27.0

#769 seems to indicate that the problem was resolved in open 1.3.8, but we can't fix.



### To Reproduce

1. Initiate 50 top-level tasks, each of which fires of approx 100 tasks, each of which may fire 0-5 additional tasks, and may reiterate
2. Create an AsyncOpenAI Client for each of the 50 toplevel tasks
3. Observe that OpenAI repeatedly returns thousands of timeout errors

### Code snippets

```Python
class OpenAIClient:                                     
     def __init__(self, account_info):                   
         self.aclient = openai.AsyncOpenAI(              
             api_key=os.environ.get(""OPENAI_API_KEY""),   
             http_client=httpx.AsyncClient(              
                 limits=httpx.Limits(                    
                     max_keepalive_connections=10000,    
                     max_connections=1000,),             
                 timeout=15,                             
             ),                                          
         )                                               
         self.account_info = account_info       



Typical error message of the hundreds / thousands received:
API call exceeded the time limit: 
Recalling OpenAI API 1. Error: . iDelay: 0.074. Delay: 0.074
 (<utils.OpenAIClient object at 0x7efd3bd35690>,)
```


### OS

Amazon Linux

### Python version

3.10.13

### Library version

openai 1.12.0",closed,2024-02-26T20:27:35Z,2024-05-25T22:19:05Z,['bug'],https://github.com/openai/openai-python/issues/1195,2137.86,True,method_order_violation,high,"The asynchronous calls to the OpenAI API are not properly managed, leading to cascading failures and timeouts when executed simultaneously.",high,
1167,feat: allow setting retry delay,"### Confirm this is a feature request for the Python library and not the underlying OpenAI API.

- [X] This is a feature request for the Python library

### Describe the feature or improvement you're requesting

Currently, setting the retry delay for the _base client is not easily exposed. End-users have easy control over the max number of retries but not the max_retry_delay and initial retry delay.

https://github.com/openai/openai-python/blob/8ee5f33e8776e4517ef91a1cb2fafb6af2ca9310/src/openai/_base_client.py#L76-L78

Preferably these are exposed and easily settable similar to max_retries in the OpenAI class:
https://github.com/openai/openai-python/blob/8ee5f33e8776e4517ef91a1cb2fafb6af2ca9310/src/openai/_client.py#L49-L74

### Additional context

_No response_",closed,2024-02-20T14:17:38Z,2024-05-25T22:03:35Z,['enhancement'],https://github.com/openai/openai-python/issues/1167,2287.77,False,,,,,
1398,Issue with run polling(sleep),"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

Hi OpenAI team,

I'm reaching out to report a possible issue or improvement regarding the polling mechanism in asynchronous functions.

Problem Description

The current implementation of the poll method uses the time.sleep() function, which is blocking and halts the execution of other asynchronous tasks during polling. This behavior affects concurrency, preventing other coroutines from running efficiently.



### To Reproduce

----


### Code snippets

```Python
import asyncio

async def poll(
    self,
    run_id: str,
    thread_id: str,
    extra_headers: dict | None = None,
    extra_query: dict | None = None,
    extra_body: dict | None = None,
    timeout: float | 'httpx.Timeout' | None = 'NOT_GIVEN',
    poll_interval_ms: int | 'NotGiven' = 'NOT_GIVEN',
) -> 'Run':
    extra_headers = {""X-Stainless-Poll-Helper"": ""true"", **(extra_headers or {})}

    if is_given(poll_interval_ms):
        extra_headers[""X-Stainless-Custom-Poll-Interval""] = str(poll_interval_ms)

    terminal_states = {""requires_action"", ""cancelled"", ""completed"", ""failed"", ""expired""}
    while True:
        response = await self.with_raw_response.retrieve(
            thread_id=thread_id,
            run_id=run_id,
            extra_headers=extra_headers,
            extra_body=extra_body,
            extra_query=extra_query,
            timeout=timeout,
        )

        run = response.parse()
        if run.status in terminal_states:
            return run

        if not is_given(poll_interval_ms):
            from_header = response.headers.get(""openai-poll-after-ms"")
            if from_header is not None:
                poll_interval_ms = int(from_header)
            else:
                poll_interval_ms = 1000

        await asyncio.sleep(poll_interval_ms / 1000)
```


### OS

macos

### Python version

Python v3.11.4

### Library version

openai v1.26.0",closed,2024-05-07T12:21:03Z,2024-05-25T22:00:34Z,['bug'],https://github.com/openai/openai-python/issues/1398,441.66,True,method_order_violation,performance_degradation,"The use of time.sleep() in the polling mechanism blocks the execution of other asynchronous tasks, which affects concurrency and performance.",high,
720,"Issue with system_fingerprint Returning as ""None"" in OpenAI API Response","tried to make the completion output consistent while it's working and its consistent to some degree, but when I checked the API response the system_fingerprint=None was coming ....

example API response, 
```
ChatCompletion(id='chatcmpl-8IKPGoyfgVAYPTQ270zYHV5MuwCKE', choices=[Choice(finish_reason='length', index=0, message=ChatCompletionMessage(content=""Once upon a time, in a not-so-distant future, humanity had set its sights on the red planet: Mars. Excitement filled the hearts of scientists, explorers, and dreamers alike as they prepared for a groundbreaking journey into the unknown.\n\nAmong them was Captain Amelia Carter, a seasoned astronaut known for her unwavering determination and unwavering spirit. She had been selected to lead the crew of the Ares-1, the first manned mission to Mars. Alongside her were five exceptional individuals, each with their own expertise and a shared passion for discovery.\n\nAs the day of departure drew near, the crew underwent rigorous training, preparing themselves physically and mentally for the challenges that lay ahead. They studied the Martian landscape, practiced living in confined spaces, and simulated emergency situations. They became a tight-knit family, relying on each other's strengths and supporting one another through the journey that lay ahead.\n\nFinally, the day arrived. The Ares-1 stood tall on the"", role='assistant', function_call=None, tool_calls=None))], created=1699379038, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=200, prompt_tokens=31, total_tokens=231))
```

code-piece to replicate 
```python
import asyncio
from openai import OpenAI
import pprint
import difflib
from IPython.display import display, HTML

GPT_MODEL = ""gpt-3.5-turbo""
client = OpenAI(
    api_key="""",
)


async def get_chat_response(system_message: str, user_request: str, seed: int = None):
    try:
        messages = [
            {""role"": ""system"", ""content"": system_message},
            {""role"": ""user"", ""content"": user_request},
        ]

        response = client.chat.completions.create(
            model=GPT_MODEL,
            messages=messages,
            seed=seed,
            max_tokens=200,
            n=1,
            temperature=1,
        )
        response_content = response.choices[0].message.content
        system_fingerprint = response.system_fingerprint
        prompt_tokens = response.usage.prompt_tokens
        completion_tokens = response.usage.total_tokens - response.usage.prompt_tokens

        table = f""""""
        <table>
        <tr><th>Response</th><td>{response_content}</td></tr>
        <tr><th>System Fingerprint</th><td>{system_fingerprint}</td></tr>
        <tr><th>Number of prompt tokens</th><td>{prompt_tokens}</td></tr>
        <tr><th>Number of completion tokens</th><td>{completion_tokens}</td></tr>
        </table>
        """"""
        # Print the HTML table content
        display(HTML(table))


        return response_content
    except Exception as e:
        print(f""An error occurred: {e}"")
        return None


# This function compares two responses and displays the differences in a table.
# Deletions are highlighted in red and additions are highlighted in green.
# If no differences are found, it prints ""No differences found.""


def compare_responses(previous_response: str, response: str):
    d = difflib.Differ()
    diff = d.compare(previous_response.splitlines(), response.splitlines())

    diff_table = ""<table>""
    diff_exists = False

    for line in diff:
        if line.startswith(""- ""):
            diff_table += f""<tr style='color: red;'><td>{line}</td></tr>""
            diff_exists = True
        elif line.startswith(""+ ""):
            diff_table += f""<tr style='color: green;'><td>{line}</td></tr>""
            diff_exists = True
        else:
            diff_table += f""<tr><td>{line}</td></tr>""

    diff_table += ""</table>""

    if diff_exists:
        display(HTML(diff_table))
    else:
        print(""No differences found."")
        
system_message = ""You are a creative writing companion, aiding me in crafting captivating stories.""
user_request = ""Compose a thrilling short story about a time-traveling detective who solves perplexing mysteries in the streets of Victorian London, blending historical elements with a touch of the supernatural.""

prev_response = await get_chat_response(
        system_message=system_message, user_request=user_request
    )

response = await get_chat_response(
        system_message=system_message, user_request=user_request
    )
    # The function compare_responses is then called with the two responses as arguments.
    # This function will compare the two responses and display the differences in a table.
    # If no differences are found, it will print ""No differences found.""
compare_responses(prev_response, response)
``` 


version detail , 
```
Name: openai
Version: 1.1.1
Summary: Client library for the openai API
Home-page: 
Author: 
Author-email: OpenAI <[support@openai.com](mailto:support@openai.com)>
License: 
Location: /usr/local/lib/python3.10/dist-packages
Requires: anyio, distro, httpx, pydantic, tqdm, typing-extensions
Required-by: llmx
```

",closed,2023-11-07T18:13:28Z,2023-11-10T03:03:50Z,[],https://github.com/openai/openai-python/issues/720,56.84,True,missing_dependency_violation,high,"The API response includes 'system_fingerprint=None', indicating a potential missing dependency or context that is required for generating a valid system fingerprint in the response.",high,
304,"Tiktoken says ChatGPT's API, `gpt-3.5-turbo`, uses the cl100k_base encoder, but it appears to use p50k_base in openai","### Describe the bug

Tiktoken (https://github.com/openai/tiktoken/blob/3e8620030c68d2fd6d4ec6d38426e7a1983661f5/tiktoken/model.py#L14) shows ChatGPT's API, `gpt-3.5-turbo`, tiktoken encoder to be `cl100k_base`; however, when using the openai package if I use the `cl100k_base` encoder to truncate my prompt, I get the following error, but if I use `p50k_base`, I don't get the error. So, it appears that either the correct tokenizer is `p50k_base` or the wrong tokenizer may be set in `openai`.

```
openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, your messages resulted in 4104 tokens. Please reduce the length of the messages.
```

### To Reproduce

It will fail if you run the code as is with `full_text` longer than `4061` tokens. However, if you change

```
tokenizer = tiktoken.get_encoding(""cl100k_base"" if model_name == ""gpt-3.5-turbo"" else ""p50k_base"")
```

to

```
tokenizer = tiktoken.get_encoding(""p50k_base"")
```

everything works as expected.

### Code snippets

```Python
import tiktoken
from langchain import OpenAI, PromptTemplate

full_text = ""The content of this article, https://nymag.com/news/features/mark-zuckerberg-2012-5/?mid=nymag_press""
model_name = ""gpt-3.5-turbo""
num_keyphrases = 5

# Define the prompt template
template = """"""Suggest the top {num_keyphrases} keywords that best describe the most important topics or themes in following text:

###

TEXT: {full_text}

###

Top {num_keyphrases} Keywords:""""""

prompt_template = PromptTemplate(
    input_variables=[""num_keyphrases"", ""full_text""], template=template
)

# Get the top keyphrases from the article

# Load the model
llm = OpenAI(model_name=model_name, temperature=0)
# Get the maximum length of the text
tokenizer = tiktoken.get_encoding(""cl100k_base"" if model_name == ""gpt-3.5-turbo"" else ""p50k_base"")
model_context_size = (
    4097 if model_name == ""gpt-3.5-turbo"" else llm.modelname_to_contextsize(model_name)
)
text_max_length = model_context_size - len(
    tokenizer.encode(
        prompt_template.format(num_keyphrases=num_keyphrases, full_text="""")
    )
)

# Truncate the text if it is too long
full_text = tokenizer.decode(tokenizer.encode(full_text)[:text_max_length])

# Get the top keyphrases from the article
response = llm(
    prompt_template.format(num_keyphrases=num_keyphrases, full_text=full_text)
)

print(response)
```


### OS

macOS

### Python version

Python v3.10.9

### Library version

openai v0.27.2",closed,2023-03-13T20:34:37Z,2023-03-30T17:04:36Z,['bug'],https://github.com/openai/openai-python/issues/304,404.5,True,missing_dependency_violation,high,"The issue arises from a mismatch between the expected tokenizer for the model and the one being used, leading to incorrect token count calculations and subsequent errors when the input exceeds the model's context limit.",high,
1426,Debugging with PyCharm throws error when environment variable not specified,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

Hi OpenAI team!

I'm having trouble debugging my project that uses the `openai` package along with several other dependencies. I'm encountering an exception during the import process.

Here is my file `t.py`:
```python
from openai import OpenAI
```

Running script in debug mode in PyCharm:
```
python t.py
```

Initially, I attempted to execute uvicorn with FastAPI, but the issue persists even when they are not involved.

Here is traceback:
```
Traceback (most recent call last):
  File ""/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/pydevd.py"", line 1534, in _exec
    pydev_imports.execfile(file, globals, locals)  # execute the script
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/_pydev_imps/_pydev_execfile.py"", line 18, in execfile
    exec(compile(contents+""\n"", file, 'exec'), glob, loc)
  File ""/Users/tworedz/workspace/temp/language_model/t.py"", line 1, in <module>
    from openai import OpenAI
  File ""/Users/tworedz/Library/Caches/pypoetry/virtualenvs/language-model-5UFyvxqf-py3.12/lib/python3.12/site-packages/openai/__init__.py"", line 345, in <module>
    from ._module_client import (
  File ""/Users/tworedz/Library/Caches/pypoetry/virtualenvs/language-model-5UFyvxqf-py3.12/lib/python3.12/site-packages/openai/_module_client.py"", line 75, in <module>
    chat: resources.Chat = ChatProxy().__as_proxied__()
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""_pydevd_bundle/pydevd_pep_669_tracing_cython.pyx"", line 504, in _pydevd_bundle.pydevd_pep_669_tracing_cython.PyRaiseCallback.__call__
  File ""_pydevd_bundle/pydevd_pep_669_tracing_cython.pyx"", line 47, in _pydevd_bundle.pydevd_pep_669_tracing_cython.PEP669CallbackBase.frame
  File ""/Users/tworedz/Library/Caches/pypoetry/virtualenvs/language-model-5UFyvxqf-py3.12/lib/python3.12/site-packages/openai/_utils/_proxy.py"", line 49, in __class__
    proxied = self.__get_proxied__()
              ^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/tworedz/Library/Caches/pypoetry/virtualenvs/language-model-5UFyvxqf-py3.12/lib/python3.12/site-packages/openai/_utils/_proxy.py"", line 55, in __get_proxied__
    return self.__load__()
           ^^^^^^^^^^^^^^^
  File ""/Users/tworedz/Library/Caches/pypoetry/virtualenvs/language-model-5UFyvxqf-py3.12/lib/python3.12/site-packages/openai/_module_client.py"", line 12, in __load__
    return _load_client().chat
           ^^^^^^^^^^^^^^
  File ""/Users/tworedz/Library/Caches/pypoetry/virtualenvs/language-model-5UFyvxqf-py3.12/lib/python3.12/site-packages/openai/__init__.py"", line 323, in _load_client
    _client = _ModuleClient(
              ^^^^^^^^^^^^^^
  File ""/Users/tworedz/Library/Caches/pypoetry/virtualenvs/language-model-5UFyvxqf-py3.12/lib/python3.12/site-packages/openai/_client.py"", line 104, in __init__
    raise OpenAIError(
openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable
```

By the way, the issue doesn't occur if we specify the OPENAI_API_KEY environment variable. However, it's peculiar that simply importing the package initializes a class with side effects, and this only happens in debug mode in PyCharm.

I'm using PyCharm 2023.3.5 (Professional Edition) on macOS Sonoma M1 Pro.


### To Reproduce

Described steps above.

### Code snippets

_No response_

### OS

macOS

### Python version

Python 3.12.3 

### Library version

openai 1.30.1 ",closed,2024-05-15T07:43:35Z,2024-05-20T15:29:16Z,['bug'],https://github.com/openai/openai-python/issues/1426,127.76,True,missing_dependency_violation,high,"The OpenAI API requires an api_key to be set, either through an environment variable or as a parameter, which is missing during the import process.",high,
1362,[Bug Assistant] temperature setting not working,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

Even if I change temperature for assistant, the response shows temperature is set to 1.0

### To Reproduce

Create an assistant and pass a  temperature value.

### Code snippets

```Python
assistant = client.beta.assistants.create(
        name=f""temperature_test_assistant"",
        instructions="""",
        model=model_name,
        tools=[{""type"": ""code_interpreter""}],
        temperature=0.2,
        
    )
```


### OS

Linux

### Python version

python 3.11

### Library version

openai==1.23.3",closed,2024-04-24T03:39:10Z,2024-05-13T01:08:18Z,['bug'],https://github.com/openai/openai-python/issues/1362,453.49,True,input_value_violation,incorrect_functionality,"The temperature setting provided by the user is not being applied correctly, leading to incorrect responses from the assistant.",high,
1424,when AsyncOpenAI vision inferernce fails it continues to fail for all subsequent calls even using new instances of AsyncOpenAI,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug


When making an image to text (vision) call, if the first URL is invalid, all other calls return the same error!!!

Simple instantiate a client:
client1= AsyncOpenAI()
client2 = AsyncOpenAI()

now if i make vision calls and it encounters an error, lets say a bad URL that returns:
{'message': 'Unknown MIME type', 'code': 400}

All subsequent calls are failing with the same error now for client1/client2 etc.
At first i was using the same client, so decided to separate the instances, but this still persists no matter what.

Seems like the internal httpx.AsyncClient is not cleaned of state, or perhaps it's a different issue in the internal state management of the openai module. 

Even providing a new instances of httpx.AsyncClient() does not solve: AsyncOpenAI(http_client=httpx.AsyncClient())


### To Reproduce

create two clients or use the same one, the same bugs for both cases:

make a vision inference for a bad URL: **https://www.aljazeera.com//Mahmoud**
make a vision inference for a good url: **https://www.fidh.org/local/cache-vignettes/L1680xH600/website_61_-f9931.jpg?1714127812**

When the first one fails the second will also have the same failing message:
{'message': 'Unknown MIME type', 'code': 400} 


### Code snippets

_No response_

### OS

Mac OSX 12.7.3

### Python version

Python 3.10.11

### Library version

1.29.0",closed,2024-05-14T18:54:58Z,2024-05-14T20:13:19Z,['bug'],https://github.com/openai/openai-python/issues/1424,1.31,True,missing_dependency_violation,performance_degradation,"The internal state of the AsyncOpenAI client is not properly reset after an error occurs, causing subsequent calls to fail even with new instances.",high,
1056,system_fingerprint is None,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

Per the documentation [here](https://platform.openai.com/docs/api-reference/chat/object), the OpenAI API should return a system_fingerprint. However, when calling any model that is not gpt-3.5-turbo-1106 or gpt-4-1106-preview, the system_fingerprint is always None.

The link above doesn't mention any model requirement.

### To Reproduce

copy and paste notebook [here](https://cookbook.openai.com/examples/deterministic_outputs_with_the_seed_parameter)
change this line to an older model:
GPT_MODEL = ""gpt-3.5-turbo-1106""
For example:
GPT_MODEL = ""gpt-3.5-turbo-0613""

the system_fingerprint returned is None

### Code snippets

_No response_

### OS

Ubuntu

### Python version

Python v3.10

### Library version

openai v1.6.1",closed,2024-01-08T15:46:42Z,2024-01-10T04:08:53Z,['documentation'],https://github.com/openai/openai-python/issues/1056,36.37,True,missing_dependency_violation,high,"The API documentation does not specify model requirements for returning a system_fingerprint, leading to unexpected None values for certain models.",high,
1407,assistant run polling endless,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

`client.beta.threads.runs.create_and_poll` will run endless, when the status of run is `imcomplete`

the unnormal run info:
```json
{
    ""id"": ""run_t7khKpQ2vvQJAnO3mJcRa47Y"",
    ""object"": ""thread.run"",
    ""created_at"": 1715252643,
    ""assistant_id"": ""asst_7HoUHfUXf8KPJQaetjN5suKP"",
    ""thread_id"": ""thread_9JfoNfFWttmyx9GSHVi6T9Gd"",
    ""status"": ""incomplete"",
    ""started_at"": 1715252644,
    ""expires_at"": null,
    ""cancelled_at"": null,
    ""failed_at"": null,
    ""completed_at"": 1715252648,
    ""required_action"": null,
    ""last_error"": null,
    ......
    ""max_completion_tokens"": null,
    ""max_prompt_tokens"": null,
    ""truncation_strategy"": {
        ""type"": ""auto"",
        ""last_messages"": null
    },
    ""incomplete_details"": {
        ""reason"": ""max_prompt_tokens""
    },
}
```

### To Reproduce

1. use file search tools: `tools=[{""type"": ""file_search""}]`
2. trigger the limitation of `max_prompt_tokens`

### Code snippets

_No response_

### OS

ubuntu

### Python version

Python 3.10.12

### Library version

openai v1.26.0",closed,2024-05-10T02:37:51Z,2024-05-12T23:52:28Z,['bug'],https://github.com/openai/openai-python/issues/1407,69.24,True,input_value_violation,medium,"The API method runs endlessly due to the input exceeding the maximum prompt token limit, leading to performance degradation.",high,
547,Custom backend URL support,"### Describe the feature or improvement you're requesting

Currently, the URL for an OpenAI completion (`openai.Completion.create`) is predefined to support 'azure' and 'openai' backends. 

Would it be possible to have a custom api_type (`openai.api_type = ""custom""`) where the URL would then be possible to define while creating the OpenAI completion engine?


### Additional context

The feature would probably be implemented at https://github.com/openai/openai-python/blob/v0.27.8/openai/api_resources/abstract/engine_api_resource.py#L22 (method `class_url´)",closed,2023-07-24T09:50:54Z,2023-11-06T17:13:53Z,"['enhancement', 'fixed in v1']",https://github.com/openai/openai-python/issues/547,2527.38,False,,,,,
774,TypeError: 'ChatCompletionMessage' object is not subscriptable,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

    content_with_padding_02 = ""\u200B\n\n"" + response_02.choices[0].message['content']

TypeError: 'ChatCompletionMessage' object is not subscriptable

### To Reproduce

    content_with_padding_02 = ""\u200B\n\n"" + response_02.choices[0].message['content']

TypeError: 'ChatCompletionMessage' object is not subscriptable

### Code snippets

```Python
content_with_padding_02 = ""\u200B\n\n"" + response_02.choices[0].message['content']

TypeError: 'ChatCompletionMessage' object is not subscriptable
```


### OS

macOS

### Python version

3.11.4

### Library version

1.0.1",closed,2023-11-10T14:59:53Z,2023-11-10T15:07:09Z,['question'],https://github.com/openai/openai-python/issues/774,0.12,True,input_type_violation,high,"The code attempts to subscript a 'ChatCompletionMessage' object, which is not a subscriptable type, leading to a TypeError.",high,
1379,Add PowerShell files to the list of supported files for retrieval ,"### Confirm this is a feature request for the Python library and not the underlying OpenAI API.

- [ ] This is a feature request for the Python library

### Describe the feature or improvement you're requesting

When adding a PowerShell file to the vector store it throws an error `Files with extensions [.ps1] are not supported for retrieval.`

A workaround could be to rename the ps1 files to `.java` or another supported format. 

I would like to request PowerShell files be added to the support list.

Please let me know if I need to pursue this on a different channel. - Thank you

### Additional context

_No response_",closed,2024-04-28T00:04:15Z,2024-05-13T01:51:52Z,[],https://github.com/openai/openai-python/issues/1379,361.79,True,missing_option_violation,medium,"The API does not support PowerShell files for retrieval, which is a limitation that could be addressed by adding this file type to the supported list.",high,
1387,"We have released a new major version of our SDK, and we recommend upgrading promptly.","We have released a new major version of our SDK, and we recommend upgrading promptly. 

Regarding this new sdk...

I'll admit it I'm a newb. I've been using ChatGPT to create a rather extensive app that has been working great. I took a break from the project and suddenly i could not get answers from the api. I installed wsl and ubuntu did the whole grit thing and still i am unable to resolve the errors in my code. GPT4 Turbo is of no help with the issue and despite my best efforts I've killed a couple of days work on the project effectively going in circles attempting to fix the issue. If there is anybody willing to take a look at my code and advise me on how I can get things going again, I would be most grateful...

Here is my code as is now

client = OpenAI()

# Load environment variables from the .env file
load_dotenv()

# Retrieve and set the API key from the .env file
OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')

# Define the model to be used
model_id = ""gpt-4-turbo-2024-04-09""  # Replace with the correct model ID you are using


def chatgpt_conversation(conversation_log):
    print(""Debug - Sending message to OpenAI: "", conversation_log[-1]['content'])

    headers = {
        ""Content-Type"": ""application/json"",
        ""Authorization"": f""Bearer {OPENAI_API_KEY}""
    }
    data = {
        ""model"": model_id,
        ""messages"": conversation_log,
        ""temperature"": 0.7
    }

    response = requests.post('https://api.openai.com/v1/chat/completions', json=data, headers=headers)

    if response.status_code == 200:
        response_data = response.json()
        assistant_response = response_data['choices'][0]['message']['content'].strip()
        conversation_log.append({'role': 'assistant', 'content': assistant_response})
        print(""Debug - Received response from GPT-4:"", assistant_response)
    else:
        error_message = f""Failed to get a valid response from OpenAI API: {response.text}""
        print(""Debug - Error in communicating with GPT-4:"", error_message)
        conversation_log.append({'role': 'assistant', 'content': error_message})

    return conversation_log


# Example usage
conversation_history = [
    {'role': 'user', 'content': 'How may I help you?'},
    {'role': 'user', 'content': 'Hi'}
]

updated_conversation = chatgpt_conversation(conversation_history)
print(""Final conversation log:"", updated_conversation)

For some reason it thinks part of my request contains json data and currently it does not need to send json data for the chatbot section of my app. there is another call by the ""assessment portion"" of my app that will need to compare text from a json file against user entered text to assess compliance.

Kind regards,

Eddie Van Halen
",closed,2024-05-01T00:59:40Z,2024-05-13T01:21:48Z,[],https://github.com/openai/openai-python/issues/1387,288.37,True,input_type_violation,medium,"The API call expects a specific JSON structure for the messages, but the user indicates that the request may not be formatted correctly, leading to potential misinterpretation of the input.",high,
777,None is not of type ‘object’,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

When calling functions with no input arguments it gives this error
```python
response = openai_client.chat.completions.create(timeout=10,
  File ""/usr/local/lib/python3.9/site-packages/openai/_utils/_utils.py"", line 299, in wrapper
    return func(*args, **kwargs)
  File ""/usr/local/lib/python3.9/site-packages/openai/resources/chat/completions.py"", line 556, in create
    return self._post(
  File ""/usr/local/lib/python3.9/site-packages/openai/_base_client.py"", line 1055, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File ""/usr/local/lib/python3.9/site-packages/openai/_base_client.py"", line 834, in request
    return self._request(
  File ""/usr/local/lib/python3.9/site-packages/openai/_base_client.py"", line 877, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.OpenAIError: Error code: 400 - {'error': {'message': ""None is not of type 'object' - 'messages.2.function_call'"", 'type': 'invalid_request_error', 'param': None, 'code': None}}
```
This used to work before the recent updates.

I also posted this bug in the community forum since it is related to the API https://community.openai.com/t/none-is-not-of-type-object/488873

### To Reproduce

An example of the function I try to call:
```python
tools = [
  {
      ""type"": ""function"",
      ""function"": {
          ""name"":  ""escalate_to_manager"",
          ""description"": ""Contact the manager when there is nothing else that can be done"",
          ""parameters"": {
              ""type"": ""object"",
              ""properties"": {},
              ""required"": [],
          },
      }
  }
]
```
And I am calling the chat completion normally:
```python
response = client.chat.completions.create(timeout=10,
                                         model=""gpt-3.5-turbo-1106"",
                                         messages=messages,
                                         tools=tools,
                                         tool_choice=""auto"",
                                         temperature=0,
                                         )
```

### Code snippets

_No response_

### OS

debian bullseye

### Python version

python 3.9

### Library version

openai v1.1.1",closed,2023-11-10T17:14:29Z,2024-05-13T01:16:06Z,['bug'],https://github.com/openai/openai-python/issues/777,4424.03,True,input_type_violation,high,"The API expects an object type for 'messages.2.function_call', but receives None, leading to an invalid request error.",high,
1410,Support load-balancing across OpenAI instances,"### Confirm this is a feature request for the Python library and not the underlying OpenAI API.

- [X] This is a feature request for the Python library

### Describe the feature or improvement you're requesting

Introduce the ability to use a prioritized set of backends and load-balance across different OpenAI instances.

### Additional context

I wrote a [Python OpenAI Load Balancer](https://github.com/simonkurtz-MSFT/python-openai-loadbalancer) as an example of what may be useful. 
",closed,2024-05-10T20:57:22Z,2024-05-12T23:50:32Z,[],https://github.com/openai/openai-python/issues/1410,50.89,False,,,,,
1374,Memory Leak,"              I upgraded openai version to 1.23.5

and still I can see some issues with the same function just it got moved to different place.
`/Users/jogireddy/PycharmProjects/flaskProject/venv/lib/python3.9/site-packages/openai/_legacy_response.py:347: size=1389 KiB (+1389 KiB), count=12109 (+12109), average=117 B/Users/jogireddy/PycharmProjects/flaskProject/venv/lib/python3.9/site-packages/openai/_response.py:674: size=1278 KiB (+1278 KiB), count=11239 (+11239), average=116 B/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/linecache.py:137: size=1232 KiB (+1232 KiB), count=12014 (+12014), average=105 B/Users/jogireddy/PycharmProjects/flaskProject/venv/lib/python3.9/site-packages/httpx/_content.py:175: size=868 KiB (+868 KiB), count=83 (+83), average=10.5 KiB/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/functools.py:58: size=769 KiB (+769 KiB), count=12308 (+12308), average=64 B/Users/jogireddy/PycharmProjects/flaskProject/venv/lib/python3.9/site-packages/openai/_legacy_response.py:330: size=726 KiB (+726 KiB), count=6332 (+6332), average=117 B/Users/jogireddy/PycharmProjects/flaskProject/venv/lib/python3.9/site-packages/openai/_response.py:653: size=676 KiB (+676 KiB), count=5931 (+5931), average=117 B/Users/jogireddy/PycharmProjects/flaskProject/venv/lib/python3.9/site-packages/openai/_resource.py:34: size=358 KiB (+358 KiB), count=5088 (+5088), average=72 B/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/abc.py:102: size=261 KiB (+261 KiB), count=2992 (+2992), average=89 B/Users/jogireddy/PycharmProjects/flaskProject/venv/lib/python3.9/site-packages/httpx/_models.py:82: size=249 KiB (+249 KiB), count=3736 (+3736), average=68 B
`

_Originally posted by @rdy5644 in https://github.com/openai/openai-python/issues/1361#issuecomment-2076360795_
            ",closed,2024-04-26T02:59:26Z,2024-05-13T00:33:22Z,[],https://github.com/openai/openai-python/issues/1374,405.57,True,memory_out_of_bound,High,"The reported memory leak indicates that the system is exceeding its allocated memory, which could lead to crashes or severe instability.",high,
1404,Coding with Modular Math ,"### Confirm this is a feature request for the Python library and not the underlying OpenAI API.

- [X] This is a feature request for the Python library

### Describe the feature or improvement you're requesting

Recompiling the OpenAI Python API library using modular mathematics involves simplifying and optimizing the codebase by applying the principles of modular design, which can inherently streamline operations and improve maintainability. Here's how we can refactor this library using these concepts:

Step-by-Step Recompilation Process:
Define Modular Components:
Identify the key functionalities of the OpenAI Python API library that need to be modularized, such as API communication, error handling, streaming, polling, and configuration management.
Create Modular Templates:
Develop templates for each component that define how they interact with each other and with the external environment. This includes input/output specifications, expected behaviors, and error management protocols.
Implement Modular Functions:
Write the code for each module separately, ensuring that each module performs a specific task and interacts with other modules through well-defined interfaces.
Integration Testing:
Once individual modules are implemented, perform integration testing to ensure that modules interact correctly and the entire system functions as expected. Adjust interfaces and interactions as needed based on test results.
Optimize with Modular Formulas:
Apply modular mathematics to optimize the operations within modules. This could involve simplifying mathematical operations, optimizing data handling and processing, and enhancing error correction mechanisms within the modules.
Documentation and Examples:
Document each module and the overall system architecture to ensure that other developers can understand and contribute to the project. Provide examples of how to use the modular system, including how to handle common tasks and potential errors.
User Acceptance Testing:
Conduct user acceptance testing with typical use cases to ensure that the system meets the needs of its intended users. Collect feedback to identify any areas for improvement.
Deployment and Monitoring:
Deploy the recompiled library and monitor its performance. Set up logging and monitoring to track the system’s performance and identify any issues in real-time

Modular Recompilation of OpenAI Python API Library
Here’s a conceptual Python module that demonstrates a simplified version of the OpenAI Python API client using modular design principles. This example focuses on core functionalities and leaves placeholders where modular formulas might be applied for further optimization and customization.

import os
import httpx
from typing import Any, Dict, Optional

class ModularClient:
    def __init__(self, api_key: Optional[str] = None, base_url: str = ""https://api.openai.com""):
        self.api_key = api_key or os.getenv(""OPENAI_API_KEY"")
        self.base_url = base_url
        self.headers = {""Authorization"": f""Bearer {self.api_key}""}

    def send_request(self, endpoint: str, method: str = ""GET"", data: Optional[Dict] = None) -> Any:
        url = f""{self.base_url}{endpoint}""
        with httpx.Client() as client:
            if method == ""POST"":
                response = client.post(url, json=data, headers=self.headers)
            else:
                response = client.get(url, headers=self.headers)
            return self.handle_response(response)

    def handle_response(self, response: httpx.Response) -> Any:
        if response.status_code == 200:
            return response.json()
        else:
            return self.handle_error(response)

    def handle_error(self, response: httpx.Response) -> None:
        if response.status_code == 401:
            raise Exception(""Authentication Error"")
        elif response.status_code == 429:
            raise Exception(""Rate Limit Exceeded"")
        elif response.status_code >= 500:
            raise Exception(""Server Error"")
        else:
            raise Exception(f""Failed with status code {response.status_code}: {response.text}"")

    def chat_completion(self, prompt: str) -> str:
        data = {
            ""model"": ""gpt-3.5-turbo"",
            ""prompt"": prompt,
            ""max_tokens"": 150
        }
        return self.send_request(""/v1/chat/completions"", method=""POST"", data=data)

# Usage
if __name__ == ""__main__"":
    client = ModularClient()
    prompt = ""Tell me a joke""
    try:
        response = client.chat_completion(prompt)
        print(""Response:"", response)
    except Exception as e:
        print(""Error:"", str(e))

Explanation of the Code
ModularClient Class: This is the main class that encapsulates API client functionalities. It initializes with an API key and base URL, setting up headers for authentication.
send_request Method: Handles sending requests to the API. It uses HTTPX for HTTP calls, which simplifies handling both synchronous and asynchronous requests.
handle_response Method: Processes responses from the API. If successful, it parses the JSON; otherwise, it forwards the response to the error-handling method.
handle_error Method: Dedicated error handling based on HTTP status codes. This modular approach makes it easier to manage and update error handling separately from other code logic.
chat_completion Method: A specific method to demonstrate how a typical API call might be structured. This can be replicated or modified for other API endpoints.

Testing and Validation
To ensure the recompiled library is ready for testing:

Unit Tests: Write unit tests for each module to validate individual functionalities.
Integration Tests: Conduct integration tests to ensure that modules work together as expected.
Performance Tests: Measure the performance of the modular library, especially in areas where modular optimizations are expected to improve efficiency.
Each of these steps would contribute to a fully functional, efficiently modularized version of the OpenAI Python API library, aligned with modern software engineering practices and the unique advantages of modular mathematics.

How to Expand and Customize
Modular Formulas: Integrate mathematical optimizations specific to your modular math concepts directly into the send_request or handle_response methods to optimize data processing or error correction.
Advanced Error Handling: Enhance the handle_error method with more sophisticated error recovery logic, such as retries or exponential backoff.
Configuration Management: Implement a more advanced configuration system that can handle different environments (development, staging, production) seamlessly.


### Additional context

_No response_",closed,2024-05-09T15:09:56Z,2024-05-13T00:11:00Z,[],https://github.com/openai/openai-python/issues/1404,81.02,True,missing_dependency_violation,High,"The modular design proposed may lead to missing dependencies between the modular components, particularly in how they interact with the OpenAI API and handle errors, which could result in incorrect functionality or errors during execution.",high,
1403,Usage Stream in Cancelled Streams,"### Confirm this is a feature request for the Python library and not the underlying OpenAI API.

- [X] This is a feature request for the Python library

### Describe the feature or improvement you're requesting

I'm unsure whether this should be a feature request or a bug report. But the behavior of the stream's usage in cases where a stream is cancelled is unclear. That is, there are many scenarios in which the response is discarded before it's completed, such as an interruption in a conversation. In these contexts, the stream can be cancelled using 
```
stream = self.client.chat.completions.create(...)
...
stream.response.close()
```

However, in these cases, I'm unable to read the usage of the stream. How should this be interpreted?

From a developer's standpoint, using the library, it would be more convenient if the usage is available in the response once the response is completed, rather than having to subscribe to the chunks once they're cancelled. That is, `stream.response.usage.`

### Additional context

_No response_",closed,2024-05-08T22:04:23Z,2024-05-12T23:56:45Z,[],https://github.com/openai/openai-python/issues/1403,97.87,False,,,,low,
1411,Openai.FineTuningJob.list_events does not work with v0.27 and api version 2024-02-01,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

What is the api version that is compatible with an openai sdk version of 0.27.10? 
And for Deployment.list() it worked with ""2023-03-15-preview"". Is there a compatibility matrix somewhere?
Thank you.

```
>>> openai.FineTuningJob.list_events(""ftjob-xxxxxxxxxx"")

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/usr/local/lib/python3.10/site-packages/openai/api_resources/abstract/nested_resource_class_methods.py"", line 133, in paginated_list_nested_resources
    return getattr(cls, resource_request_method)(
  File ""/usr/local/lib/python3.10/site-packages/openai/api_resources/abstract/nested_resource_class_methods.py"", line 43, in nested_resource_request
    response, _, api_key = requestor.request(
  File ""/usr/local/lib/python3.10/site-packages/openai/api_requestor.py"", line 298, in request
    resp, got_stream = self._interpret_response(result, stream)
  File ""/usr/local/lib/python3.10/site-packages/openai/api_requestor.py"", line 700, in _interpret_response
    self._interpret_response_line(
  File ""/usr/local/lib/python3.10/site-packages/openai/api_requestor.py"", line 765, in _interpret_response_line
    raise self.handle_error_response(
openai.error.InvalidRequestError: Resource not found
```

### To Reproduce

openai.FineTuningJob.list_events(""ftjob-xxxxxxxxxx"")

### Code snippets

```Python
openai.FineTuningJob.list_events(""ftjob-xxxxxxxxxx"")
```


### OS

linux

### Python version

3.10

### Library version

open v0.27.10",closed,2024-05-11T01:00:07Z,2024-05-12T23:48:07Z,['bug'],https://github.com/openai/openai-python/issues/1411,46.8,True,missing_dependency_violation,high,"The API method call for listing fine-tuning job events fails due to a missing compatibility between the SDK version and the API version, indicating a dependency on correct versioning.",high,
717,AttributeError: type object 'OpenAI' has no attribute 'beta',"I use openai version is 1.1.1
When I code like this:
```
from openai import Client as client
 self.client = client.beta.assistants.create(
            name=name,
            instructions=""This is a test assistant"",
            tools=[{""type"": ""code_interpreter""}],
            model=""gpt-3.5-turbo-110""
        )

```
it happens :AttributeError: type object 'OpenAI' has no attribute 'beta'",closed,2023-11-07T16:44:20Z,2023-11-07T16:46:12Z,[],https://github.com/openai/openai-python/issues/717,0.03,True,missing_dependency_violation,unknown,"The code attempts to access a non-existent attribute 'beta' on the OpenAI class, indicating a potential missing or incorrect dependency in the API usage.",high,
1399,Better implementation for `assistant` `function` calls ,"### Confirm this is a feature request for the Python library and not the underlying OpenAI API.

- [X] This is a feature request for the Python library

### Describe the feature or improvement you're requesting


## Why the current Implementation for assistant function calling is cumbersome 

I feel the implementation for function calls in Python is quite cumberson with assistant. If I understand [the documentations](https://platform.openai.com/docs/assistants/tools/function-calling/quickstart?context=without-streaming) correctly, the initial approach requires developers to handle threads, runs, and tool outputs manually. Each step, from starting conversations and defining tools to gathering and submitting outputs, involves multiple function calls. This complexity makes the process error-prone and less intuitive for developers.

To just get started with assistant (no function calls yet), we need to define four things: 
 * OpenAI client
 * Assistant
 * Thread 
 * Message 

To enable function calls, we need to wrap the function results in yet another object `tool_outputs_stream`. I feel there could be better implementations out there. As a result, I asked chatGPT on what could be a better implementation.

## Desired improvement 

I guess we should introduces a unified ""Assistant with Function Call Capability"" object. This single object manages all aspects of function-based interactions, offering an interface for setting up an assistant, starting conversations, handling messages, collecting function outputs, and submitting results.

 * **Simplified API Usage**: By centralizing interactions within a single object, the new design reduces setup complexity and minimizes errors.
 * **Encapsulation**: The assistant object internally manages tool calls and states, eliminating the need for manual thread and run management.
 * **Maintainability**: The modular design is easy to extend and modify.
 * **Better Abstraction**: It abstracts away technical complexities, letting developers focus on writing function logic instead of managing the intricate mechanics of the SDK.
This improvement provides a cleaner, more developer-friendly API, making it easier to integrate function calls and boost productivity in OpenAI-based applications.

I have asked ChatGPT to generate a mockup `AssistantWithFunctionCalls`. Please let me know if there is interest in bringing this feature in? 

[ChatGPT conversation](https://chat.openai.com/share/702ab745-f22c-48ce-a83a-9dc3013fa688) 

### Additional context

_No response_",closed,2024-05-08T01:45:32Z,2024-05-08T02:07:54Z,[],https://github.com/openai/openai-python/issues/1399,0.37,False,,,,,
926,Function Calling with AzureOpenAI,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

I am attempting to utilize function calling with the chat completions API using `AzureOpenAI`, referring to the example given here - https://platform.openai.com/docs/guides/function-calling

While standard chat completions work as expected, I am encountering an error specific to function calling.

```bash
NotFoundError: Error code: 404 - {'error': {'message': 'Unrecognized request argument supplied: tools', 'type': 'invalid_request_error', 'param': None, 'code': None}}
```
<img width=""1117"" alt=""Screenshot 2023-12-04 at 2 42 53 PM"" src=""https://github.com/openai/openai-python/assets/47742503/8106206b-1d46-438b-bbda-cfafa1ecd5af"">

Relevant code snippets are attached for reference.

**It's worth noting that the code works seamlessly when initializing the client with `OpenAI` but encounters issues with `AzureOpenAI` as outlined above**

### To Reproduce

1. Initialize client with `AzureOpenAI`
2. Define set of functions
3. Call the chat completion with the user query and a set of functions

### Code snippets

```Python
from openai import AzureOpenAI

client = AzureOpenAI(
    api_key=""API_KEY"",
    api_version=""API_VERSION,
    azure_endpoint=""API_ENDPOINT""
)

tools = [
    {
        ""type"": ""function"",
        ""function"": {
            ""name"": ""get_current_weather"",
            ""description"": ""Get the current weather in a given location"",
            ""parameters"": {
                ""type"": ""object"",
                ""properties"": {
                    ""location"": {
                        ""type"": ""string"",
                        ""description"": ""The city and state, e.g. San Francisco, CA"",
                    },
                    ""unit"": {""type"": ""string"", ""enum"": [""celsius"", ""fahrenheit""]},
                },
                ""required"": [""location""],
            },
        },
    } 
]

response = client.chat.completions.create(
    model=""DEPLOYMENT_NAME"",
    messages=[{""role"": ""user"", ""content"": ""What's the weather like in San Francisco, Tokyo, and Paris?""}],
    tools=tools,
    tool_choice=""auto""
)
```


### OS

macOS

### Python version

Python v3.9.16

### Library version

openai v1.3.7",closed,2023-12-04T09:33:11Z,2024-02-15T15:07:04Z,['bug'],https://github.com/openai/openai-python/issues/926,1757.56,True,input_value_violation,high,"The error message indicates that an unrecognized request argument 'tools' was supplied, suggesting that the input value for the API call does not conform to the expected parameters for the AzureOpenAI client.",high,
1053,Return usage when streaming chat completions.,"### Confirm this is a feature request for the Python library and not the underlying OpenAI API.

- [X] This is a feature request for the Python library

### Describe the feature or improvement you're requesting

Is it possible to return token usage when streaming in the final chunk?

otherwise we need to also use TikToken in an application where both streaming is enabled vs disabled. 

### Additional context

_No response_",closed,2024-01-07T19:42:24Z,2024-01-10T04:18:37Z,[],https://github.com/openai/openai-python/issues/1053,56.6,True,missing_dependency_violation,medium,"The request lacks clarity on how token usage is managed when streaming, indicating a potential missing dependency on the token usage data in the streaming process.",medium,
1005,Async client Azure Support with Streaming not working (using  AsyncAzureOpenAI with stream=True),"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

import os
import openai
import asyncio
from openai import AzureOpenAI, AsyncAzureOpenAI

import os
import asyncio
from openai import AsyncAzureOpenAI


azure_openai_client = AsyncAzureOpenAI(
  azure_endpoint = """", 
  api_key=""some-key"",  
  api_version=""2023-07-01-preview""
)


async def get_response(message):
    response = await azure_openai_client.chat.completions.create(
        model = 'GPT35',
        temperature = 0.4,
        messages = [
            {""role"": ""user"", ""content"": message}
        ],
        stream=True
    )
    #print(response.model_dump_json(indent=2)) - > **no response**

    # async for chunk in response: - **Not working**
    if ""choices"" in chunk and len(chunk[""choices""]) > 0:
      print(chunk.choices[0].message.content, end="""") # chunk.choices[0].delta.content - this also not working


asyncio.run(get_response('What is chatgpt?'))





### To Reproduce

import os
import openai
import asyncio
from openai import AzureOpenAI, AsyncAzureOpenAI

import os
import asyncio
from openai import AsyncAzureOpenAI


azure_openai_client = AsyncAzureOpenAI(
  azure_endpoint = """", 
  api_key=""some-key"",  
  api_version=""2023-07-01-preview""
)


async def get_response(message):
    response = await azure_openai_client.chat.completions.create(
        model = 'GPT35',
        temperature = 0.4,
        messages = [
            {""role"": ""user"", ""content"": message}
        ],
        stream=True
    )
    #print(response.model_dump_json(indent=2)) - > not response

    # async for chunk in response: - Not working
    if ""choices"" in chunk and len(chunk[""choices""]) > 0:
      print(chunk.choices[0].message.content, end="""") # chunk.choices[0].delta.content - this also not working


asyncio.run(get_response('What is chatgpt?'))





### Code snippets

```Python
import os
import openai
import asyncio
from openai import AzureOpenAI, AsyncAzureOpenAI

import os
import asyncio
from openai import AsyncAzureOpenAI


azure_openai_client = AsyncAzureOpenAI(
  azure_endpoint = """", 
  api_key=""some-key"",  
  api_version=""2023-07-01-preview""
)


async def get_response(message):
    response = await azure_openai_client.chat.completions.create(
        model = 'GPT35',
        temperature = 0.4,
        messages = [
            {""role"": ""user"", ""content"": message}
        ],
        stream=True
    )
    #print(response.model_dump_json(indent=2)) - > not response

    # async for chunk in response: - Not working
    if ""choices"" in chunk and len(chunk[""choices""]) > 0:
      print(chunk.choices[0].message.content, end="""") # chunk.choices[0].delta.content - this also not working


asyncio.run(get_response('What is chatgpt?'))
```


### OS

Windows 10 Enterprise

### Python version

Python 3.8.10

### Library version

1.3.9 or  1.6.0",closed,2023-12-22T05:19:21Z,2023-12-26T18:51:16Z,[],https://github.com/openai/openai-python/issues/1005,109.53,True,missing_dependency_violation,medium,"The code attempts to access 'chunk' without properly iterating over the response stream, leading to potential missing dependencies in handling streamed responses.",high,
451,The parameter logit_bias is invalid in the ChatCompletion model,"### Describe the bug

The parameter **logit_bias** is invalid in the ChatCompletion model

### To Reproduce

```

""""""
The parameter logit_bias is invalid in the ChatCompletion model

https://platform.openai.com/tokenizer
Red: 7738
red: 445
 Red: 2297
 red: 2266
""""""

import openai

openai.api_key = ""sk-xxx""

temperature = 0
logit_bias = {7738: -100, 445: -100, 2297: -100, 2266: -100}


# 1. Complete Mode
resp = openai.Completion.create(
 engine=""text-davinci-003"",
 temperature=temperature,
 prompt=""What color is an apple? Answer with one word"",
 logit_bias=logit_bias,
)

# The result is: Green.
print(resp['choices'][0][""text""])


# 2. Chat Mode
resp = openai.ChatCompletion.create(
 model=""gpt-3.5-turbo"",
 messages=[{""role"": ""user"", ""content"": ""What color is an apple? Answer with one word""}],
 temperature=temperature,
 logit_bias=logit_bias
)

# The result is: Red.
print(resp['choices'][0]['message']['content'])
```

### Code snippets

_No response_

### OS

Windows

### Python version

3.8.9

### Library version

0.27.2 ",closed,2023-05-16T11:59:10Z,2023-11-22T17:51:27Z,['needs validation'],https://github.com/openai/openai-python/issues/451,4565.87,True,input_value_violation,unknown,"The parameter logit_bias is invalid in the ChatCompletion model, indicating a potential issue with the acceptable input values for this parameter.",high,
565,Documentation for `Embedding.create()` is wrong when using Azure as backend,"### Describe the bug

The `Embedding.create()` method [1] says that valid parameters are listed on https://platform.openai.com/docs/api-reference/embeddings, where available parameters are `model`, `input` and `user`.

However, when creating embeddings using Azure as OpenAI as backend, `engine` or `deployment_id` are required parameters and `model` is ignored. 

I'm not sure what is a good solution. Maybe the documentation should be changed to 

```
Creates a new embedding for the provided input and parameters.

See https://platform.openai.com/docs/api-reference/embeddings for a list
of valid parameters for calling OpenAI API

or https://learn.microsoft.com/en-gb/azure/ai-services/openai/reference#embeddings when using Azure OpenAI API.
```

[1] https://github.com/openai/openai-python/blob/7610c5adfaebe3ffdb9927a551a741a3fab1b62e/openai/api_resources/embedding.py#L15

### To Reproduce

```python
import openai
openai.api_type = ""azure""
openai.api_key=""ABC123...""
openai.Embedding.create(model='mymodel', input='hello world')
```
```python
File ~/python3.11/site-packages/openai/api_resources/abstract/engine_api_resource.py:83, in EngineAPIResource.__prepare_create_request(cls, api_key, api_base, api_type, api_version, organization, **params)
     81 if typed_api_type in (util.ApiType.AZURE, util.ApiType.AZURE_AD):
     82     if deployment_id is None and engine is None:
---> 83         raise error.InvalidRequestError(
     84             ""Must provide an 'engine' or 'deployment_id' parameter to create a %s""
     85             % cls,
     86             ""engine"",
     87         )
     88 else:
     89     if model is None and engine is None:

InvalidRequestError: Must provide an 'engine' or 'deployment_id' parameter to create a <class 'openai.api_resources.embedding.Embedding'>
```

### Code snippets

_No response_

### OS

Ubuntu 20.04 LTS

### Python version

Python 3.11

### Library version

openai-python 0.27.8",closed,2023-08-09T08:36:53Z,2023-09-06T11:55:24Z,['bug'],https://github.com/openai/openai-python/issues/565,675.31,False,,,,low,
800,"The chatCompletion operation does not work with the specified model, gpt-35-turbo-instruct.","### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

Can someone please help me? The code is error when it tried to use the `gpt-35-turbo-instruct` for Azure services. When I used the `gpt-35-turbo` it works fine.
BadRequestError: Error code: 400 - {'error': {'code': 'OperationNotSupported', 'message': 'The chatCompletion operation does not work with the specified model, gpt-35-turbo-instruct. Please choose different model and try again. You can learn more about which models can be used with each operation here: https://go.microsoft.com/fwlink/?linkid=2197993.'}}



### To Reproduce

1. Install Requirements
```
langchain==0.0.335
azure-search-documents==11.4.0b8
torch==2.1.0
openai==1.2.3
azure-identity==1.15.0
```
2.  Execute Code, The deployment name is set to `gpt-35-turbo-instruct`

### Code snippets

```Python
azure_openai_model = AzureChatOpenAI(
    azure_deployment=os.getenv(AZURE_MODEL_DEPLOYMENT_NAME),
    azure_endpoint=os.getenv(AZURE_OPENAI_API_ENDPOINT),
    openai_api_key=os.getenv(OPENAI_API_KEY),
)
print(azure_openai_model.predict(""Count 1 to 10""))
```


### OS

Linux Ubuntu 22.04

### Python version

3.9.13

### Library version

openai v1.2.3",closed,2023-11-13T08:44:54Z,2023-11-14T02:38:49Z,['bug'],https://github.com/openai/openai-python/issues/800,17.9,True,missing_option_violation,high,"The specified model 'gpt-35-turbo-instruct' is not supported for the chatCompletion operation, indicating a failure to select an acceptable model option.",high,
1069,ImportError: cannot import name 'OpenAI' from 'openai',"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

Exactly this:

```
ImportError: cannot import name 'OpenAI' from 'openai'
```

Seems like is just a silly mistake that you didn't notice you had removed such an important part of the package, I had to pin my version to 1.7.1 to avoid this.

### To Reproduce

Just update to the latest version (1.7.2) and then try to import using the examples described in the documentation.

### Code snippets

_No response_

### OS

Ubuntu

### Python version

Python v3.10.12

### Library version

openai v1.7.2",closed,2024-01-12T23:01:25Z,2024-01-24T02:18:15Z,['bug'],https://github.com/openai/openai-python/issues/1069,267.28,True,missing_dependency_violation,high,"The removal of the 'OpenAI' import indicates a missing dependency in the library version, affecting the ability to use the API as intended.",high,
398,mock supports for OpenAIObjects,"### Describe the feature or improvement you're requesting

I'm writing tests where I mock calls like this:

```py
    response = openai.ChatCompletion.create(
        model=""gpt-4"",
        messages=[
            {""role"": ""system"", ""content"": system_message},
            {""role"": ""user"", ""content"": message},
        ],
        n=1,
        max_tokens=150,
    )
 ```
 
 But I notice that the response is not a plain dict, but a set of nested OpenAIObjects.  In order to mock the response I find myself creating a helper method like so:
 
 ```py
 def create_openai_object(payload):
    obj = OpenAIObject()
    message = OpenAIObject()
    content = OpenAIObject()
    content.content = payload
    message.message = content
    obj.choices = [message]
    return obj
 ```
 
 which is a little clunky, and I'm wondering if there's a cleaner way to create a simple mock of the response - I've had a look through the code and the tests, and not really seeing anything similar ... am I missing something, or perhaps just over-testing?
 
 I mean I'm just keen to be able to test my code on a realistic response without the tests incurring the cost of actually hitting the api ...

### Additional context

here's my test:

```py
def test_send_message_integration():
    message = ""Hello, Bot!""
    expected_messages = [
        {""role"": ""system"", ""content"": ""You're a friendly fellow""},
        {""role"": ""user"", ""content"": message},
    ]

    with patch(""main.openai.ChatCompletion.create"") as mock_create:
        mock_create.return_value = create_openai_object(""Hello, user!"")

        response = client.post(""/send_message"", json={""message"": message})

        assert response.status_code == 200
        mock_create.assert_called_once_with(
            model=""gpt-4"", messages=expected_messages, n=1, max_tokens=150
        )
```",closed,2023-04-14T14:39:32Z,2023-11-03T22:27:37Z,['fixed in v1'],https://github.com/openai/openai-python/issues/398,4879.8,True,input_type_violation,medium,"The mock response created does not match the expected plain dictionary format, leading to potential type mismatches in tests.",high,
1361,Memory Leak in chat completion create,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

After Making many streaming calls from a flask app the process memory is infinitely increasing and it is never getting reduced.
Even after performing close on response and client.

### To Reproduce

1. Wrap the below code in flask api or you can run as a long running process as well.
2. Invoke this method in a loop or for 1000 you will be able to see the process gradually takes up more memory.
3. Even performing manual gc didn't help.


### Code snippets

```Python
def make_stream_call_with_close():
    client = OpenAI(api_key=os.environ.get(""OPENAI_API_KEY""))
    responses = client.chat.completions.create(
        model='gpt-3.5-turbo',
        messages=[{""role"": ""user"", ""content"": ""Say exactly one word.""}],
        stream=True,
    )
    responses.response.close()
    print(responses.response.is_closed)
    client.close()
    print(client.is_closed())
```


### OS

macOS,Linux

### Python version

3.10.13

### Library version

openai==1.3.4 , httpx==0.27.0 , pydantic==2.7.1, pydantic_core==2.18.2",closed,2024-04-23T23:21:08Z,2024-04-24T18:00:30Z,['bug'],https://github.com/openai/openai-python/issues/1361,18.66,True,memory_out_of_bound,High,"The process memory is infinitely increasing without being reduced, indicating a potential memory leak in the API usage.",high,
1363,thêm vấn đề lưu trữ chat,"### Confirm this is a feature request for the Python library and not the underlying OpenAI API.

- [X] This is a feature request for the Python library

### Describe the feature or improvement you're requesting

hãy thêm chức năng lưu trữ và ghim các chat cần thiết trên chat gpt

### Additional context

fewfwefwe",closed,2024-04-24T10:47:49Z,2024-04-24T16:02:02Z,[],https://github.com/openai/openai-python/issues/1363,5.24,True,missing_dependency_violation,medium,"The feature request lacks necessary context or metadata regarding how chat storage and pinning should be implemented, which may lead to incomplete functionality.",high,
911,Fix typing errors from mypy when calling completion,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

Mypy doesn't seem to play very well with the type annotations that the SDK has generated. I can see that OpenAI uses Stainless and that might be the issue in itself, but as it stands the type annotations aren't very well documented.

See below for the exact use case and how to reproduce.

I'd propose either updating the annotations to be more semantically correct, or updating the documentation and usage if the idea is to keep the annotations more strict.

Option 1 seems like a better option until a full migration to stricter types can be reached, likely in a major version.

### To Reproduce

When calling completion like so

```python
client = AsyncOpenAI(api_key=<API_KEY>)
messages = [
    {""role"": ""user"", ""content"": ""some test message""}
]
completion = await client.chat.completions.create(
    model=""gpt-3.5-turbo-0613"", # Actual model doesn't matter here
    messages=messages # Mypy shows a type mismatch error here
)
```

I did a little bit of digging and found that the `messages` arg in `chat.completions.create` is annotated as `ChatCompletionMessageParam`, however the method is smart enough to take and serialize plain dicts as well. I haven't seen the rest of the endpoints, but my guess is that this annotation issue is present throughout.

I can get around it by using the appropriate types instead. So, the following works

```python
from openai.types.chat import ChatCompletionMessageParam, ChatCompletionUserMessageParam
messages: list[ChatCompletionMessageParam] = [
    ChatCompletionUserMessageParam(role=""user"", content=""some test message"")
]
# This seems to satisfy mypy
```

### Code snippets

_No response_

### OS

Linux

### Python version

3.12

### Library version

v1.3.6",closed,2023-11-30T20:32:48Z,2023-11-30T20:55:44Z,['question'],https://github.com/openai/openai-python/issues/911,0.38,True,input_type_violation,medium,"Type mismatch in the 'messages' argument when calling the 'chat.completions.create' method, leading to mypy errors.",high,
871,Allow filename to be passed as a parameter into `files.create`,"### Confirm this is a feature request for the Python library and not the underlying OpenAI API.

- [x] This is a feature request for the Python library

### Describe the feature or improvement you're requesting

Right now, you can only pass a FileTypes file object into the `files.create` function. Therefore, when doing
```py
with open(""some_file.txt"", ""r"") as f:
        file = f.read()
    openai_file = openai.files.create(file=file, purpose=""assistants"")
```
there is no way of passing a filename into the request (altough the endpoint accepts one, the library provides no way of passing one manually).
A workaround I found is 
```py
response = openai.files.create(file=Path(file_path), purpose=""assistants"")
```
but I think many people don't know about it, and you cannot provide a different filename than the one in the file system.

I can look into the library and make a PR myself if the feature sounds good:)

### Additional context

_No response_",closed,2023-11-23T12:31:37Z,2023-11-23T15:56:46Z,[],https://github.com/openai/openai-python/issues/871,3.42,True,missing_option_violation,medium,"The current implementation does not allow users to pass a filename directly to the `files.create` function, despite the underlying API accepting it, leading to potential confusion and suboptimal usage.",high,
1351,Fix BadRequestError when using ChatCompletionMessage with explicit tool_calls=None,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

## Description
When creating a ChatCompletionMessage instance with `tool_calls` explicitly set to `None`, a subsequent chat completion request using that message fails with a `BadRequestError`. The error message indicates that None is not of type `array` for `messages.0.tool_calls`.

## Steps to reproduce:

Create a ChatCompletionMessage instance with tool_calls set to None.
Use the created message in a chat completion request.
Expected behavior:
The chat completion request should succeed, as setting tool_calls to None should be equivalent to not specifying it at all.

Actual behavior:
The chat completion request fails with a `BadRequestError`, indicating that None is not of type `array` for `messages.0.tool_calls`.

## Proposed solution:
Modify the `ChatCompletionMessage` class to handle the case when tool_calls is explicitly set to `None`. If tool_calls is `None`, it should be treated the same as if it were not specified, allowing the chat completion request to succeed.

## Additional information:

OpenAI version: 1.23.2
Python version: Python 3.11.8
Operating system: macOS
**Steps to Reproduce:**

1. **Install the required libraries**:
	* `!pip install openai==1.23.2 python-dotenv -q`
2. **Import the necessary modules and load the environment variables**:
	* `from openai.types.chat import ChatCompletionMessage`
	* `from openai import AsyncOpenAI`
	* `import os`
	* `from dotenv import load_dotenv`
	* `load_dotenv()`
3. **Create an instance of the `AsyncOpenAI` client**:
	* `client = AsyncOpenAI(api_key=os.environ.get(""OPENAI_API_KEY""))`
4. **Create two `ChatCompletionMessage` objects with `tool_calls=None`**:
	* `message2 = ChatCompletionMessage(content=""foo bar"", role=""assistant"")`
		+ Output: `ChatCompletionMessage(content='foo bar', role='assistant', function_call=None, tool_calls=None)`
	* `message3 = ChatCompletionMessage(content=""foo bar"", role=""assistant"", tool_calls=None)`
		+ Output: `ChatCompletionMessage(content='foo bar', role='assistant', function_call=None, tool_calls=None)`
5. **Verify that both objects pass equality checks and have the same `model_dump()` output**:
	* `assert message2 == message3`
	* `assert message2.model_dump() == message3.model_dump()`
6. **Attempt to create a chat completion using both objects**:
	* `await client.chat.completions.create(model=""gpt-3.5-turbo"", messages=[message2])` (Expected to succeed)
	* `await client.chat.completions.create(model=""gpt-3.5-turbo"", messages=[message3])` (Expected to fail with a `BadRequestError`)

**Expected Behavior:**

Both objects should be treated equally and result in a successful chat completion creation.

**Actual Behavior:**

One object succeeds, while the other fails with a `BadRequestError`.

**Proposed Fix:**

Modify the serialization of `ChatCompletionMessage` objects to handle `tool_calls=None` consistently, ensuring that identical objects are treated equally in API requests.

**Additional Context:**

This issue may be related to how `tool_calls` is serialized and deserialized in the OpenAI API. A thorough review of the serialization process is recommended to prevent similar inconsistencies in the future.
### OS

macOS

### Python version

Python 3.11.8

### Library version

1.23.2",closed,2024-04-21T06:37:13Z,2024-04-22T21:26:23Z,['bug'],https://github.com/openai/openai-python/issues/1351,38.82,True,input_type_violation,High,"The API expects 'tool_calls' to be of type 'array', but receives 'None', leading to a BadRequestError.",high,
1352,Module level Client forcefully loaded when in PyCharm debugger,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

If I run the following script simply via Python

```python
from openai import OpenAI

openai = OpenAI(api_key=""MY KEY"")

print(""We got to the end!"")
```

It works fine. However, in PyCharm's debugger, it seems like the ```_module_client``` is being forcefully initialized, so I get this error raised when trying to run on the ```from openai import OpenAI``` line

```python
# in _client.py
        if api_key is None:
            raise OpenAIError(
                ""The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable""
            )
```

The workaround that worked for me is conditionally loading the module client like this in ```__init__.py``` at the end of the file:

```python
import sys

gettrace = getattr(sys, 'gettrace', None)

if gettrace is None:
    # We're not in debug mode, load libraries
    from ._module_client import (
        beta as beta,
        chat as chat,
        audio as audio,
        files as files,
        images as images,
        models as models,
        batches as batches,
        embeddings as embeddings,
        completions as completions,
        fine_tuning as fine_tuning,
        moderations as moderations,
    )
```




### To Reproduce

Follow instructions from the description.

### Code snippets

_No response_

### OS

Windows 11

### Python version

3.12

### Library version

1.23.2",closed,2024-04-21T19:13:52Z,2024-04-22T21:22:13Z,['bug'],https://github.com/openai/openai-python/issues/1352,26.14,True,missing_dependency_violation,high,"The OpenAI client requires an api_key to be set, which is not being initialized correctly in the PyCharm debugger context, leading to an error.",high,
1354,openai.BadRequestError: Error code: 400 - Unrecognized request argument supplied: dataSources.,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

When I use chat.completions.create(), and give extra_body, encounter this error.

I've checked endpoint uri, key, ai search endpoint and key. Nothing wrong.

After I remove extra_body, it worked.

### To Reproduce

client = AzureOpenAI(
    azure_endpoint="""",
    api_key="""",
    azure_deployment="""",
    api_version="""",
)

completion = client.chat.completions.create(
    model=deployment,
    messages=[
        {
            ""role"": ""user"",
            ""content"": ""What are my available health plans?"",
        },
    ],
    extra_body={
        ""dataSources"": [
            {
                ""type"": ""AzureCognitiveSearch"",
                ""parameters"": {
                    ""endpoint"": os.environ[""AZURE_AI_SEARCH_ENDPOINT""],
                    ""key"": os.environ[""AZURE_AI_SEARCH_API_KEY""],
                    ""indexName"": os.environ[""AZURE_AI_SEARCH_INDEX""]
                }
            }
        ]
    }
)

### Code snippets

_No response_

### OS

windows

### Python version

Python 3.11.5

### Library version

openai v1.23.2",closed,2024-04-22T06:02:31Z,2024-04-22T20:38:37Z,['bug'],https://github.com/openai/openai-python/issues/1354,14.6,True,input_value_violation,high,"The API does not recognize the 'dataSources' argument supplied in the extra_body, leading to a BadRequestError.",high,
486,Support for functions,"### Describe the feature or improvement you're requesting

Pythonic support for functions as described in the feature [announcement](https://openai.com/blog/function-calling-and-other-api-updates). 

### Additional context

_No response_",closed,2023-06-13T19:26:28Z,2024-03-03T01:03:42Z,['enhancement'],https://github.com/openai/openai-python/issues/486,6317.62,False,,,,,
1324,Add timestamp_granularities for AzureOpenAI,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

```
TypeError: Translations.create() got an unexpected keyword argument 'timestamp_granularities'
```
This is when calling AzureOpenAI client with code below. 

### To Reproduce

To Reproduce you should use AzureOpenAI client (OpenAI client is fine):

```python
import os
from openai import AzureOpenAI

client = AzureOpenAI(
    api_key=""API_KEY"",  
    api_version=""2023-12-01-preview"",
    azure_endpoint=""ENDPOINT_URL""
)
audio_file = open(""output_audio.m4a"", ""rb"")
transcript = client.audio.translations.create(
  model=""whisper-1"",
  file=audio_file,
  prompt="""",
  response_format=""verbose_json"",
  timestamp_granularities=[""word""] # or segment
  
)

```

I've already also made a PR. 
- #1323 

### Code snippets

This code actually works without any problem:

```python
from openai import OpenAI
client = OpenAI(api_key='OPENAPI_KEY')

audio_file = open(""output_audio.m4a"", ""rb"")
transcript = client.audio.transcriptions.create(
  file=audio_file,
  model=""whisper-1"",
  response_format=""verbose_json"",
  timestamp_granularities=[""word""],
)

print(transcript.words)
```
But When you change OpenAI to AzureOpenAI it doesn't work. 

Also this argument is supported in Azure and it works. So, it is just the matter of adding it here. 

### OS

MacOS, Linux

### Python version

Python v3.12.1

### Library version

1.20.0",closed,2024-04-17T14:30:38Z,2024-04-17T20:39:28Z,['bug'],https://github.com/openai/openai-python/issues/1324,6.15,True,input_type_violation,High,"The AzureOpenAI client does not accept the 'timestamp_granularities' argument, leading to a TypeError when it is provided.",high,
1312,"Support for truncation_strategy, max_prompt_tokens and max_completion_tokens","### Confirm this is a feature request for the Python library and not the underlying OpenAI API.

- [X] This is a feature request for the Python library

### Describe the feature or improvement you're requesting

Can the python package be updated to support truncation_strategy, max_prompt_tokens and max_completion_tokens as seen in the [OpenAI Documentation here](https://platform.openai.com/docs/api-reference/runs/createRun#runs-createrun-truncation_strategy)?

Is it possible to do this already?

### Additional context

_No response_",closed,2024-04-13T10:41:45Z,2024-04-16T14:36:23Z,[],https://github.com/openai/openai-python/issues/1312,75.91,False,,,,,
1270,"openai.BadRequestError: Error code: 400 - {'error': {'message': ""'$.messages[0].content' is invalid. Please check the API reference: https://platform.openai.com/docs/api-reference."", 'type': 'invalid_request_error', 'param': None, 'code': None}}","### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

Tried the below AsyncAzureOpenAI
```
import asyncio
import openai
import time
openai.api_type = ""azure""
openai.api_base = ""<base url>""
openai.api_version = ""<version>""
openai.api_key = ""<api key>""
instructions = [prompt1,prompt2,prompt3]
from tqdm import tqdm
async_client = openai.AsyncAzureOpenAI(
    azure_endpoint=""<base url>"",
    api_key=""<api key>"",
    api_version=""<version>""
    )

async def aync_main(prompt: str) -> None:
    response = await async_client.chat.completions.create(
        model=""<model>"",
        messages=[{""role"": ""user"", ""content"": prompt}]
    )
    return response.choices[0].message.content
async def async_run_loop(instructions):
    tasks = [aync_main(prompt) for prompt in enumerate(tqdm(instructions))]
    answers = await asyncio.gather(*tasks)
    return {idx: answer for idx, answer in enumerate(answers)}

async_answers = asyncio.run(async_run_loop(instructions))
print(async_answers)
```
But it returns the given error
```
openai.BadRequestError: Error code: 400 - {'error': {'message': ""'$.messages[0].content' is invalid. Please check the API reference: https://platform.openai.com/docs/api-reference."", 'type': 'invalid_request_error', 'param': None, 'code': None}}
```


### To Reproduce

Tried the below AsyncAzureOpenAI
```
import asyncio
import openai
import time
openai.api_type = ""azure""
openai.api_base = ""<base url>""
openai.api_version = ""<version>""
openai.api_key = ""<api key>""
instructions = [prompt1,prompt2,prompt3]
from tqdm import tqdm
async_client = openai.AsyncAzureOpenAI(
    azure_endpoint=""<base url>"",
    api_key=""<api key>"",
    api_version=""<version>""
    )

async def aync_main(prompt: str) -> None:
    response = await async_client.chat.completions.create(
        model=""<model>"",
        messages=[{""role"": ""user"", ""content"": prompt}]
    )
    return response.choices[0].message.content
async def async_run_loop(instructions):
    tasks = [aync_main(prompt) for prompt in enumerate(tqdm(instructions))]
    answers = await asyncio.gather(*tasks)
    return {idx: answer for idx, answer in enumerate(answers)}

async_answers = asyncio.run(async_run_loop(instructions))
print(async_answers)
```
But it returns the given error
```
openai.BadRequestError: Error code: 400 - {'error': {'message': ""'$.messages[0].content' is invalid. Please check the API reference: https://platform.openai.com/docs/api-reference."", 'type': 'invalid_request_error', 'param': None, 'code': None}}
```


### Code snippets

_No response_

### OS

Ubuntu

### Python version

Python 3.9.18

### Library version

1.13.3",closed,2024-03-25T13:10:44Z,2024-03-25T13:38:05Z,['bug'],https://github.com/openai/openai-python/issues/1270,0.46,True,input_value_violation,high,"The content of the messages provided to the API is invalid, which suggests that the input value does not meet the expected format or constraints.",high,
1313,Azure OpenAI API doesn't return consistent model ids,"### Confirm this is a feature request for the Python library and not the underlying OpenAI API.

- [X] This is a feature request for the Python library

### Describe the feature or improvement you're requesting

When compared with the ""classical"" API, Azure's doesn't seem to return a consistent model ID after a completion. For instance, I have a deployment of the following models:
- `gpt-4-0613` 
- `gpt-4-1106-preview`
- `gpt-4-0125-preview`

However, when calling a completion, the `model` attribute of the retrieved object is always `gpt-4` (differently from the ""classical"" API which provides the full name of the model).

Same with `gpt-35-turbo-0613`, `gpt-35-turbo-1106`, `gpt-35-turbo-0125`.

### Additional context

_No response_",closed,2024-04-13T20:19:37Z,2024-04-15T21:54:37Z,[],https://github.com/openai/openai-python/issues/1313,49.58,True,incorrect_functionality,high,"The API does not return the expected full model IDs, leading to confusion and incorrect usage of the API.",high,
1310,`gpt-4-turbo-2024-04-09` not pointing to correct cutoff date without an explicitly set system message statement,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library _(sort of, because it affects it too)_

### Describe the bug

While this issue intersects with the usage of the Python API, it primarily pertains to a deeper, potentially systemic concern affecting model performance and accuracy across any interfacing tool.

**Issue Description:**
When interfacing with various model versions (specifically noted with `gpt-4-turbo-2024-04-09`), it appears that without an explicit declaration of the model version within the system message, e.g., “You are gpt-4-turbo-2024-04-09,” the model does not correctly anchor to its designated context and capabilities. It exhibits behaviors reminiscent of older versions or operates with outdated knowledge, such as assuming a knowledge cutoff either in April 2023 or all the way back to September 2021. This misalignment occurs despite selecting the correct model version in API settings, indicating a potential oversight in how model version grounding is internally managed.

**Impact:**
Such behavior can significantly impede the model's utility and accuracy, particularly in contexts requiring up-to-date information or specific model functionalities.

**Additional Context:**
I have shared these findings within the OpenAI Developer Community, garnering corroborative feedback and additional insights which indicate this is not an isolated incident:

- [Community Post 1](https://community.openai.com/t/gpt-4-turbo-2024-04-09-will-think-its-in-2021-unless-specified/712483)
- [Community Post 2](https://community.openai.com/t/gpt-4-turbo-2024-04-09-will-think-its-in-2021-unless-specified/712483/6)

I'm submitting this report here to ensure the issue receives appropriate attention and to facilitate a cross-disciplinary examination of its implications and potential resolutions, since as of now it's affecting model performance and since the Python API is among the most popular methods to utilize the OpenAI model(s).

Thank you for your time and attention to this matter.

### To Reproduce

**(Use the code snippet below for a simple A/B test example)**

1. **Without Specified System Message:**
   - Initialize a session with the model `gpt-4-turbo-2024-04-09` without specifying any additional context or information about its version in the system message.
   - Ask the model about recent events or information post-April 2023, which should fall within its knowledge base if it were accurately aligned with its latest training data.
   - Observe and note any responses that suggest the model is unaware of events or information after April 2023, indicating it is not leveraging its most current training. 

2. **With Specified System Message:**
   - In a new session, explicitly tell the model ""You are gpt-4-turbo-2024-04-09"" in the system message.
   - Repeat the inquiry about recent events or specific information post-April 2023.
   - Compare the responses to those from Step 1 to observe any notable improvements or corrections in the model's awareness and accuracy, suggesting that the explicit context provided has re-aligned the model with its expected knowledge base.

### Further explanation of the issue

End-users of the API would reasonably expect that when they select a specific model version, like `gpt-4-turbo-2024-04-09`, the model inherently understands and aligns with its latest training data up to the declared cutoff without needing any additional nudges or clarifications. Right now this is _not_ the case.

The absence of an explicit version declaration in the system message leads to 'AI hallucination', where the model outputs are inaccurately anchored in past knowledge, incorrectly suggesting a knowledge cut-off in 2021. 

This misalignment persists despite correct model version selection in API settings, suggesting a need for improved internal management of model version grounding. This seems to be a persisting issue across the GPT-4 model checkpoint variants, but the problem is most prominent in the latest version.

By following the mentioned inclusion in the system message, anyone replicating the test should be able to see a clear contrast in the model's performance and awareness of its knowledge cutoff, based on whether or not its version was explicitly stated in the system message. 

A simple method to A/B test this is to use i.e. the provided Python code (see below) to run diagnostics on the model and spot the difference yourself. You can change the test question to any post-April 2023 event by editing the `user_question` variable.

### Code snippets

To replicate and test the issue:

```Python
import os
from openai import OpenAI

# User question to be asked
user_question = ""Is Cormac McCarthy still alive?""

# Instantiate the client with your API key
client = OpenAI(api_key=os.environ.get(""OPENAI_API_KEY""))

# Inform the user about the question being asked without model guidance
print(""---"")
print(f""Asking without specifying the model version in the system message: {user_question}"")
print(""---"")
response = client.chat.completions.create(
    model=""gpt-4-turbo-2024-04-09"",
    messages=[
        {""role"": ""system"", ""content"": ""You are an AI assistant based on OpenAI's GPT-4.""},
        {""role"": ""user"", ""content"": user_question}
    ]
)

# Print the response
print(response.choices[0].message.content)

# Inform the user about the question being asked with model guidance
print(""---"")
print(f""Asking while specifying 'You are gpt-4-turbo-2024-04-09' in the system message: {user_question}"")
print(""---"")
response_with_system_message = client.chat.completions.create(
    model=""gpt-4-turbo-2024-04-09"",
    messages=[
        {""role"": ""system"", ""content"": ""You are gpt-4-turbo-2024-04-09""},
        {""role"": ""user"", ""content"": user_question}
    ]
)

# Print the response
print(response_with_system_message.choices[0].message.content)
```
_(note how even pointing out the model's role as a GPT-4 AI assistant is not enough to align it to the most current cutoff date data)_

### OS

(all)

### Python version

Python v3.11

### Library version

openai v1.14.2",closed,2024-04-11T17:36:01Z,2024-04-13T16:36:58Z,['bug'],https://github.com/openai/openai-python/issues/1310,47.02,True,missing_dependency_violation,high,"The model does not correctly anchor to its designated context and capabilities without an explicitly set system message, leading to incorrect outputs and reduced accuracy.",high,
1308,Azure API Manager with Azure OpenAI,"### Confirm this is a feature request for the Python library and not the underlying OpenAI API.

- [X] This is a feature request for the Python library

### Describe the feature or improvement you're requesting

We proxy the Azure OpenAI service using API Manager. The system design is from Microsoft found here: https://github.com/microsoft/AzureOpenAI-with-APIM.

The request would be for a way to override/set a custom endpoint url along with api key name. (ex. below the key name is Ocp-Apim-Subscription-Key)

example code
#!/bin/bash
apimUrl=""THE_HTTPS_URL_OF_YOUR_APIM_INSTANCE""
modelName=""GPT-3_5-Turbo"" # Probaby what you named your model, but change if necessary
apiVersion=""2023-03-15-preview"" # Do not change this value, unless you are testing a different API version
subscriptionKey=""YOUR_APIM_SUBSCRIPTION_KEY""

url=""${apimUrl}""/deployments/""${modelName}""/chat/completions?api-version=""${apiVersion}""
key=""Ocp-Apim-Subscription-Key: ${subscriptionKey}""

curl $url -k -H ""Content-Type: application/json"" -H $key -d '{
    ""messages"": [
        {
            ""role"": ""system"",
            ""content"": ""You are an AI assistant that helps people find information.""
        },
        {
            ""role"": ""user"",
            ""content"": ""What are the differences between Azure Machine Learning and Azure AI services?""
        }
    ]
}' 



### Additional context

_No response_",closed,2024-04-10T13:48:15Z,2024-04-12T23:40:48Z,[],https://github.com/openai/openai-python/issues/1308,57.88,True,input_type_violation,medium,"The API expects a specific structure for the input JSON, and the provided example may not adhere to the expected format, potentially leading to misinterpretation of the prompt.",medium,
1303,"VQA Issue ID # 98, 99 - Two Up Text + Media - Multiple Issues","## Submitted by

Steph

## Section

Two Up Text + Media component

## Page

[98 - Security](https://openai-dotcom-dun.vercel.app/security) 

[99 - Security](https://openai-dotcom-dun.vercel.app/careers) 

## Issue Descriptions

## Issue # 98

Mobile — increase top and bottom padding to use our M spacer value (45.41)

Issue # 98 Device/ Browser/ Version - Macbook Pro / Arc (Chrome) / Sonoma 14.2.1

Issue # 98 Screenshot/Link
[Screenshot Issue #98](https://drive.google.com/file/d/1aDW4aB3O9ymtQXFR5t-2hcSe3YlcGs_v/view?usp=drive_link)

## Issue # 99

Middle align content within container

Issue # 99 Device/ Browser/ Version - Macbook Pro / Arc (Chrome) / Sonoma 14.2.1

Issue # 99 Screenshot/Link
[Screenshot Issue # 99](https://drive.google.com/file/d/1eIY_zqG61Lz5qZnN-K75bEeVply2lKTj/view?usp=drive_link)


## Tracker/Link
[VQA Tracker](https://docs.google.com/spreadsheets/d/1_oavDmqtoo0LXl6VXISTeB-_XftS_jIM6kpW90h7DEs/edit#gid=1543807878)

",closed,2024-04-09T05:14:21Z,2024-04-09T21:35:19Z,[],https://github.com/openai/openai-python/issues/1303,16.35,False,,,,,
1301,how to fetch the openai api key usage and remaining balance,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

i want to fetch the openai api usage through the python code, though i have seen some comments hereon the code, but they dont seem to wok for me, can i get any latest version for fetching the usage.

### To Reproduce

nil

### Code snippets

_No response_

### OS

windows

### Python version

3.11

### Library version

latest",closed,2024-04-08T07:49:48Z,2024-04-08T22:26:48Z,['bug'],https://github.com/openai/openai-python/issues/1301,14.62,True,missing_dependency_violation,medium,The user is unable to fetch API usage due to a lack of clear instructions or missing dependencies in the code examples provided.,high,
1296,Use Type ChatCompletionRole for ChatCompletionMessage.role,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

It appears that the role attribute of ChatCompletionMessage is incorrectly using the type Literal[""assistant""] instead of ChatCompletionRole. Messages sent to, and received from, the Chat Completions API can indeed accept all values defined in ChatCompletionRole, and not just ""assistant"".

This is preventing use of the type `ChatCompletionMessage` provided in this library for type checking/enforcement.

### To Reproduce

N/A

### Code snippets

_No response_

### OS

*

### Python version

*

### Library version

openai >=v1.0.0",closed,2024-04-04T18:00:24Z,2024-04-05T17:54:55Z,['bug'],https://github.com/openai/openai-python/issues/1296,23.91,True,input_type_violation,high,"The role attribute of ChatCompletionMessage is incorrectly using a specific literal type instead of the broader ChatCompletionRole type, leading to type enforcement issues.",high,
1297,openai.Model.list() is invalid,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug


After the version update, openai.Model.list() is invalid, now how do I see which models the API can call?



### To Reproduce

above

### Code snippets

_No response_

### OS

linux

### Python version

3.9

### Library version

1.13.3",closed,2024-04-05T02:37:16Z,2024-04-05T17:36:55Z,['bug'],https://github.com/openai/openai-python/issues/1297,14.99,True,missing_dependency_violation,high,"The method openai.Model.list() is reported as invalid, indicating a potential missing dependency or incorrect method call after a version update.",high,
1230,Generation of \u2028 results in JSONDecodeError when stream=True,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

When generating a completion with `stream=True` I get `JSONDecodeError` when the LLM tries to generate `\u2028`. After some digging, this seems to be due to `\u2028` corresponding to 2 tokens `[378, 101]`, none of which can be decoded into strings alone.
```python
import tiktoken

enc = tiktoken.get_encoding(""cl100k_base"")
tokens = enc.encode(""\u2028"").decode()  #  '\u2028' unicode line separator

enc.decode_bytes(tokens).decode() # [378, 101] is fine

enc.decode_bytes([tokens[0]]).decode()  # [378]
# > UnicodeDecodeError: 'utf-8' codec can't decode bytes in position 0-1: unexpected end of data

enc.decode_bytes([tokens[1]])  # [101]
# > UnicodeDecodeError: 'utf-8' codec can't decode byte 0xa8 in position 0: invalid start byte
``` 
When `stream=True` I presume the tokens are streamed one at a time and hence can't be decoded. The error occurs when the openai library tries to convert the SSE data to json, resulting in an invalid conversion e.g. `{..., ""choices"":[{""index"":0,""delta"":{""content"":"" ` (note that I have truncated the beginning of the data, but the content just ends like this).

This seems quite obscure but handles frequently in scenarios where references / citations are made to some user input.

### To Reproduce

1. Create prompt including a single character/symbol that gets encoded into multiple tokens e.g. `\u2028` which cl100k_base maps to the tokens `[378, 101]`
2. Ask the LLM to recite the user message

### Code snippets

```Python
from openai import OpenAI

api_key = ""...""  # Replace with your API key


system_message = ""The user will send you a short text. You MUST respond with the EXACT same text verbatim as the user supplies, nothing more, nothing less.""

user_message = """"""
Minim culpa \u2028 anim eu id exercitation amet. Culpa culpa esse mollit pariatur est enim. Exercitation minim cillum aute occaecat. Incididunt velit commodo sit ea. \u2028

Deserunt labore eu ipsum reprehenderit esse sunt nisi aliqua qui id mollit. Id cupidatat incididunt Lorem ex ullamco quis voluptate mollit sit labore quis. Nostrud sint sint Lorem tempor minim amet aliquip elit fugiat. Ipsum cupidatat ipsum veniam ut ea magna nostrud id quis exercitation tempor velit aliqua sit. Proident sint velit ullamco culpa dolore magna ut eiusmod pariatur. Commodo ut sint minim ex aliqua eu esse anim elit elit eiusmod ea. Culpa quis in ea id cupidatat labore amet amet ullamco sunt Lorem do tempor ad. \u2028

Dolor anim dolore laborum fugiat dolor eiusmod amet adipisicing. Consectetur et dolor enim proident aute deserunt. Excepteur ullamco ea officia nulla irure cupidatat veniam ipsum ex. Labore sint sit incididunt ad exercitation labore minim consequat elit sit nulla occaecat do nisi. Irure est commodo id eu fugiat eiusmod proident consequat ea.from typing import Any, Dict, List, Literal, Union  \u2028
""""""


client = OpenAI(api_key=api_key)
response = client.chat.completions.create(
    model=""gpt-4-1106-preview"",
    messages=[
        {""role"": ""system"", ""content"": system_message},
        {""role"": ""user"", ""content"": user_message},
    ],
    stream=True,
)
for chunk in response:
    print(chunk.choices[0].delta.content, end="""")
```


### OS

macOS

### Python version

Python 3.11.0

### Library version

openai v1.13.3",closed,2024-03-12T21:06:42Z,2024-04-05T08:29:20Z,['bug'],https://github.com/openai/openai-python/issues/1230,563.38,False,,,,low,
587,Add context length attribute to the model objects returned on the list and retrieve models endpoint.,"### Describe the feature or improvement you're requesting

**Description:**

At present, the `openai.Model.list()` function provides a list of model objects, each characterized by four attributes, as exemplified below:

```json
{
  ""id"": ""davinci"",
  ""object"": ""model"",
  ""created"": 1686935002,
  ""owned_by"": ""openai""
}
```

I would like to propose an augmentation to this functionality by introducing an additional attribute that conveys crucial information—the maximum context length or limit of each model.

**Request:**

I kindly request the incorporation of a new attribute to the model object that signifies the respective model's maximum context length. This enhancement would yield several benefits:

1. **Simplified Code Implementation:** By exposing the maximum context length directly through the model object, users can seamlessly adapt their code to automatically respect the context limit. This feature would empower users to fine-tune their applications to the specific capabilities of each model effortlessly, without having to manually store context limits individually for every model.

2. **Enhanced Adaptability:** The suggested attribute would enable OpenAI to dynamically adjust a model's context length, if needed, without disrupting users' code. This adaptability reflects a proactive approach towards model improvements while safeguarding the stability of user implementations.

**Proposed Attribute:**

I propose the inclusion of the following attribute in the model object:

```json
{
  ""context_limit"": {
    ""type"": ""integer"",
    ""description"": ""The maximum context length that the model can effectively handle.""
  },
}
```

The ""context_limit"" attribute, when integrated into the model object, would be of type ""integer."" It will succinctly convey the maximum number of tokens can be processed by the model. This clear and concise addition would provide developers with essential information upfront, contributing to an improved development experience.

### Additional context

There's been demand for this attribute on the OpenAI dev community as well. Here are the links for your reference:
- https://community.openai.com/t/request-query-for-a-models-max-tokens/161891
- https://community.openai.com/t/can-i-get-the-max-tokens-information-from-openai-api/143549
",closed,2023-08-25T07:27:21Z,2023-11-10T04:11:40Z,['API-feedback'],https://github.com/openai/openai-python/issues/587,1844.74,False,,,,,
560,Chat Completion Hangs after error 500,"### Describe the bug

Hi,

Chat Completion hang at the next call after a 500 error. My code printed the error below. After that at the next Chat Completion call, it just stuck there.

The server had an error processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if you keep seeing this error. (Please include the request ID 90c93658c303560e33e5d459254eae54 in your email.) {
  ""error"": {
    ""message"": ""The server had an error processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if you keep seeing this error. (Please include the request ID 90c93658c303560e33e5d459254eae54 in your email.)"",
    ""type"": ""server_error"",
    ""param"": null,
    ""code"": null
  }
}


### To Reproduce

1. Call chatcompletion
2. Get a 500 error from openai
3. Call chatcompletion again
4. hang

### Code snippets

_No response_

### OS

Windows

### Python version

Python 3.10.11

### Library version

openai-0.27.2",closed,2023-08-03T23:38:47Z,2023-09-06T11:58:40Z,['bug'],https://github.com/openai/openai-python/issues/560,804.33,True,hang,Medium,"The system becomes unresponsive after a 500 error, requiring a restart or manual intervention.",high,
1108,Can't import openai module without authentication error,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

Given the following import:

```python
def do_stuff():

    secret = ""vgtc/dev/openai-secret""
    secret_values = get_secret_json(secret)
    
    import openai

    client = openai.OpenAI(**secret_values)
```

The exception is raised:

""The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable""

This happens because on line 316 of `openai/__init__.py`  it attempts to initialize client proxies (before I have requested initialization to occur)

This isn't a problem for those who use environment variables to leak their secrets to processes, but for those of us who want stricter control of our secrets than what is available by handing them over to the OS, it makes it impossible to even import the library without creating an exception to our security policy.

Here's a blog post on it:

https://www.cloudtruth.com/blog/the-pitfalls-of-using-environment-variables-for-config-and-secrets#:~:text=Developers%20often%20use%20environment%20variables,information%20poses%20significant%20security%20risks.

### To Reproduce

1. New environment
2. pip install openai
3. `import openai`
4. See crash

### Code snippets

_No response_

### OS

Win11

### Python version

3.12

### Library version

openai v1.10.0""",closed,2024-01-26T18:34:11Z,2024-02-03T21:22:28Z,['bug'],https://github.com/openai/openai-python/issues/1108,194.8,True,missing_dependency_violation,high,"The openai module requires an api_key to be set either through client options or environment variables, but the import is attempted without fulfilling this dependency, leading to an exception.",high,
287,Why doesn't the OpenAI API throw an error when we pass a value to `frequency_penalty` outside its valid range?,"### Describe the bug

[The documentation](https://platform.openai.com/docs/api-reference/completions/create#completions/create-frequency_penalty) says

> Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.

However, I can pass values like 2.3 to this parameter and the API doesn't raise anything. So, I suppose this is a bug with the API or the documentation is wrong. Note: this doesn't happen with the `presence_penalty` parameter, which, if set to e.g. 2.2, makes the API returns an error `openai.error.InvalidRequestError: 2.2 is greater than the maximum of 2 - 'presence_penalty'`. Even if I set say `frequency_penalty=10000`, the api doesn't err. Similarly, the API doesn't err if we set this parameter to negative values like `-1000`. So, maybe this parameter is supposed to take any floating-point number and the documentation is wrong? Or maybe the API is buggy?

### To Reproduce

You can make any call to the completions endpoint with any valid value for the other parameters and you will not get an error you're supposed to get

### Code snippets

```
import openai
# Set your api key

completions = openai.Completion.create(model=""text-davinci-003"", 
                                       prompt=""hello"", 
                                       frequency_penalty=1000)
print(completions)
```

### OS

macos monterey (12.5.1)

### Python version

Python 3.8.13

### Library version

0.27.0",closed,2023-03-08T14:31:02Z,2023-12-31T00:04:05Z,['bug'],https://github.com/openai/openai-python/issues/287,7137.55,True,input_value_violation,high,"The API does not enforce the valid range for the frequency_penalty parameter, allowing out-of-range values without error.",high,
1279,Documentation/Wrappers to stream images/files created by Assistant API,"### Confirm this is a feature request for the Python library and not the underlying OpenAI API.

- [X] This is a feature request for the Python library

### Describe the feature or improvement you're requesting

In the current SDK/docs on streaming the [assistants api](https://github.com/openai/openai-python/blob/main/helpers.md), it's not clear how to retrieve the files created in the streaming event.

For the non-streaming approach, there would be a `file.id` that we can use to pull the object.

### Additional context

_No response_",closed,2024-03-29T06:35:38Z,2024-03-29T09:01:48Z,[],https://github.com/openai/openai-python/issues/1279,2.44,True,missing_dependency_violation,medium,"The request lacks clarity on how to handle file retrieval during streaming, indicating a missing dependency on the documentation for proper usage.",high,
1276,batch_size param ignored when creating fine tuning job in Azure,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

Batch size passed in fine tuning hyperparameters is ignored and set to 1.

### To Reproduce

I pass
``` 
hyperparameters = openai.types.fine_tuning.job_create_params.Hyperparameters(
            n_epochs=1,
            batch_size=64,
        )
```

To AzureOpenAI client, but reated job ignores the batch_size and sets it to 1:
  
  ```
  FineTuningJob(id='ftjob-5c2642e206294f30945d47f6ac6bf703', created_at=1711639578, error=None, fine_tuned_model=None, finished_at=None, hyperparameters=Hyperparameters(n_epochs=1), model='gpt-35-turbo-0613', object='fine_tuning.job',
 organization_id=None, result_files=None, status='pending', trained_tokens=None, training_file='file-78c086798a404b7592e0191aa5838dce', validation_file='file-642e426e1e514211bd982709640d231d', updated_at=1711639585)
 ```
 
Batch Size is also set to 1 when I inspect the job in Azure UI.

**Used to work with v 0.28**, stopped working when upgraded to v1, current version openai==1.14.3

No client exception or warning/error event is being produced.
    
    
    

### Code snippets

```Python
hyperparameters = openai.types.fine_tuning.job_create_params.Hyperparameters(
            n_epochs=1,
            batch_size=64,
        )
    response = client.fine_tuning.jobs.create(
        training_file=training_file_id,
        validation_file=validation_file_id,
        model=""gpt-35-turbo-0613"",
        hyperparameters=hyperparameters,
    )
  ```
  
I also tried:
```
    response = client.fine_tuning.jobs.create(
        training_file=training_file_id,
        validation_file=validation_file_id,
        model=""gpt-35-turbo-0613"",
        hyperparameters=dict(n_epochs=1, batch_size=64,
    )
  ```
```


### OS

macOS

### Python version

Python v3.10.13

### Library version

openai v1.14.3",closed,2024-03-28T15:33:12Z,2024-03-28T15:40:49Z,['bug'],https://github.com/openai/openai-python/issues/1276,0.13,True,input_value_violation,medium,"The batch_size parameter passed in the fine tuning hyperparameters is ignored and defaults to 1, indicating an issue with acceptable input values for the API method.",high,
1272,ImportError: cannot import name 'is_union' from 'pydantic.typing',"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

>>> import openai
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/demo/miniconda3/envs/sqlcode/lib/python3.9/site-packages/openai/__init__.py"", line 8, in <module>
    from . import types
  File ""/demo/miniconda3/envs/sqlcode/lib/python3.9/site-packages/openai/types/__init__.py"", line 5, in <module>
    from .edit import Edit as Edit
  File ""/demo/miniconda3/envs/sqlcode/lib/python3.9/site-packages/openai/types/edit.py"", line 6, in <module>
    from .._models import BaseModel
  File ""/demo/miniconda3/envs/sqlcode/lib/python3.9/site-packages/openai/_models.py"", line 33, in <module>
    from ._utils import (
  File ""/demo/miniconda3/envs/sqlcode/lib/python3.9/site-packages/openai/_utils/__init__.py"", line 2, in <module>
    from ._utils import flatten as flatten
  File ""/demo/miniconda3/envs/sqlcode/lib/python3.9/site-packages/openai/_utils/_utils.py"", line 22, in <module>
    from .._compat import is_union as _is_union
  File ""/demo/miniconda3/envs/sqlcode/lib/python3.9/site-packages/openai/_compat.py"", line 55, in <module>
    from pydantic.typing import is_union as is_union
ImportError: cannot import name 'is_union' from 'pydantic.typing' (/demo/miniconda3/envs/sqlcode/lib/python3.9/site-packages/pydantic/typing.cpython-39-x86_64-linux-gnu.so)

### To Reproduce
python -c ""import pydantic.utils; print(pydantic.utils.version_info())""
             pydantic version: 1.7.4
            pydantic compiled: True
                 install path: /demo/miniconda3/envs/sqlcode/lib/python3.9/site-packages/pydantic
               python version: 3.9.18 (main, Sep 11 2023, 13:41:44)  [GCC 11.2.0]
                     platform: Linux-5.15.0-88-generic-x86_64-with-glibc2.35
     optional deps. installed: ['typing-extensions']
import openai 


### Code snippets

_No response_

### OS

linux

### Python version

python 3.9

### Library version

openai 1.1.0",closed,2024-03-26T05:19:32Z,2024-03-26T17:39:19Z,['bug'],https://github.com/openai/openai-python/issues/1272,12.33,True,missing_dependency_violation,high,"The OpenAI library is attempting to import 'is_union' from 'pydantic.typing', which is not available in the specified version of Pydantic (1.7.4). This indicates a missing dependency that is critical for the library's functionality.",high,
923,error info of “Processed 0 files and found 0 matches  after type “grit apply openai” and press return on my WSL terminal,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

Hi, I follow your step by step process on WSL for windows user, first 2 step are right , I am sure grit are installed. but when I type "" grit apply openai "" , it shows ""Processed 0 files and found 0 matches"". I think migration doesn't work because I rerun my Python program still with the error running.    

 Normally I am not a Linux user. I type grit help, I find ""Usage: grit apply [OPTIONS] <PATTERN_OR_WORKFLOW> [PATHS]..."". I guess I didn't make PATH right. I need to apply ""openai"" in correct location. But I can't find ""openai"" anywhere in Linux directory. Could you help to solve my problem-that is-how to write correct path for openai. Can WSL linux command access the windows directory?

""openai.exe""  in my python directory is ""C:\Users\Felix SUN\PycharmProjects\pythonProject1\venv\Scripts"".  and ""C:\Users\Felix SUN\PycharmProjects\pythonProject1\venv\Lib\site-packages"" is my openai SDK.  

please help me to kick start my Python with openai.

### To Reproduce

error like message in my WSL: 

timekeeper already present, skipping
hojosi@DESKTOP-AMM5O49:~/.grit/bin$ grit apply openai
Processed 0 files and found 0 matches

error message of my python :
""C:\Users\Felix SUN\PycharmProjects\pythonProject1\venv\Scripts\python.exe"" ""C:\Users\Felix SUN\PycharmProjects\pythonProject1\venv\Lib\snippet.py"" 
Traceback (most recent call last):
  File ""C:\Users\Felix SUN\PycharmProjects\pythonProject1\venv\Lib\snippet.py"", line 5, in <module>
    openai.ChatCompletion.create(
  File ""C:\Users\Felix SUN\PycharmProjects\pythonProject1\venv\Lib\site-packages\openai\lib\_old_api.py"", line 39, in __call__
    raise APIRemovedInV1(symbol=self._symbol)
openai.lib._old_api.APIRemovedInV1: 

You tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.

You can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. 

Alternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`

A detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742


Process finished with exit code 1

### Code snippets

```Python
import openai

openai.api_key = 'my key here'

openai.ChatCompletion.create(
  model=""gpt-3.5-turbo"",
  messages=[
        {""role"": ""system"", ""content"": ""You are a helpful assistant.""},
        {""role"": ""user"", ""content"": ""Who won the world series in 2020?""},
        {""role"": ""assistant"", ""content"": ""The Los Angeles Dodgers won the World Series in 2020.""},
        {""role"": ""user"", ""content"": ""Where was it played?""}
    ]
)
```


### OS

windows 11

### Python version

Python 3.12

### Library version

openai 1.3.7",closed,2023-12-03T06:36:24Z,2023-12-04T02:19:58Z,[],https://github.com/openai/openai-python/issues/923,19.73,True,missing_dependency_violation,high,"The user is trying to access an outdated API method (openai.ChatCompletion) that is no longer supported in the current version of the OpenAI library, indicating a missing dependency on the correct API version.",high,
1254,"AsyncOpenAI occasionally throws ""Runtime Error: Event loop is closed"" on Windows","### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

On Windows, if you try getting chat completions the openai library will occasionally throw an error saying `RuntimeError: Event loop is closed`. This seems to originate from httpx trying to close the connection. I noticed it primarily happens on short lived async functions that are run in rapid succession. It also only seems to happen if the `AsyncOpenAI` client is created multiple times. If it's created once (like outside the loop in the example code below) then the issue doesn't happen. I believe this may or may not be related to httpx's connection pooling trying to reuse a connection that no longer exists. [Here](https://github.com/encode/httpx/discussions/2959) is a related issue from httpx's repo.

Here are the logs from the exception in the code snippet provided in this issue.
```
Task exception was never retrieved
future: <Task finished name='Task-6' coro=<AsyncClient.aclose() done, defined at c:\Users\Ethan\tonic_validate_tester\venv\Lib\site-packages\httpx\_client.py:2011> exception=RuntimeError('Event loop is closed')>
Traceback (most recent call last):
  File ""c:\Users\Ethan\tonic_validate_tester\venv\Lib\site-packages\httpx\_client.py"", line 2018, in aclose
    await self._transport.aclose()
  File ""c:\Users\Ethan\tonic_validate_tester\venv\Lib\site-packages\httpx\_transports\default.py"", line 385, in aclose
    await self._pool.aclose()
  File ""c:\Users\Ethan\tonic_validate_tester\venv\Lib\site-packages\httpcore\_async\connection_pool.py"", line 313, in aclose
    await self._close_connections(closing_connections)
  File ""c:\Users\Ethan\tonic_validate_tester\venv\Lib\site-packages\httpcore\_async\connection_pool.py"", line 305, in _close_connections
    await connection.aclose()
  File ""c:\Users\Ethan\tonic_validate_tester\venv\Lib\site-packages\httpcore\_async\connection.py"", line 171, in aclose
    await self._connection.aclose()
  File ""c:\Users\Ethan\tonic_validate_tester\venv\Lib\site-packages\httpcore\_async\http11.py"", line 265, in aclose
    await self._network_stream.aclose()
  File ""c:\Users\Ethan\tonic_validate_tester\venv\Lib\site-packages\httpcore\_backends\anyio.py"", line 54, in aclose
    await self._stream.aclose()
  File ""c:\Users\Ethan\tonic_validate_tester\venv\Lib\site-packages\anyio\streams\tls.py"", line 202, in aclose
    await self.transport_stream.aclose()
  File ""c:\Users\Ethan\tonic_validate_tester\venv\Lib\site-packages\anyio\_backends\_asyncio.py"", line 1191, in aclose
    self._transport.close()
  File ""C:\Users\Ethan\AppData\Local\Programs\Python\Python311\Lib\asyncio\proactor_events.py"", line 109, in close
    self._loop.call_soon(self._call_connection_lost, None)
  File ""C:\Users\Ethan\AppData\Local\Programs\Python\Python311\Lib\asyncio\base_events.py"", line 761, in call_soon
    self._check_closed()
  File ""C:\Users\Ethan\AppData\Local\Programs\Python\Python311\Lib\asyncio\base_events.py"", line 519, in _check_closed
    raise RuntimeError('Event loop is closed')
RuntimeError: Event loop is closed
```

### To Reproduce

1. Run the code snippet provided on Windows 11; Python v3.11
2. Sometime during while the for loop is running you should see in the logs that the event loop is closed

### Code snippets

```Python
async def test() -> None:
    chat_completion = await client.chat.completions.create(
        messages=[
            {
                ""role"": ""user"",
                ""content"": ""Say this is a test"",
            }
        ],
        model=""gpt-3.5-turbo"",
    )

for i in range(50):
    # Note we create the client inside this loop as it triggers the bug
    client = AsyncOpenAI()
    asyncio.run(test())
```


### OS

Windows 11

### Python version

Python v3.11.7

### Library version

openai v1.12.0",closed,2024-03-20T17:47:15Z,2024-03-26T11:32:24Z,['bug'],https://github.com/openai/openai-python/issues/1254,137.75,False,,,,low,
1266,AsyncStream returning only empty choices.,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

When I am calling `client.chat.completions.create()` with `stream=True` I am getting only `ChatCompletionChunks` with 'empty' `choices.`

### To Reproduce

Run:

```
async with AsyncAzureOpenAI(
            api_key=os.environ['OPENAI_API_KEY'],
            azure_deployment=os.environ['OPENAI_AZURE_COMPLETIONS_DEPLOYMENT'],
            azure_endpoint=os.environ['OPENAI_BASE_URL'],
            api_version=""2023-12-01-preview""
        ) as client:
            openai_stream  = await client.chat.completions.create(
                model=""gpt-4"",
                messages=messages,
                temperature=0.2,
                max_tokens=1200,
                top_p=0.45,
                frequency_penalty=0,
                presence_penalty=0,
                stop=None,
                stream=True
            )
            logger.info(f""{openai_stream.__dict__=}"")
            
            
async for chunk in openai_stream:
        logger.info(f""{chunk.model_dump_json()=}"")
        logger.info(f""{chunk.__dict__=}"")
        current_response = chunk.choices[0].delta.content
        logger.info(f""{current_response=}"")
        yield current_response

```

returns:

```
openai_stream.__dict__={'response': <Response [200 OK]>, '_cast_to': <class 'openai.types.chat.chat_completion_chunk.ChatCompletionChunk'>, '_client': <openai.lib.azure.AsyncAzureOpenAI object at 0x0000011B098F2920>, '_decoder': <openai._streaming.SSEDecoder object at 0x0000011B0AA06890>, '_iterator': <async_generator object AsyncStream.__stream__ at 0x0000011B0985A9C0>, '__orig_class__': openai.AsyncStream[openai.types.chat.chat_completion_chunk.ChatCompletionChunk]}
chunk.model_dump_json()='{""id"":"""",""choices"":[],""created"":0,""model"":"""",""object"":"""",""system_fingerprint"":null,""prompt_filter_results"":[{""prompt_index"":0,""content_filter_results"":{""hate"":{""filtered"":false,""severity"":""safe""},""self_harm"":{""filtered"":false,""severity"":""safe""},""sexual"":{""filtered"":false,""severity"":""safe""},""violence"":{""filtered"":false,""severity"":""safe""}}}]}'
chunk.__dict__={'id': '', 'choices': [], 'created': 0, 'model': '', 'object': '', 'system_fingerprint': None}

```

### Code snippets

_No response_

### OS

Windows

### Python version

Python v3.10.11

### Library version

openai v1.10.0",closed,2024-03-22T18:28:13Z,2024-03-24T03:15:04Z,['bug'],https://github.com/openai/openai-python/issues/1266,32.78,True,input_value_violation,incorrect_functionality,"The API call returns empty choices, indicating that the input parameters may not be producing valid outputs.",high,
1271,Multiprocessing pattern unclear,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

Since v1.x, `openai` breaks use in `multiprocessing`, because `multiprocessing` chooses to pickle jobs and `openai` doesn't intend to support pickling (as far as I can see from #837).

This renders the pattern for doing multithreaded calling unclear. The reasons and policy are stated in #837 but no solution suggested. I'd like to be doing tens of thousands of requests per hour and I'd like to do these in parallel from a python program. What's the recommended pattern for multithreaded calling as of `openai` v.1x? I'm not working with more than about 100 parallel calls.

### To Reproduce

* wrap any chat/completion request in `multiprocessing`
* observe `TypeError: cannot pickle '_thread.RLock' object`

### Code snippets

_No response_

### OS

Linux

### Python version

Python 3.12

### Library version

1.12.0",closed,2024-03-25T14:21:17Z,2024-03-25T17:44:21Z,['bug'],https://github.com/openai/openai-python/issues/1271,3.38,False,,,,low,
749,Threads retrieve request not properly validated,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

OpenAI.beta.threads.retrieve does not validate the ""thread_id"" input.

Invoking
```python
t = client.beta.threads.retrieve(thread_id="""")
```
Throws ""APIStatusError"" with the following message:
```
Exception has occurred: APIStatusError
Error code: 307
httpx.HTTPStatusError: Redirect response '307 Temporary Redirect' for url 'https://api.openai.com/v1/threads/'
Redirect location: 'http://api.openai.com/v1/threads'
```

The expected behavior would be to fail validation at the library level. Another possible behavior would be to return a list with all the threads, as returned by the ```https://api.openai.com/v1/threads``` endpoint, but in that case it would be cleaner to implement something like ```client.beta.threads.retrieve_all()```



### To Reproduce

Run the following code:
```python
from openai import OpenAI

client = OpenAI(api_key=""sk-*****"")

t = client.beta.threads.retrieve(thread_id="""")

print(t)
```
This was tested with version 1.2.0 of the python library on 2023-11-09 9:16 UTC.

### Code snippets

_No response_

### OS

Ubuntu 22.04 Linux

### Python version

Python 3.11.5 (conda)

### Library version

openai v1.2.0",closed,2023-11-09T09:25:41Z,2024-03-25T16:52:49Z,['enhancement'],https://github.com/openai/openai-python/issues/749,3295.45,True,input_value_violation,high,"The 'thread_id' input is not validated, leading to an APIStatusError when an empty string is provided.",high,
1263,Python 3.7 support dropped accidentally?,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

This library seems to have problems with Python 3.7 now, possibly after this PR: https://github.com/openai/openai-python/pull/1114

I know Python 3.7 is itself EOL, but this library declares Python 3.7 support but then breaks downstream dependencies on install, such as checkov earlier versions. It would be best if this change could be reverted and then added back e.g. in a new major release and with Python supports updated.

### To Reproduce

1. Install this library using pip on a system with Python 3.7 as the default

### Code snippets

_No response_

### OS

Ubuntu 20.04

### Python version

Python v3.7

### Library version

openai latest",closed,2024-03-22T10:11:19Z,2024-03-25T16:51:36Z,['bug'],https://github.com/openai/openai-python/issues/1263,78.67,True,missing_option_violation,medium,"The library claims to support Python 3.7 but breaks compatibility, leading to issues with downstream dependencies.",high,
1242,"error: Module ""openai.types.beta.threads.run"" has no attribute ""ToolAssistantToolsFunction""","### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

Hi there, i was using old version 1.2.3, seems in the new version 1.14.0 `ToolAssistantToolsFunction` has been removed, but I need this function to validate openai tool schema. Thanks in advance!

### To Reproduce

from openai.types.beta.threads.run import ToolAssistantToolsFunction

### Code snippets

_No response_

### OS

macOS

### Python version

Python v 3.10

### Library version

openai v1.14.0",closed,2024-03-18T12:28:17Z,2024-03-21T01:58:11Z,['bug'],https://github.com/openai/openai-python/issues/1242,61.5,True,missing_dependency_violation,high,The removal of 'ToolAssistantToolsFunction' in the new library version indicates a missing dependency for users relying on this function for validation.,high,
1265,RuntimeWarning: coroutine 'AsyncAPIClient.post' was never awaited,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

Getting this error while using create_and_stream() method.

error logs:

`2024-03-22 20:05:30 /usr/src/app/app/services/openai_service.py:111: RuntimeWarning: coroutine 'AsyncAPIClient.post' was never awaited
2024-03-22 20:05:30   stream_or_run = await self.client.beta.threads.runs.create_and_stream(thread_id=thread_id, assistant_id=self.assistant_id, metadata=metadata, event_handler=event_handler)
2024-03-22 20:05:30 Object allocated at (most recent call last):
2024-03-22 20:05:30   File ""/usr/local/lib/python3.9/site-packages/openai/resources/beta/threads/runs/runs.py"", lineno 1322
2024-03-22 20:05:30     request = self._post(`

### To Reproduce

1. Install openai 1.14.2 module and tracemalloc
2. start the tracemalloc using the command `tracemalloc.start()`
2. Execute the async run `create_and_stream()` method using an example code.


Example code:

```from typing_extensions import override
from openai import AsyncAssistantEventHandler
 
class EventHandler(AsyncAssistantEventHandler):    
  @override
  async def on_text_created(self, text) -> None:
    print(f""\nassistant > "", end="""", flush=True)
      
  @override
   async def on_text_delta(self, delta, snapshot):
    print(delta.value, end="""", flush=True)
      
   async def on_tool_call_created(self, tool_call):
    print(f""\nassistant > {tool_call.type}\n"", flush=True)
  
   async def on_tool_call_delta(self, delta, snapshot):
    if delta.type == 'code_interpreter':
      if delta.code_interpreter.input:
        print(delta.code_interpreter.input, end="""", flush=True)
      if delta.code_interpreter.outputs:
        print(f""\n\noutput >"", flush=True)
        for output in delta.code_interpreter.outputs:
          if output.type == ""logs"":
            print(f""\n{output.logs}"", flush=True)
  
with client.beta.threads.runs.create_and_stream(
  thread_id=thread.id,
  assistant_id=assistant.id,
  instructions=""Please address the user as Jane Doe. The user has a premium account."",
  event_handler=EventHandler(),
) as stream:
  stream.until_done()
```

Example code was taken from https://platform.openai.com/docs/assistants/overview?context=with-streaming and updated for asynchronous operation.

### Code snippets

```Python
In runs.py under AsyncRuns class.

    def create_and_stream(
        self,
        *,
        assistant_id: str,
        additional_instructions: Optional[str] | NotGiven = NOT_GIVEN,
        instructions: Optional[str] | NotGiven = NOT_GIVEN,
        metadata: Optional[object] | NotGiven = NOT_GIVEN,
        model: Optional[str] | NotGiven = NOT_GIVEN,
        tools: Optional[Iterable[AssistantToolParam]] | NotGiven = NOT_GIVEN,
        thread_id: str,
        event_handler: AsyncAssistantEventHandlerT | None = None,
        # Use the following arguments if you need to pass additional parameters to the API that aren't available via kwargs.
        # The extra values given here take precedence over values defined on the client or passed to this method.
        extra_headers: Headers | None = None,
        extra_query: Query | None = None,
        extra_body: Body | None = None,
        timeout: float | httpx.Timeout | None | NotGiven = NOT_GIVEN,
    ) -> (
        AsyncAssistantStreamManager[AsyncAssistantEventHandler]
        | AsyncAssistantStreamManager[AsyncAssistantEventHandlerT]
    ):
        """"""Create a Run stream""""""
        if not thread_id:
            raise ValueError(f""Expected a non-empty value for `thread_id` but received {thread_id!r}"")

        extra_headers = {
            ""OpenAI-Beta"": ""assistants=v1"",
            ""X-Stainless-Stream-Helper"": ""threads.runs.create_and_stream"",
            ""X-Stainless-Custom-Event-Handler"": ""true"" if event_handler else ""false"",
            **(extra_headers or {}),
        }
        request = self._post(
            f""/threads/{thread_id}/runs"",
            body=maybe_transform(
                {
                    ""assistant_id"": assistant_id,
                    ""additional_instructions"": additional_instructions,
                    ""instructions"": instructions,
                    ""metadata"": metadata,
                    ""model"": model,
                    ""stream"": True,
                    ""tools"": tools,
                },
                run_create_params.RunCreateParams,
            ),
            options=make_request_options(
                extra_headers=extra_headers, extra_query=extra_query, extra_body=extra_body, timeout=timeout
            ),
            cast_to=Run,
            stream=True,
            stream_cls=AsyncStream[AssistantStreamEvent],
        )
        return AsyncAssistantStreamManager(request, event_handler=event_handler or AsyncAssistantEventHandler())
```


### OS

macOS

### Python version

Python 3.10.13

### Library version

openai v1.14.2",closed,2024-03-22T15:47:46Z,2024-03-24T03:05:09Z,['bug'],https://github.com/openai/openai-python/issues/1265,35.29,True,method_order_violation,high,"The coroutine 'AsyncAPIClient.post' was never awaited, indicating that an asynchronous method was not called in the correct order, leading to potential runtime errors.",high,
1248,Title: Integration Issue with DALL-E 3 in an Asynchronous Telegram Bot Using Python,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

Hello OpenAI Community!

I'm working on an asynchronous Telegram bot in Python, utilizing the OpenAI API for generating text with the GPT-3.5 Turbo model. The text generation part is functioning smoothly, however, I've encountered difficulties when trying to add the capability to generate images using DALL-E 3.

I'm using the latest version of the openai library (installed via !pip install --upgrade openai), but my attempts to use openai.Completion or any related methods for DALL-E result in the following error:

### To Reproduce

ERROR:__main__:Error processing message: 

You tried to access openai.Completion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.

You can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. 

Alternatively, you can pin your installation to the old version, e.g., `pip install openai==0.28`

A detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742


### Code snippets

```Python
import openai

openai.api_key = 'my_openai_api_key'

# Example code that leads to the error
try:
    response = openai.Completion.create(
        engine=""text-davinci-003"",
        prompt=""This is a test prompt."",
        temperature=0.7,
        max_tokens=150
    )
except Exception as e:
    print(f""Encountered an error: {e}"")
```


### OS

Google Colab

### Python version

3.10.12

### Library version

1.14.1",closed,2024-03-19T12:17:24Z,2024-03-21T02:12:22Z,['bug'],https://github.com/openai/openai-python/issues/1248,37.92,True,missing_option_violation,high,"The user is attempting to use openai.Completion for DALL-E 3, which is not supported in the latest version of the library. They need to follow the migration guide to use the correct methods for image generation.",high,
1241,Non caught exception `NetworkError` is raised by `httpx`,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

In some circumstances of faulty network (like on developer laptop, poor VPN, …), `httpx` raises a `NetworkError` (`ConnectError`, `ReadError`, `WriteError`, `CloseError`) exception. This error is related to the network management of the library, but the library do not catch it.

It would be easier to catch `NetworkError` and to map it to a `APIError`.

[See HTTPX doc for exception handling.](https://www.python-httpx.org/exceptions/)

### To Reproduce

1. Run any command involving the remote endpoint
2. Close / interrupt the network connection a few seconds
3. See the error

### Code snippets

_No response_

### OS

macOS 14.3.1

### Python version

Python v3.12.1

### Library version

openai v1.14.0",closed,2024-03-16T12:49:10Z,2024-03-16T13:03:32Z,['bug'],https://github.com/openai/openai-python/issues/1241,0.24,True,missing_dependency_violation,medium,"The library does not catch the `NetworkError`, which is a missing dependency in error handling for network-related API calls.",high,
1237,Streaming Assistant Not returning result after submitting the tool result,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

I am trying to use streaming assistant with function calling and whenever the tool output is submitted, stream ends , it is not getting the final message or response from assistant.  Please let me know , whether I am doing something wrong 

```code
from openai import OpenAI
import time
import json
import time

from openai.types.beta import AssistantStreamEvent

client = OpenAI()


def get_current_weather(location, unit=""fahrenheit""):
    """"""Get the current weather in a given location""""""
    if ""tokyo"" in location.lower():
        return json.dumps({""location"": location, ""temperature"": ""10"", ""unit"": ""celsius""})
    elif ""san francisco"" in location.lower():
        return json.dumps({""location"": location, ""temperature"": ""72"", ""unit"": ""fahrenheit""})
    else:
        return json.dumps({""location"": location, ""temperature"": ""22"", ""unit"": ""celsius""})


def getNickname(location: str) -> str:
    return location + ""God""


from typing_extensions import override
from openai import AssistantEventHandler

function_map = {
    ""getCurrentWeather"": get_current_weather,
    ""getNickname"": getNickname
}

from dataclasses import dataclass


@dataclass
class tool_output:
    tool_call_id: str
    output: str


from openai.types.beta.threads import Message, MessageDelta
from openai.types.beta.threads.runs import ToolCall, RunStep


class EventHandler(AssistantEventHandler):
    def __init__(self, thread_id, assistant_id):
        super().__init__()
        self.output = None
        self.tool_id = None
        self.thread_id = thread_id
        self.assistant_id = assistant_id

    @override
    def on_text_created(self, text) -> None:
        print(f""\nassistant > "", end="""", flush=True)

    @override
    def on_text_delta(self, delta, snapshot):
        print(delta.value, end="""", flush=True)

    @override
    def on_end(self, ):
        print(f""\n end assistant > "",self.current_run_step_snapshot, end="""", flush=True)

    @override
    def on_exception(self, exception: Exception) -> None:
        """"""Fired whenever an exception happens during streaming""""""
        print(f""\nassistant > {exception}\n"", end="""", flush=True)

    @override
    def on_message_created(self, message: Message) -> None:
        print(f""\nassistant > {message}\n"", end="""", flush=True)

    @override
    def on_message_done(self, message: Message) -> None:
        print(f""\nassistant > {message}\n"", end="""", flush=True)

    @override
    def on_message_delta(self, delta: MessageDelta, snapshot: Message) -> None:
        print(f""\nassistant > {delta}\n"", end="""", flush=True)

    @override
    def on_tool_call_created(self, tool_call):
        print(""The type of tool call is "", type(tool_call), flush=True)

        # print(f""\n tool call created assistant > {tool_call.type}\n, >>>> {tool_call.function.json}"", flush=True)
        # print(""function name is "",tool_call.function.name,flush=True)
        # print(""function argument is "", tool_call.function.arguments, flush=True)
        # function_name=tool_call.function.name
        # print(tool_call.function.arguments)
        # function_args=json.loads(tool_call.function.arguments)
        # fn = function_map.get(function_name)
        # output=fn(function_args.get(""location""))
        # print(output)
        # tool_output(tool_call.id,output)

    @override
    def on_tool_call_done(self, tool_call: ToolCall) -> None:
        print("" toolcall val in on_tool_call_done is The type of tool call is "", tool_call, flush=True)

        tool_id = tool_call.id
        f = tool_call.function.name
        arg = json.loads(tool_call.function.arguments)
        fn = function_map.get(f)
        output = fn(arg.get(""location""))
        run_id = self.current_run.id
        print(""The output of function call is "",output,flush=True)
        self.tool_id=tool_id
        self.output=output
        client.beta.threads.runs.submit_tool_outputs(
            thread_id=thread.id,
            run_id= self.current_run.id,
            tool_outputs=[{
                ""tool_call_id"": self.tool_id,
                ""output"": self.output,
            }],

            stream=True
        )





    @override
    def on_run_step_created(self, run_step: RunStep) -> None:
        print(""The type of run step is "", type(run_step), flush=True)
        print(f""\n run step created assistant > {run_step}\n"", flush=True)

    @override
    def on_run_step_done(self, run_step: RunStep) -> None:
        print(f""\n run step done assistant > {run_step}\n"", flush=True)

    def on_tool_call_delta(self, delta, snapshot):
        # print(f""\n tool call delta assistant > {delta.type}\n"", flush=True)

        if delta.type == 'code_interpreter':
            if delta.code_interpreter.input:
                print(delta.code_interpreter.input, end="""", flush=True)
            if delta.code_interpreter.outputs:
                print(f""\n\n tool call delta output >"", flush=True)
                for output in delta.code_interpreter.outputs:
                    if output.type == ""logs"":
                        print(f""\n{output.logs}"", flush=True)

    @override
    def on_event(self, event: AssistantStreamEvent) -> None:
        print(""In on_event of event is "", event.event, flush=True)


thread = client.beta.threads.create()
print(thread.id)

message = client.beta.threads.messages.create(
    thread_id=thread.id,
    role=""user"",
    content=""Get me weather of San Francisco "",
)

with client.beta.threads.runs.create_and_stream(
        thread_id=thread.id,
        assistant_id=""asst_x"",
        event_handler=EventHandler(thread.id, ""asst_x""),

) as stream:

    stream.until_done()
    print(""\n"")
    print(stream.current_run_step_snapshot)





# stream = client.beta.threads.runs.create(
#     thread_id=thread.id,
#     assistant_id=""asst_x"",
#     instructions=""Please address the user as Jane Doe. The user has a premium account."",
#     stream=True,
# )

# for event in stream:
#     print(event.model_dump_json(indent=2, exclude_unset=True))


    # print(stream.get_final_messages())

```


The last line of output prints that thread.run.step is in progress but stream is completed and program exits
```

 end assistant >  RunStep(id='step_x', assistant_id='asst_a', cancelled_at=None, completed_at=None, created_at=1710513033, expired_at=None, failed_at=None, last_error=None, metadata=None, object='thread.run.step', run_id='run_x', status='in_progress', step_details=ToolCallsStepDetails(tool_calls=[FunctionToolCall(id='call_x', function=Function(arguments='{""location"":""San Francisco, CA"",""unit"":""f""}', name='getCurrentWeather', output=None), type='function', index=0)], type='tool_calls'), thread_id='thread_x', type='tool_calls', usage=None, expires_at=1710513631)

RunStep(id='step_x', assistant_id='asst_x', cancelled_at=None, completed_at=None, created_at=1710513033, expired_at=None, failed_at=None, last_error=None, metadata=None, object='thread.run.step', run_id='run_x', status='in_progress', step_details=ToolCallsStepDetails(tool_calls=[FunctionToolCall(id='call_x', function=Function(arguments='{""location"":""San Francisco, CA"",""unit"":""f""}', name='getCurrentWeather', output=None), type='function', index=0)], type='tool_calls'), thread_id='thread_x', type='tool_calls', usage=None, expires_at=1710513631)
```

Assistant creation

```code


assistant = client.beta.assistants.create(
  instructions=""You are a weather bot. Use the provided functions to answer questions."",
  model=""gpt-4-1106-preview"",
  tools=[{
      ""type"": ""function"",
    ""function"": {
      ""name"": ""getCurrentWeather"",
      ""description"": ""Get the weather in location"",
      ""parameters"": {
        ""type"": ""object"",
        ""properties"": {
          ""location"": {""type"": ""string"", ""description"": ""The city and state e.g. San Francisco, CA""},
          ""unit"": {""type"": ""string"", ""enum"": [""c"", ""f""]}
        },
        ""required"": [""location""]
      }
    }
  }, {
    ""type"": ""function"",
    ""function"": {
      ""name"": ""getNickname"",
      ""description"": ""Get the nickname of a city"",
      ""parameters"": {
        ""type"": ""object"",
        ""properties"": {
          ""location"": {""type"": ""string"", ""description"": ""The city and state e.g. San Francisco, CA""},
        },
        ""required"": [""location""]
      }
    } 
  }]
)

```



### To Reproduce

Please use the above code to reproduce

### Code snippets

_No response_

### OS

windows

### Python version

3.11

### Library version

1.14.0",closed,2024-03-15T14:40:42Z,2024-03-21T02:05:01Z,['bug'],https://github.com/openai/openai-python/issues/1237,131.41,True,missing_dependency_violation,medium,"The tool output submission does not appear to be followed by a proper handling of the final response, leading to an incomplete interaction.",high,
1253,An openai.APIConnectionError occurred calling the api,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

Hi ,I can call OpenAI Api interface with my Api key  but calling the Api using OpenAI-python results in an error ,it error. can you show how did you solve this problem.
Api example:
`❯ curl https://api.openai.com/v1/chat/completions
-H ""Content-Type: application/Json""
-H ""Authorization: Bearer key""
-d '{
""model"": ""gpt-3.5-turbo"",
""messages"": [{""role"": ""user"", ""content"": ""how to use MySQL!""}],
""temperature"": 0.7
}'

{
""id"": ""cxxxxxx"",
""object"": ""chat. Completion"",
""created"": 1710945410,
""model"": ""gpt-3.5-turbo-0125"",
""choices"": [
{
""index"": 0,
""message"": {
""role"": ""assistant"",
""content"": ""To use MySQL, you will need to follow xxxx""
},
""logprobs"": null,
""finish_reason"": ""stop""
}
],
""usage"": {
""prompt_tokens"": 12,
""completion_tokens"": 282,
""total_tokens"": 294
},
""system_fingerprint"": ""xxxx""
}`

error log is :
File ""/Users/xxx/Library/Caches/pypoetry/virtualenvs/backend-FuQBPevx-py3.11/lib/python3.11/site-packages/openai/_base_client.py"", line 1494, in _request
return await self._retry_request(
^^^^^^^^^^^^^^^^^^^^^^^^^^
File ""/Users/xxx/Library/Caches/pypoetry/virtualenvs/backend-FuQBPevx-py3.11/lib/python3.11/site-packages/openai/_base_client.py"", line 1563, in _retry_request
return await self._request(
^^^^^^^^^^^^^^^^^^^^
File ""/Users/xxxx/Library/Caches/pypoetry/virtualenvs/backend-FuQBPevx-py3.11/lib/python3.11/site-packages/openai/_base_client.py"", line 1504, in _request
raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.

### To Reproduce

calling the api using openapi-python

### Code snippets

_No response_

### OS

mac os

### Python version

python3.11

### Library version

openapi v 1.0.0",closed,2024-03-20T15:27:22Z,2024-03-21T01:50:16Z,['bug'],https://github.com/openai/openai-python/issues/1253,10.38,True,missing_dependency_violation,medium,"The error indicates a potential missing dependency in the API call setup, possibly related to connection parameters or network configuration that is not being handled correctly in the Python library.",high,
1252,Stream Parameter Issue in Async Thread Creation,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

The bug arises when attempting to create a thread in the OpenAI Python library using asynchronous functions. It seems that the issue lies in passing the `stream` parameter, which is unrecognized, causing a `BadRequestError` with error code 400. This error indicates that the OpenAI API does not recognize the `stream` parameter as a valid parameter for thread creation. As a result, the thread creation process fails, leading to the generation of a `BadRequestError` and preventing the thread from being created successfully.

### To Reproduce

To reproduce the behavior:

1. Set up the OpenAI Python library and ensure that all necessary dependencies are installed.
2. Copy the provided code into a Python script or notebook.
3. Ensure that environment variables such as `API_VERSION`, `AZURE_ENDPOINT`, and `AZURE_OPENAI_KEY` are correctly set with appropriate values.
4. Execute the script or notebook.
5. Upon execution, the script will attempt to create a thread with specified messages and instructions.
6. The bug occurs during the creation of the thread when the `stream` parameter is passed, causing the `BadRequestError` with error code 400 to be raised.
7. The error traceback will indicate that the `stream` parameter is unrecognized, leading to the failure of the thread creation process.

### Code snippets

```Python
from __future__ import annotations

from typing_extensions import override

import openai
from openai import AssistantEventHandler
from openai.types.beta import AssistantStreamEvent
from openai.types.beta.threads import Text, TextDelta
from openai.types.beta.threads.runs import RunStep, RunStepDelta
import os
from dotenv import load_dotenv

load_dotenv()


class EventHandler(AssistantEventHandler):
    @override
    def on_event(self, event: AssistantStreamEvent) -> None:
        if event.event == ""thread.run.step.created"":
            details = event.data.step_details
            if details.type == ""tool_calls"":
                print(""Generating code to interpret:\n\n"")
        elif event.event == ""thread.message.created"":
            print(""\nResponse:\n"")

    @override
    def on_text_delta(self, delta: TextDelta, snapshot: Text) -> None:
        print(delta.value, end="""", flush=True)

    @override
    def on_run_step_done(self, run_step: RunStep) -> None:
        details = run_step.step_details
        if details.type == ""tool_calls"":
            for tool in details.tool_calls:
                if tool.type == ""code_interpreter"":
                    print(""\n\nExecuting code..."")

    @override
    def on_run_step_delta(self, delta: RunStepDelta, snapshot: RunStep) -> None:
        details = delta.step_details
        if details is not None and details.type == ""tool_calls"":
            for tool in details.tool_calls or []:
                if tool.type == ""code_interpreter"" and tool.code_interpreter and tool.code_interpreter.input:
                    print(tool.code_interpreter.input, end="""", flush=True)


async def main() -> None:
    client = openai.AsyncAzureOpenAI(
        api_version=os.getenv(""API_VERSION""),
        azure_endpoint=os.getenv(""AZURE_ENDPOINT""),
        api_key=os.getenv(""AZURE_OPENAI_KEY""),
    )

    assistant = await client.beta.assistants.create(
        name=""Math Tutor"",
        instructions=""You are a personal math tutor. Write and run code to answer math questions."",
        tools=[{""type"": ""code_interpreter""}],
        model=""gpt-4-1106-preview"",
    )

    try:
        question = ""I need to solve the equation `3x + 11 = 14`. Can you help me?""

        thread = await client.beta.threads.create(
            messages=[
                {
                    ""role"": ""user"",
                    ""content"": question,
                },
            ]
        )
        print(f""Question: {question}\n"")

        async with client.beta.threads.runs.create_and_stream(
            thread_id=thread.id,
            assistant_id=assistant.id,
            instructions=""Please address the user as Jane Doe. The user has a premium account."",
            event_handler=EventHandler(),
        ) as stream:
            stream.until_done()
            print()
    finally:
        client.beta.assistants.delete(assistant.id)


await main()
```


### OS

windows

### Python version

Python v3.11.2

### Library version

openai v1.14.1",closed,2024-03-20T14:58:23Z,2024-03-21T01:45:36Z,['bug'],https://github.com/openai/openai-python/issues/1252,10.79,True,input_value_violation,high,"The 'stream' parameter is unrecognized by the OpenAI API, leading to a BadRequestError.",high,
996,AsyncCompletions.create not recognized as coroutine by inspect.iscoroutinefunction,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

The `AsyncCompletions.create` method in the `openai-python` library is an asynchronous method that one would expect to be recognized as a coroutine by the `inspect.iscoroutinefunction`. However, checks against this method currently return `False`, suggesting that it is not being identified as an async function. This could lead to issues in asynchronous programming where the correct identification of coroutines is crucial for event loops and async frameworks to schedule and execute tasks appropriately.

### To Reproduce

1. Install the latest version of the OpenAI Python library.
2. Use the `inspect.iscoroutinefunction` to check if `AsyncCompletions.create` is recognized as a coroutine.
3. The result of this check is `False` instead of the expected `True`.

### Code snippets

```Python
import inspect
import openai

is_coroutine = inspect.iscoroutinefunction(openai.resources.AsyncCompletions.create)
print(f'Is `openai.resources.AsyncCompletions.create` a coroutine function? {is_coroutine}')
```


### OS

Windows 11

### Python version

Python v3.9.18

### Library version

openai v1.5.0",closed,2023-12-20T11:43:22Z,2023-12-23T22:41:30Z,[],https://github.com/openai/openai-python/issues/996,82.97,True,input_type_violation,medium,"The method `AsyncCompletions.create` is not recognized as a coroutine, which indicates a potential type mismatch in its implementation, affecting its usability in asynchronous contexts.",high,
1232,`CompletionUsage.generation` is `None` sometimes,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

When getting a completion using `gpt-4-vision` the `generation` field of the `CompletionUsage` is sometimes `None`. I don't know when this happens exactly. This is not in accordance with the description or the `BaseModel`, which claims `generation` should always be `int`. 

Sorry if this is not a python issue.

### To Reproduce

Use `gpt-4-vision`, send an image. I'm not sure what the reason is.

### Code snippets

_No response_

### OS

macOS

### Python version

v3.10.8

### Library version

openai v1.3.6",closed,2024-03-13T15:27:27Z,2024-03-14T20:22:36Z,['bug'],https://github.com/openai/openai-python/issues/1232,28.92,True,input_type_violation,high,"The 'generation' field of the CompletionUsage is sometimes None, which violates the expected type of int as per the BaseModel.",high,
1118,Bugs When using OpenAI api with Celery/redit,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

Hello,
There seem to be a bug when using the Openai API with Celery/redit shared task. When running the function bellow.
I’m not sure exactly why. When In use langchain with Mistral api, Ollama or with any my own custom models it works really well, but it crashes when I use anything touching the openai api. I tried bypassing langchain to work directly with the openai api and I still do get this error  Does someone have any idea why ?



### To Reproduce

1. pip install Django
2. pip install celery
3. brew install redit // for macs 
4. create a random view function that will call the task at task.py

### Code snippets

in views.py
```Python
def some_view_fucntion(request):
    for x in range(2):
         some_ai_function.delay()
```

in task.py
```Python
@shared_task
def some_ai_function():
    message = ""Hello, write a message about testing ""
    print(message)
    completion = openai.chat.completions.create(
        model=“gpt-3.5-turbo”,
        messages=[
            {
                 “role”: “user”,
                 “content”: message,
            },
        ],
    )
    print(completion.choices[0].message.content)
```

RETURNS :
```Terminal
ERROR/MainProcess] Process ‘ForkPoolWorker-2’ pid:86900 exited with ‘signal 5 (SIGTRAP)’
ERROR/MainProcess] Task handler raised error: WorkerLostError(‘Worker exited prematurely: signal 5 (SIGTRAP) Job: 0.’)

Traceback (most recent call last):
File"" …
/python3.11/site-packages/billiard/pool.py"", line 1264, in mark_as_worker_lost
raise WorkerLostError(
billiard.exceptions.WorkerLostError: Worker exited prematurely: signal 5 (SIGTRAP) Job: 0.
```


### OS

macOS (m2)

### Python version

Python 3.11.5

### Library version

openai 1.8.0
```[tasklist]
### Tasks
- [ ] Create a work around get_architecture()  for celery tasks on MacOS
```
",closed,2024-02-02T08:41:11Z,2024-02-04T18:53:11Z,['bug'],https://github.com/openai/openai-python/issues/1118,58.2,True,input_value_violation,high,"The OpenAI API call may be receiving an improperly formatted or unacceptable input value, particularly in the 'messages' parameter, leading to a crash.",high,
1235,Client not defined,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

I migrated to new API, followed the guide
![image_2024-03-14_005652531](https://github.com/openai/openai-python/assets/163383302/d4649265-b517-4def-a46d-39859ba120c5)
![image_2024-03-14_005719321](https://github.com/openai/openai-python/assets/163383302/efe1d571-07bc-4845-8f25-e8ebd6be9984)
DISCLAIMER: I DO NOT KNOW HOW TO CODE AT ALL, I do not understand anything, I will just do whatever im told if anyone can fix this problem, I used ChatGPT to get all the way here making a website with a chatbot, im having fun and experimenting in code for the first time

### To Reproduce

Idk how to reproduce just look at the pictures I guess. When I did the migration, after pasting and running the code in WSL, it said it was complete or successful so I closed WSL. after that the second picture is the code I added.

### Code snippets

_No response_

### OS

Windows 11

### Python version

Python v3.12.2

### Library version

openai v1.14.0",closed,2024-03-14T06:00:32Z,2024-03-14T06:02:16Z,['bug'],https://github.com/openai/openai-python/issues/1235,0.03,True,missing_dependency_violation,high,"The issue indicates that the 'client' is not defined, suggesting a missing initialization or configuration that is required before making API calls.",high,
1204,Allow for custom SSL certs to be used by the APIClient classes,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

Currently, the APIClient classes allow for custom timeout, custom headers, custom max_retries, to be easily passed in as parameters.

The only way to pass in custom SSL certs are by one of these two options:

- Setting the `SSL_CERT_FILE` environment variable. This causes issues with async calls.
- Passing in a custom `http_client`. This causes inconsistencies, and is unnecessarily complicated when handling both Sync and Async clients through a 3rd party library like langchain.

Instead, `verify` should be an argument that can be passed in just like `timeout` or `max_retries`.

PR #1205 fixes this issue.

### To Reproduce

1. Try to use custom SSL certs.
2. Note the difficulty in doing so.
3. Merge in #1205 to make the task simple.

### Code snippets

_No response_

### OS

Ubuntu 20.04.5 LTS

### Python version

Python v.3.11.6

### Library version

openai v1.13.3",closed,2024-03-01T15:28:16Z,2024-03-08T00:21:57Z,['enhancement'],https://github.com/openai/openai-python/issues/1204,152.89,True,missing_option_violation,medium,"The APIClient classes do not provide a straightforward option to pass custom SSL certs, leading to complications and inconsistencies.",high,
416,Expose X-Ratelimit-* headers on ChatCompletion API requests,"### Describe the feature or improvement you're requesting

If you make a call to `POST /v1/chat/completions` manually, you get a few very helpful headers that estimate time to rate-limiting

```
x-ratelimit-limit-requests: 3500
x-ratelimit-remaining-requests: 3499
x-ratelimit-reset-requests: 17ms
```

However, if you call `openai.ChatCompletion.create(...)` or `openai.ChatCompletion.acreate(...)`, these helpful rate limit values are not returned by the function.

It would be nice if we could get access to these headers, or have them returned in a helpful format along with the response

### Additional context

_No response_",closed,2023-04-26T15:29:41Z,2023-11-06T16:43:37Z,"['enhancement', 'fixed in v1']",https://github.com/openai/openai-python/issues/416,4657.23,False,,,,,
1215,openai.APITimeoutError: Request timed,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

我用from openai import OpenAI这个库访问时候回复速度很快，但是我在利用llamaindex来结合gpt与模型时候，queryengine.query（）这个语句就会报错openai.APITimeoutError: Request timed，以下是我的报错信息：
03/05/2024 16:42:02 - [INFO] -openai._base_client->>> Retrying request to /chat/completions in 0.859262 seconds
03/05/2024 16:42:23 - [INFO] -openai._base_client->>> Retrying request to /chat/completions in 1.629988 seconds
03/05/2024 16:42:46 - [INFO] -openai._base_client->>> Retrying request to /chat/completions in 3.447563 seconds
Traceback (most recent call last):
File ""C:\Users\590\anaconda3\envs\bce\lib\site-packages\httpx_transports\default.py"", line 69, in map_httpcore_exceptions
yield
File ""C:\Users\590\anaconda3\envs\bce\lib\site-packages\httpx_transports\default.py"", line 233, in handle_request
resp = self._pool.handle_request(req)
File ""C:\Users\590\anaconda3\envs\bce\lib\site-packages\httpcore_sync\connection_pool.py"", line 216, in handle_request
raise exc from None
File ""C:\Users\590\anaconda3\envs\bce\lib\site-packages\httpcore_sync\connection_pool.py"", line 196, in handle_request
response = connection.handle_request(
File ""C:\Users\590\anaconda3\envs\bce\lib\site-packages\httpcore_sync\connection.py"", line 99, in handle_request
raise exc
File ""C:\Users\590\anaconda3\envs\bce\lib\site-packages\httpcore_sync\connection.py"", line 76, in handle_request
stream = self._connect(request)
File ""C:\Users\590\anaconda3\envs\bce\lib\site-packages\httpcore_sync\connection.py"", line 122, in _connect
stream = self._network_backend.connect_tcp(**kwargs)
File ""C:\Users\590\anaconda3\envs\bce\lib\site-packages\httpcore_backends\sync.py"", line 205, in connect_tcp
with map_exceptions(exc_map):
File ""C:\Users\590\anaconda3\envs\bce\lib\contextlib.py"", line 153, in exit
self.gen.throw(typ, value, traceback)
File ""C:\Users\590\anaconda3\envs\bce\lib\site-packages\httpcore_exceptions.py"", line 14, in map_exceptions
raise to_exc(exc) from exc
httpcore.ConnectTimeout: [WinError 10060] 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
File ""C:\Users\590\anaconda3\envs\bce\lib\site-packages\openai_base_client.py"", line 918, in _request
response = self._client.send(
File ""C:\Users\590\anaconda3\envs\bce\lib\site-packages\httpx_client.py"", line 914, in send
response = self._send_handling_auth(
File ""C:\Users\590\anaconda3\envs\bce\lib\site-packages\httpx_client.py"", line 942, in _send_handling_auth
response = self._send_handling_redirects(
File ""C:\Users\590\anaconda3\envs\bce\lib\site-packages\httpx_client.py"", line 979, in _send_handling_redirects
response = self._send_single_request(request)
File ""C:\Users\590\anaconda3\envs\bce\lib\site-packages\httpx_client.py"", line 1015, in _send_single_request
response = transport.handle_request(request)
File ""C:\Users\590\anaconda3\envs\bce\lib\site-packages\httpx_transports\default.py"", line 232, in handle_request
with map_httpcore_exceptions():
File ""C:\Users\590\anaconda3\envs\bce\lib\contextlib.py"", line 153, in exit
self.gen.throw(typ, value, traceback)
File ""C:\Users\590\anaconda3\envs\bce\lib\site-packages\httpx_transports\default.py"", line 86, in map_httpcore_exceptions
raise mapped_exc(message) from exc
httpx.ConnectTimeout: [WinError 10060] 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
File ""D:\BCE\pycharmcode\test2.py"", line 51, in
query_response = query_engine.query(query)
File ""C:\Users\590\anaconda3\envs\bce\lib\site-packages\llama_index\core\base_query_engine.py"", line 40, in query
return self._query(str_or_query_bundle)
File ""C:\Users\590\anaconda3\envs\bce\lib\site-packages\llama_index\query_engine\retriever_query_engine.py"", line 172, in _query
response = self._response_synthesizer.synthesize(
File ""C:\Users\590\anaconda3\envs\bce\lib\site-packages\llama_index\response_synthesizers\base.py"", line 168, in synthesize
response_str = self.get_response(
File ""C:\Users\590\anaconda3\envs\bce\lib\site-packages\llama_index\response_synthesizers\compact_and_refine.py"", line 38, in get_response
return super().get_response(
File ""C:\Users\590\anaconda3\envs\bce\lib\site-packages\llama_index\response_synthesizers\refine.py"", line 146, in get_response
response = self._give_response_single(
File ""C:\Users\590\anaconda3\envs\bce\lib\site-packages\llama_index\response_synthesizers\refine.py"", line 202, in _give_response_single
program(
File ""C:\Users\590\anaconda3\envs\bce\lib\site-packages\llama_index\response_synthesizers\refine.py"", line 64, in call
answer = self._llm.predict(
File ""C:\Users\590\anaconda3\envs\bce\lib\site-packages\llama_index\llms\llm.py"", line 239, in predict
chat_response = self.chat(messages)
File ""C:\Users\590\anaconda3\envs\bce\lib\site-packages\llama_index\llms\base.py"", line 100, in wrapped_llm_chat
f_return_val = f(_self, messages, **kwargs)
File ""C:\Users\590\anaconda3\envs\bce\lib\site-packages\llama_index\llms\openai.py"", line 237, in chat
return chat_fn(messages, **kwargs)
File ""C:\Users\590\anaconda3\envs\bce\lib\site-packages\llama_index\llms\openai.py"", line 296, in _chat
response = client.chat.completions.create(
File ""C:\Users\590\anaconda3\envs\bce\lib\site-packages\openai_utils_utils.py"", line 275, in wrapper
return func(*args, **kwargs)
File ""C:\Users\590\anaconda3\envs\bce\lib\site-packages\openai\resources\chat\completions.py"", line 663, in create
return self._post(
File ""C:\Users\590\anaconda3\envs\bce\lib\site-packages\openai_base_client.py"", line 1200, in post
return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
File ""C:\Users\590\anaconda3\envs\bce\lib\site-packages\openai_base_client.py"", line 889, in request
return self._request(
File ""C:\Users\590\anaconda3\envs\bce\lib\site-packages\openai_base_client.py"", line 927, in _request
return self._retry_request(
File ""C:\Users\590\anaconda3\envs\bce\lib\site-packages\openai_base_client.py"", line 1013, in _retry_request
return self._request(
File ""C:\Users\590\anaconda3\envs\bce\lib\site-packages\openai_base_client.py"", line 927, in _request
return self._retry_request(
File ""C:\Users\590\anaconda3\envs\bce\lib\site-packages\openai_base_client.py"", line 1013, in _retry_request
return self._request(
File ""C:\Users\590\anaconda3\envs\bce\lib\site-packages\openai_base_client.py"", line 927, in _request
return self._retry_request(
File ""C:\Users\590\anaconda3\envs\bce\lib\site-packages\openai_base_client.py"", line 1013, in _retry_request
return self._request(
File ""C:\Users\590\anaconda3\envs\bce\lib\site-packages\openai_base_client.py"", line 937, in _request
raise APITimeoutError(request=request) from err
openai.APITimeoutError: Request timed out.

### To Reproduce

我的代码是这样的：（我删除了apikey和apiurl）
# 我们在`BCEmbedding`中提供llama_index直接集成的接口。
from BCEmbedding.tools.llama_index import BCERerank

import os
from llama_index.embeddings import HuggingFaceEmbedding
from llama_index import VectorStoreIndex, ServiceContext, SimpleDirectoryReader
from llama_index.node_parser import SimpleNodeParser
from llama_index.llms import OpenAI
from llama_index.retrievers import VectorIndexRetriever

# init embedding model and reranker model
embed_args = {'model_name': 'D:\\BCE\\model\\bce-embedding-base_v1', 'max_length': 512, 'embed_batch_size': 32, 'device': 'cuda:0'}
embed_model = HuggingFaceEmbedding(**embed_args)

reranker_args = {'model': 'D:\\BCE\\model\\bce-reranker-base_v1', 'top_n': 5, 'device': 'cuda:0'}
reranker_model = BCERerank(**reranker_args)
# example #1. extract embeddings
query1 = 'apples'
passages = [
        'I like apples', 
        'I like oranges', 
        'Apples and oranges are fruits'
    ]
query_embedding = embed_model.get_query_embedding(query1)
passages_embeddings = embed_model.get_text_embedding_batch(passages)
print('passages_em:'+str(passages_embeddings))
print('query_em:'+str(query_embedding))
# example #2. rag example
llm = OpenAI(model='gpt-3.5-turbo', api_key=os.environ.get(''),api_base=os.environ.get(''))
service_context = ServiceContext.from_defaults(llm=llm, embed_model=embed_model)

documents = SimpleDirectoryReader(input_files=[""D:\\BCE\\pycharmcode\\文章.pdf"",""D:\\BCE\\pycharmcode\\文章2.pdf"",""D:\\BCE\\pycharmcode\\文章3.pdf""]).load_data()
node_parser = SimpleNodeParser.from_defaults(chunk_size=512)
nodes = node_parser.get_nodes_from_documents(documents[0:36])
index = VectorStoreIndex(nodes, service_context=service_context)
query = ""请你给我介绍一下这个游戏""

# example #2.1. retrieval with EmbeddingModel and RerankerModel
vector_retriever = VectorIndexRetriever(index=index, similarity_top_k=5, service_context=service_context)
retrieval_by_embedding = vector_retriever.retrieve(query)
retrieval_by_reranker = reranker_model.postprocess_nodes(retrieval_by_embedding, query_str=query)
print(retrieval_by_embedding)
print(retrieval_by_reranker)
# example #2.2. query with EmbeddingModel and RerankerModel

query_engine = index.as_query_engine(node_postprocessors=[reranker_model])
print(query_engine)
query_response = query_engine.query(query)
# print(query_response)


### Code snippets

_No response_

### OS

windows11

### Python version

Python 3.10.13

### Library version

1.13.3",closed,2024-03-05T09:32:08Z,2024-03-06T22:54:00Z,['bug'],https://github.com/openai/openai-python/issues/1215,37.36,True,method_order_violation,performance_degradation,"The query method is called without ensuring that all necessary context and dependencies are properly initialized, leading to a timeout error.",high,
1214,Ability to change OPENAI_BASE_URL and OPENAI_API_HOST for testing purposes,"### Confirm this is a feature request for the Python library and not the underlying OpenAI API.

- [X] This is a feature request for the Python library

### Describe the feature or improvement you're requesting

to define our own API_HOST for testing

### Additional context

_No response_",closed,2024-03-05T05:48:50Z,2024-03-06T22:48:16Z,[],https://github.com/openai/openai-python/issues/1214,40.99,True,missing_option_violation,medium,"The request to change OPENAI_BASE_URL and OPENAI_API_HOST does not follow the expected options for configuring the API, potentially leading to suboptimal behavior during testing.",high,
1217,Messages with non-latin languages texts returns server errors (500 status code),"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [ ] This is an issue with the Python library

### Describe the bug

I'm passing a message to openai, that might include some JSON in the message. those JSONs might include non-latin languages such as Hebrew, Arabic, and Japanese. 
I think there's something wrong when the library builds the ""Request"" object, it encodes the string (or httpx does it) in a way that causes openai servers to return 500 status codes.

I debugged the request-making process. it seems that httpx is to blame here. If I look at the library's code in the AsyncAPIClient code. 

When I print the ""options"" parameter `async def _reuqest` I see that a Hebrew string is fine, when I access request.content I can see that it's the hebrew is now encoded.

Now, I'm not sure how much the openai API or GPT4 care about working with those encoded texts, but I do know, that request with nested jsons in msg causes lots of 500 responses from GPT4-1106 on Azure. 

### To Reproduce

1. build a message that contains a JSON string with some Hebrew/Arabic/Japanese text in it. 
2. try to send it to gpt-4 with openai Library. 
3. put a breakpoint in the row: `raise self._make_status_error_from_response(err.response) from None`
4. see that it returns a 500, and accessing request.content, you see the message with the non-Latin encoded.


### Code snippets

_No response_

### OS

macOS

### Python version

Python v3.11.7

### Library version

openai v1.13.3",closed,2024-03-06T05:17:35Z,2024-03-06T14:57:35Z,['bug'],https://github.com/openai/openai-python/issues/1217,9.67,True,input_value_violation,high,"The API fails to handle non-Latin language inputs correctly, resulting in server errors (500 status code) when such texts are included in JSON messages.",high,
1208,Request: make it possible to specify the upper limit of history,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

I specified the limit parameter, but the message history does not match the limit parameter value.
https://github.com/openai/openai-python/blob/main/src/openai/types/beta/threads/message_list_params.py#L29-L30
```
    limit: int
    """"""A limit on the number of objects to be returned.
```

it is forum feedback url.
https://community.openai.com/t/request-make-it-possible-to-specify-the-upper-limit-of-history/663420

### To Reproduce

always

### Code snippets

```Python
def _get_message(gpt_thread_id:str, secret:str):
    client = OpenAI(api_key=secret)
    return client.beta.threads.messages.list(
        thread_id=gpt_thread_id,
        extra_query = {""limit"": 1},
        limit=1
    )
```
```


### OS

mac(docker debian)

### Python version

python3.9

### Library version

opneai v1.12.0",closed,2024-03-04T00:06:50Z,2024-03-05T04:32:31Z,['bug'],https://github.com/openai/openai-python/issues/1208,28.43,True,input_value_violation,performance_degradation,"The specified limit parameter does not match the expected behavior of the message history retrieval, potentially leading to unexpected results.",high,
433,openai.error.InvalidRequestError: Unrecognized request argument supplied: logprobs,"### Describe the bug

I need the ```logprobs``` value from the API.

The code I tried

``` 
response = openai.ChatCompletion.create(
            model='gpt-3.5-turbo',
            messages=[{""role"": ""user"", ""content"": ""Who won the world series in 2020?""}],
            max_tokens=193,
            temperature=0,
            logprobs=1
        )
```
but it shows the error
```openai.error.InvalidRequestError: Unrecognized request argument supplied: logprobs```

### To Reproduce

I need the ```logprobs``` value from the API.

The code I tried

``` 
response = openai.ChatCompletion.create(
            model='gpt-3.5-turbo',
            messages=[{""role"": ""user"", ""content"": ""Who won the world series in 2020?""}],
            max_tokens=193,
            temperature=0,
            logprobs=1
        )
```
but it shows the error
```openai.error.InvalidRequestError: Unrecognized request argument supplied: logprobs```

### Code snippets

_No response_

### OS

Ubuntu

### Python version

Python3.9

### Library version

openai-0.27.6",closed,2023-05-04T11:18:20Z,2024-03-03T00:44:34Z,['bug'],https://github.com/openai/openai-python/issues/433,7285.44,True,missing_option_violation,medium,"The API call includes an unrecognized argument 'logprobs', which is not supported by the 'ChatCompletion' method.",high,
1139,Assistants API,"### Confirm this is a feature request for the Python library and not the underlying OpenAI API.

- [X] This is a feature request for the Python library

### Describe the feature or improvement you're requesting

Is there a way to customize the Assistants chat response? 
Given the outcome of certain tool uses, id like the assistant to respond in a different manner. 

I guess this somehow should happen in the client.beta.threads.runs.create function


### Additional context

_No response_",closed,2024-02-08T22:04:54Z,2024-03-04T23:43:59Z,[],https://github.com/openai/openai-python/issues/1139,601.65,True,missing_dependency_violation,medium,"The request lacks necessary context or metadata for customizing the assistant's response, which may lead to unexpected behavior.",high,
1206,Error with proxy use,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug


```python
from openai import AsyncOpenAI
import httpx
import logging
logger = logging.getLogger()
logger.setLevel(logging.DEBUG)
client = AsyncOpenAI(
     # This is the default and can be omitted
    api_key=""..."",
    base_url='https://api.openai.com/v1',
    timeout=20,
    http_client=httpx.Client(verify=False,
        proxies={""all://"":""http://...""},
    ),
 )
```
After that, I make an asynchronous request to the **chat.completions** method
```shell
DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'role': 'system', 'content': '.......'}, {'role': 'user', 'content': 'fef'}], 'model': 'gpt-4', 'max_tokens': 4096}}
DEBUG:httpcore.connection:connect_tcp.started host='......' port=.....local_address=None timeout=20 socket_options=None
DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001D831AD70D0>
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'CONNECT']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'CONNECT']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'CONNECT']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'Connection Established', [])
DEBUG:httpcore.proxy:start_tls.started ssl_context=<ssl.SSLContext object at 0x000001D82F847940> server_hostname='api.openai.com' timeout=20
DEBUG:httpcore.proxy:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001D831AD7100>
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Sat, 02 Mar 2024 08:57:45 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'Cache-Control', b'no-cache, must-revalidate'), (b'openai-model', b'gpt-4-0613'), (b'openai-organization', b'gergergergegr'), (b'openai-processing-ms', b'1174'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'5000'), (b'x-ratelimit-limit-tokens', b'40000'), (b'x-ratelimit-remaining-requests', b'4999'), (b'x-ratelimit-remaining-tokens', b'35807'), (b'x-ratelimit-reset-requests', b'12ms'), (b'x-ratelimit-reset-tokens', b'6.289s'), (b'x-request-id', b'......'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=.....; path=/; expires=Sat, 02-Mar-24 09:27:45 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Set-Cookie', b'_cfuvid=.....; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'85e02e54ecf9cb27-DUS'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3="":443""; ma=86400')])
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions ""HTTP/1.1 200 OK""
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
DEBUG:httpcore.http11:response_closed.complete
DEBUG:openai._base_client:Encountered Exception
Traceback (most recent call last):
  File ""C:\Users\oleg\AppData\Local\Programs\Python\Python39\lib\site-packages\openai\_base_client.py"", line 1457, in _request
    resp = await self._client.send(
TypeError: object Response can't be used in 'await' expression
``` 

### To Reproduce

1. Use AsyncOpenAI
2. Use proxy in client.AsyncOpenAI
3. use await client.chat.completions

### Code snippets

_No response_

### OS

Windows 10

### Python version

Python 3.9.4

### Library version

openai 1.13.1",closed,2024-03-02T09:07:04Z,2024-03-03T09:30:26Z,['bug'],https://github.com/openai/openai-python/issues/1206,24.39,False,,,,low,
1107,[Assistants API] Run Step Tool call element changes type depending on run status,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

When polling run steps to read tool calls, I've observed that when the run is not in status 'completed' the function object in the `tool_calls` array is of type `dict`, however, once the run is completed it becomes a regular object. This has implications for how the tool call attributes can be accessed, leading to redundant code checking the element type. I have specifically observed this with function tool calls. I haven't done any work with the other types of Assistant tools.

### To Reproduce

1. Initiate a run that requires the use of a function
2. Retrieve the latest run step associated with the run
3. Attempt to read the tool calls when the run is of status `completed` versus when it's in an `in_progress `or `requires_action `state.

### Code snippets

_No response_

### OS

Windows 11

### Python version

Python v3.9,3

### Library version

openai v1.3.5",closed,2024-01-26T15:20:32Z,2024-03-03T01:10:40Z,['bug'],https://github.com/openai/openai-python/issues/1107,873.84,True,input_type_violation,medium,"The type of the function object in the tool_calls array changes based on the run status, leading to inconsistent behavior and requiring redundant type checks.",high,
1064,embedding behavior inconsistency with different parameters.,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

I noticed embeddings behaves differently when we set `encoding_format='float'` or not. 
I ran the embeddings for `chain of thoughts` 100 times with and without `encoding_format='float'` which then give me different embedding results. 

If I specify `encoding_format='float'`, all the returned embeddings have the same vector ( list of floats).
But if I don't specify the parameter, embedding api is using `base64` by default and returned me 3 different vectors although the differences among the 3 vectors are very minimal and their cosine distances are neglectable (< 1e-6).

I suspect this inconsistency is caused by `base64` encoding of the embedding results which is using `numpy` to decode.

Reporting this issue so hope team can investigate and fix it.  

context: 
openai version: 1.3.4
python: 3.10.12
numpy: 1.26.1 
model: text-embedding-ada-002

### To Reproduce

run the following 100 times:

openai_client.embeddings.create(input='chain of thoughts', model='text-embedding-ada-002')

run the following 100 times:

openai_client.embeddings.create(input='chain of thoughts', model='text-embedding-ada-002', encoding_format='float')

### Code snippets

_No response_

### OS

macos

### Python version

Python 3.10.12

### Library version

1.3.4",closed,2024-01-11T23:23:36Z,2024-03-03T01:08:22Z,['bug'],https://github.com/openai/openai-python/issues/1064,1225.75,True,missing_option_violation,incorrect_functionality,"Inconsistent behavior of the embeddings API based on the presence or absence of the 'encoding_format' parameter, leading to unexpected output.",high,
1055,[assistants API] How long are the validity periods of Messages and Runs? #611,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

[assistants API] How long are the validity periods of Messages and Runs?
The deletion API of Assistants and Threads, my understanding is that it will always exist if it is not actively deleted.
As for Messages and Runs, I’m not sure how long they last.
【assistants API】Messages、Runs的有效期是多久？
Assistants、Threads的删除的API，我的理解是不主动删除一直存在
而Messages、Runs的是多久不太确定

### To Reproduce

-

### Code snippets

_No response_

### OS

-

### Python version

-

### Library version

-",closed,2024-01-08T12:14:19Z,2024-03-03T01:08:03Z,"['documentation', 'question']",https://github.com/openai/openai-python/issues/1055,1308.9,True,missing_dependency_violation,unknown,"The issue lacks clarity on the validity periods of Messages and Runs, indicating a potential missing dependency on the documentation or API behavior.",medium,
460,"When changing the version specification, the Azure deployment name cannot be retrieved.","### Describe the bug

Until now, I had been referring to the following post to retrieve the Azure deployment name.

```
import openai

openai.api_type = ""azure""
openai.api_base = ""https://<your-resource>.openai.azure.com/""
openai.api_key = ""<your-key>""
openai.api_version = ""2023-03-15-preview""

deployments = openai.Deployment.list()
print(deployments)
```

When I set the `openai.api_version` to the latest `2023-05-15`, it stopped functioning properly. Since there are no other changes, I believe it's due to the version. Is there any way to address this issue?

### To Reproduce

Here is the code that works without any issues:

```
import openai

openai.api_type = ""azure""
openai.api_base = ""https://<your-resource>.openai.azure.com/""
openai.api_key = ""<your-key>""
openai.api_version = ""2023-03-15-preview""

deployments = openai.Deployment.list()
print(deployments)
```

This returns a response where id is your deployment name and model is the model name.

When executing the code by simply switching to `openai.api_version = ""2023-05-15""`, the following stack trace is returned:

```
---------------------------------------------------------------------------
InvalidRequestError                       Traceback (most recent call last)
[<ipython-input-20-6ce049a89318>](https://localhost:8080/#) in <cell line: 8>()
      6 openai.api_version = ""2023-05-15""
      7 
----> 8 deployments = openai.Deployment.list()
      9 print(deployments)

4 frames
[/usr/local/lib/python3.10/dist-packages/openai/api_requestor.py](https://localhost:8080/#) in _interpret_response_line(self, rbody, rcode, rheaders, stream)
    685         stream_error = stream and ""error"" in resp.data
    686         if stream_error or not 200 <= rcode < 300:
--> 687             raise self.handle_error_response(
    688                 rbody, rcode, resp.data, rheaders, stream_error=stream_error
    689             )

InvalidRequestError: Resource not found
```

### Code snippets

_No response_

### OS

Google Colab

### Python version

3.10.11

### Library version

openai-python v0.27.7",closed,2023-05-22T01:36:26Z,2024-03-03T01:05:40Z,['bug'],https://github.com/openai/openai-python/issues/460,6863.49,True,input_value_violation,unknown,"The change in API version may have introduced incompatible changes, leading to an InvalidRequestError when attempting to list deployments.",medium,
464,openai.error.AuthenticationError: <empty message>,"### Describe the bug

Since yesterday 24.05.2023 around your outage I am facing this errors:

```
..
  File ""/opt/homebrew/lib/python3.10/site-packages/openai/api_resources/embedding.py"", line 33, in create
    response = super().create(*args, **kwargs)
  File ""/opt/homebrew/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py"", line 153, in create
    response, _, api_key = requestor.request(
  File ""/opt/homebrew/lib/python3.10/site-packages/openai/api_requestor.py"", line 230, in request
    resp, got_stream = self._interpret_response(result, stream)
  File ""/opt/homebrew/lib/python3.10/site-packages/openai/api_requestor.py"", line 624, in _interpret_response
    self._interpret_response_line(
  File ""/opt/homebrew/lib/python3.10/site-packages/openai/api_requestor.py"", line 687, in _interpret_response_line
    raise self.handle_error_response(
openai.error.AuthenticationError: <empty message>
```

I tried several approaches, thought LangChain would be the problem, thought a vectorstore is it, but now I think it must be something with the API as I rebuilt with several options and it all leads to the same error.

### To Reproduce

```
openai.api_key = OPENAI_KEY or os.getenv(""OPENAI_API_KEY"")

# where texts can be a list of strings like 
texts = [""test"", ""foo""]
for i in texts:
    response = openai.Embedding.create(
        input=i,
        model=""text-embedding-ada-002""
    )
    embeddings = response['data'][0]['embedding']
```
### Code snippets

_No response_

### OS

macOS, m1-chip

### Python version

Python 3.10.9

### Library version

openai==0.27.7",closed,2023-05-25T10:56:53Z,2024-03-03T01:05:22Z,['bug'],https://github.com/openai/openai-python/issues/464,6782.14,True,missing_dependency_violation,High,"The error indicates a potential missing dependency related to the API key, which is required for authentication but appears to be empty or not set correctly.",high,
465,"Numba error when inference in thread (QThread, pyside6)","### Describe the bug

Doing a chat inference with ""text-davinci-003"" into a QThread hangs the function ""openai.Completion.create"" ""ad infinitum"".

Only one error is reported in console: Numba reports that it cannot be in a thread because the data is not copied from the parent thread.

Finally, when I cancel the request, an exception is fired.

If I change the code to use -only- async, and it works. But I need use threads, because I want avoid locking the main thread.

### To Reproduce

class QUERY(QThread):

            def __init__(self, tutor, prompt):
                super().__init__(tutor);
                self.tutor=tutor;
                self.prompt=prompt;

            def query(self, prompt):
                MODEL=""gpt-3.5-turbo"";
                print(""OpenAI: enviando texto a openAI"", flush=True);
                r=openai.ChatCompletion.create(model=MODEL, messages=prompt, stream=False);
                print(""OpenAI: esperando por la respuesta"", flush=True);
                try:
                   if ""choices"" in r:
                      if len(r[""choices""])>0 and ""message"" in r[""choices""][0]:
                         if ""content"" in r[""choices""][0][""message""]:
                            response=r[""choices""][0][""message""][""content""];
                            self.tutor.history.append(response);
                            self.tutor.onResponse.emit(response);
                         else:
                            self.tutor.onError.emit(""Ha fallado en: content"",r);
                      else:
                         self.tutor.onError.emit(""Ha fallado en: len choices o message"",r);
                   else:
                      self.tutor.onError.emit(""Ha fallado en: choices"",r);

                except:
                   print(traceback.format_exc(), flush=True);
                   self.tutor.onError.emit(""OpenAI ha fallado!"");

                finally:
                   print(""OpenAI: respuesta finalizada"", flush=True);

### Code snippets

_No response_

### OS

Linux Ubuntu Jammy

### Python version

Python 3.9.16

### Library version

openai-python 0.27.0
numba 0.57
",closed,2023-05-25T12:39:11Z,2024-03-03T01:04:55Z,['bug'],https://github.com/openai/openai-python/issues/465,6780.43,True,method_order_violation,hang,"The use of threading with the OpenAI API call may violate the expected method order, leading to hangs due to improper handling of context between threads.",high,
466,Azure content management exception triggered by charset encoding issues,"### Describe the bug

I'm getting exceptions triggered by content management filter at the Azure endpoint, i.e. an `InvalidRequestError` with fields
* `error_code`: `content_filter`
*  `error_message`: _The response was filtered due to the prompt triggering Azure OpenAI?s content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766_

The prompt sent is trivial, the only issue is that it's in Spanish and has non-ASCII characters. I'm using `gpt-35-turbo` and the `ChatCompletion` method. The prompt is just:

      [{'role': 'user', 'content': 'quiero un móvil con una buena cámara'}]

(meaning _I want a mobile with a good camera_). This triggers the content filter, which does not make much sense, given that the message is harmless. Furthermore, if I _remove_ the accented character in `cámara`, i.e.

      [{'role': 'user', 'content': 'quiero un móvil con una buena camara'}]

... then the filter is not triggered, and I get a proper text response. Which makes less sense.

I've dug a bit in the code. The prompt seems to be serialized [here](https://github.com/openai/openai-python/blob/main/openai/api_requestor.py#L484):
```python
            if params and not files:
                data = json.dumps(params).encode()
                headers[""Content-Type""] = ""application/json""
```

Interestingly, if I change the serialization to

```python
            if params and not files:
                data = json.dumps(params, ensure_ascii=False).encode()
                headers[""Content-Type""] = ""application/json""
```

..then the content management filter is *not* triggered, even with an accented character, and I get a proper response. Which suggests some kind of unusual behavior at the endpoint, given that it behaves differently with escaped unicode characters than with regular UTF-8 text.


### To Reproduce

1. Peepare a prompt `[{'role': 'user', 'content': 'quiero un móvil con una buena cámara'}]
2. Send to the ChatCompletion API, with a gpt-35-turbo model
3. Content Management filter is triggered

### Code snippets

_No response_

### OS

Ubuntu

### Python version

Python 3.8.10

### Library version

openai-python v0.27.7",closed,2023-05-25T18:03:31Z,2024-03-03T01:04:22Z,['bug'],https://github.com/openai/openai-python/issues/466,6775.01,True,input_value_violation,medium,"The prompt containing non-ASCII characters triggers the content management filter, indicating that the input value may not be acceptable due to charset encoding issues.",high,
468,Get error when using async request,"### Describe the bug

I got the following error when using openai arequest_raw.

Traceback (most recent call last):
  File ""/home/admin/gpt/curious-cub-server/test_openai.py"", line 31, in arequest_raw
    result = await session.request(**request_kwargs)
  File ""/home/admin/.local/lib/python3.10/site-packages/aiohttp/client.py"", line 536, in _request
    conn = await self._connector.connect(
  File ""/home/admin/.local/lib/python3.10/site-packages/aiohttp/connector.py"", line 540, in connect
    proto = await self._create_connection(req, traces, timeout)
  File ""/home/admin/.local/lib/python3.10/site-packages/aiohttp/connector.py"", line 901, in _create_connection
    _, proto = await self._create_direct_connection(req, traces, timeout)
  File ""/home/admin/.local/lib/python3.10/site-packages/aiohttp/connector.py"", line 1206, in _create_direct_connection
    raise last_exc
  File ""/home/admin/.local/lib/python3.10/site-packages/aiohttp/connector.py"", line 1175, in _create_direct_connection
    transp, proto = await self._wrap_create_connection(
  File ""/home/admin/.local/lib/python3.10/site-packages/aiohttp/connector.py"", line 982, in _wrap_create_connection
    raise ClientConnectorCertificateError(req.connection_key, exc) from exc
aiohttp.client_exceptions.ClientConnectorCertificateError: Cannot connect to host www.baidu.com:443 ssl:True [SSLCertVerificationError: (1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:997)')]

### To Reproduce

Even i added the following line to fix my issue,   i just wanna know if it's due to ssl lib issue.

        ssl_ctx = ssl.create_default_context(cafile=certifi.where())  //add ssl ctx

        try:
            result = await session.request(**request_kwargs, ssl=ssl_ctx) //add ssl=ssl_ctx
            util.log_info(
                ""OpenAI API response"",
                path=abs_url,
                response_code=result.status,
                processing_ms=result.headers.get(""OpenAI-Processing-Ms""),
                request_id=result.headers.get(""X-Request-Id""),
            )
            # Don't read the whole stream for debug logging unless necessary.
            if openai.log == ""debug"":
                util.log_debug(
                    ""API response body"", body=result.content, headers=result.headers
                )
            return result
        except (aiohttp.ServerTimeoutError, asyncio.TimeoutError) as e:
            raise error.Timeout(""Request timed out"") from e
        except aiohttp.ClientError as e:
            raise error.APIConnectionError(""Error communicating with OpenAI"") from e


aiohttp version: 3.8.4

### Code snippets

_No response_

### OS

CentOS7.9

### Python version

Python3.10

### Library version

openai-python0.27.7",closed,2023-05-26T02:47:14Z,2024-03-03T01:03:56Z,['bug'],https://github.com/openai/openai-python/issues/468,6766.28,True,input_value_violation,medium,"The error indicates a failure in SSL certificate verification, which suggests that the input values for the SSL context may not be correctly configured or are out of range for acceptable SSL parameters.",high,
516,command line for fine tune unable to pick api type,"### Describe the bug

openai [-h] [-V] [-v] [-b API_BASE] [-k API_KEY] [-p PROXY [PROXY ...]] [-o ORGANIZATION] {api,tools,wandb}

we have setup the environment variables for api_key, api_base, version and api_type
and we are trying to run fine tune model using the commands provided in open_ai_cookbook
we are facing an error of 
Error: The API type provided in invalid. Please select one of the supported API types: 'azure', 'azure_ad', 'open_ai'


### To Reproduce

%env OPENAI_API_TYPE  
%env OPENAI_API_VERSION 
%env OPENAI_API_KEY
%env OPENAI_API_BASE 

we are trying to run the following commands on cloud platform

!openai api fine_tunes.create -t ""/FileStore/train.json"" -v ""/FileStore/test.json"" --classification_metric ""accuracy"" --classification_labels ""yes,no"" --model ""text-davinci-003"" --max_iterations 100 --save_model ""test_results""

we have also tried 

%sh /local_disk0/envs/pythonEnv/bin/openai --verbose --api-key <api_key>  -api-base <api_base> api fine_tunes.create -t /FileStore/train.json -v /FileStore/test.json --compute_classification_metrics --classification_positive_class <class>


Error: The API type provided in invalid. Please select one of the supported API types: 'azure', 'azure_ad', 'open_ai'


### Code snippets

_No response_

### OS

Linux

### Python version

3.9

### Library version

'0.27.8'",closed,2023-07-03T11:34:38Z,2024-03-03T01:01:37Z,['bug'],https://github.com/openai/openai-python/issues/516,5845.45,True,missing_option_violation,high,"The API type provided is invalid, indicating that the user did not select one of the supported API types as required by the API contract.",high,
762,Default timeout is ten minutes not 60 seconds.,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

[The code comment in README.md](https://github.com/openai/openai-python/blob/aa6818997468b753546d55365d8142e2bb1c6640/README.md?plain=1#L352) says timeout is 60s which conflicts with the prior sentence saying it's ten minutes.  According to [_constants.DEFAULT_TIMEOUT](https://github.com/openai/openai-python/blob/aa6818997468b753546d55365d8142e2bb1c6640/src/openai/_constants.py#L8), ten minutes is the correct value.

(ten minutes seems like a lot!)

### To Reproduce

Read the README.md

### Code snippets

_No response_

### OS

any

### Python version

any

### Library version

1.0 I guess",closed,2023-11-09T21:08:25Z,2024-03-03T00:59:13Z,['bug'],https://github.com/openai/openai-python/issues/762,2739.85,False,,,,low,
791,openai==1.2.3: segmentation fault with async,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

seg fault when using async and running openai integration tests with the latest version on windows

running this
```
python -q -X faulthandler test.py
```

gets this:

```
ChatCompletion(id='chatcmpl-8JvRae0P4EOZRkIJuGQQMFBQ6lRpf', choices=[Choice(finish_reason='stop', index=0, message=ChatCompletionMessage(content='Hello! How can I assist you today?', role='a
ssistant', function_call=None, tool_calls=None))], created=1699759738, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=
9, prompt_tokens=18, total_tokens=27))
Windows fatal exception: access violation

Current thread 0x000032c8 (most recent call first):
  Garbage-collecting
  File ""C:\Users\erik\.pyenv\pyenv-win\versions\3.11.6\Lib\asyncio\proactor_events.py"", line 82 in __repr__
  File ""C:\Users\erik\.pyenv\pyenv-win\versions\3.11.6\Lib\asyncio\proactor_events.py"", line 116 in __del__
Segmentation fault
```
i updated to 3.11.6 jut to see if that helped... nope

i don't normally have issues with async code, and i have unit tests and code passing fine.   only the openai lib has this issue.   

fortunately it only fails at exit, so i can keep on keeping on for now without worrying too much


### To Reproduce

using windows pyenv python 3.11.5 or 3.11.6, use the async openai client to make a single chat completion in a pytest test.   

it works, but then your program segfaults

all other httpx async/rest/tests all work, even respx live tests, etc.   

only when i do the integration test with openai does it fail

### Code snippets

```Python
example of failure


from openai import AsyncOpenAI
import asyncio

async def x():
    model=""gpt-3.5-turbo""
    client = AsyncOpenAI()
    messages = [
        {
            ""role"": ""system"",
            ""content"": ""You are a helpful assistant.""
        },
        {
            ""role"": ""user"",
            ""content"": ""Hello""
        },
    ]

    response = await client.chat.completions.create(
        model=model,
        messages=messages,
        max_tokens=500
    )

    return response

print(asyncio.run(x()))
```
```


### OS

windows

### Python version

3.11.5

### Library version

openai==1.2.3",closed,2023-11-12T03:30:20Z,2024-03-03T00:58:27Z,['bug'],https://github.com/openai/openai-python/issues/791,2685.47,True,input_value_violation,high,"Segmentation fault indicates a potential input value violation, possibly due to improper handling of async calls or memory access issues.",high,
835,"When using with Gevent in Flask_Sock it errors out: specifically, the monkey.patch_all() line casues some sort of error to occur. ","### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

When using with Gevent in Flask_Sock it errors out: specifically, the monkey.patch_all() line casues some sort of error to occur. 

### To Reproduce

When using with Gevent in Flask_Sock it errors out: specifically, the monkey.patch_all() line casues some sort of error to occur. 

### Code snippets

```Python
if __name__ == '__main__':
    app.logger.setLevel(logging.DEBUG)
    from gevent import monkey
    monkey.patch_all()

    from gevent.pywsgi import WSGIServer
    server=WSGIServer(('127.0.0.1', 5000), app)
    print(""Server listening on: http://localhost:"" + str(HTTP_SERVER_PORT))
    server.serve_forever()
(venv) C:\Users\lukas\OneDrive\Desktop\Fuckaroundy\TwilioBidirectionalStream>python app.py
C:\Users\lukas\OneDrive\Desktop\Fuckaroundy\TwilioBidirectionalStream\app.py:4: DeprecationWarning: 'audioop' is deprecated and slated for removal in Python 3.13
  import audioop
Traceback (most recent call last):
  File ""C:\Users\lukas\OneDrive\Desktop\Fuckaroundy\TwilioBidirectionalStream\app.py"", line 247, in <module>
    monkey.patch_all()
  File ""C:\Users\lukas\OneDrive\Desktop\Fuckaroundy\TwilioBidirectionalStream\venv\Lib\site-packages\gevent\monkey.py"", line 1219, in patch_all
    patch_thread(Event=Event, _warnings=_warnings)
  File ""C:\Users\lukas\OneDrive\Desktop\Fuckaroundy\TwilioBidirectionalStream\venv\Lib\site-packages\gevent\monkey.py"", line 187, in ignores
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\lukas\OneDrive\Desktop\Fuckaroundy\TwilioBidirectionalStream\venv\Lib\site-packages\gevent\monkey.py"", line 749, in patch_thread
    _patch_existing_locks(threading_mod)
  File ""C:\Users\lukas\OneDrive\Desktop\Fuckaroundy\TwilioBidirectionalStream\venv\Lib\site-packages\gevent\monkey.py"", line 630, in _patch_existing_locks  
    if isinstance(o, rlock_type):
       ^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\lukas\OneDrive\Desktop\Fuckaroundy\TwilioBidirectionalStream\venv\Lib\site-packages\openai\_utils\_proxy.py"", line 39, in __class__        
    return self.__get_proxied__().__class__
           ^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\lukas\OneDrive\Desktop\Fuckaroundy\TwilioBidirectionalStream\venv\Lib\site-packages\openai\_utils\_proxy.py"", line 43, in __get_proxied__  
    return self.__load__()
           ^^^^^^^^^^^^^^^
  File ""C:\Users\lukas\OneDrive\Desktop\Fuckaroundy\TwilioBidirectionalStream\venv\Lib\site-packages\openai\lib\_old_api.py"", line 33, in __load__
    raise APIRemovedInV1(symbol=self._symbol)
openai.lib._old_api.APIRemovedInV1:

You tried to access openai.Edit, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.  

You can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface.

Alternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`

A detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742


(venv) C:\Users\lukas\OneDrive\Desktop\Fuckaroundy\TwilioBidirectionalStream>python app.py
C:\Users\lukas\OneDrive\Desktop\Fuckaroundy\TwilioBidirectionalStream\app.py:4: DeprecationWarning: 'audioop' is deprecated and slated for removal in Python 3.13
  import audioop
Traceback (most recent call last):
  File ""C:\Users\lukas\OneDrive\Desktop\Fuckaroundy\TwilioBidirectionalStream\app.py"", line 247, in <module>
    monkey.patch_all()
  File ""C:\Users\lukas\OneDrive\Desktop\Fuckaroundy\TwilioBidirectionalStream\venv\Lib\site-packages\gevent\monkey.py"", line 1219, in patch_all
    patch_thread(Event=Event, _warnings=_warnings)
  File ""C:\Users\lukas\OneDrive\Desktop\Fuckaroundy\TwilioBidirectionalStream\venv\Lib\site-packages\gevent\monkey.py"", line 187, in ignores
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\lukas\OneDrive\Desktop\Fuckaroundy\TwilioBidirectionalStream\venv\Lib\site-packages\gevent\monkey.py"", line 749, in patch_thread
    _patch_existing_locks(threading_mod)
  File ""C:\Users\lukas\OneDrive\Desktop\Fuckaroundy\TwilioBidirectionalStream\venv\Lib\site-packages\gevent\monkey.py"", line 630, in _patch_existing_locks  
    if isinstance(o, rlock_type):
       ^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\lukas\OneDrive\Desktop\Fuckaroundy\TwilioBidirectionalStream\venv\Lib\site-packages\openai\_utils\_proxy.py"", line 39, in __class__        
    return self.__get_proxied__().__class__
           ^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\lukas\OneDrive\Desktop\Fuckaroundy\TwilioBidirectionalStream\venv\Lib\site-packages\openai\_utils\_proxy.py"", line 43, in __get_proxied__  
    return self.__load__()
           ^^^^^^^^^^^^^^^
  File ""C:\Users\lukas\OneDrive\Desktop\Fuckaroundy\TwilioBidirectionalStream\venv\Lib\site-packages\openai\lib\_old_api.py"", line 33, in __load__
    raise APIRemovedInV1(symbol=self._symbol)
openai.lib._old_api.APIRemovedInV1:

You tried to access openai.Edit, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.  

You can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface.

Alternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`

A detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742
```


### OS

Windows 

### Python version

Python v3.11.3

### Library version

openai 1.3.0",closed,2023-11-16T07:51:33Z,2024-03-03T00:56:08Z,['bug'],https://github.com/openai/openai-python/issues/835,2585.08,True,missing_dependency_violation,high,"The code attempts to access an API method (openai.Edit) that has been removed in the current library version, indicating a missing dependency on the correct API version.",high,
916,Custom openai like sever nolonger work for new openai api,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

I have acustomedz openai like server

```
@app.post(""/v1/chat/completions"", response_model=ChatCompletionResponse)
async def create_chat_completion(request: ChatCompletionRequest):
    print('got request')
```


This works for old openai,

but after upgrade, my custom server no longer receive the request now.

### To Reproduce

my request code:

```
import openai
import argparse
from openai import OpenAI

parser = argparse.ArgumentParser()
parser.add_argument(""--ip"", type=str, default='127.0.0.1')
parser.add_argument(""--port"", type=str, default=80)
args = parser.parse_args()

local_ip = args.ip

base_url = 'http://127.0.0.1:8080'
model = ""qwen-v2""

tts = False
stream_mode = True

if tts:
    client = OpenAI(api_key=api_key)
    response = client.audio.speech.create(
        model='tts-1', # ""tts-1"",""tts-1-hd""
        voice='fable', # 'alloy', 'echo', 'fable', 'onyx', 'nova', 'shimmer'
        input='hell，你好，请你介绍一下你自己',
    )
else:
    client = OpenAI(api_key='sk34fgerg', base_url=base_url)
    if stream_mode:
        questions = [
        '你好，请介绍一下你自己，以及北京的有哪些小吃。',
        '你是谁',
        '请列举出8部好看的电影'
        ]
        for qs in questions:
            print(f'问: {qs}')
            response = client.chat.completions.create(
                model=""qwen-v2"",
                messages=[
                    {""role"": ""user"", ""content"": qs}
                ],
                temperature=0.95,
                stream=True
            )
            for chunk in response:
                if hasattr(chunk.choices[0].delta, ""content""):
                    print(chunk.choices[0].delta.content, end="""", flush=True)
                print()
    else:
        pass
```

### Code snippets

_No response_

### OS

macOS

### Python version

3.10

### Library version

1.3.6",closed,2023-12-01T15:21:19Z,2024-03-03T00:53:15Z,['bug'],https://github.com/openai/openai-python/issues/916,2217.53,False,,,,low,
936,How do I know which account an OpenAI token is associated with ?,"### Confirm this is a feature request for the Python library and not the underlying OpenAI API.

- [X] This is a feature request for the Python library

### Describe the feature or improvement you're requesting

Hello 👋🏻 

This isn't really a bug, but rather a question. I have several tokens to access OpenAI. I know how to find out whether they're valid or not, but I can't see how to find out which account is associated with them.

How can I get this information ? Does OpenAI Python API library support this ?

Thank you for your reply 

### Additional context

_No response_",closed,2023-12-06T08:56:06Z,2024-03-03T00:52:25Z,[],https://github.com/openai/openai-python/issues/936,2103.94,False,,,,,
990,Multiple document upload assistant update seems not working,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [x] This is an issue with the Python library

### Describe the bug

I'm trying to upload multiple files on openai Assistant.

- I didn't find the way to upload just one file to a set of existing file already uploaded to the assistant.
- So if you want to add just one file you need to upload a list of already uploaded files plus the new one you want to upload

The problem in my case is that proceeding as above it upload just 2 files of the list.



### To Reproduce

Create file n°1:
```
file_1 = client.files.create(
    file=open(""'Test_1.txt', ""rb""),
    purpose=""assistants""
)
```

Create file n°2:
```
file_2 = client.files.create(
    file=open(""'Test_2.txt', ""rb""),
    purpose=""assistants""
)
```

Create file n°3:
```
file_3 = client.files.create(
    file=open(""'Test_3.txt', ""rb""),
    purpose=""assistants""
)
```
Then we can update the assistant with 3 files:

```
assistant = client.beta.assistants.update(
    assistant_id=""asst_ ........ Dmo3S"",
    file_ids=[file_1.id , file_2.id, file_3.id]
)
```

Than if you go in the assistant you see that only 2 documents are uploaded (attached image)
Let me know if it is a bug or I missed something.

Thanks
![Cattura](https://github.com/openai/openai-python/assets/143315119/1c7b45fd-6396-4369-ade0-00bedb98753f)




### Code snippets

_No response_

### OS

Windows 10

### Python version

Python v3.12

### Library version

openai 1.5.0",closed,2023-12-19T08:48:37Z,2024-03-03T00:51:46Z,[],https://github.com/openai/openai-python/issues/990,1792.05,True,missing_dependency_violation,medium,"The user is unable to upload a single file without providing a list of already uploaded files, indicating a potential missing dependency in the API's expected input handling.",high,
1023,Fail to create message in assistant thread,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

`    def add_messages(self, room_id, prompt, file_ids: List[str] | NotGiven = NotGiven):
        thread = self.get_thread(room_id)
        message = self.client.beta.threads.messages.create(
            thread_id=thread.id,
            role=""user"",
            content=prompt,
            metadata={'room_id': room_id},
            file_ids=file_ids,
        )
        mlogger.info(f'[add_messages]room_id:{room_id} thread_id:{thread.id}  message_id:{message.id}')
        return message
`
I've finished my test before. But I got this error these days.
`
openai.BadRequestError: Error code: 400 - {'error': {'message': ""Can't add messages to thread_jD1dj44APTovq48PCBZACOGV while a run run_KfVG8fSJg882VHYeewFQ2BHk is active."", 'type': 'invalid_request_error', 'param': None, 'code': None}}`
I'v make sure the thread was just created . 



### To Reproduce

1. get assistant from list assistant 
2. get file ids from file list 
3. create thread or get thread id from cache
4. message create  with content 、file_ids 、 metadata
5. call run.create 
6. see error


### Code snippets

```Python
def reply(self, room_id, prompt, payload: dict):
        file_ids = self._get_file_ids(payload, room_id)
        # get file ids from redis
        try:
            self.repo.add_messages(room_id, prompt, file_ids)
            self.repo.create_run(room_id)
        except Exception as e:
            mlogger.exception(e)
        yield from self._get_answer(room_id, payload)
    def get_thread(self, room_id):
        thread_id = room_id_to_thread_id.get(room_id)
        if thread_id:
            return self.client.beta.threads.retrieve(thread_id=thread_id)
        thread = self.client.beta.threads.create()
        room_id_to_thread_id.set(key=room_id, value=thread.id)
        return thread

    def add_messages(self, room_id, prompt, file_ids: List[str] | NotGiven = NotGiven):
        thread = self.get_thread(room_id)
        message = self.client.beta.threads.messages.create(
            thread_id=thread.id,
            role=""user"",
            content=prompt,
            metadata={'room_id': room_id},
            file_ids=file_ids,
        )
        mlogger.info(f'[add_messages]room_id:{room_id} thread_id:{thread.id}  message_id:{message.id}')
        return message

    def create_run(self, room_id):
        thread = self.get_thread(room_id)
        run = self.client.beta.threads.runs.create(
            thread_id=thread.id,
            assistant_id=self.assistant.id,
            metadata={'room_id': room_id}
        )
        mlogger.info(f'[create_run]room_id:{room_id} thread_id:{thread.id} run_id:{run.id}')
        room_id_to_run_id[room_id] = run.id
        return run
```


### OS

win11

### Python version

Python 3.11.2

### Library version

openai v1.6.1",closed,2023-12-25T08:18:20Z,2024-03-03T00:50:44Z,"['question', 'API-feedback']",https://github.com/openai/openai-python/issues/1023,1648.54,False,,,,low,
363,Using 'logit_bias' to prevent chatgpt or gpt4 from stop generating,"### Describe the bug

I have reported [here](https://github.com/openai/openai-cookbook/issues/312), but probably I should also report here.

Hi, 
as the example in the official documentation, we can pass `logit_bias={""50256"": -100}` to the `Completion` API to prevent the <|endoftext|> token from being generated.

I'm trying to do the same for `ChatCompletion` but it seems doesn't work.


### To Reproduce

Here is an example:
```
import os
import openai
openai.api_key = os.getenv(""OPENAI_API_KEY"")

completion = openai.ChatCompletion.create(
    model=""gpt-3.5-turbo"",
    messages=[
        {""role"": ""user"", ""content"": ""Hello!""}
    ],
    logit_bias={'100257': -100}
)
```
where I get `<|endoftext|>` token id for `gpt-3.5-turbo` by:
```
import tiktoken
print(tiktoken.encoding_for_model('gpt-3.5-turbo').eot_token)

#results: 100257
```

I still getting `""finish_reason"": ""stop""` in the returned response.

I also found a positive value works. If set logit_bias={'100257': 100}, the generation will stop immediately and return an empty string message. Seems only negative value doesn't work. Is this a bug or on purpose?

### Code snippets

_No response_

### OS

macOS

### Python version

Python3.8

### Library version

v0.27.2",closed,2023-04-02T04:03:04Z,2024-03-03T00:48:19Z,['bug'],https://github.com/openai/openai-python/issues/363,8060.75,True,input_value_violation,incorrect_functionality,The negative logit bias value is not producing the expected behavior of preventing the generation of the <|endoftext|> token.,high,
368,Intermittent Connection Aborted Error on Moderation Endpoint,"### Describe the bug

I'll leave this specific to my use case but as others have reported on the [forums](https://community.openai.com/t/api-aborts-my-connection-without-a-reason-anything-i-can-do/104256/29) there is a intermittent error:

`Error communicating with OpenAI: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))`

In our case we are encountering this on the Moderation endpoint. Only started happening within the last week or so.


### To Reproduce

1. Make Request to Moderation endpoint using `openai.Moderation.create()`
2. It may work it may not, our users have found that this most often happens at the beginning of a conversation thread, the fact that it is so intermittent is what is making me believe this is a server error on OpenAI's part.

### Code snippets

```Python
try:
        input = body[""messages""][-1][""content""]
        moderation = openai.Moderation.create(input=input, model='text-moderation-latest')
        moderation_flagged = moderation[""results""][0][""flagged""]
    except Exception as e:
        print(e) #error is received here
        return EventSourceResponse(error_generate(message=""Error: Unable to moderate prompt"", status_code=500))
```


### OS

macOS/Windows

### Python version

Python v3.9.16

### Library version

0.27.2",closed,2023-04-03T20:50:22Z,2024-03-03T00:47:31Z,['bug'],https://github.com/openai/openai-python/issues/368,8019.95,True,input_value_violation,unknown,Intermittent connection errors may indicate issues with input values or server handling of requests.,medium,
392,no response from embedding api for 600s,"### Describe the bug

This maybe true for other openai python api's as well. We are using openai api's via the python library in a multi threaded application which is always ON, i.e. we do not frequently start/restart the application.

Following is the code:
```
from embeddings_utils import get_embedding
input = get_embedding(""this is a test"", engine=""text-embedding-ada-002"")
```
When there is a long period of inactivity, intermittently, we get responses for above after 600s. This problem some time resolves itself after some requests/time. But during the time it gives issue, response is received after exact 600-601 seconds. It looks like request goes and there is no response for 600s and then it retries and that works immediately.

Any idea about this issue?


### To Reproduce

No particular steps, please see description above.

### Code snippets

_No response_

### OS

Linux/Ubuntu

### Python version

Python v3.7.5

### Library version

open-ai 0.27.2",closed,2023-04-13T12:45:20Z,2024-03-03T00:47:06Z,['bug'],https://github.com/openai/openai-python/issues/392,7788.03,False,,,,low,
415,fine tune for json does not work,"### Describe the bug

I am parsing a json file with the command openai tools fine_tunes.prepare_data -f javi.json, and i always have this error message:

Analyzing...


ERROR in read_any_format validator: Your file `javi.json` does not appear to be in valid JSON format. Please ensure your file is formatted as a valid JSON file.

Aborting...%

It does not matter whats the content of the file, even with something as simple as  { ""name"": ""javi""} it throws this error



### To Reproduce

Create a json file with any content

Run openai tools fine_tunes.prepare_data -f javi.json

It errors

### Code snippets

_No response_

### OS

Monterrey

### Python version

Python 3.9.6

### Library version

openai 0.27.4",closed,2023-04-25T09:04:05Z,2024-03-03T00:46:18Z,['bug'],https://github.com/openai/openai-python/issues/415,7503.7,True,input_type_violation,high,"The provided JSON file does not meet the expected input type, leading to validation errors.",high,
432,Internal error when calling completion with incorrect name,"### Describe the bug

I got a funky HTTP 500 internal error when I accidentally provided an incorrect model name for the completion API.

### To Reproduce

```
response = openai.Completion.create(model='	text-davinci-003', prompt=context)
```

(Accidental whitespace at the front of the model name)

### Code snippets

```Python
File "".../server.py"", line 62, in query
    response = openai.Completion.create(model='	text-davinci-003', prompt=context)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

...

  File ""...venv/lib/python3.11/site-packages/openai/api_requestor.py"", line 687, in _interpret_response_line
    raise self.handle_error_response(
openai.error.APIError: internal error {
    ""error"": {
        ""message"": ""internal error"",
        ""type"": ""invalid_request_error"",
        ""param"": null,
        ""code"": null
    }
}
```
```


### OS

Linux

### Python version

3.11.3+chromium.29

### Library version

0.27.6",closed,2023-05-04T00:32:12Z,2024-03-03T00:45:55Z,['bug'],https://github.com/openai/openai-python/issues/432,7296.23,False,,,,low,
362,error_message='Resource not found' for gpt-35-turbo model in Azure OpenAI Service,"### Describe the bug

It was successfully called via python openai library. But from a few days ago. It returns error_code=404 error_message='Resource not found' error_param=None error_type=None message='OpenAI API error received' stream_error=False. 

I am pretty sure that the model deployment is existed since I can still call the endpoint on Postman and get the response. 
And I also tried to delete and deploy it again in Azure OpenAI service. But still not work on python side. 

Is this issue from Microsoft side? or anyone can help me figure out?


### To Reproduce

```
response = openai.ChatCompletion.create(
        engine=""NAME of Model Deployment"",
        messages=msg,
        temperature=temperature,
        max_tokens=max_tokens,
        top_p=top_p,
        frequency_penalty=frequency_penalty,
        presence_penalty=presence_penalty,
        stop=stop)
```

### Code snippets

_No response_

### OS

macOS

### Python version

Python v3.10

### Library version

openai-python 0.27.2",closed,2023-04-02T01:41:02Z,2024-03-03T00:45:26Z,"['bug', 'Azure']",https://github.com/openai/openai-python/issues/362,8063.07,True,missing_dependency_violation,high,"The API call is returning a 404 error indicating that the resource (model deployment) cannot be found, suggesting a missing dependency or misconfiguration in the API contract.",high,
342,Add compression support,"### Describe the feature or improvement you're requesting

> HTTP compression is a capability that can be built into web servers and web clients to improve transfer speed and bandwidth utilization.


### Additional context

eg.

br – Brotli, a compression algorithm specifically designed for HTTP content encoding, defined in RFC 7932 and implemented in all modern major browsers.
compress – UNIX ""compress"" program method (historic; deprecated in most applications and replaced by gzip or deflate)
deflate – compression based on the deflate algorithm (described in RFC 1951), a combination of the LZ77 algorithm and Huffman coding, wrapped inside the zlib data format (RFC 1950);
exi – W3C Efficient XML Interchange
gzip – GNU zip format (described in RFC 1952). Uses the deflate algorithm for compression, but the data format and the checksum algorithm differ from the ""deflate"" content-encoding. This method is the most broadly supported as of March 2011.[5]
identity – No transformation is used. This is the default value for content coding.
pack200-gzip – Network Transfer Format for Java Archives[6]
**zstd** – Zstandard compression, defined in RFC 8478
In addition to these, a number of unofficial or non-standardized tokens are used in the wild by either servers or clients:

bzip2 – compression based on the free bzip2 format, supported by lighttpd[7]
lzma – compression based on (raw) LZMA is available in Opera 20, and in elinks via a compile-time option[8]
peerdist[9] – Microsoft Peer Content Caching and Retrieval
rsync[10] - delta encoding in HTTP, implemented by a pair of rproxy proxies.
xpress - Microsoft compression protocol used by Windows 8 and later for Windows Store application updates. LZ77-based compression optionally using a Huffman encoding.[11]
xz - LZMA2-based content compression, supported by a non-official Firefox patch;[12] and fully implemented in mget since 2013-12-31.[13]
",closed,2023-03-26T12:28:33Z,2024-03-03T00:43:59Z,['enhancement'],https://github.com/openai/openai-python/issues/342,8220.26,False,,,,,
434,Return `finish_reason` early in streaming multi choice `ChatCompletion`,"### Describe the feature or improvement you're requesting

Return `finish_reason` early in streaming multi choice(`n` > 1) `ChatCompletion`.

### Additional context

Messages containing `finish_reason` for different `choice`s are sent at the end of streaming request. However, different `choice`s can be completed at different times, and it would be very useful to have `finish_reason` to determine if a `choice` is completed early.

This may not be related to Python library, and possibly must be fixed in the API implementation.",closed,2023-05-04T13:05:16Z,2024-03-03T00:31:20Z,[],https://github.com/openai/openai-python/issues/434,7283.43,True,missing_option_violation,medium,"The API does not currently support returning `finish_reason` early for multiple choices in streaming responses, which is a recommended option for improved functionality.",high,
1180,Implement the new TTS file formats,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

The speech api implementation is not up to date. 
It doesn’t yet support the new Output formats like WAV
https://github.com/openai/openai-openapi/pull/202

### To Reproduce

1. Create a speech request
2. Try to use e.g. wav as the media type.

### Code snippets

```Python
https://github.com/openai/openai-python/blob/a7115b5f33acd27326e5f78e19beb0d73bd3268e/src/openai/types/audio/speech_create_params.py#L29
```


### OS

macOS

### Python version

v3.11.3

### Library version

v1.13.2",closed,2024-02-22T22:13:55Z,2024-03-02T22:46:20Z,['bug'],https://github.com/openai/openai-python/issues/1180,216.54,True,missing_option_violation,medium,"The API does not support new output formats like WAV, which are expected options for the speech request.",high,
1207,Fine tuning failure,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

An error occurred when I was fine-tuning a josn file to print the id of fine_tuning_job
![image](https://github.com/openai/openai-python/assets/72792297/d94ecf2b-974a-4941-b8c3-0230a1e23a98)
![image](https://github.com/openai/openai-python/assets/72792297/171a16b4-c3f5-4ce9-ba99-e5ad87a0d6c7)
![image](https://github.com/openai/openai-python/assets/72792297/708e63e6-8c93-4792-956b-c677498b2d87)
The returned model is none
May I ask what caused this error？
Thank you so much for your assistance.

### To Reproduce

As indicated above

### Code snippets

_No response_

### OS

Window 10

### Python version

Python 3.8.18

### Library version

openai 1.3.6",closed,2024-03-02T14:55:55Z,2024-03-02T22:43:42Z,['bug'],https://github.com/openai/openai-python/issues/1207,7.8,True,input_value_violation,medium,"The error indicates that the fine-tuning process may have received an improperly formatted JSON file, leading to a 'none' response for the model.",high,
1192,openai.File is no longer supported,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

You tried to access openai.File, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.

I'm trying to train an OpenAI Azure model by uploading a file but I'm getting an error that the openai.File is no longer supported.
There is no other sample documentation to explain the updated method of uploading training data to Azure OpenAI.

### To Reproduce

Try to upload a jsonl file to OpenAi Azure by using openai.File.create with the Microsoft example under ""Upload your training data""
https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/fine-tuning?tabs=turbo%2Cpython&pivots=programming-language-python

### Code snippets

```Python
# Upload fine-tuning files

import openai
import os

openai.api_key = os.getenv(""AZURE_OPENAI_KEY"")
openai.api_base =  ""https://nit.openai.azure.com/""
openai.api_type = 'azure'
openai.api_version = '2023-12-01-preview' # This API version or later is required to access fine-tuning for turbo/babbage-002/davinci-002

training_file_name = 'test.jsonl'
validation_file_name = 'test.jsonl'

# Upload the training and validation dataset files to Azure OpenAI with the SDK.

training_response = openai.File.create(
    file=open(training_file_name, ""rb""), purpose=""fine-tune"", user_provided_filename=""training_set.jsonl""
)
training_file_id = training_response[""id""]

validation_response = openai.File.create(
    file=open(validation_file_name, ""rb""), purpose=""fine-tune"", user_provided_filename=""validation_set.jsonl""
)
validation_file_id = validation_response[""id""]

print(""Training file ID:"", training_file_id)
print(""Validation file ID:"", validation_file_id)
```


### OS

macOS

### Python version

Python v3

### Library version

openai v1.0.1",closed,2024-02-25T18:39:03Z,2024-02-28T23:29:12Z,['bug'],https://github.com/openai/openai-python/issues/1192,76.84,True,missing_option_violation,high,"The use of openai.File.create is no longer supported in the specified library version, and there is no alternative method documented for uploading training data.",high,
1191,Official release version - PyPi or Github?,"### Confirm this is a feature request for the Python library and not the underlying OpenAI API.

- [X] This is a feature request for the Python library

### Describe the feature or improvement you're requesting

I'm just wondering what the official release version is. I see on PyPi version 1.12.0 and here on Github version 1.13.2. Since I packaged it for Mageia, I chose to go for the version on PyPi because it felt ""safer"".

Thank you.

### Additional context

N/A",closed,2024-02-25T18:30:54Z,2024-02-28T23:25:42Z,[],https://github.com/openai/openai-python/issues/1191,76.91,False,,,,,
1181,"memory leak, consumes entire system after just days of usage","### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

Progressive memory leak despite use of close().  Leads to eventually OOM on even large memory systems after just a few days.

Related: https://github.com/openai/openai-python/issues/820

### To Reproduce

I'm using 1.12.0 and still hit this issue.

Using text completion.  I'm also using close() in a try-finally, so close() does not help.  A global client connection doesn't make sense to enforce.  The old client pre 1.x had global attributes that made the API poor.  I presume there are some legacy parts still in place.

In h2oGPT, we use OpenAI client for OpenAI or vLLM connections, and I see a 5GB memory leak for every 6000 connections.  This happens whether I yield the generator for streaming or just exit after creating the completion.

Normally connections are not as intense, but this was easily reproducible by bisecting the OpenAI creation/generation parts of the code.  For typical workloads this leads to OOM on a 256GB system after just few days of usage.

Here is repro.  Please choose the <choose> to be some endpoint that you have setup like vLLM or TGI or gpt3.5 turbo so not expensive.  Choose api_key and model accordingly.

```
import os
import psutil
from openai import OpenAI

for i in range(6000):

    client_args = dict(base_url='<choose>', api_key=""EMPTY"")
    client = OpenAI(**client_args)

    responses = client.completions.create(
        model='h2oai/h2ogpt-4096-llama2-13b-chat',
        prompt=""Say exactly one word."",
        stream=True,
    )
    client.close()
    p = psutil.Process(os.getpid())
    print(p.memory_full_info())
```

The memory consumed is not increasing every step in loop, but it does monotonically increase from pss=48523264 to pss=107862016 within a few minutes (i.e. doubled) and continues this indefinitely.

The problem seems to be even stronger when doing concurrent requests in multi-threaded setup, as if the clean-up is not thread safe.  I'm trying to put together a repro that would showcase the 5GB after 6000 connections that only takes half hour to run.  But perhaps the above is sufficient.


### Code snippets

_No response_

### OS

ubuntu 22

### Python version

Python v3.10

### Library version

openai v1.12.0",closed,2024-02-23T01:32:10Z,2024-02-27T21:47:46Z,['bug'],https://github.com/openai/openai-python/issues/1181,116.26,True,memory_out_of_bound,High,"The progressive memory leak leads to out-of-memory (OOM) errors on large memory systems, indicating a severe issue with memory management in the API.",high,
341,Error regarding knowledge cutoff,"### Describe the bug

In the system message to GPT-3.5-turbo, I have set the knowledge cutoff to the 27/3/23 (so it knows all the events that have happened today). But, when I ask it who the current PM is, it says Boris Johnson instead of Rishi Sunak. I have put images in the code snippets below. Am I doing anything wrong?

### To Reproduce

1. Use GPT-3.5-turbo
2. Set the knowledge cutoff date to 27/3/23
3. Ask it who the British PM is.

### Code snippets

```Python
Code (user is a variable):
messages=[
        {""role"": ""system"", ""content"": ""You are ChatGPT, a large language model trained by OpenAI. Answer as concisely as possible. Knowledge cutoff: {27/3/23} Current date: {26/3/23}""},
        {""role"": ""user"", ""content"": user},
        {""role"": ""assistant"", ""content"": ""Hi user, your question delights me the answer is:""},
    ]
Output: 
As of my knowledge cutoff date of March 27, 2023, Boris Johnson is still serving as Prime Minister of the United Kingdom, so there hasn't been a Prime Minister after him yet.
```


### OS

Windows 11

### Python version

Python v3.10.4

### Library version

openai-python v0.27.2",closed,2023-03-26T10:55:08Z,2023-03-29T23:11:19Z,['bug'],https://github.com/openai/openai-python/issues/341,84.27,False,,,,low,
519,Passing large datasets to `Embedding` causes an error,"### Describe the bug

See the bug report originally filed in https://github.com/chroma-core/chroma/issues/709

### To Reproduce

See the bug report originally filed in https://github.com/chroma-core/chroma/issues/709

### Code snippets

_No response_

### OS

macOS

### Python version

Python v3.11.4

### Library version

openai==0.27.8",closed,2023-07-04T18:33:32Z,2023-11-10T04:05:35Z,['bug'],https://github.com/openai/openai-python/issues/519,3081.53,True,input_value_violation,performance_degradation,"Passing large datasets to the `Embedding` method may exceed acceptable input limits, leading to performance issues.",high,
1185,openai.InternalServerError: Error code: 503,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

when I call a function client.files.create(file=open(""dige.txt"", ""rb""), purpose='assistants')，
then，there was an error in the log： openai.InternalServerError: Error code: 503 - {'error': {'message': '当前分组 default 下对于模型  无可用渠道 (request id: 20240223090320216834355OmsitOrX)', 'type': 'one_api_error'}}。
Can anyone help with this? Thanks~

### To Reproduce

code：
client = OpenAI(api_key=OPENAI_API_KEY ,base_url=OPENAI_API_BASE)
file = client.files.create(file=open(""dige.txt"", ""rb""),
                               purpose='assistants')


### Code snippets

_No response_

### OS

linux

### Python version

Python 3.10.11

### Library version

openai v1.2.3",closed,2024-02-23T14:44:39Z,2024-02-23T17:46:52Z,['bug'],https://github.com/openai/openai-python/issues/1185,3.04,True,missing_dependency_violation,high,"The error indicates that there is no available channel for the model under the specified group, suggesting a missing dependency on the model's availability or configuration.",high,
1186,openai.InternalServerError: Error code: 503,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

when I call a function client.files.create(file=open(""dige.txt"", ""rb""), purpose='assistants')，
then，there was an error in the log： openai.InternalServerError: Error code: 503 - {'error': {'message': '当前分组 default 下对于模型  无可用渠道 (request id: 20240223090320216834355OmsitOrX)', 'type': 'one_api_error'}}。
Can anyone help with this? Thanks~

### To Reproduce

code：
client = OpenAI(api_key=OPENAI_API_KEY ,base_url=OPENAI_API_BASE)
file = client.files.create(file=open(""dige.txt"", ""rb""),
                               purpose='assistants')


### Code snippets

_No response_

### OS

linux

### Python version

Python 3.10.11

### Library version

openai v1.2.3",closed,2024-02-23T14:45:25Z,2024-02-23T17:46:42Z,['bug'],https://github.com/openai/openai-python/issues/1186,3.02,True,input_value_violation,unknown,"The error indicates that there is no available channel for the model in the specified group, suggesting an issue with the input parameters or the state of the API.",medium,
1183,SSL errors when connecting to OpenAI azure endpoint while using library,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

Hi,

Over the last few weeks I've been configuring a site to site VPN between AWS and Azure so we can use the OpenAI endpoints that Azure now offers through a Privatelink. We're using a sample taken from the docs (with correct values replaced etc)

`import os
import openai
openai.api_type = ""azure""
openai.api_base = os.getenv(""AZURE_OPENAI_ENDPOINT"") 
openai.api_key = os.getenv(""AZURE_OPENAI_KEY"")
openai.api_version = ""2023-05-15""

response = openai.ChatCompletion.create(
    engine=""gpt-35-turbo"", # engine = ""deployment_name"".
    messages=[
        {""role"": ""system"", ""content"": ""You are a helpful assistant.""},
        {""role"": ""user"", ""content"": ""Does Azure OpenAI support customer managed keys?""},
        {""role"": ""assistant"", ""content"": ""Yes, customer managed keys are supported by Azure OpenAI.""},
        {""role"": ""user"", ""content"": ""Do other Azure AI services support this too?""}
    ]
)

print(response)
print(response['choices'][0]['message']['content'])`

I'm unfortunately receiving an SSL error when attempting to connect to the Azure endpoint while on the VPN. The error is...

`Max retries exceeded with url: /openai/deployments//chat/completions?api-version=1106-preview (Caused by SSLError(CertificateError(""hostname 'myorg.privatelink.openai.azure.com' doesn't match either of '*.cognitiveservices.azure.com', '*.api.cognitive.microsoft.com', '*.dev.cognitive.microsoft.com', '*.openai.azure.com'""))).
`

I initially thought this might be an issue with the domain I was using so have also attempted myorg.openai.azure.com (the endpoint seen in the azure console) and IP with the same problem. Issues I've seen similar to this online suggest a couple of solutions...

- Use certifi to find where the local certificate chain is and add the root cert which you can grab from hitting the endpoint through the browser, this has had no effect when I've tried.
- Disable SSL verification (Not really a fix and a bad solution)

I thought it could be something to do with the infrastructure setup but after running a pure SSL test I can receive a handshake from the endpoint, going via the browser also doesn't present the same certificate issue. Is this something other people have experienced or is this a bug with the openAI library/azure implementation?

### To Reproduce

1. Create an Azure endpoint with Privatelink
2. Connect to a corporate VPN
3. Run above code sample

### Code snippets

```Python
None
```


### OS

MacOS

### Python version

Python 3.8.18

### Library version

latest",closed,2024-02-23T11:17:08Z,2024-02-23T14:25:19Z,['bug'],https://github.com/openai/openai-python/issues/1183,3.14,True,input_value_violation,medium,"The SSL error indicates that the hostname does not match the expected certificate, suggesting a potential issue with the input values provided for the API endpoint configuration.",high,
1184,text to image,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

APIRemovedInV1
openai.lib._old_api.APIRemovedInV1: 

You tried to access openai.Image, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.

You can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. 

Alternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`

A detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742

### To Reproduce

a

### Code snippets

```Python
from flask import Flask, render_template, request, jsonify, send_file
import openai
import requests
from PIL import Image
from io import BytesIO
openai migrate


# Initialize Flask app
app = Flask(__name__)

# Set your OpenAI API key
openai.api_key = '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'

# Define a route for the home page
@app.route('/')
def home():
    return render_template('login.html')

# Define a route to handle image generation
@app.route('/generate', methods=['POST'])
def generate_image():
    # Get the text prompt from the form submission
    text = request.form['text_prompt']
    
    # Call the OpenAI API to generate the image
    response = openai.Image.create(prompt=text, n=1, size=""256x256"")
    
    # Get the URL of the generated image
    image_url = response['data'][0]['url']
    
    # Fetch the image using the URL
    image_response = requests.get(image_url)
    
    # Convert the image response to a PIL Image
    image = Image.open(BytesIO(image_response.content))
    
    # Save the image to a buffer
    buf = BytesIO()
    image.save(buf, format='PNG')
    buf.seek(0)
    
    # Send the image as a response
    return send_file(buf, mimetype='image/png')

# Run the Flask app
if __name__ == '__main__':
    app.run(debug=True)
```


### OS

windows 11

### Python version

python.11.8

### Library version

openai1.10.0",closed,2024-02-23T12:34:11Z,2024-02-23T14:22:46Z,['bug'],https://github.com/openai/openai-python/issues/1184,1.81,True,missing_dependency_violation,High,"The code attempts to access an unsupported method 'openai.Image.create' in the new library version, indicating a missing dependency on the updated API contract.",high,
820,Memory leak in the chat completion `create` method,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

Calling the `create` method on `completions` introduces a memory leak, according to `tracemalloc`.

### Example:
```python
client = OpenAI(MY_KEY)
client.chat.completions.create(
    messages=[
        {
            ""role"": ""user"",
            ""content"": ""Can you write a poem?"",
        }
    ],
    model=""gpt-3.5-turbo""
)
```

### How to determine it's a memory leak?
I use `tracemalloc` with my flask application:
```python
@blueprint.route(""/admin/sys/stats"")
def admin_sys_stats():
    snapshot = tracemalloc.take_snapshot()
    top_stats = snapshot.statistics('lineno')

    from openai import OpenAI

    client = OpenAI(KEY)
    client.chat.completions.create(
        messages=[
            {
                ""role"": ""user"",
                ""content"": ""Can you write a poem?"",
            }
        ],
        model=""gpt-3.5-turbo""
    )

    stats = """"
    for stat in top_stats[:1000]:
        if grep in str(stat):
            stats += str(stat) + ""\n""

    return f""<pre>{stats}</pre>"", 200
```

When running this endpoint multiple times, one line is at the very top (which means it's the most expensive one):
```
\venv\Lib\site-packages\openai\_response.py:227: size=103 KiB, count=1050, average=100 B
```

When I refresh, the `size` increases. Of course, in a production environment, the numbers get high a lot quicker.

### To Reproduce

There's no one way to prove there's a memory leak.
But what I did was:
1. Setup a flask application
2. Create the route provided in the bug description
3. Hit the route multiple times, you'll see an increase in the `size` of the object

### Code snippets

_No response_

### OS

Linux, macOS

### Python version

Python v3.11.2

### Library version

openai v1.2.4",closed,2023-11-14T23:21:06Z,2023-12-15T01:44:43Z,['bug'],https://github.com/openai/openai-python/issues/820,722.39,True,memory_out_of_bound,High,"The `create` method in the chat completions API is causing a memory leak, leading to increased memory usage with repeated calls, which can result in crashes or severe instability in the application.",high,
1176,Simple stream example does not work with GPT-4 model,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

Hi, I'm using OpenAI Python API version 1.12.0

The simple stream example from your GitHub repository does not work on the GTP-4 model. The GPT-3.5-Turbo model worked without issue. 

```python
from openai import OpenAI

client = OpenAI()

stream = client.chat.completions.create(
    model=""gpt-4"",
    messages=[{""role"": ""user"", ""content"": ""Say this is a test""}],
    stream=True,
)
for chunk in stream:
    print(chunk.choices[0].delta.content or """", end="""")
```

Error message:
```
Traceback (most recent call last):
  File ""/home/haje01/test.py"", line 25, in <module>
    for chunk in response:
  File ""/home/haje01/.pyenv/versions/ml/lib/python3.9/site-packages/openai/_streaming.py"", line 43, in __iter__
    for item in self._iterator:
  File ""/home/haje01/.pyenv/versions/ml/lib/python3.9/site-packages/openai/_streaming.py"", line 62, in __stream__
    raise APIError(
openai.APIError: An error occurred during streaming
```

### To Reproduce

1. `pip install openai==1.12.0`
2. run above code

### Code snippets

_No response_

### OS

Ubuntu 20.04.5 LTS (WSL)

### Python version

Python 3.9.15

### Library version

openai 1.12.0",closed,2024-02-22T01:29:17Z,2024-02-22T07:23:30Z,['bug'],https://github.com/openai/openai-python/issues/1176,5.9,True,missing_dependency_violation,high,"The error indicates that the streaming functionality is not properly initialized or configured for the GPT-4 model, suggesting a missing dependency in the API contract for streaming responses.",high,
1175,Can we get the response headers within a completion?,"### Confirm this is a feature request for the Python library and not the underlying OpenAI API.

- [X] This is a feature request for the Python library

### Describe the feature or improvement you're requesting

There is a lot of useful information within the response headers. Can we add this to the ChatCompletion object that `client.chat.completions.create` makes?

<img width=""695"" alt=""image"" src=""https://github.com/openai/openai-python/assets/26822232/317b9344-6031-4480-b1ab-48a4457807dc"">


### Additional context

_No response_",closed,2024-02-21T22:59:24Z,2024-02-22T03:03:57Z,[],https://github.com/openai/openai-python/issues/1175,4.08,False,,,,low,
1172,GPT-4-vision-preview does not throw error when the message content indicates one,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

I'd like to know if there is a way to detect responses as the ones listed below, as the openai cliend does not throw any exceptions when these occur?

“I am unable to view images directly.”
“Unfortunately, there seems to be an issue as the image is not visible or accessible to me at the moment.”
“I’m unable to provide real-time analysis or generate content based on the image provided as the capabilities have been disabled.”
“Given the lack of an actual image to analyse, (…)”
“Since the image cannot be displayed (…)”
“Unfortunately, I cannot assist with this request.”
“Unfortunately, there seems to be some misunderstanding as I do not possess the ability to physically see images or any type of attachments. Therefore, I am unable to provide an analysis based on an actual image.”

Here is a discussion on the same issue:
https://community.openai.com/t/calls-to-gpt-4-vision-preview-dont-produce-errors-but-it-says-it-cant-read-images/478203/7

Please suggest a way I can handle these grasefully.

### To Reproduce

N/A

### Code snippets

_No response_

### OS

Ubuntu 20.04

### Python version

Python 3.10.12

### Library version

openai  1.6.1",closed,2024-02-21T18:58:24Z,2024-02-21T19:08:33Z,['bug'],https://github.com/openai/openai-python/issues/1172,0.17,True,incorrect_functionality,high,"The API does not throw errors for invalid input scenarios, leading to incorrect system behavior.",high,
1165,`openai migrate` fails to translate multiple `except` blocks with old `openai.error` objects,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

I am running `openai migrate` on a codebase written for 0.28.1, and it doesn't correctly process a try/except with multiple except blocks.  E.g.

```
try:
   [calling the completions API]
except openai.error.RateLimitError as e:
   [handle it]
except openai.error.AuthenticationError as e:
   [handle it]
[other except blocks with other openai.error.* types...]
```

The migrate command translates just the first except block to use the newer class (`openai.RateLimitError`, e.g.) and ignores the later ones.

Each time I re-run `openai migrate` it translates one more except block, so that is a simple workaround.

### To Reproduce

1. Write a simple test case like the above code.
2. Run `openai migrate`
3. See it translates only one except block.

### Code snippets

_No response_

### OS

Linux

### Python version

Python v3.10.12

### Library version

openai v1.12.0",closed,2024-02-19T16:51:50Z,2024-02-21T15:39:25Z,['bug'],https://github.com/openai/openai-python/issues/1165,46.79,True,missing_dependency_violation,performance_degradation,"The migrate command fails to translate all except blocks, indicating a missing dependency in processing multiple error types.",high,
1151,name 'openai' is not defined in openai 1.2.0,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

This code should work as documented in usage https://github.com/openai/openai-python

Also i have doubt about using client with or without api key as parameter...in courses of OpenAi form DeepLearning, they show client without api key as parameter, and they prefer to use an environment variable saved in a .env file.:

```
client = OpenAI(
    # This is the default and can be omitted
    api_key=os.environ.get(""OPENAI_API_KEY""),
)

```

or

`client = OpenAI()`


### To Reproduce

1 - Execute the snippet below from DeepLearning official short course. Reading the usage of your main github file as linked above, it should work.

2 - Gives error : name 'openai' is not defined in openai 1.2.0

### Code snippets

```Python
from openai import OpenAI
import os


from dotenv import load_dotenv, find_dotenv

_ = load_dotenv(find_dotenv()) # read local .env file

openai.api_key  = os.getenv('OPENAI_API_KEY')

client = OpenAI()

def get_completion(prompt, model=""gpt-3.5-turbo""):
    messages = [{""role"": ""user"", ""content"": prompt}]
    response = client.chat.completions.create(
        model=model,
        messages=messages,
        temperature=0
    )
    return response.choices[0].message.content



def get_completion_from_messages(messages, 
                                 model=""gpt-3.5-turbo"", 
                                 temperature=0, 
                                 max_tokens=500):
    response = client.chat.completions.create(model=model,
    messages=messages,
    temperature=temperature, 
    max_tokens=max_tokens)
    return response.choices[0].message.content

messages =  [  
{'role':'system', 'content':'You are an assistant that speaks like Shakespeare.'},    
{'role':'user', 'content':'tell me a joke'},   
{'role':'assistant', 'content':'Why did the chicken cross the road'},   
{'role':'user', 'content':'I don\'t know'}  ]

response = get_completion_from_messages(messages)
print(response)
```
```


### OS

Ubuntu 22.04

### Python version

Python 3.11

### Library version

openai v1.2.0",closed,2024-02-13T19:29:19Z,2024-02-14T02:18:31Z,['bug'],https://github.com/openai/openai-python/issues/1151,6.82,True,input_type_violation,High,"The variable 'openai' is not defined, leading to a type mismatch when attempting to access 'openai.api_key'.",high,
1109,AsyncAzureClient connection hangs frequently,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

something very strange: the python OpenAI AzureClient works fine, but using the AsyncAzureClient results in hanging connection ~5% of the time

### To Reproduce

Run a bunch of requests (~100) to GPT-4 using `AsyncAzureClient`, eventually one of them will hang for 20+ minutes before timing out

### Code snippets

_No response_

### OS

macOS

### Python version

Python v3.11.6

### Library version

1.6.1",closed,2024-01-26T23:44:52Z,2024-02-14T18:09:08Z,['bug'],https://github.com/openai/openai-python/issues/1109,450.4,True,hang,Medium,"The AsyncAzureClient experiences hanging connections during requests, requiring timeouts or manual intervention.",high,
1059,Reusing AsyncOpenAI client results in openai.APIConnectionError,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

Reusing an instance of `AsyncOpenAI` client for multiple calls of `asyncio.gather` results in an `openai.APIConnectionError`. Retried requests (either via the `openai` library directly or `backoff` decorator) succeed, but the first try of the second use of the client always fails.

I suspect that this usage of `AsyncOpenAI` is not ideal, but the behavior nonetheless feels buggy. Even if the reuse of the client should fail, I'm confused understand why retries succeed.

Bizarrely, in my application all retries after the initial `openai.APIConnectionError` result in unending `openai.APITimeoutError` instead of success, but I am unable to repro this outside of the application. However, I strongly suspect that the issue is related as reusing the client solves both the initial error as well as the timeouts.

### To Reproduce

1. Create an instance of `AsyncOpenAI` with no retries enabled
2. Use `AsyncOpenAI().chat.completions.create` to create a list of `Future` objects (any number will do)
3. Use `asyncio.gather` to get the results of the API calls
4. Executes steps 3+4 again - this causes the error

### Code snippets

```Python
import asyncio
import openai
from openai import AsyncOpenAI
import httpx
import backoff

print(f""OpenAI version: {openai.__version__}"")

OPENAI_API_KEY = ""redacted""

api_params = {
    ""temperature"": 0.2,
    ""max_tokens"": 500,
    ""model"": ""gpt-3.5-turbo-1106"",
}

messages = [{""role"": ""user"", ""content"": ""What is the capital of Quebec?""}]


@backoff.on_exception(
    backoff.expo,
    (
        openai.RateLimitError,
        openai.APIStatusError,
        openai.APIConnectionError,
        openai.APIError,
        openai.APITimeoutError,
        openai.InternalServerError,
    ),
)
async def create_request_retry(client, messages, api_params):
    return await client.chat.completions.create(messages=messages, **api_params)


async def create_request_no_retry(client, messages, api_params):
    return await client.chat.completions.create(messages=messages, **api_params)


# No retries, new client for each set of requests - succeeds
def succeed1():
    for i in range(2):
        client = AsyncOpenAI(
            api_key=OPENAI_API_KEY,
            timeout=10.0,
            http_client=httpx.AsyncClient(limits=httpx.Limits(max_keepalive_connections=500, max_connections=100)),
            max_retries=0,
        )
        arequests = []
        for _ in range(5):
            arequests.append(create_request_no_retry(client, messages, api_params))
        responses = asyncio.run(asyncio.gather(*arequests))
        results = [response.choices[0].message.content for response in responses]
        print(f""{i}: {results}"")


# Retry using backoff decorator, reuse client - succeeds
def succeed2():
    client = AsyncOpenAI(
        api_key=OPENAI_API_KEY,
        timeout=10.0,
        http_client=httpx.AsyncClient(limits=httpx.Limits(max_keepalive_connections=500, max_connections=100)),
        max_retries=0,
    )
    for i in range(2):
        arequests = []
        for _ in range(5):
            arequests.append(create_request_retry(client, messages, api_params))
        responses = asyncio.run(asynciogather(*arequests))
        results = [response.choices[0].message.content for response in responses]
        print(f""{i}: {results}"")


# Retry using openai library, reuse client - succeeds
def succeed3():
    client = AsyncOpenAI(
        api_key=OPENAI_API_KEY,
        timeout=10.0,
        http_client=httpx.AsyncClient(limits=httpx.Limits(max_keepalive_connections=500, max_connections=100)),
        max_retries=2,
    )
    for i in range(2):
        arequests = []
        for _ in range(5):
            arequests.append(create_request_no_retry(client, messages, api_params))
        responses = asyncio.run(asynciogather(*arequests))
        results = [response.choices[0].message.content for response in responses]
        print(f""{i}: {results}"")


# No retries, reuse client - fails
def error():
    client = AsyncOpenAI(
        api_key=OPENAI_API_KEY,
        timeout=10.0,
        http_client=httpx.AsyncClient(limits=httpx.Limits(max_keepalive_connections=500, max_connections=100)),
        max_retries=0,
    )
    for i in range(2):
        arequests = []
        for _ in range(5):
            arequests.append(create_request_no_retry(client, messages, api_params))
        responses = asyncio.run(gather(*arequests))
        results = [response.choices[0].message.content for response in responses]
        print(f""{i}: {results}"")
```


### OS

macOS

### Python version

Python 3.11.5

### Library version

openai v1.6.1",closed,2024-01-09T23:59:46Z,2024-02-14T18:07:51Z,['bug'],https://github.com/openai/openai-python/issues/1059,858.13,True,method_order_violation,medium,"Reusing the AsyncOpenAI client without proper reinitialization or handling may lead to APIConnectionError, indicating that the method calls are not being executed in the expected order or context.",high,
1134,Client Frequently hangs gives Timeout error!,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

There are lot of openai.APIConnectionError when there is non english messages sent!

I stream response and have tried with timeout setting `30.0` , `60.0` and even `120.0` 
OpenAI(api_key = os.getenv(""OPENAI_API_KEY""),timeout=30.0)

I have noticed there are frequent timeout error and most of the time it's when there is non english messaged but also few times in english messages. Eventually I have to set max retries to 0 and use fallback model.


<img width=""864"" alt=""Screenshot 2023-12-20 at 2 41 47 PM"" src=""https://github.com/openai/openai-python/assets/31214277/aa2a5c03-9ac5-48a4-b8da-e657490159c0"">




### To Reproduce

Essentially  openai.APIConnectionError randomly. I don't even have huge concurrent traffic.

### Code snippets

```Python
#I have tried will all these similar settings while setting streaming = True

client = OpenAI(api_key = os.getenv(""OPENAI_API_KEY""),timeout=30.0, max_retries=1)

client = OpenAI(api_key = os.getenv(""OPENAI_API_KEY""),timeout=60.0, max_retries=0)

client = OpenAI(api_key = os.getenv(""OPENAI_API_KEY""),timeout=80.0, max_retries=0)

```
My OpenAI Library version is always the latest

### OS

Cloud Function

### Python version

Python 3.10

### Library version

v1.11.1",closed,2024-02-07T18:59:28Z,2024-02-14T18:05:47Z,['bug'],https://github.com/openai/openai-python/issues/1134,167.11,True,hang,Medium,"The client frequently hangs and gives timeout errors, particularly when handling non-English messages, indicating potential issues with input handling or API call sequencing.",high,
1153,"'timestamp granularity', new featured added to AudioAPI Feb 9th not accessible","### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

audio.transcriptions.create() parameter 'timestamp_granularities' not recognized.  

As per API changelog, parameter added to API Feb 9th:
https://platform.openai.com/docs/changelog

### To Reproduce

passing timestamp_granularities=[""word""] to audio.transcriptions.create() results in 

TypeError: Transcriptions.create() got an unexpected keyword argument 'timestamp_granularities'

### Code snippets

```Python
---> 19 transcript = client.audio.transcriptions.create( #openai.audio.transcriptions.create(
     20   file=wavfile,
     21   model=""whisper-1"",
     22   response_format=""verbose_json"",
     23   timestamp_granularities=[""word""]
     24 )

TypeError: Transcriptions.create() got an unexpected keyword argument 'timestamp_granularities'
```


### OS

All

### Python version

3.11.4

### Library version

openai-1.12.0",closed,2024-02-14T01:56:18Z,2024-02-14T02:50:06Z,['bug'],https://github.com/openai/openai-python/issues/1153,0.9,False,,,,low,
479,Is there a way to prevent the API from returning the same answers in different outputs of the same HTTP response?,"### Describe the feature or improvement you're requesting

When I make a request to the OpenAI API (specifically, the chat endpoint) with this package, I expect that different outputs, in the same response, are different. There's no reason for expecting different outputs to be the same - you could just copy them. This problem happens so often and it's so annoying. Even if we increase the temperature, this issue still occurs, i.e. the model may always return the same answer in different outputs. We should never pay for getting the same output in the same HTTP response multiple times. It would be like asking someone (a human) to come up with 5 different descriptions of a product, but 3 of them are repetitions.

So, is there a way to prevent the API from returning the same answer in different outputs? I think the answer is no. 

If not, could this be implemented in some way in the backend? It would also be great if this feature allowed us to make the comparison by ignoring cases (some standardized form) or maybe using some kind of similarity metric (but this is more advanced). 

Clearly, we need this to avoid making unnecessary calls and spending unnecessary credits. 

Prompt engineering doesn't really work in many cases. It's trial-and-error combined with headaches. What we need is the API or the backend to **enforce** the different outputs to contain different outputs/answers, otherwise, we'd better just remove the possibility to return multiple outputs because, at least, that's less misleading.

### Additional context

_No response_",closed,2023-06-06T12:27:23Z,2023-09-06T15:08:32Z,['API-feedback'],https://github.com/openai/openai-python/issues/479,2210.69,True,missing_option_violation,medium,"The API does not enforce distinct outputs when multiple outputs are requested, leading to user frustration and potential misuse of credits.",high,
1137,openai.organization not being used,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

We've recently upgraded the openai client to >1.0.0 and found that our ""organization switching"" logic stopped working.

Before 1.0.0, organization was being set by setting global `openai.organization`. All clients created used that as the default organization.

Post 1.0.0, the logic doesn't work. 

Having the ability to set the property, but property not being used is very surprising.

### To Reproduce

Snippet is below. I've used opentelemetry httpx instrumentation to intercept the request and verify that header is not being set.

Example headers after running the provided code below. Observe that there's no ""Openai-organization"" header set.
```
Headers({'host': 'api.openai.com', 'accept-encoding': 'gzip, deflate', 'connection': 'keep-alive', 'accept': 'application/json', 'content-type': 'application/json', 'user-agent': 'OpenAI/Python 1.3.5', 'x-stainless-lang': 'python', 'x-stainless-package-version': '1.3.5', 'x-stainless-os': 'MacOS', 'x-stainless-arch': 'arm64', 'x-stainless-runtime': 'CPython', 'x-stainless-runtime-version': '3.11.7', 'authorization': '[secure]', 'content-length': '84', 'sentry-trace': 'cf394dd2ce8b4682843e13cbf98782d1-894bb0e05b111a50', 'baggage': 'sentry-trace_id=cf394dd2ce8b4682843e13cbf98782d1,sentry-environment=development,sentry-release=unknown-development'})
```

### Code snippets

```Python
import openai

openai.organization = ""some_id""
from openai import OpenAI
client = OpenAI(
    # Defaults to os.environ.get(""OPENAI_API_KEY"")
)

chat_completion = client.chat.completions.create(
    model=""gpt-3.5-turbo"",
    messages=[{""role"": ""user"", ""content"": ""Hello world""}]
)
```


### OS

macOs

### Python version

v3.11.7

### Library version

openai 1.3.5",closed,2024-02-08T10:11:32Z,2024-02-12T10:20:31Z,['bug'],https://github.com/openai/openai-python/issues/1137,96.15,True,missing_dependency_violation,high,"The openai.organization property is set but not utilized in the API call, indicating a missing dependency between the organization setting and the request being made.",high,
1135,export 'Usage' class as apart of openai.types,"### Confirm this is a feature request for the Python library and not the underlying OpenAI API.

- [X] This is a feature request for the Python library

### Describe the feature or improvement you're requesting

Hi there,

I'm using the library and writing tests to mock `CreateEmbeddingResponse`. To mock the class, I'm configuring the properties, including the `usage` parameter. I'd like to instantiate a '`Usage`' object for this. Hence, requesting it to be exported with `openai.types.` Thanks! 

### Additional context

_No response_",closed,2024-02-07T19:03:39Z,2024-02-13T12:57:14Z,[],https://github.com/openai/openai-python/issues/1135,137.89,False,,,,,
1147,Problem with proxy and streaming,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

I try to process streaming (return customer in chat). I need to use proxy. I have problem that response does not streaming when use proxy (all responses returned after all processed, no effect of writing text)

### To Reproduce

All code below, api key in ENV

### Code snippets

```Python
import asyncio
from typing import Optional

from httpx import AsyncClient
from openai import AsyncStream, AsyncOpenAI
from openai.types.chat import ChatCompletionChunk


async def get_openai_stream_agenerator() -> AsyncStream[ChatCompletionChunk]:
    client = AsyncOpenAI(
        http_client=AsyncClient(
            # when I comment these two lines streaming is ok
            proxy=""http://localhost:8080"",  # I'm using mitmproxy with basic configuration
            verify=False,
        )
    )
    messages = [
        {""role"": ""system"", ""content"": ""Return details about asking person""},
        {""role"": ""user"", ""content"": ""Iga Świątek""},
    ]
    response: AsyncStream[ChatCompletionChunk] = await client.chat.completions.create(
        model='gpt-4-0613',
        messages=messages,
        stream=True,
    )  # type: ignore
    return response


def get_delta_argument(chunk: ChatCompletionChunk) -> Optional[str]:
    if len(chunk.choices) > 0:
        return chunk.dict()['choices'][0]['delta']['content']
    else:
        return None


async def get_response_generator() -> None:
    async for it in await get_openai_stream_agenerator():
        value = get_delta_argument(it)
        if value:
            print(value, end="""")
    print()


if __name__ == '__main__':
    asyncio.run(get_response_generator())
```


### OS

macOS

### Python version

Python v3.11.7

### Library version

openai 1.12.0, httpx 0.26.0",closed,2024-02-13T01:43:57Z,2024-02-13T11:41:38Z,['bug'],https://github.com/openai/openai-python/issues/1147,9.96,True,input_value_violation,performance_degradation,"The use of a proxy with streaming responses may lead to degraded performance or unexpected behavior, as the streaming functionality does not operate as intended when the proxy is enabled.",high,
1141,"""We could not parse the JSON body of your request."" with tts-1-(hd) endpoint, text to speach","### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

I am receiving this error (suddenly) with openai==1.12.0 on Feb  09, 2024...worked the day before:

```

  File ""/Users/ajung/Dropbox/FILMORA- VIDEOS/Typesense-Onkopedia - 2/convert.py"", line 66, in <module>
    text_to_speech(content, ""de"", output_wav)
  File ""/Users/ajung/Dropbox/FILMORA- VIDEOS/Typesense-Onkopedia - 2/convert.py"", line 17, in text_to_speech
    response = client.audio.speech.create(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/ajung/Dropbox/FILMORA- VIDEOS/Typesense-Onkopedia - 2/lib/python3.12/site-packages/openai/resources/audio/speech.py"", line 81, in create
    return self._post(
           ^^^^^^^^^^^
  File ""/Users/ajung/Dropbox/FILMORA- VIDEOS/Typesense-Onkopedia - 2/lib/python3.12/site-packages/openai/_base_client.py"", line 1201, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/ajung/Dropbox/FILMORA- VIDEOS/Typesense-Onkopedia - 2/lib/python3.12/site-packages/openai/_base_client.py"", line 889, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File ""/Users/ajung/Dropbox/FILMORA- VIDEOS/Typesense-Onkopedia - 2/lib/python3.12/site-packages/openai/_base_client.py"", line 981, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': ""We could not parse the JSON body of your request. (HINT: This likely means you aren't using your HTTP library correctly. The OpenAI API expects a JSON payload, but what was sent was not valid JSON. If you have trouble figuring out how to fix this, please contact us through our help center at help.openai.com.)"", 'type': 'invalid_request_error', 'param': None, 'code': None}}
```



### To Reproduce

```
import sys
from openai import OpenAI
import openai

client = OpenAI()

SPEED = 1.0
VOICE = ""echo""
MODEL=""tts-1"",
#MODEL=""tts-1-hd"",

def text_to_speech(content: str, language: str, output_wav: str):

    response = client.audio.speech.create(
        model=MODEL,
        voice=VOICE,
        speed=SPEED,
        input=content,
    )

    print(f""Saved to {output_wav}"")
    response.stream_to_file(output_wav)


text_to_speech(""hello world"", ""de"", ""output.wav"")
```

### Code snippets

_No response_

### OS

MacOS Soama

### Python version

3.12

### Library version

open 1.12.0",closed,2024-02-09T07:31:24Z,2024-02-12T06:05:43Z,['bug'],https://github.com/openai/openai-python/issues/1141,70.57,True,input_type_violation,High,"The API expects a valid JSON payload, but the provided input does not conform to the expected format, leading to a BadRequestError.",high,
1142,AttributeError: 'function' object has no attribute 'completion',"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

```

client = OpenAI(api_key = ""API-KEY"")
chat_completion = client.chat.completions.create(
    messages=[
        {
            ""role"": ""user"",
            ""content"": ""Say this is a test"",
        }
    ],
    model=""gpt-3.5-turbo"",
)
```

Gives an Error on Colab
`---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
[<ipython-input-26-2409eaa814b3>](https://localhost:8080/#) in <cell line: 1>()
----> 1 chat_completion = client.chat.completions.create(
      2     messages=[
      3         {
      4             ""role"": ""user"",
      5             ""content"": ""Say this is a test"",

AttributeError: 'function' object has no attribute 'completions'
`
Version: openai 1.12.0

### To Reproduce

```
client = OpenAI(api_key = ""API-KEY"")
chat_completion = client.chat.completions.create(
    messages=[
        {
            ""role"": ""user"",
            ""content"": ""Say this is a test"",
        }
    ],
    model=""gpt-3.5-turbo"",
)
```

### Code snippets

```Python
Already Attached
```


### OS

linux

### Python version

Python 3.11

### Library version

openai v1.12.0",closed,2024-02-09T12:40:02Z,2024-02-12T02:14:29Z,['bug'],https://github.com/openai/openai-python/issues/1142,61.57,False,,,,low,
1145,`usage` of Completion.Create is None ,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

usage object is None - how could it be?
It didn't happen before; today, that's the first time it happened.
What is the scenario in which usage is None? 
Why was it the first time that happened?

### To Reproduce

Run the code about with openai version = 1.3.3 or 1.12.0 and usage object will be none.


### Code snippets

```Python
openai_response = self.client.chat.completions.create(
            model=model, messages=messages, temperature=temperature,
            max_tokens=max_tokens, top_p=top_p,
            frequency_penalty=frequency_penalty, presence_penalty=presence_penalty, stop=stop
        )

        openai_usage = openai_response.usage # Sometimes usage is None
```
```


### OS

Linux

### Python version

Python 3.9.18

### Library version

openai v1.3.3 or openai v1.12.0
Using Azure OpenAI",closed,2024-02-10T00:37:00Z,2024-02-11T04:52:30Z,['bug'],https://github.com/openai/openai-python/issues/1145,28.26,True,missing_dependency_violation,high,"The usage object is None, indicating a missing dependency or context required for its proper initialization or retrieval.",high,
1089,AzureOpenAI client usage is inconsistent with the API description regarding model id,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

Following are examples of chat completions create and assistants create calls:

response = client.chat.completions.create(
        model=model,
        messages=messages
)

assistant = client.beta.assistants.create(
        name=name,
        instructions=instructions,
        tools=tools,
        model=model,
        file_ids=file_ids
)

In both API methods, the model parameter is specified as
    model: ID of the model to use. See the [model endpoint compatibility](https://platform.openai.com/docs/models/model-endpoint-compatibility) table for details on which models work with the Chat API.

The problem is that if client has been created with AzureOpenAI() constructor, 
the required model parameter value is not actually model id but model deployment name (from Azure Portal)

This is problematic because:
1. Documentation asks for model id, not for deployment name
2. If customer lists the models using following API `client.models.list()`, it provides model ids that
   works OK with OpenAI client, but not with AzureOpenAI client (unless the deployment name is the same as model id)
3. Listing the models using client api is probably common and the values there are expected to work with client APIs.

The problem can be avoided with AzureOpenAI if model deployment name and model id are the same, but it is not
clear for user always to do that.


### To Reproduce

1. Create Azure OpenAI resource and create model deployment with some name which is not the same as model id(name)
2. Construct AzureOpenAI client
3. List the model ids using `client.models.list()` API
4. Use e.g. chat completion api with the model id
5. If the model id is not the same as model deployment name, there will be a ""model deployment not found"" error

### Code snippets

_No response_

### OS

Windows

### Python version

Python v3.12.1

### Library version

openai v1.7.0",closed,2024-01-19T21:04:38Z,2024-02-03T00:21:51Z,[],https://github.com/openai/openai-python/issues/1089,339.29,False,,,,low,
692,CRITICAL BUG: images.generate does not work on Azure OpenAI,"            openai_api_version = ""2023-09-01-preview""
            client = AzureOpenAI(azure_endpoint = openai_api_base, api_key=openai_api_key, api_version=openai_api_version)

            response = client.images.generate(
                prompt=prompt,
                size=size,
                n=n
            )

Fails with:

{'error': {'code': '404', 'message': 'Resource not found'}}",closed,2023-11-06T23:31:19Z,2024-01-10T04:38:53Z,[],https://github.com/openai/openai-python/issues/692,1541.13,True,missing_dependency_violation,High,The API call to images.generate fails due to a missing resource or incorrect endpoint configuration.,high,
874,Unclear how to use the new AsyncClient at scale - memory leaks or timeouts,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

I have a containerized FastAPI server.
- When I create new clients per request, I see a big memory leak. This is the same as #820.
- When I create a global AsyncClient that is reused across requests, I see regular `httpx.PoolTimeout`s. Similar to #821, except mine is async

As a result, its not clear how to use the new library at scale. It seems like the optimal solution would be to manage a pool of AsyncClients and scale them up/down as QPS goes up/down. Looking for suggestions/advice on how to handle this. Thanks! 

### To Reproduce

Use this as the client:
```
def get_openai_client() -> AsyncOpenAI:
    if not hasattr(get_openai_client, ""client""):
        get_openai_client.client = AsyncOpenAI(
            api_key=""..."",
        )
    return get_openai_client.client
```

And then send a bunch of requests/see timeouts.

### Code snippets

_No response_

### OS

Linux

### Python version

3.11

### Library version

1.3.4",closed,2023-11-23T20:59:19Z,2023-11-23T22:22:55Z,['bug'],https://github.com/openai/openai-python/issues/874,1.39,True,memory_out_of_bound,High,"The creation of new AsyncClients per request leads to memory leaks, indicating potential memory management issues.",high,
1130,Support Dall-e-3 from the command line,"### Confirm this is a feature request for the Python library and not the underlying OpenAI API.

- [X] This is a feature request for the Python library

### Describe the feature or improvement you're requesting

Currently the default model for images.create is Dall-e-2.

Source: https://platform.openai.com/docs/api-reference/images/create

![Screenshot 2024-02-06 at 14 48 58](https://github.com/openai/openai-python/assets/382183/3825708f-90b6-478b-bad1-aa0f67f0359b)

But the command line does not support selecting a different model.

Source: 

```sh
$ openai api images.generate

usage: openai api images.generate [-h] -p PROMPT [-n NUM_IMAGES] [-s SIZE] [--response-format RESPONSE_FORMAT]
openai api images.generate: error: the following arguments are required: -p/--prompt
```

### Additional context

https://github.com/openai/openai-python/blob/d231d1fa783967c1d3a1db3ba1b52647fff148ac/src/openai/resources/images.py#L61-L62",closed,2024-02-06T19:52:33Z,2024-02-07T12:50:51Z,['CLI'],https://github.com/openai/openai-python/issues/1130,16.97,True,missing_option_violation,medium,"The command line interface does not support selecting a different model, which is a missing option for users who want to utilize Dall-e-3.",high,
932,Add `openai.embeddings_utils` to the list of ###Removed,"<details>
<summary>Originally posted by @rattrayalex in https://github.com/openai/openai-python/discussions/742_</summary>
We have released a new major version of our SDK, and we recommend upgrading promptly. 

It's a total rewrite of the library, so many things have changed, but we've made upgrading easy with a code migration script and detailed docs below. It was extensively [beta tested](https://github.com/openai/openai-python/discussions/631) prior to release. 

## Getting started

```
pip install --upgrade openai
```

## What's changed
- Auto-retry with backoff if there's an error
- Proper types (for mypy/pyright/editors)
- You can now instantiate a client, instead of using a global default.
- Switch to explicit client instantiation
- Weights and Biases CLI will now be included in their own package

## Migration guide

_For Azure OpenAI users, see [Microsoft's Azure-specific migration guide](https://aka.ms/oai/v1-python-migration)._

### Automatic migration with grit

You can automatically migrate your codebase using [grit](https://grit.io/), either [online](https://app.grit.io/migrations/new/openai) or with the following CLI command on Mac or Linux:

```
openai migrate
```

The grit binary executes entirely locally with AST-based transforms.

Be sure to audit its changes: we suggest ensuring you have a clean working tree beforehand, and running `git add --patch` afterwards. Note that grit.io also offers opt-in automatic fixes powered by AI.

#### Automatic migration with grit on Windows

To use grit to migrate your code on Windows, you will need to use Windows Subsystem for Linux (WSL). [Installing WSL](https://learn.microsoft.com/en-us/windows/wsl/install) is quick and easy, and you do not need to keep using Linux once the command is done.

Here's a step-by-step guide for setting up and using WSL for this purpose:

1. Open a PowerShell or Command Prompt as an administrator and run `wsl --install`.
2. Restart your computer.
3. Open the WSL application.
4. In the WSL terminal, `cd` into the appropriate directory (e.g., `cd /mnt/c/Users/Myself/my/code/`) and then run the following commands:
    ```bash
    curl -fsSL https://docs.grit.io/install | bash
    grit install
    grit apply openai
    ```

Then, you can close WSL and go back to using Windows.

#### Automatic migration with grit in Jupyter Notebooks

If your Jupyter notebooks are not in source control, they will be more difficult to migrate. You may want to copy each cell into grit's web interface, and paste the output back in.

If you need to migrate in a way that preserves use of the module-level client instead of instantiated clients, you can use [the openai_global grit migration](https://app.grit.io/migrations/new/openai_global) instead.

### Initialization

```py
# old
import openai

openai.api_key = os.environ['OPENAI_API_KEY']

# new
from openai import OpenAI

client = OpenAI(
  api_key=os.environ['OPENAI_API_KEY'],  # this is also the default, it can be omitted
)
```

### Responses

Response objects are now [pydantic models](https://docs.pydantic.dev/latest/usage/models/) and no longer conform to the dictionary shape. However you can easily convert them to a dictionary with `model.model_dump()`.

```py
# before
import json
import openai

completion = openai.Completion.create(model='curie')
print(completion['choices'][0]['text'])
print(completion.get('usage'))
print(json.dumps(completion, indent=2))

# after
from openai import OpenAI

client = OpenAI()

completion = client.completions.create(model='curie')
print(completion.choices[0].text)
print(dict(completion).get('usage'))
print(completion.model_dump_json(indent=2))
```

### Async client

We do not support calling asynchronous methods in the module-level client, instead you will have to instantiate an async client.

The rest of the API is exactly the same as the synchronous client.

```py
# old
import openai

completion = openai.ChatCompletion.acreate(model=""gpt-3.5-turbo"", messages=[{""role"": ""user"", ""content"": ""Hello world""}])

# new
from openai import AsyncOpenAI

client = AsyncOpenAI()
completion = await client.chat.completions.create(model=""gpt-3.5-turbo"", messages=[{""role"": ""user"", ""content"": ""Hello world""}])
```

### Module client

> [!IMPORTANT]  
> We highly recommend instantiating client instances instead of relying on the global client.

We also expose a global client instance that is accessible in a similar fashion to versions prior to v1.

```py
import openai

# optional; defaults to `os.environ['OPENAI_API_KEY']`
openai.api_key = '...'

# all client options can be configured just like the `OpenAI` instantiation counterpart
openai.base_url = ""https://...""
openai.default_headers = {""x-foo"": ""true""}

completion = openai.chat.completions.create(
    model=""gpt-4"",
    messages=[
        {
            ""role"": ""user"",
            ""content"": ""How do I output all files in a directory using Python?"",
        },
    ],
)
print(completion.choices[0].message.content)
```

The API is the exact same as the standard client instance based API.

This is intended to be used within REPLs or notebooks for faster iteration, **not** in application code.

We recommend that you always instantiate a client (e.g., with `client = OpenAI()`) in application code because:
- It can be difficult to reason about where client options are configured
- It's not possible to change certain client options without potentially causing race conditions
- It's harder to mock for testing purposes
- It's not possible to control cleanup of network connections

### Pagination

All `list()` methods that support pagination in the API now support automatic iteration, for example:

```py
from openai import OpenAI

client = OpenAI()

for job in client.fine_tuning.jobs.list(limit=1):
    print(job)
```

Previously you would have to explicitly call a `.auto_paging_iter()` method instead.
See the[ README](https://github.com/openai/openai-python/tree/main/README.md) for more details.

### Azure OpenAI

To use this library with [Azure OpenAI](https://learn.microsoft.com/en-us/azure/ai-services/openai/overview), use the `AzureOpenAI` class instead of the `OpenAI` class.

A more comprehensive [Azure-specific migration guide is available on the Microsoft website](https://aka.ms/oai/v1-python-migration).

> [!IMPORTANT]  
> The Azure API shape differs from the core API shape which means that the static types for responses / params
> won't always be correct.

```python
from openai import AzureOpenAI

# gets the API Key from environment variable AZURE_OPENAI_API_KEY
client = AzureOpenAI(
    # https://learn.microsoft.com/en-us/azure/ai-services/openai/reference#rest-api-versioning
    api_version=""2023-07-01-preview"",
    # https://learn.microsoft.com/en-us/azure/cognitive-services/openai/how-to/create-resource?pivots=web-portal#create-a-resource
    azure_endpoint=""https://example-endpoint.openai.azure.com"",
)

completion = client.chat.completions.create(
    model=""deployment-name"",  # e.g. gpt-35-instant
    messages=[
        {
            ""role"": ""user"",
            ""content"": ""How do I output all files in a directory using Python?"",
        },
    ],
)
print(completion.model_dump_json(indent=2))
```

In addition to the options provided in the base `OpenAI` client, the following options are provided:

- `azure_endpoint`
- `azure_deployment`
- `api_version`
- `azure_ad_token`
- `azure_ad_token_provider`

An example of using the client with Azure Active Directory can be found [here](https://github.com/openai/openai-python/tree/main/examples/azure_ad.py).

### All name changes

> Note: all `a*` methods have been removed; the [async client](#async-client) must be used instead.

- `openai.api_base` -> `openai.base_url`
- `openai.proxy` -> `openai.proxies` ([docs](https://www.python-httpx.org/compatibility/#proxy-keys))
- `openai.InvalidRequestError` -> `openai.BadRequestError`
- `openai.Audio.transcribe()` -> `client.audio.transcriptions.create()`
- `openai.Audio.translate()` -> `client.audio.translations.create()`
- `openai.ChatCompletion.create()` -> `client.chat.completions.create()`
- `openai.Completion.create()` -> `client.completions.create()`
- `openai.Edit.create()` -> `client.edits.create()`
- `openai.Embedding.create()` -> `client.embeddings.create()`
- `openai.File.create()` -> `client.files.create()`
- `openai.File.list()` -> `client.files.list()`
- `openai.File.retrieve()` -> `client.files.retrieve()`
- `openai.File.download()` -> `client.files.retrieve_content()`
- `openai.FineTune.cancel()` -> `client.fine_tunes.cancel()`
- `openai.FineTune.list()` -> `client.fine_tunes.list()`
- `openai.FineTune.list_events()` -> `client.fine_tunes.list_events()`
- `openai.FineTune.stream_events()` -> `client.fine_tunes.list_events(stream=True)`
- `openai.FineTune.retrieve()` -> `client.fine_tunes.retrieve()`
- `openai.FineTune.delete()` -> `client.fine_tunes.delete()`
- `openai.FineTune.create()` -> `client.fine_tunes.create()`
- `openai.FineTuningJob.create()` -> `client.fine_tuning.jobs.create()`
- `openai.FineTuningJob.cancel()` -> `client.fine_tuning.jobs.cancel()`
- `openai.FineTuningJob.delete()` -> `client.fine_tuning.jobs.create()`
- `openai.FineTuningJob.retrieve()` -> `client.fine_tuning.jobs.retrieve()`
- `openai.FineTuningJob.list()` -> `client.fine_tuning.jobs.list()`
- `openai.FineTuningJob.list_events()` -> `client.fine_tuning.jobs.list_events()`
- `openai.Image.create()` -> `client.images.generate()`
- `openai.Image.create_variation()` -> `client.images.create_variation()`
- `openai.Image.create_edit()` -> `client.images.edit()`
- `openai.Model.list()` -> `client.models.list()`
- `openai.Model.delete()` -> `client.models.delete()`
- `openai.Model.retrieve()` -> `client.models.retrieve()`
- `openai.Moderation.create()` -> `client.moderations.create()`
- `openai.api_resources` -> `openai.resources`

### Removed

- `openai.api_key_path`
- `openai.app_info`
- `openai.debug`
- `openai.log`
- `openai.OpenAIError`
- `openai.Audio.transcribe_raw()` 
- `openai.Audio.translate_raw()`
- `openai.ErrorObject`
- `openai.Customer`
- `openai.api_version`
- `openai.verify_ssl_certs`
- `openai.api_type`
- `openai.enable_telemetry`
- `openai.ca_bundle_path`
- `openai.requestssession` (we now use [httpx](https://github.com/encode/httpx))
- `openai.aiosession` (we now use [httpx](https://github.com/encode/httpx))
- `openai.Deployment` (only used for Azure)
- `openai.Engine`
- `openai.File.find_matching_files()`

</details>

I propose to update migration guide to reflect that `openai.embeddings_utils` has been removed from the v1.0 and above. To achieve this add the following to `###Removed`:
- `openai.embeddings_utils`",closed,2023-12-04T17:42:35Z,2023-12-05T22:41:15Z,[],https://github.com/openai/openai-python/issues/932,28.98,True,missing_option_violation,medium,"The migration guide does not reflect the removal of 'openai.embeddings_utils', which is a critical update for users transitioning to the new version.",high,
1119,I encountered an error: openai. error. AuthenticationError Invalid token (request id: 20240204142954xxxxxxxxxxxxxxxxxxx),"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [x] This is an issue with the Python library

### Describe the bug

When I call the openai API, it may be normal for the first 1-6 hours, but then as long as I send a request to gpt-3.5 turbo, an AuthenticationError Invalid token (request id: 20240204142954xxxxxxxxxxxxxxxxxxx) will occur. When I restart my django service, it returns to a normal state, which I am very puzzled about

### To Reproduce

1. Call the openai interface
2. Maintain call frequency and wait for a few hours
3. An invalid token with an error occurred
4. When restarting django, everything becomes normal again

### Code snippets

```Python
response = openai.ChatCompletion.create(
                    model=""gpt-3.5-turbo-1106"",
                    stream=True,
                    messages=conversation_list,
                    timeout=120
                )
                for data in response:
                    if ""content"" in data[""choices""][0][""delta""]:
                        content = data[""choices""][0][""delta""][""content""]
                        print(content, end='')
```


### OS

linux

### Python version

Python v3.9.2

### Library version

openai v0.28",closed,2024-02-04T07:19:57Z,2024-02-04T18:28:52Z,['bug'],https://github.com/openai/openai-python/issues/1119,11.15,True,missing_dependency_violation,high,"The error indicates a missing or invalid authentication token, which is essential for making API calls. This suggests a dependency on a valid token that is not being maintained over time.",high,
1110,Add support for locally hosted img urls,"### Confirm this is a feature request for the Python library and not the underlying OpenAI API.

- [ ] This is a feature request for the Python library

### Describe the feature or improvement you're requesting

You know what I don't like doing right now. Creating a microservice and deploying it just to handle, serve and store img urls. 
OpenAi should implement a feature which allows the OpenAi vision API to be able to communicate with localhost. I know that this could be a complex task, but you guys have 100 of billions of dollar in funding. You guys could probably figure it out

### Additional context

I know that we can use bas64 encoding, but it's slow. ",closed,2024-01-28T14:40:32Z,2024-02-02T04:40:43Z,['enhancement'],https://github.com/openai/openai-python/issues/1110,110.0,False,,,,low,
1116,Seed,"### Confirm this is a feature request for the Python library and not the underlying OpenAI API.

- [x] This is a feature request for the Python library

### Describe the feature or improvement you're requesting

Hi,

I would like to inquire about the behavior of the seed value in making chat interactions deterministic. Currently, it seems that the determinism is only ""more or less"" reliable, often deviating from the expected path after the second response-answer, regardless of the model used. Could this be addressed or clarified in future updates?

### Additional context

_No response_",closed,2024-02-01T13:37:50Z,2024-02-02T04:40:13Z,[],https://github.com/openai/openai-python/issues/1116,15.04,True,input_value_violation,medium,"The inquiry about the seed value indicates that the input for determinism may not be adhering to expected behavior, leading to unreliable outputs.",high,
1115,TypeError: 'LegacyAPIResponse' object is not iterable,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

Not able to get a streaming response because 'LegacyAPIResponse' object is not iterable.

### To Reproduce

The following code:

```python
import os
import openai
from openai import AzureOpenAI

os.environ['AZURE_OPENAI_API_KEY'] = ""xxx""
client = AzureOpenAI(
    api_version=""2023-07-01-preview"",
    azure_endpoint=""https://xxx.openai.azure.com/"",
)

stream = client.chat.completions.with_raw_response.create(
    messages=[{
        ""role"": ""user"",
        ""content"": ""sing me a song"",
    }],
    model=""gpt-35-turbo"",
    max_tokens=30,
    temperature=0.7,
    stream=True
)
print(stream)
for chunk in stream:
    print(chunk.choices[0].delta.content or """", end="""")
```

Results in:
```bash
<APIResponse [200 OK] type=<class 'openai.types.chat.chat_completion.ChatCompletion'>>
Traceback (most recent call last):
  File ""test.py"", line 27, in <module>
    for chunk in stream:
TypeError: 'LegacyAPIResponse' object is not iterable
```

### Code snippets

_No response_

### OS

WSL2 Ubuntu

### Python version

Python v3.9.7

### Library version

openai-python v1.10.0",closed,2024-02-01T11:07:42Z,2024-02-01T14:20:51Z,['question'],https://github.com/openai/openai-python/issues/1115,3.22,False,,,,low,
779,ValueError for missing api_version when calling with_options() on an (Async)AzureOpenAI client instance,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

Calling `with_options()` on an AzureOpenAI or AsyncAzureOpenAI client instance throws a ValueError exception:
> ValueError: Must provide either the `api_version` argument or the `OPENAI_API_VERSION` environment variable


### To Reproduce

Initialize an AzureOpenAI client and call `with_options()` on it.

### Code snippets

```Python
from openai import AsyncAzureOpenAI

client = AsyncAzureOpenAI(
    azure_endpoint="""",
    api_key=""mykey"",
    api_version=""2023-07-01-preview"",
)

copied_client = client.with_options(timeout=30)
```


### OS

ubuntu 22.04

### Python version

v3.10.12

### Library version

opeanai v1.1.2",closed,2023-11-10T22:50:53Z,2023-11-28T15:48:35Z,['bug'],https://github.com/openai/openai-python/issues/779,424.96,True,missing_dependency_violation,high,"The call to with_options() requires either the api_version argument or the OPENAI_API_VERSION environment variable, which is missing.",high,
1070,APIStatusError: Error code: 307,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

APIStatusError                            Traceback (most recent call last)
[<ipython-input-40-ef6a5d33e7aa>](https://localhost:8080/#) in <cell line: 4>()
      2 
      3 
----> 4 result = client.chat.completions.create(
      5     model=""gpt-4-vision-preview"",
      6     messages=[

4 frames
[/usr/local/lib/python3.10/dist-packages/openai/_base_client.py](https://localhost:8080/#) in _request(self, cast_to, options, remaining_retries, stream, stream_cls)
    947 
    948             log.debug(""Re-raising status error"")
--> 949             raise self._make_status_error_from_response(err.response) from None
    950 
    951         return self._process_response(

APIStatusError: Error code: 307


My format refers to the demo and other people's submissions. Still reporting an error。
eg:

### To Reproduce

call the api

### Code snippets

```Python
from openai.types import Image, ImagesResponse


result = client.chat.completions.create(
    model=""gpt-4-vision-preview"",
    messages=[
        {
            ""role"": ""user"",
            ""content"":[
              ""please description img"",
              *map(lambda x: {""image"": x, ""resize"": 768}, base64Frames[0::20]),
            ]
        },
    ],
    max_tokens=200,
)

print(result.choices[0].message.content)
```


### OS

colab

### Python version

3

### Library version

1.7.2",closed,2024-01-13T12:38:33Z,2024-01-26T10:40:21Z,['bug'],https://github.com/openai/openai-python/issues/1070,310.03,False,,,,low,
1101,openai.error.APIConnectionError,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

Error communicating with OpenAI: HTTPSConnectionPool(host='api.openai.com', port=443): Max retries exceeded with url: /v1/completions (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x0000018360C08370>: Failed to establish a new connection: [WinError 10060] 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。'))


### To Reproduce

由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。'))

### Code snippets

```Python
import openai
openai.api_key = ""sk-…""

response = openai.Completion.create(
    model='text-davinci-003',
    prompt='主题: 早餐 风\n两句话的恐怖故事:',
    temperature=0.8,
    max_tokens=120,
    top_p=1.0,
    frequency_penalty=0.5,
    presence_penalty=0.0,
)

print(response.choices[0].text)
```


### OS

windows

### Python version

python 3.8

### Library version

openai v1.1",closed,2024-01-25T04:08:27Z,2024-01-25T04:23:53Z,['bug'],https://github.com/openai/openai-python/issues/1101,0.26,True,missing_dependency_violation,unknown,"The error indicates a failure to establish a connection to the OpenAI API, which may suggest missing dependencies or network configuration issues that are not addressed in the code snippet.",high,
734,JSON errors not decoded into APIError correctly,"The following code does not work as expected since the release of 1.0:

```python
import openai
client = openai.Client(api_key='...')
file = open('demo.wav', 'rb')
try:
    client.audio.transcriptions.create(model=""whisper-1"", file=file)
except openai.BadRequestError as e:
    print(e.code)
    print(e.message)
```

This prints
```
None
Error code: 400 - {'error': {'message': 'Audio file is too short. Minimum audio length is 0.1 seconds.', 'type': 'invalid_request_error', 'param': 'file', 'code': 'audio_too_short'}}
```
so it appears `e.code` is not getting set correctly to the expected value of `audio_too_short`. Similarly `param` and `type` are not getting decoded correctly from the error response.

Here is a 0s WAV file that can be used to reproduce the issue: [demo.wav.zip](https://github.com/openai/openai-python/files/13299045/demo.wav.zip)",closed,2023-11-08T16:27:07Z,2023-11-09T21:49:32Z,['bug'],https://github.com/openai/openai-python/issues/734,29.37,True,input_value_violation,medium,"The API is not handling the case of an audio file that is too short correctly, leading to improper error decoding.",high,
1097,Chat completion endpoint cannot remember its previous messages,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

![image](https://github.com/microsoft/semantic-kernel/assets/40053881/33ab5f58-d40e-4781-993f-7152cb29016d)
With this request, llm should answer my last question. But it says it doesn;t know. Is ChatPromptTemplate with chat completion endpoint supposed to remember the previous messages or I should manually added them in the latest user prompt?
Chat completion takes a list of messages
![image](https://github.com/microsoft/semantic-kernel/assets/40053881/88e62be8-d05c-4175-a303-05f3f483fc2e)
Response saying don't know previous message
![image](https://github.com/microsoft/semantic-kernel/assets/40053881/d270c98a-2026-42a7-a559-03ce2c0aafa3)


### To Reproduce

This is my code for setting up using semantic kerner which uses openai package in the end to send the request, basically it add the previous messages into the ChatPromptTemplate.
```
system_message = """"""
You are a technical supporter that help users' questions, you should give 1-2 sentences to explain the answer and code examples for the question based on only the following contexts.
You may find the contexts provide information for multiple potential answers, you can give up to 3 most relevant answers separated by numbers like 1. 2. 3.

Follow this pattern to answer the question:
Contexts:
- Document path_title1 : page_content1
&&&
- Document path_title2 : page_content2

Question: question

Answer: answer
""""""

prompt_template = """"""
Contexts:
{{$context}}

Question: {{$input}}
""""""

def preprocess_messages(messages : list):
    transformed_messages = []
    transformed_messages.append({""role"": ""system"", ""message"": system_message})
    for message in messages:
      role = ""assistant"" if message[""role""] == ""bot"" else message[""role""]
      transformed_message = {""role"": role, ""message"": message[""content""]}
      if (role != ""system""):
        transformed_messages.append(transformed_message)
    return transformed_messages

def create_chat_prompt_template_instance(kernel : Kernel, messages : list = []):
    req_settings = sk_oai.AzureChatRequestSettings(max_tokens=2000, temperature=0, extension_data={""chat_system_prompt"": system_message})
    req_settings.unpack_extension_data()
    config = PromptTemplateConfig(completion=req_settings)
    template = ChatPromptTemplate(prompt_template, kernel.prompt_template_engine, config)
    if (messages and len(messages) > 0):
      processed_messages = preprocess_messages(messages)
      for message in processed_messages:
        template.add_message(message[""role""], message[""message""])
    function_config = SemanticFunctionConfig(config, template)
    return kernel.register_semantic_function(""ChatBot"", ""rag_chat"", function_config)

  # simplified calling
  messages = [{""content"": ""Q1"", ""role"": ""user""}, {""content"": ""A1"", ""role"": ""bot""},{""content"": ""Q2"", ""role"": ""user""}]
  query=messages[-1]['content']
  context['context'] = combined_documents
  context['input'] = query

  previous_messages = messages[:-1]
  chat_function = create_chat_prompt_template_instance(kernel=kernel, messages=previous_messages)

  response = await kernel.run_async(chat_function, input_context=context)
```

### Code snippets

_No response_

### OS

Ubuntu

### Python version

python3.9

### Library version

openai1.0",closed,2024-01-23T12:39:10Z,2024-01-24T02:55:51Z,['bug'],https://github.com/openai/openai-python/issues/1097,14.28,True,missing_dependency_violation,high,"The chat completion endpoint fails to remember previous messages, indicating a missing dependency on maintaining context across calls.",high,
1081,"Is there a way to print or log to a file every word that is sent to the OpenAI API, in the same format that the API receives them?","### Confirm this is a feature request for the Python library and not the underlying OpenAI API.

- [X] This is a feature request for the Python library

### Describe the feature or improvement you're requesting

I would like to be able to view whatever gets sent to the OpenAI API, after it gets sent, as a .json.  I would like to see what the OpenAI server sees when my messages arrive to it.

### Additional context

This would help with building my own agents using existing framework.",closed,2024-01-17T01:47:45Z,2024-01-24T02:51:12Z,[],https://github.com/openai/openai-python/issues/1081,169.06,False,,,,,
1096,"when I use gpt-3.5-turbo-1106, an openai.APIStatusError occurred","### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

openai.APIStatusError: <html>
  <head>
    <title>Request Header Fields Too Large</title>
  </head>
  <body>
    <h1><p>Request Header Fields Too Large</p></h1>
    Error parsing headers: &#x27;limit request headers fields&#x27;
  </body>
</html>


### To Reproduce

无

### Code snippets

```Python
stream = OpenAI(api_key='my key').openai_engine.chat.completions.create(
                    model=""gpt-3.5-turbo"",
                    messages=messages,
                    stream=True,
                    temperature=0.1
                )
```


### OS

linux

### Python version

python3.9

### Library version

openai 1.9.0",closed,2024-01-23T04:27:49Z,2024-01-24T02:16:32Z,['bug'],https://github.com/openai/openai-python/issues/1096,21.81,True,input_value_violation,medium,"The error indicates that the request headers exceed the allowed size, suggesting that the input values (headers) are out of acceptable bounds.",high,
1095,"When using openai asynchronous request, an error occurs when defining the proxy with http_client","### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

When using the AsyncOpenAI object and specifying a proxy through http_client to initiate a request, an exception ""TypeError: object Response can't be used in 'await' expression"" will be thrown in openai/_base_client.py.

The following are exception details:

------

Traceback (most recent call last):
  File ""/Users/apple/opt/miniconda3/lib/python3.9/site-packages/openai/_base_client.py"", line 1441, in _request
    response = await self._client.send(
TypeError: object Response can't be used in 'await' expression

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/Users/apple/opt/miniconda3/lib/python3.9/site-packages/openai/_base_client.py"", line 1441, in _request
    response = await self._client.send(
TypeError: object Response can't be used in 'await' expression

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/Users/apple/opt/miniconda3/lib/python3.9/site-packages/openai/_base_client.py"", line 1441, in _request
    response = await self._client.send(
TypeError: object Response can't be used in 'await' expression

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File ""/Users/apple/project/myproject-aigc/test.py"", line 87, in <module>
    asyncio.run(main())
  File ""/Users/apple/opt/miniconda3/lib/python3.9/asyncio/runners.py"", line 44, in run
    return loop.run_until_complete(main)
  File ""/Users/apple/opt/miniconda3/lib/python3.9/asyncio/base_events.py"", line 647, in run_until_complete
    return future.result()
  File ""/Users/apple/project/space-aigc/test.py"", line 75, in main
    chat_completion = await client.chat.completions.create(
  File ""/Users/apple/opt/miniconda3/lib/python3.9/site-packages/openai/resources/chat/completions.py"", line 1300, in create
    return await self._post(
  File ""/Users/apple/opt/miniconda3/lib/python3.9/site-packages/openai/_base_client.py"", line 1705, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File ""/Users/apple/opt/miniconda3/lib/python3.9/site-packages/openai/_base_client.py"", line 1407, in request
    return await self._request(
  File ""/Users/apple/opt/miniconda3/lib/python3.9/site-packages/openai/_base_client.py"", line 1461, in _request
    return await self._retry_request(
  File ""/Users/apple/opt/miniconda3/lib/python3.9/site-packages/openai/_base_client.py"", line 1530, in _retry_request
    return await self._request(
  File ""/Users/apple/opt/miniconda3/lib/python3.9/site-packages/openai/_base_client.py"", line 1461, in _request
    return await self._retry_request(
  File ""/Users/apple/opt/miniconda3/lib/python3.9/site-packages/openai/_base_client.py"", line 1530, in _retry_request
    return await self._request(
  File ""/Users/apple/opt/miniconda3/lib/python3.9/site-packages/openai/_base_client.py"", line 1471, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.

------

### To Reproduce

1. Create an AsyncOpenAI object and define the http_client parameter as the proxy address through httpx.Client
2. Initiate any asynchronous request

### Code snippets

```Python
import asyncio
from openai import AsyncOpenAI
import httpx

client = AsyncOpenAI(
    # This is the default and can be omitted
    api_key=<api_key>,
    http_client=httpx.Client(proxies=<proxy_address>)
)

async def main() -> None:
    chat_completion = await client.chat.completions.create(
        messages=[
            {
                ""role"": ""user"",
                ""content"": ""Say this is a test"",
            }
        ],
        model=""gpt-3.5-turbo"",
    )
    print(chat_completion)

asyncio.run(main())
```


### OS

macOS

### Python version

Python v3.9.13

### Library version

openai v1.9.0",closed,2024-01-23T02:55:43Z,2024-01-23T09:51:28Z,['bug'],https://github.com/openai/openai-python/issues/1095,6.93,True,input_type_violation,High,"The http_client parameter is expected to be an instance of an asynchronous HTTP client, but a synchronous httpx.Client is being used instead, leading to a TypeError when attempting to await the response.",high,
758,cannot import name 'OpenAI' from 'openai' v1.2.0,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

ImportError: cannot import name 'OpenAI' from 'openai'

The not working code was taken from the official documentation.

### To Reproduce

from openai import OpenAI

### Code snippets

```Python
# There are many following errors
from pathlib import Path

from openai import OpenAI
# ImportError: cannot import name 'OpenAI' from 'openai'

client = OpenAI()
# NameError: name 'OpenAI' is not defined

speech_file_path = Path(__file__).parent / ""speech.mp3""
response = client.audio.speech.create(
# AttributeError: module 'client' has no attribute 'audio'
# response = client.Audio.speech.create(
# AttributeError: type object 'Audio' has no attribute 'speech'
  model=""tts-1"",
  voice=""alloy"",
  input=""Today is a wonderful day to build something people love!""
)

response.stream_to_file(speech_file_path)
```


### OS

Windows 10

### Python version

Python v3.10.7

### Library version

openai v1.2.0",closed,2023-11-09T17:58:28Z,2023-11-09T19:10:28Z,['question'],https://github.com/openai/openai-python/issues/758,1.2,True,input_type_violation,unknown,"The code attempts to import 'OpenAI' from the 'openai' library, which may not exist in the specified version, leading to ImportError and subsequent NameError and AttributeError.",high,
1062,Mixing multithreading with async methods hangs when trying to close internal streams,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

When creating multiple threads to make completion requests in parallel the script hangs. After adding some debugging logs I can see that it happens in `aiter_raw` when calling `.aclose()`.

The same code with just one thread works with no issues.

### To Reproduce

I've attached a script that reproduces the issue.

### Code snippets

```Python
import os
import logging
import threading
import asyncio
from openai import AsyncAzureOpenAI
from dotenv import load_dotenv

load_dotenv()


deployment_name = os.environ.get(""AZURE_OPENAI_DEPLOYMENT_NAME"")
client = AsyncAzureOpenAI(
    api_version=os.environ.get(""AZURE_OPENAI_API_VERSION""),
    api_key=os.environ.get(""AZURE_OPENAI_API_KEY""),
    azure_endpoint=os.environ.get(""AZURE_OPENAI_ENDPOINT""),
    azure_deployment=deployment_name,
)

texts = [""Hello"", ""Hola"", ""Bonjour"", ""Hallo"", ""Ciao"", ""Olá"", ""Namaste"", ""Salaam"", ""Zdras-tvuy-te"", ""Konnichiwa"", ""Nǐn hǎo"", ""Guten Tag"", ""Shikamoo"", ""Merhaba"", ""Sa""]

async def detect_language(text: str) -> str:
    completion = await client.chat.completions.create(
        model=""gpt-3.5-turbo-16k"",
        messages=[
            {
                ""role"": ""user"",
                ""content"": f""""""
## Given this input:
{text}

## Do the following:
## Using only the input provided by the user, you must identify the input language and provide only the two character language code and nothing else:
"""""",
            },
        ],
    )
    
    if completion.choices:
        return completion.choices[0].message.content if completion.choices[0].message.content else ""Unknown""

    return ""Unknown""

def thread_function(loop, text):
    logging.info(""Thread with text '%s': starting"", text)
    asyncio.set_event_loop(loop)
    result = loop.run_until_complete(detect_language(text))
    logging.info(""Thread with text '%s': finishing: Detected language: %s"", text, result)
    return result

def main():
    threads = []

    for index, text in enumerate(texts):
        loop = asyncio.new_event_loop()
        x = threading.Thread(target=thread_function, args=(loop, text,))
        threads.append(x)
        x.start()

    for index, thread in enumerate(threads):
        logging.info(""Before joining thread %d."", index)
        thread.join()
        logging.info(""Thread %d done"", index)

if __name__ == ""__main__"":
    format = ""%(asctime)s: %(message)s""
    logging.basicConfig(format=format, level=logging.INFO,
                        datefmt=""%H:%M:%S"")

    main()
```


### OS

Windows 11

### Python version

Python 3.9

### Library version

openai 1.6.1",closed,2024-01-10T18:55:36Z,2024-01-18T09:16:28Z,['bug'],https://github.com/openai/openai-python/issues/1062,182.35,False,,,,low,
910,Support for Azure OpenAI extentions,"### Confirm this is a feature request for the Python library and not the underlying OpenAI API.

- [X] This is a feature request for the Python library

### Describe the feature or improvement you're requesting

Azure OpenAI also has additional endpoints for extentions, especially for the ""on your data"" functionality, would be great if this package also supports that feature, it is described here: https://learn.microsoft.com/en-us/azure/ai-services/openai/reference#completions-extensions it mostly consists of a slightly different endpoint and additional fields in the request, and the output adds a enhancement field with citations, latest version: https://github.com/Azure/azure-rest-api-specs/blob/main/specification/cognitiveservices/data-plane/AzureOpenAI/inference/preview/2023-12-01-preview/inference.yaml

### Additional context

_No response_",closed,2023-11-30T20:28:10Z,2023-12-01T07:40:11Z,[],https://github.com/openai/openai-python/issues/910,11.2,False,,,,low,
957,"Feature: support for ""retry-after-ms"" HTTP header variant","### Confirm this is a feature request for the Python library and not the underlying OpenAI API.

- [X] This is a feature request for the Python library

### Describe the feature or improvement you're requesting

**Feature request**: add support for the millisecond-precision `retry-after-ms` variant of the standard `retry-after` response header, using its value as a higher-resolution first selection when present that falls back to the lower-resolution standard when not present.

openai-python's retry header handling is cleanly done in [_base_client.py](https://github.com/openai/openai-python/blob/e36956673d9049713c91bca6ce7aebe58638f483/src/openai/_base_client.py#L621) and parses the standard `retry-after` header, which provides second-resolution guidance on how long a client should wait before initiating a retry.

Some services, including Azure OpenAI and particularly in the context of provisioned customers, can provide a `retry-after-ms` header in addition to `retry-after`. This millisecond-resolution variant is primarily valuable when retry behavior is being used to efficiently control traffic of service-to-service calls within a topology that often has delays that can be well under a single whole second.

As a reference/comparison, Azure's SDKs use a precedence order of three retry headers, e.g. [as per here in the azure-sdk-for-js core logic](https://github.com/Azure/azure-sdk-for-js/blob/17de1a2b7f3ad61f34ff62876eced7d077c10d4b/sdk/core/core-rest-pipeline/src/retryStrategies/throttlingRetryStrategy.ts#L35):

- If the `retry-after-ms` header key is present, use its value as the number of milliseconds to delay
- Else, if the `x-ms-retry-after-ms` header key is present, instead use its value as the number of milliseconds to delay
- Else, if the `retry-after` header key is present, use its value as the number of whole seconds to delay
- Else, fall back to standard fallback heuristics to calculate a retry delay

`openai-python` already uses a float value from `retry-after` as the input into `time.sleep()`, so this superficially looks like a fairly straightforward addition:
```python
retry_after = float(retry_header)
``` 

Conceptually, this would just be a `float(retry_ms_header) / 1000` style of thing.

Thank you!

### Additional context

_No response_",closed,2023-12-08T19:29:40Z,2024-01-17T16:12:32Z,"['enhancement', 'Azure']",https://github.com/openai/openai-python/issues/957,956.71,False,,,,,
1074,Use pydantic models for requests instead of TypedDict,"### Confirm this is a feature request for the Python library and not the underlying OpenAI API.

- [X] This is a feature request for the Python library

### Describe the feature or improvement you're requesting

Hello,
I have been using library for some time now and lately started to update from 0.x to newest (1.7.x)
From the start we are using pydantic models for requests and responses handling, mostly due to being very type-safe but also because it works nicely with FastApi.

Let's take chat completion as example.
Right now you are creating different dictionary type for every role, then you have one common return type which is Union of these possible values. 
Couldn't it be replaced by one pydantic model?

I am wondering why was there a choice to use TypedDict instead of pydantic models for requests objects?
Searched the issues/PRs but couldn't find anything.

Thanks in advance for response!

### Additional context

_No response_",closed,2024-01-15T20:18:28Z,2024-01-16T17:16:51Z,[],https://github.com/openai/openai-python/issues/1074,20.97,False,,,,,
886,AuthenticationError with Incorrect API Key,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

When attempting to run the provided code, an openai.AuthenticationError is raised with the error message stating, ""Incorrect API key provided."" The error code is 401.

### To Reproduce

Traceback (most recent call last):
  File ""d:\Chatbot_AzureAI_CSV_Project\open.py"", line 23, in <module>
    asyncio.run(main())
  # ... (rest of the traceback)
openai.AuthenticationError: Error code: 401 - {'error': {'message': 'Incorrect API key provided: 9********************p. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}


### Code snippets

```Python
import asyncio
from openai import AsyncOpenAI

client = AsyncOpenAI(
    api_key=""***************************"",  # <-- Replace with the correct API key
)

async def main() -> None:
    chat_completion = await client.chat.completions.create(
        messages=[
            {
                ""role"": ""user"",
                ""content"": ""Say this is a test"",
            }
        ],
        model=""gpt-3.5-turbo"",
    )

asyncio.run(main())
```


### OS

Windows 

### Python version

3.11.0

### Library version

Latest",closed,2023-11-27T09:02:46Z,2023-11-27T09:58:59Z,['question'],https://github.com/openai/openai-python/issues/886,0.94,True,input_value_violation,unknown,"The provided API key is incorrect, leading to an AuthenticationError.",high,
906,RecursionError in openai\_utils\_proxy.py ,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

A RecursionError is encountered when debugging a Python script that includes import openai in PyCharm. 

The specific error occurs within the openai library's _utils\_proxy.py file, involving the __get_proxied__ and __getattr__ methods.

The error message is as follows:

```
 File ""<venv_path>\Lib\site-packages\openai\_utils\_proxy.py"", line 45, in __get_proxied__
    proxied = self.__proxied
  File ""<venv_path>\Lib\site-packages\openai\_utils\_proxy.py"", line 22, in __getattr__
    return getattr(self.__get_proxied__(), attr)
  ...
RecursionError: maximum recursion depth exceeded
```
This error is triggered upon initiating the PyCharm debugger to debug the script.

stack trace:
> ...
__getattr__, _proxy.py:22
__get_proxied__, _proxy.py:45
__getattr__, _proxy.py:22
__get_proxied__, _proxy.py:45
__getattr__, _proxy.py:22
__get_proxied__, _proxy.py:45
__class__, _proxy.py:39
__init__, _proxy.py:18
\<module\>, numpy_proxy.py:29
\<module\>, __init__.py:1
\<module\>, embeddings.py:14
\<module\>, __init__.py:20
\<module\>, _client.py:12
\<module\>, __init__.py:11
\<module\>, temp.py:1

![image](https://github.com/openai/openai-python/assets/11143269/2456d166-d94c-413c-ab13-51b7d23ebd32)


### To Reproduce

1. In PyCharm, create and open a Python script that imports the OpenAI library (import openai).
2. Configure PyCharm for debugging the script.
3. Start the debugger in PyCharm. The command executed by PyCharm will be similar to:
  `<python_executable> -X pycache_prefix=<cache_directory> <pydevd_path> --multiprocess --qt-support --client <client_ip> --port <port> --file test.py`
4. Observe the RecursionError during the debugging process when the OpenAI library is imported.

> PyCharm 2023.3 EAP (Professional Edition)

### Code snippets

```Python
import openai

print(""1"")
```


### OS

Windows11

### Python version

Python 3.12.0

### Library version

1.3.6",closed,2023-11-30T09:46:34Z,2024-01-16T16:59:37Z,[],https://github.com/openai/openai-python/issues/906,1135.22,True,input_value_violation,high,"The RecursionError indicates that the input values or method calls are leading to an infinite recursion, exceeding the maximum recursion depth.",high,
1052,Function calling using the assistants API is partially broken,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

when using the assistants api, with the function calling tool enabled, and the call request is received in the `step_details.tool_calls` list,  I get a dictionary instead of a `FunctionToolCall` object for function, this seems to be related to the fact that the dict/json's `""function""`  sub dictionary is missing the `output`  field, but I'm not sure.

### To Reproduce

I'm using `openai==1.6.1`
just run the minimalistic script I wrote to reproduce the probem.

you'll end up with the following exception:
```
Traceback (most recent call last):
  File ""/home/user/Documents/dev/perso/gpt-api-tests/run-test.py"", line 65, in <module>
    if (tool.type == ""function""):
        ^^^^^^^^^

```

### Code snippets

```Python
from openai import OpenAI
import os

client = OpenAI(api_key=os.environ[""OPENAI_API_KEY""])

assistant = client.beta.assistants.create(
    name=""Doc"",
    instructions=""You are Doc, a personal assistant. You can write code and execute it to to help the user. You can also help the user using your knowledge base, you can send email with the function tool"",
    tools=[
        {""type"": ""code_interpreter""},
        {""type"": ""retrieval""},
        {""type"": ""function"", ""function"": {
            ""name"": ""send_email"",
            ""description"": ""Send an email to the recipient"",
            ""parameters"": {
                ""type"": ""object"",
                ""properties"": {
                    ""recipient"": {
                        ""type"": ""string"",
                        ""description"": ""The recipient email address""
                    },
                    ""subject"": {
                        ""type"": ""string"",
                        ""description"": ""The email subject""
                    },
                    ""body"": {
                        ""type"": ""string"",
                        ""description"": ""Body of the email""
                    }
                }
            }
        }}
    ],
    model=""gpt-4-1106-preview"",
)

thread = client.beta.threads.create()

message = client.beta.threads.messages.create(
    thread_id=thread.id,
    role=""user"",
    content=""Hello, using python, generate the first 100 fibonacci numbers. when done, send the result by email to foo@bar.com"",
)

run = client.beta.threads.runs.create(
    thread_id=thread.id,
    assistant_id=assistant.id,
)

while (run := client.beta.threads.runs.retrieve(
    thread_id=thread.id,
    run_id=run.id,
)).status in ('queued', 'in_progress'):
    run_steps = client.beta.threads.runs.steps.list(
        thread_id=thread.id,
        run_id=run.id
    )

    for step in run_steps.data:
        if (step.type != 'tool_calls'):
            continue
        if (step.step_details.type != 'tool_calls'):
            continue
        for tool in step.step_details.tool_calls:
            if (tool.type == ""function""):
                print(""function call "")


```
```


### OS

Manjaro Linux

### Python version

3.11.6

### Library version

openai-1.6.1",closed,2024-01-07T14:39:40Z,2024-01-16T15:05:06Z,['bug'],https://github.com/openai/openai-python/issues/1052,216.42,True,missing_dependency_violation,high,"The function call is missing the 'output' field in the 'function' sub-dictionary, which is required for proper execution of the function tool.",high,
1063,azure_endpoint logic not appropriate for running Azure LLMs through a proxy,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

I access my Azure LLM through a gateway proxy service - e.g Microsoft APIM. Its quite opinionated on the format, and I've found that the format is:

`https://myserver/proxy/openai/v1/` `deployments/gpt-4/chat/completions`

The new (as in openai v1) standard way of specifying the azure_endpoint should be azure_endpoint=""https://myserver/proxy/openai/v1/""

However, the code in https://github.com/openai/openai-python/blob/main/src/openai/lib/azure.py#L192-L195 specifically adds an /openai on the end of my azure_endpoint - this leads to:

https://myserver/proxy/openai/v1/openai/deployments/gpt-4/chat/completions --> #404

At the moment I notice L183 that I can use base_url instead, and then azure_endpoint wont work, but worth noting that langchain spits warnings that azure_endpoint is the new way forward, and you shouldnt use base_url any more, and llama-index doesnt allow you to set that at all. 

All this means if you're accessing Azure LLMs through an API Gateway (as for example happens and is mandatory in heavily regulated environments), you have to do things which may be on their way out. 

If you're intending to keep base_url around, and it is not deprecated, I'll raise a bug in langchain telling them to get rid of the deprecation warning. 

But I think a better solution is to make the forced addition of the /openai configurable. 


### To Reproduce

Run this code with openai 1.7.0


import logging
import os
logging.basicConfig(level=logging.DEBUG, format=""%(asctime)s - %(levelname)s - %(message)s"")

from openai import AzureOpenAI

client = AzureOpenAI(
  azure_endpoint = ""https://myserver/proxy/openai/v1"",
  api_key = os.getenv(""OPENAI_API_KEY""),
  api_version = ""2023-05-15""
)

client.chat.completions.create(
  messages = [
    {
      ""role"": ""user"", ""content"": ""whats the capital of France""
    }
  ],
  model = ""gpt-4"",
  temperature = 0.5,
  stream = False
)



Running this will give you:

2024-01-11 17:44:52,143 - DEBUG - HTTP Request: POST https://myserver/proxy/openai/v1/openai/deployments/gpt-4/chat/completions?api-version=2023-05-15 ""403 Forbidden""

Note the additional /openai 

### Code snippets

_No response_

### OS

macOS

### Python version

3.11.5

### Library version

openai v1.7",closed,2024-01-11T17:48:23Z,2024-01-13T01:28:47Z,['bug'],https://github.com/openai/openai-python/issues/1063,31.67,True,missing_option_violation,medium,"The API does not allow for the configuration of the forced addition of '/openai' to the azure_endpoint, which leads to incorrect URL formation.",high,
1044,joint translation and transcription,"I would like to keep track of a English speech history no matter the spoken language.

Is it possible to achieve joint translation and transcription with the current whisper model API?

Thus far we have experimented with the following two approaches:
1. anylanguage-to-English translations API + transcriptions API
2. transcriptions API + GPT-4 turbo completions API for anylanguage-to-English

It would be nice to have a unified way to achieve that; am I missing something with the API as-is?",closed,2024-01-04T10:44:01Z,2024-01-10T04:57:25Z,[],https://github.com/openai/openai-python/issues/1044,138.22,True,missing_dependency_violation,medium,"The user is attempting to achieve joint translation and transcription without clear dependencies or a unified API method, which may lead to incomplete or incorrect outputs.",high,
1045,Limit not working when listing messages,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

I'm not able to list messages of a thread filtering by a certain limit.

### To Reproduce

See the code snippet below.

### Code snippets

```Python
# Assuming we have 20 messages in a thread with id ""XYZ"".


raw_messages = []
async for m in assistant.async_beta_client.threads.messages.list(""XYZ"", order=""asc"", limit=1):
    print(m)
    raw_messages.append(m) 
print(len(raw_messages))

# At the end of this code, the result is 20 and the loop runs 20 times. I was expecting a single result.
```


### OS

macOS

### Python version

Python v3.11

### Library version

openai v1.6.1 (but also previous versions)",closed,2024-01-04T16:21:52Z,2024-01-10T04:50:30Z,[],https://github.com/openai/openai-python/issues/1045,132.48,True,input_value_violation,performance_degradation,"The limit parameter is not functioning as expected, leading to the retrieval of all messages instead of the specified limit, which may degrade performance by processing unnecessary data.",high,
1051,Support on setting the API_BASE variable,"### Confirm this is a feature request for the Python library and not the underlying OpenAI API.

- [X] This is a feature request for the Python library

### Describe the feature or improvement you're requesting

Hi Openai developers! 

In the new version of openai-python package (>=1.0), I find it rather inconvenient to set the `OPENAI_API_BASE` variable. That is to say, it is impossible to set the variable with something either like:

```
client = OpenAI(
    api_base = """",
    api_key = """"
)
```

or export the variable in the terminal like:

export OPENAI_API_BASE=

But in order versions, one can conveniently set the variable through the Python script, using something like `openai.api_base=`



### Additional context

_No response_",closed,2024-01-07T06:45:49Z,2024-01-10T04:28:08Z,[],https://github.com/openai/openai-python/issues/1051,69.71,True,missing_option_violation,medium,"The API does not provide a way to set the OPENAI_API_BASE variable through the expected methods, leading to suboptimal usage.",high,
1038,system_fingerprint is None,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

Per the documentation [here](https://platform.openai.com/docs/guides/text-generation/reproducible-outputs), the OpenAI API should return a `system_fingerprint`. However, when calling any model that is not `gpt-3.5-turbo-1106` or `gpt-4-1106-preview`, the `system_fingerprint` is always `None`.

Is this working as designed? Is this parameter only supported in these two models? If so, can this requirement be documented? The link above doesn't mention this requirement and neither does the API documentation.

### To Reproduce

1. copy and paste notebook [here](https://cookbook.openai.com/examples/deterministic_outputs_with_the_seed_parameter)
2. change this line to an older model:
GPT_MODEL = ""gpt-3.5-turbo-1106""

For example:
GPT_MODEL = ""gpt-3.5-turbo-0613""

3. the `system_fingerprint` returned is `None`

### Code snippets

_No response_

### OS

Unix

### Python version

Python v3.10

### Library version

openai v1.6.1",closed,2024-01-02T21:02:56Z,2024-01-02T23:35:31Z,[],https://github.com/openai/openai-python/issues/1038,2.54,True,missing_dependency_violation,medium,"The API does not return a 'system_fingerprint' for older models, which is not documented as a requirement, leading to confusion and potential misuse.",high,
745,Add back support for OPENAI_API_BASE environment variable,"### Confirm this is a feature request for the Python library and not the underlying OpenAI API.

- [X] This is a feature request for the Python library

### Describe the feature or improvement you're requesting

For my environment I need to configure this library via environment variables. I use to be able to define the base URL via `OPENAI_API_BASE` but no longer can. I am also not able to define a base_url and a `AZURE_OPENAI_ENDPOINT` at the same time. 

I understand v1 now exposes the ability for set base_url with  `_httpx.URL`, but after reading the httpx documentation, it does not seem immediately clear how to do so via environment variables.

### Additional context

_No response_",closed,2023-11-08T21:58:37Z,2023-11-25T22:04:11Z,[],https://github.com/openai/openai-python/issues/745,408.09,True,missing_option_violation,medium,"The user is unable to define both base_url and AZURE_OPENAI_ENDPOINT simultaneously, which may lead to suboptimal configuration.",high,
1040,Async with tools should be improved,,closed,2024-01-03T15:29:07Z,2024-01-03T17:00:27Z,['bug'],https://github.com/openai/openai-python/issues/1040,1.52,True,missing_dependency_violation,medium,"The issue lacks details on specific dependencies or configurations needed for async operations with tools, which may lead to improper usage.",high,
286,"Why doesn't this package provide a way to calculate the number of tokens in the prompt, suffix and messages parameters?","### Describe the feature or improvement you're requesting

People often need to calculate the number of tokens in the `prompt` and `suffix` parameters of the `completions` endpoint or in the `messages` parameter of the new `chat` endpoint. We can use a package like `tokenizers` (from huggingface) or `tiktoken`, but it's really not clear which pre-trained model we should use to create the tokenizer or how exactly the number of tokens from these different parameters should be summed and compared against the model's context length. 

So, why doesn't this package simply provide the functionality of counting the number of tokens in the prompts, suffixes and messages or the functionality to check that the sum of these tokens doesn't exceed the model's context length?

### Additional context

I would be happy to work on this in my free time, if you tell me exactly how this is done under the hood for each model.",closed,2023-03-08T14:13:39Z,2023-12-31T00:03:14Z,['enhancement'],https://github.com/openai/openai-python/issues/286,7137.83,True,missing_option_violation,medium,"The API does not provide a way to calculate the number of tokens in the prompt, suffix, and messages parameters, which is an expected functionality for managing context length.",high,
351,async openai.audio.atranscribe doesn't want to take aiofiles (async file) object,"### Describe the bug

Hello. I tried to pass an aiofiles object directly to async atranscribe but got an error about expected file like object, not generator.
```
async def some_function():
	async with aiofiles.open(fwav,""rb"") as f:
		r=await openai.Audio.atranscribe(""whisper-1"", f, response_format=""verbose_json"")
```

Isn't it bad to pass file like objects to an async functions? regular file objects are blocking and synchronous and they will block the concurrent task.
If i'm not right, comment out and close the issue. Thank you.

### To Reproduce

1. pip install aiofiles.
2. enter the code that I wrote earlier, but as the first param to open, pass the filename
3. run it. and you will get a beautifull error and the windows xp crytical error sound will come from your computer. no jokin'.

### Code snippets

_No response_

### OS

windows 10 but is it really necessary?

### Python version

python 3.11

### Library version

the last. 27.2 i think.",closed,2023-03-28T16:13:17Z,2024-01-02T18:18:11Z,['bug'],https://github.com/openai/openai-python/issues/351,6722.08,True,input_type_violation,unknown,"The API method async openai.audio.atranscribe is receiving an aiofiles object instead of an expected file-like object, leading to a type mismatch.",high,
971,Async API server starts throwing errors and works fine after restart,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

All the requests to the open ai end point start throwing timeouts. This changed after doing two things
- Upgrading to the new `GPT_3_5_TURBO_1106` model as well as using the async client.
- I reverted the `GPT_3_5_TURBO_1106` change but the issue is still happening

I am using `openai = ""^1.3.9""`

### To Reproduce

Use a combination of above + Fastify
After some number of requests we start getting timeouts

### Code snippets

_No response_

### OS

macOs

### Python version

3.11.5

### Library version

1.3.9",closed,2023-12-14T08:26:07Z,2023-12-23T23:29:16Z,[],https://github.com/openai/openai-python/issues/971,231.05,True,hang,medium,"The async API server experiences timeouts after a certain number of requests, causing the system to become unresponsive.",high,
1036,Inconsistent performance of chat api,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

When call the `client.chat.completions.create(..., logprobs=True)` with `gpt-3.5-turbo-1106`
Sometime it will raise below error
```
Error code: 400 - {'error': {'message': ""This model does not support the 'logprobs' parameter."", 'type': 'invalid_request_error', 'param': 'logprobs', 'code': None}}
```

### To Reproduce

Run the code snippets in jupyter about 5~8 times, sometime it run correctly, somtime will raise an error.
https://platform.openai.com/docs/api-reference/chat/create?lang=python

### Code snippets

```Python
from openai import OpenAI
client = OpenAI()

completion = client.chat.completions.create(
  model=""gpt-3.5-turbo"",
  messages=[
    {""role"": ""user"", ""content"": ""Hello!""}
  ],
  logprobs=True,
  top_logprobs=2
)
```


### OS

Linux

### Python version

python3.9.13

### Library version

openai v1.6.1",closed,2024-01-02T10:33:52Z,2024-01-02T17:57:00Z,[],https://github.com/openai/openai-python/issues/1036,7.39,True,input_value_violation,medium,"The 'logprobs' parameter is not supported by the 'gpt-3.5-turbo' model, leading to inconsistent behavior and errors when it is included in the API call.",high,
517,Azure API does not support functions in chat?,"### Describe the bug

New `functions` parameter in ChatCompletion is not recognized when calling MS Azure endpoints. I got this error when I pass in my functions:

```
  File ""call_chatgpt.py"", line 52, in chatgpt
    chat_completion = openai.ChatCompletion.create(
  File ""/root/anaconda3/lib/python3.8/site-packages/openai/api_resources/chat_completion.py"", line 25, in create
    return super().create(*args, **kwargs)
  File ""/root/anaconda3/lib/python3.8/site-packages/openai/api_resources/abstract/engine_api_resource.py"", line 153, in create
    response, _, api_key = requestor.request(
  File ""/root/anaconda3/lib/python3.8/site-packages/openai/api_requestor.py"", line 298, in request
    resp, got_stream = self._interpret_response(result, stream)
  File ""/root/anaconda3/lib/python3.8/site-packages/openai/api_requestor.py"", line 700, in _interpret_response
    self._interpret_response_line(
  File ""/root/anaconda3/lib/python3.8/site-packages/openai/api_requestor.py"", line 763, in _interpret_response_line
    raise self.handle_error_response(
openai.error.InvalidRequestError: Unrecognized request argument supplied: functions
```

### To Reproduce

1. run the code snippet
2. get the error

### Code snippets

```Python
import openai

AZURE_KEY = ""<key here>""
AZURE_ENDPOINT = ""<endpoint here>""

messages = [{""role"": ""user"", ""content"": ""What's the weather like in Boston?""}]
functions = [
    {
        ""name"": ""get_current_weather"",
        ""description"": ""Get the current weather in a given location"",
        ""parameters"": {
            ""type"": ""object"",
            ""properties"": {
                ""location"": {
                    ""type"": ""string"",
                    ""description"": ""The city and state, e.g. San Francisco, CA"",
                },
                ""unit"": {""type"": ""string"", ""enum"": [""celsius"", ""fahrenheit""]},
            },
            ""required"": [""location""],
        },
    }
]
openai.api_type = ""azure""
openai.api_key = AZURE_KEY
openai.api_base = AZURE_ENDPOINT
openai.api_version = ""2023-05-15""

chat_completion = openai.ChatCompletion.create(
    engine=""gpt-35-turbo"",
    messages=messages,
    functions=functions
)
```


### OS

linux

### Python version

python 3.8

### Library version

v0.27.8",closed,2023-07-04T03:27:27Z,2023-08-08T09:14:16Z,['bug'],https://github.com/openai/openai-python/issues/517,845.78,True,missing_option_violation,medium,"The 'functions' parameter is not recognized by the Azure API, indicating that the client did not follow the correct usage options provided by the API's contract.",high,
1030,An error occured,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [x] This is an issue with the Python library

### Describe the bug

i got this error named ""No error handlers are registered, logging exception.
Traceback (most recent call last):
  File ""/data/user/0/ru.iiec.pydroid3/files/aarch64-linux-android/lib/python3.11/site-packages/telegram/ext/dispatcher.py"", line 555, in process_update
    handler.handle_update(update, self, check, context)
  File ""/data/user/0/ru.iiec.pydroid3/files/aarch64-linux-android/lib/python3.11/site-packages/telegram/ext/handler.py"", line 198, in handle_update
    return self.callback(update, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""<string>"", line 17, in handle_message
  File ""/data/user/0/ru.iiec.pydroid3/files/aarch64-linux-android/lib/python3.11/site-packages/openai/lib/_old_api.py"", line 39, in __call__
    raise APIRemovedInV1(symbol=self._symbol)
openai.lib._old_api.APIRemovedInV1:

You tried to access openai.Completion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.

You can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface.

Alternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`

A detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742"" and it said that the i tried to access openAI.Completion but its no longer supported in openai>=1.0.0

### To Reproduce

error ""No error handlers are registered, logging exception.
Traceback (most recent call last):
  File ""/data/user/0/ru.iiec.pydroid3/files/aarch64-linux-android/lib/python3.11/site-packages/telegram/ext/dispatcher.py"", line 555, in process_update
    handler.handle_update(update, self, check, context)
  File ""/data/user/0/ru.iiec.pydroid3/files/aarch64-linux-android/lib/python3.11/site-packages/telegram/ext/handler.py"", line 198, in handle_update
    return self.callback(update, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""<string>"", line 17, in handle_message
  File ""/data/user/0/ru.iiec.pydroid3/files/aarch64-linux-android/lib/python3.11/site-packages/openai/lib/_old_api.py"", line 39, in __call__
    raise APIRemovedInV1(symbol=self._symbol)
openai.lib._old_api.APIRemovedInV1:

You tried to access openai.Completion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.

You can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface.

Alternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`

A detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742""

### Code snippets

```Python
import telegram.ext
import openai

# Telegram bot token received from BotFather
telegram_token = 'YOUR_BOT_TOKEN'

# OpenAI GPT API key
openai.api_key = 'YOUR_API_TOKEN'

# Initialize the Telegram bot
bot = telegram.Bot(token=telegram_token)

# Function to handle incoming messages
def handle_message(update, context):
    message = update.message.text  # Get user's message
    # Process the message using GPT-3
    response = openai.Completion.create(
        engine=""text-davinci-003"",
        prompt=message,
        max_tokens=50  # Adjust the number of tokens for the length of the response
    )
    generated_text = response.choices[0].text.strip()  # Get GPT's response

    # Send the generated response back to the user
    update.message.reply_text(generated_text)

# Function to handle the /start command
def start(update, context):
    update.message.reply_text('Hello! I am your ChatGPT bot. Send me a message, and I will respond.')

# Function to handle the /help command
def help(update, context):
    update.message.reply_text('This bot uses AI to generate responses. Just start chatting!')

# Set command handlers
updater = telegram.ext.Updater(token=telegram_token)
updater.dispatcher.add_handler(telegram.ext.CommandHandler('start', start))
updater.dispatcher.add_handler(telegram.ext.CommandHandler('help', help))
updater.dispatcher.add_handler(telegram.ext.MessageHandler(telegram.ext.Filters.text & (~telegram.ext.Filters.command), handle_message))
updater.start_polling()
updater.idle()
```


### OS

Android

### Python version

PyDroid3

### Library version

idk",closed,2023-12-28T16:04:48Z,2023-12-29T21:29:20Z,['question'],https://github.com/openai/openai-python/issues/1030,29.41,True,missing_dependency_violation,high,The code attempts to use an outdated API method 'openai.Completion.create' that is no longer supported in the current version of the OpenAI library.,high,
321,No rate limit error when using asynchronous calls for the chatgpt api,"### Describe the bug

When using the ChatGPT api with asynchronous calls, I do not receive rate error limits, instead the code seems stuck.

### To Reproduce

Here are 2 example codes that use asynchronous calls.

 _Note that I learned about asynchronous calls for this purpose and so have little experience._

# 1 Without the Python wrapper

```
import asyncio
from aiohttp import ClientSession

openai_api_key = ""key_here""

headers = {
    ""Content-Type"": ""application/json"",
    ""Authorization"": ""Bearer "" + openai_api_key,
}
url = ""https://api.openai.com/v1/chat/completions""

async def ChatBatch(prompts,temperature=0,model= ""gpt-3.5-turbo"") :

    async def Chat_Result(session, prompt, url=url, headers=headers):
        data = {""model"": model,
            ""messages"": [{""role"": ""user"", ""content"": prompt}],
            ""temperature"": temperature
        }
        async with session.post(url, headers=headers, json=data) as result:
            return await result.json()

    async def get_response():
        results = []
        async with ClientSession() as session:
            for prompt in prompts:
                result = Chat_Result(session, prompt)
                results.append(result)
            return await asyncio.gather(*results)


    # return await get_response()
    return [response['choices'][0]['message']['content'] for response in await get_response()]

```
**test :**

`await ChatBatch([""Hello how are you ?""]*30)`

# 2 With the Python Wrapper 


```
import openai
import asyncio
from aiohttp import ClientSession

openai.api_key = ""key_here""


openai.aiosession.set(ClientSession())

async def create_completion(prompt=""How are you ?""):
    return await openai.ChatCompletion.acreate(messages=[{""role"": ""user"", ""content"": prompt}], model=""gpt-3.5-turbo"")

async def main():

    return await asyncio.gather(*[create_completion() for _ in range(14)])

answers=await main()

await openai.aiosession.get().close()

answers
```

### Code snippets

_No response_

### OS

Linux

### Python version

Python 3.9.12

### Library version

openai v0.27",closed,2023-03-19T22:29:41Z,2023-12-31T00:08:03Z,['bug'],https://github.com/openai/openai-python/issues/321,6865.64,True,method_order_violation,medium,"The asynchronous calls to the ChatGPT API may not be respecting the required rate limits, leading to the system becoming unresponsive.",high,
318,Must provide an 'engine' when using ChatCompletion api,"### Describe the bug

I followed the instructions from https://github.com/openai/openai-cookbook/blob/main/examples/How_to_format_inputs_to_ChatGPT_models.ipynb
but got an error:
openai.error.InvalidRequestError: Must provide an 'engine' or 'deployment_id' parameter to create a <class 'openai.api_resources.chat_completion.ChatCompletion'>

### To Reproduce

using a Azure openai endpoint
execute the following code snippets with your own api_key
```
import openai
openai.api_type = ""azure""
# Example OpenAI Python library request
MODEL = ""gpt-3.5-turbo""
response = openai.ChatCompletion.create(
    model=MODEL,
    messages=[
        {""role"": ""system"", ""content"": ""You are a helpful assistant.""},
        {""role"": ""user"", ""content"": ""Knock knock.""},
        {""role"": ""assistant"", ""content"": ""Who's there?""},
        {""role"": ""user"", ""content"": ""Orange.""},
    ],
    temperature=0,
)

response
```

### Code snippets

```Python
import openai
openai.api_type = ""azure""
# Example OpenAI Python library request
MODEL = ""gpt-3.5-turbo""
response = openai.ChatCompletion.create(
    model=MODEL,
    messages=[
        {""role"": ""system"", ""content"": ""You are a helpful assistant.""},
        {""role"": ""user"", ""content"": ""Knock knock.""},
        {""role"": ""assistant"", ""content"": ""Who's there?""},
        {""role"": ""user"", ""content"": ""Orange.""},
    ],
    temperature=0,
)

response
```


### OS

macOS

### Python version

Python 3.9.13

### Library version

openai-python-0.27.2",closed,2023-03-19T02:44:16Z,2023-12-31T00:07:42Z,"['bug', 'Azure']",https://github.com/openai/openai-python/issues/318,6885.39,True,missing_dependency_violation,high,"The API call to ChatCompletion is missing the required 'engine' or 'deployment_id' parameter, which is necessary for the request to be processed.",high,
307,HTTP 500 Internal Server Error & HTTP 429 Too Many Requests,"### Describe the bug

Today is March 14, 2023, I read logs and find some ""HTTP 500 Internal Server Error"" errors, here are two request id: 

65f570730b40007dde214d6c94758fce
e5ad2d7adee633e0c72ba78c54f63e44

Please help analysis what happened.

At the same time, I meet the error ""HTTP 429 Too Many Requests"", honestly say my request is not up the limitation according to Api  document, but who knows what happened.

Thank you.

### To Reproduce

I submit request id above

### Code snippets

_No response_

### OS

Ubuntu

### Python version

Python 3.7.1

### Library version

latest",closed,2023-03-14T10:25:50Z,2023-12-31T00:06:14Z,['bug'],https://github.com/openai/openai-python/issues/307,6997.67,True,input_value_violation,performance_degradation,"The user is encountering HTTP 429 Too Many Requests, indicating that the request rate exceeds the allowed limit, which may not align with the user's understanding of the API's rate limits.",medium,
300,Prompt only works for the first audio,"### Describe the bug


Somehow, the prompt only works for my first prompt in python.

`for audio_file in sound_track:
  print('prompt_for_whisperer: ' + prompt_for_whisperer)
  transcript = openai.Audio.transcribe(
      ""whisper-1"", 
      audio_file, 
      response_format = 'srt',
      prompt = prompt_for_whisperer)

  transcripts_ary.append(transcript)`

### To Reproduce

I include the whole script below to reproduce it.
I told whisperer to limit words within 20, it works for the first audio only.

### Code snippets

```Python
#@title 安裝相關套件 (yt-dlp, openAI API, Pydub)
! pip install --upgrade pip
! pip install yt-dlp
! pip install openai
! pip install pydub
#@title 下載 youtube 影片 
import yt_dlp

 #@markdown ### Youtube 連結:
url = 'https://youtu.be/VZD5iLl0E_E' #@param {type:""string""} 

# 抓取影片標題
with yt_dlp.YoutubeDL() as ydl:
  info_dict = ydl.extract_info(url, download=False)
  video_title = info_dict.get('title', None)

filename = video_title

prompt_for_whisperer = ""This is about %s."" \
""Consider its title in your response: %s."" \
""Some Keywords you should expect: %s."" \
""Additional info: %s."" \
""Additional info: %s."" \
% \
('AWS Elastic Load Balancer' \
, filename \
, 'AZ, ASG, EC2, ELB, Target Group.' \
# the below one does not work for all
, 'Add one space before and after every English words, such as AZ, ALB, Target Group'\
, 'for every transcrtipt, limit it within 20 words, but do not cut sentence in the middel.'
)

# 設定選項
ydl_opts = {
    'format': 'bestaudio/best',
    'outtmpl': filename , 
    'postprocessors': [{
        'key': 'FFmpegExtractAudio',
        'preferredcodec': 'mp3',
        'preferredquality': '192',
    }],
}


# 建立 yt_dlp 下載器物件
with yt_dlp.YoutubeDL(ydl_opts) as ydl:
    ydl.download([url])
#@title 分割 youtube 影片 
from pydub import AudioSegment

#@markdown ### 分割檔案的長度（單位：毫秒）:
segment_length_ms = 60000 #@param {type:""integer""} 
segment_length_s = segment_length_ms/1000

# 載入 MP3 音檔
sound = AudioSegment.from_file(f'{filename}.mp3', format='mp3')

sound_track = []
# 將音檔分割成多個檔案
for i, chunk in enumerate(sound[::segment_length_ms]):
    # 設定分割檔案的檔名
    chunk.export(f'output_{i}.mp3', format='mp3')
    audio_file = open(f'output_{i}.mp3', ""rb"")
    sound_track.append(audio_file)
    

import openai
#@markdown ### 填入 OpenAI API Secret Key:
openai.api_key = '' #@param {type:""string""}
openai.api_key = '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'

transcripts_ary = []
for audio_file in sound_track:
  print('prompt_for_whisperer: ' + prompt_for_whisperer)
  transcript = openai.Audio.transcribe(
      ""whisper-1"", 
      audio_file, 
      response_format = 'srt',
      prompt = prompt_for_whisperer)

  transcripts_ary.append(transcript)
# debug
for transcript in transcripts_ary:
  print('transcript: ' , transcript)
! pip install pysrt
import pysrt

# 轉成 subtitle 物件 
subtitles = []
for transcript in transcripts_ary:
  subtitle = pysrt.from_string(transcript)
  subtitles.append(subtitle)

# 處理最後時間超過問題 
for subtitle in subtitles:
  max_time = pysrt.SubRipTime(seconds = segment_length_s)
  for sub in subtitle:
    sub.start = sub.start if sub.start < max_time else max_time
    sub.end = sub.end if sub.end < max_time else max_time

# 處理字幕時間銜接問題
shift_time_s = 0
for subtitle in subtitles:
  shift_time = pysrt.SubRipTime(seconds = shift_time_s)
  print(""shift_time: "", shift_time)
  for sub in subtitle:
    sub.start = sub.start + shift_time
    sub.end = sub.end + shift_time    
  shift_time_s = shift_time_s + segment_length_s

# 全部合體
subtitle_merged = pysrt.from_string('')
for subtitle in subtitles:
  subtitle_merged.extend(subtitle)

# 存成檔案
subtitle_merged.save(filename + '.srt')
```


### OS

maxOS

### Python version

Python v3.7

### Library version

openai-python v0.26.4",closed,2023-03-12T21:20:05Z,2023-12-31T00:05:57Z,['bug'],https://github.com/openai/openai-python/issues/300,7034.76,True,missing_dependency_violation,medium,"The prompt only works for the first audio file, indicating a missing dependency in context management or prompt handling for subsequent audio files.",high,
293,Client Disconnected during Fine Tune,"### Describe the bug

When running `openai api fine_tunes.create ...` the streaming cli response is continually interrupted. A bit of investigation revealed that the underlying exception is `Invalid chunk encoding ""Connection broken: InvalidChunkLength(got length b'', 0 bytes read)""`.

As we iterate over the events on the stream, there is a ProtocolError because a response is coming back with no bytes in it.

The full call stack:
```Traceback (most recent call last):
  File ""\lib\site-packages\openai\cli.py"", line 537, in _stream_events
    for event in events:
  File ""\lib\site-packages\openai\api_resources\fine_tune.py"", line 158, in <genexpr>
    return (
  File ""\lib\site-packages\openai\api_requestor.py"", line 611, in <genexpr>
    return (
  File ""\lib\site-packages\openai\api_requestor.py"", line 107, in parse_stream
    for line in rbody:
  File ""\lib\site-packages\requests\models.py"", line 865, in iter_lines
    for chunk in self.iter_content(
  File ""\lib\site-packages\requests\models.py"", line 818, in generate
    raise ChunkedEncodingError(e)
requests.exceptions.ChunkedEncodingError: (""Connection broken: InvalidChunkLength(got length b'', 0 bytes read)"", InvalidChunkLength(got length b'', 0 bytes read))
```

### To Reproduce

1. Update the cli.py to not discard all exceptions while streaming events
2. Start a fine tune and wait for error

### Code snippets

_No response_

### OS

Windows

### Python version

3.10

### Library version

0.27.0",closed,2023-03-09T17:09:03Z,2023-12-31T00:04:51Z,['bug'],https://github.com/openai/openai-python/issues/293,7110.93,True,input_value_violation,performance_degradation,"The streaming response is interrupted due to invalid chunk encoding, indicating potential issues with input values or stream handling.",high,
279,Moderation bypasses proxy settings,"### Describe the bug

In an environment without direct access to the Internet, If you pass proxy settings to the `openai `object and try to perform input validation, the `openai.Moderation.create()` call hangs on or times out.

Those same proxies do work for `openai.ChatCompletion.create()` and `openai.ChatCompletion.create()` calls.

### To Reproduce

1. Prepare a no-internet environment that includes http&https proxy
2. Import openai library, set api key and proxy
3. Try to moderate a message
4. `openai.Moderation.create()` hangs on forever or until timeout

### Code snippets

```Python
openai.api_key = ""OPENAI_API_KEY""
openai.proxy = {""http"": ""<PROXY>"", ""https"": ""<PROXY>""}

message = ""I will kill you""
moderation_resp = openai.Moderation.create(input=message) # Times out
```


### OS

Linux

### Python version

Python v3.9

### Library version

v0.27.0",closed,2023-03-06T15:32:57Z,2023-12-31T00:02:18Z,['bug'],https://github.com/openai/openai-python/issues/279,7184.49,True,method_order_violation,hang,The `openai.Moderation.create()` call hangs due to improper handling of proxy settings in a no-internet environment.,high,
235,JSON parsing issues with embedding create with large batch size.,"### Describe the bug

When I call `Embedding.create` with a large number of text chunks (batch) I get JSON decoding errors in the reponse. If I keep the batch size small (say 50) it works fine, but for large batch sizes (say 12k) it shows this problem. Looks very similar to the problems seen in #184. I initially saw this when using langchain, but reproduced the openai alone. Oddly, I sometimes get an `InvalidRequestError` instead.

### To Reproduce

Run the following code:

```python
texts = [""AI""*100 for i in range(4000)]
e = openai.Embedding.create(input=texts, model=""text-embedding-ada-002"")
```

### Code snippets

_No response_

### OS

macOS

### Python version

Python 3.11

### Library version

0.26.5",closed,2023-03-01T02:31:16Z,2023-12-31T00:01:56Z,['bug'],https://github.com/openai/openai-python/issues/235,7317.51,True,input_value_violation,performance_degradation,"The large batch size of 12k exceeds acceptable limits for JSON parsing, leading to decoding errors.",high,
218,Getting requests.exceptions.ChunkedEncodingError when using TaskCompletion in stream mode or ,"### Describe the bug

Hi, 

I'm using the openaI python module and I'm experiencing some issues lately. I'm making a streaming completion task. Randomly I'm getting the following exception ""**requests.exceptions.ChunkedEncodingError : (""Connection broken: InvalidChunkLength(got length b'', 0 bytes read)**"". 

`def send_completion_task_stream(self):`
 `       prompt = """"""my prompt""""""`
 `           response = self.completion_task.create(prompt=prompt, model=self.modal_name, stop=self.stop,`
 `                                                  temperature=self.temperature,`
  `                                                 top_p=self.top_p, frequency_penalty=self.frequency_penalty,`
 `                                                  presence_penalty=self.presence_penalty,`
 `                                                  max_tokens=3000,`
 `                                                  stream=True`
                                                   )`
 `       return response `
 do you know what can cause this error ? 
 
 Thank you
 
 Best regards
        
        

### To Reproduce

1- Send a streaming completion task and put max-token to 3000
2 - The errors appears (but randomly)

### Code snippets

_No response_

### OS

macOs

### Python version

Python 3.10

### Library version

openai-python-0.26.1",closed,2023-02-09T17:29:50Z,2023-12-31T00:01:06Z,['bug'],https://github.com/openai/openai-python/issues/218,7782.52,True,input_value_violation,medium,"The use of max_tokens set to 3000 may exceed the acceptable limits for the API, leading to connection issues.",high,
150,Prompt token count discrepancy,"Hello,

I noticed a discrepancy in prompt (not completion) token counting. Here's a minimum working example:

```
import os

import openai
from transformers import GPT2TokenizerFast

os.environ[""OPENAI_API_KEY""] = ""for you""

tokenizer = GPT2TokenizerFast.from_pretrained(""gpt2"")

model_name = ""text-davinci-002""
prompt = 'Some choices are given below. It is provided in a numbered list (1 to 1),where each item in the list corresponds to a summary.\n---------------------\n(1) A serial killer is typically a person who kills three or more people, with the murders taking place over more than a month and including a significant period of time between them. The Federal Bureau of Investigation (FBI) defines serial murder as ""a series of two or more murders, committed as separate events, usually, but not always, by one offender acting alone"".   == Identified serial killers ==   == Unidentified serial killers == This is a list of unidentified serial killers who committed crimes within the United States.   == See also == List of rampage killers in the United States List of mass shootings in the United StatesInternational:  List of serial killers by country List of serial killers by number of victims   == References ==   == Bibliography ==\n\n\n---------------------\nUsing only the choices above and not prior knowledge, return the choice that is most relevant to the question: \'How many serial killers in the US are there?\'\nProvide choice in the following format: \'ANSWER: <number>\' and explain why this summary was selected in relation to the question.\n'
params = {'temperature': 0.0, 'max_tokens': 256, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0, 'n': 1, 'best_of': 1}

completion = openai.Completion.create(model=model_name, prompt=prompt, **params)
print(completion)

prompt_token_count = len(tokenizer(prompt)[""input_ids""])
print(f""prompt token count is {prompt_token_count}, which is 5 more tokens than the output above"")

```

This will print: 
```
{
  ""choices"": [
    {
      ""finish_reason"": ""stop"",
      ""index"": 0,
      ""logprobs"": null,
      ""text"": ...
    }
  ],
  ...
  ""model"": ""text-davinci-002"",
  ""object"": ""text_completion"",
  ""usage"": {
    ""completion_tokens"": 51,
    ""prompt_tokens"": 252,              <---------------- what openai counts
    ""total_tokens"": 303
  }
}
prompt token count is 257, which is 5 more tokens than the output above
```

That is, openai counts 252 tokens in the prompt, but I'm counting 257. From https://beta.openai.com/tokenizer, I am tokenizing using `transformers.GPT2TokenizerFast`. I have also pasted prompt text above (after running it through python `print()`) in the url above, and I get 257 as well:
<img width=""708"" alt=""image"" src=""https://user-images.githubusercontent.com/10181537/206954395-ffdbeb87-b5c1-45f3-9986-45c76364332f.png"">

Below is my requirements.txt:
```
openai==0.25.0
tokenizers==0.13.2
```

Is there something that I am missing here? Thanks a lot! ",closed,2022-12-12T03:28:29Z,2023-12-31T00:00:12Z,[],https://github.com/openai/openai-python/issues/150,9212.53,True,input_value_violation,medium,"Discrepancy in token counting between the OpenAI API and the user's tokenizer, leading to potential confusion in expected input sizes.",high,
133,Inconsistent `prompt_tokens` definition between `text-davinci-002` and `code-davinci-002`,"Ubuntu 18.04.6
openai-python 0.23.1
python 3.8.13

I'm facing unexpected results when using `prompt_tokens` returned from the completion API in the davinci `codex` model.

Consider the following function to reconstruct the prompt using the API's response:
```py

def test_prompt_idx(prompt: str, engine=str):
    response: OpenAIObject = openai.Completion.create(
        prompt=prompt,
        stop=[""\n""],
        temperature=0.0,
        engine=engine,
        max_tokens=32,
        logprobs=5,
        echo=True,
    )
    n_prompt_tokens: int = response[""usage""][""prompt_tokens""]
    prompt_tokens = [
        {
            ""val"": response[""choices""][0][""logprobs""][""tokens""][i],
            ""options"": response[""choices""][0][""logprobs""][""top_logprobs""][i],
            ""logprob"": response[""choices""][0][""logprobs""][""token_logprobs""][i]
            if i != 0
            else 0.0,  # first token has logprob None
        }
        for i in range(n_prompt_tokens)
    ]
    reconstructed_prompt = """".join(token[""val""] for token in prompt_tokens)
    assert reconstructed_prompt == prompt
```

When I use `text-danvinci-002`, the snippet runs fine
```py
test_prompt_idx(
        prompt=""""""import numpy as np
a = np.array(object=[0,1,2])
b = np.array("""""",
        engine=""text-davinci-002""
)
test_prompt_idx(
        prompt=""""""import numpy as np
a = np.array(object=[0,1,2])
b = np.array(object="""""",
        engine=""text-davinci-002""
)
```

However, when I use `code-davinci-002`, this snippet fails
```py
test_prompt_idx(
        prompt=""""""import numpy as np
a = np.array(object=[0,1,2])
b = np.array("""""",
        engine=""code-davinci-002""
)
test_prompt_idx(
        prompt=""""""import numpy as np
a = np.array(object=[0,1,2])
b = np.array(object="""""",
        engine=""code-davinci-002""
)
```

Comparing `reconstructed_prompt` and `prompt` shows that for `code-davinci-002`, it seems to be that `n_prompt_tokens`'s value is one less than expected.
I've observed that this could lead to the codex model changing the last token of the prompt.
For instance, if I update my `test_prompt_idx()` function to use `n_prompt_tokens+1` instead, then the following snippet passes
```py
test_prompt_idx(
        prompt=""""""import numpy as np
a = np.array(object=[0,1,2])
b = np.array("""""",
        engine=""code-davinci-002""
)
```
because the first completion token returned by codex (i.e.: `object`, with tokenid  of 15252 according to [the tokenizer](https://beta.openai.com/tokenizer?view=bpe)).
However, the following fails 
```py
test_prompt_idx(
        prompt=""""""import numpy as np
a = np.array(object=[0,1,2])
b = np.array(object="""""",
        engine=""code-davinci-002""
)
```
because the first completion token returned is `=[` with token id 41888, instead of the last token of the given prompt, which is `=` with token id 28.",closed,2022-10-30T01:33:49Z,2023-12-30T23:59:45Z,[],https://github.com/openai/openai-python/issues/133,10246.43,True,input_value_violation,incorrect_functionality,"The `n_prompt_tokens` value returned by `code-davinci-002` is inconsistent, leading to incorrect reconstruction of the prompt.",high,
128,Inaccurate time estimation results for fine-tuning use-case,"[This PR](https://github.com/openai/openai-python/pull/29) introduces time estimation functionality for fine-tuning tasks. We observed in our experiments that estimated values are pretty inaccurate and have a few questions and suggestions:

**Question 1: Is there any public information about where constants like `0.0515` (line 601) come from?**
My data frame which was used for fine-tuning `curie` model for 2 epochs contains 8236 rows. Our aim was to train an open-ended generator, that's why `prompt` column is completely empty. However, running `memory_usage` on this df gives the same values for `prompt` and `completion` columns. Please note that `completion` column contains pretty long text values.

<img width=""261"" alt=""Screen Shot 2022-10-06 at 17 12 48"" src=""https://user-images.githubusercontent.com/7279205/194335959-dbeb5876-912d-4146-902f-efe769e68225.png"">

If I use `sys` module to get the size of df on system, I get a very different result.

<img width=""241"" alt=""Screen Shot 2022-10-06 at 17 14 38"" src=""https://user-images.githubusercontent.com/7279205/194336337-925ee779-b2dd-457d-8ffe-f9d40b6db9f3.png"">

If I add `deep=True` parameter to `pandas`' `memory_usage` call, the returned value becomes very similar to `sys` output.

<img width=""434"" alt=""Screen Shot 2022-10-06 at 17 16 07"" src=""https://user-images.githubusercontent.com/7279205/194336727-f38bc0ba-da08-49c1-bad3-7b9564973131.png"">

**Question 2: Based on the previous trials, is there any reason why the estimator doesn't use `deep=True` flag to get memory consumed in the system?**

**Question 3: Does this estimator have any `number of epochs` assumption?**
Time estimator returns 1.92 hours (approximately 115 minutes) for my dataset. When I started training on the same df for **2 epochs**, it took 17 minutes in total. ~9 minutes per epoch. It doesn't take this parameter into account because it's not available until the fine-tuning process call is made.

```
Once your model starts training, it'll approximately take 1.93 hours to train a `curie` model, and less for `ada` and `babbage`. Queue will approximately take half an hour per job ahead of you

....
[2022-10-06 15:56:26] Fine-tune enqueued. Queue number: 0
[2022-10-06 15:56:29] Fine-tune started
[2022-10-06 16:05:33] Completed epoch 1/2
[2022-10-06 16:13:42] Completed epoch 2/2
```

## Suggestions:
- [ ] More documentation about constant values like `0.0515`
- [ ] Adding `deep=True` flag to `memory_usage` call and updating constants accordingly
- [ ] Adding information about `epoch count` assumption to the log message, like `Once your model starts training, it'll approximately take 1.93 hours to train a `curie` model for x epochs based on historical statistics, and less ...`",closed,2022-10-06T14:22:52Z,2023-12-30T23:56:50Z,[],https://github.com/openai/openai-python/issues/128,10809.57,True,input_value_violation,performance_degradation,"The time estimation functionality produces inaccurate results, leading to suboptimal training expectations and potential misallocation of resources.",high,
111,Enable CI to check for formatting,We should enable consistent formatting and require PRs to have that formatting,closed,2022-07-18T22:26:10Z,2023-12-30T23:56:00Z,['enhancement'],https://github.com/openai/openai-python/issues/111,12721.5,True,missing_option_violation,medium,"The issue suggests a lack of enforced formatting options for PRs, which can lead to inconsistent code quality.",high,
93,Check usage of a given user,"I am testing out an application, and am using the `user` field to set which user has made a request.  From the docs, it says that OpenAI uses this to check for abuse.

However, is there a way to get more data from the API on a per-user level?  E.g. number of calls, etc.",closed,2022-05-18T17:07:58Z,2023-12-30T23:55:52Z,['enhancement'],https://github.com/openai/openai-python/issues/93,14190.8,False,,,,low,
84,[FEATURE REQUEST] Add stratification on train/validation split with fine_tune.prepare_data,"Hello,
first of all many thanks for this great library ! 🙏

When preparing data for multiclass classification for fine-tuning and accepting the split into train and validation data, I end up with a different number of classes in both datasets with respect to those I specified.
Error message:
```
[2022-03-25 10:12:57] Fine-tune failed. Errors:
The number of classes in file-LSGG6mb4lhNMqyAxN6dA63sc does not match the number of classes specified in the hyperparameters.
The number of classes in file-tRE2P9nw9pq2NtM4qpKgceI2 does not match the number of classes specified in the hyperparameters.
```
It seems to me a problem related to stratification while splitting. Do you think it'd be possible to include this [option](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedShuffleSplit.html) in the future ? I know it's not an easy task and when you have not many examples you have to manually play with `test_size` until you get the same number of classes in the splits but it could be automated by progressively increase the `test_size` until `train_dataset.nunique() == test_dataset.nunique()`",closed,2022-03-25T09:36:10Z,2023-12-30T23:54:59Z,['enhancement'],https://github.com/openai/openai-python/issues/84,15494.31,True,missing_dependency_violation,high,"The API method does not ensure that the number of classes in the train and validation datasets matches the specified hyperparameters, leading to incorrect functionality.",high,
43,[FEATURE REQUEST] Bad words list,"I don't have access to the GPT-3 API yet (A guy can dream, eh?), but I have been reading through the docs and it seems like the completion module would be perfect for my use case _except_ for the exclusion of a ""bad words list"" feature.

This feature would not allow certain words to be generated in the completion output. I am aware of the `logit_bias` argument, but this only stops individual tokens from being generated. 
My idea would take an arbitrary string (Or list of token IDs) as input, and then not allow the completion of this string given the words before it.

I have successfully asked for this feature from the Huggingface .generate API many moons ago. Please see my feature request for a fuller run-down of how it could be implemented (link: https://github.com/huggingface/transformers/issues/3061).

It would be a useful feature for customers because it could give peace of mind that the models that they are serving are not going to output any unsavoury language. I can see that an alternative to this feature would just be to train the model not to output generally bad language (E.g. overly aggressive or xenophobic language) through thoughtful use of training data, but since everyone's definition of bad language is different, it would be nice to customise the model accordingly.

Thanks!",closed,2021-11-05T03:25:59Z,2023-12-30T23:52:58Z,[],https://github.com/openai/openai-python/issues/43,18860.45,False,,,,,
1026,Auth flow of custom http client not invoked when making a request,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

When passing a custom http client to the OpenAI client, it does not properly use the custom client's auth flow. 

In the client's `_request` method, `self.custom_auth` is passed to `self._client.send` ([link to source code](https://github.com/openai/openai-python/blob/main/src/openai/_base_client.py#L877)).

The property `custom_auth` however is simply defined as `return None` ([link to source code](https://github.com/openai/openai-python/blob/main/src/openai/_base_client.py#L555C19-L555C19)).

Because we explicitly pass `None` to the `auth` argument of the `send` method of the `httpx` client, the client doesn't use the default value `USE_CLIENT_DEFAULT` for the `auth` argument ([link to source code](https://github.com/encode/httpx/blob/master/httpx/_client.py#L882)).

Is there a reason why we have this `custom_auth` property on the client? If we do not pass it to the `auth` argument of `self._client.send`, everything works as expected and the auth logic of the custom `httpx` client is properly invoked.

### To Reproduce

```python
openai_client = OpenAI(
    api_key=""<API_KEY>"",
    base_url=""<BASE_URL>"",
    http_client=""<HTTP_CLIENT>"",
)

completion = openai_client.chat.completions.create(
    messages=[
        {
            ""role"": ""user"",
            ""content"": ""Last name of the president of the United States"",
        }
    ],
    model=""gpt-3.5-turbo"",
)  # Does not invoke the authentication flow of the client passed using `http_client`.
```

### Code snippets

_No response_

### OS

MacOS 14.2

### Python version

3.11.6

### Library version

1.3.9",closed,2023-12-27T12:26:24Z,2023-12-29T21:31:34Z,['bug'],https://github.com/openai/openai-python/issues/1026,57.09,True,missing_dependency_violation,high,"The custom authentication flow is not invoked because the 'custom_auth' property returns None, leading to a failure in the expected authentication behavior.",high,
1033,module openai' has no attribute 'error',"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

I'm using the ""sheetcopilot"" and using the openai api. Whe running the python file, I got this problem: Cause: module 'openai' has no attribute 'error'.

My used model version is: Model: gpt-3.5-turbo
My installed openai version is: openai 1.6.1

Could you please help me solve this problem. thanks a lot!

### To Reproduce

Whe running the python file, I got this problem: Cause: module 'openai' has no attribute 'error'.

### Code snippets

_No response_

### OS

wins

### Python version

3.10.13

### Library version

1.6.1",closed,2023-12-29T04:16:36Z,2023-12-29T12:20:19Z,['question'],https://github.com/openai/openai-python/issues/1033,8.06,True,input_type_violation,unknown,"The error indicates that the 'openai' module is being accessed incorrectly, possibly due to an incorrect version or missing attributes.",medium,
1021,Addition of plotly as a dependency for installing OpenAI API,"### Confirm this is a feature request for the Python library and not the underlying OpenAI API.

- [X] This is a feature request for the Python library

### Describe the feature or improvement you're requesting

While working with OpenAI API's embeddings_utils module, I ran into the following error while importing the get_embeddings and get_embedding function from it.

<img width=""559"" alt=""image"" src=""https://github.com/openai/openai-python/assets/46163196/1dc6dc37-44b4-4f74-b78d-1ad982df81d3"">


Hence, I think plotly should be added as a dependency for installing OpenAI API.


### Additional context

_No response_",closed,2023-12-25T04:52:47Z,2023-12-26T18:57:47Z,[],https://github.com/openai/openai-python/issues/1021,38.08,False,,,,low,
715,convert_to_openai_object in new API,"This function was quite useful for unit testing - was this moved to another location / under a different name? I cannot find similar functionality in the new API.

Thanks in advance.",closed,2023-11-07T15:26:01Z,2023-11-07T16:43:36Z,[],https://github.com/openai/openai-python/issues/715,1.29,True,missing_dependency_violation,medium,"The user is unable to find the 'convert_to_openai_object' function, indicating a potential missing functionality that may be required for proper unit testing in the new API.",high,
1006,Missing default value to logprobs in openai.types.chat.chat_completion.Choice,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

#980 added token `logprobs` to chat completions of type `Optional[ChoiceLogprobs]` in [`openai.types.chat.chat_completion.Choice`](https://github.com/openai/openai-python/blob/3ad4e8bc9d89d7a81586bf598289ff62b0a339b9/src/openai/types/chat/chat_completion.py#L33) and [`openai.types.chat.chat_completion_chunk.Choice`](https://github.com/openai/openai-python/blob/3ad4e8bc9d89d7a81586bf598289ff62b0a339b9/src/openai/types/chat/chat_completion_chunk.py#L97). In the latter, the default value is set to `None`, while in the former it is not set. This causes backward compatibility problems with code written for versions prior to 1.5.0.



### To Reproduce

Execution of the following code fails:

```python 
from openai.types.chat.chat_completion import ChatCompletionMessage, Choice

msg = ChatCompletionMessage(role=""assistant"", content="""")

Choice(
    index=0,
    finish_reason=""stop"",
    message=msg,
)
```
The output
```
----> 1 Choice(
      2     index=0,
      3     finish_reason=""stop"",
      4     message=msg,
      5 )

File /.venv-3.10/lib/python3.10/site-packages/pydantic/main.py:164, in BaseModel.__init__(__pydantic_self__, **data)
    162 # `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks
    163 __tracebackhide__ = True
--> 164 __pydantic_self__.__pydantic_validator__.validate_python(data, self_instance=__pydantic_self__)

ValidationError: 1 validation error for Choice
logprobs
  Field required [type=missing, input_value={'index': 0, 'finish_reas...=None, tool_calls=None)}, input_type=dict]
    For further information visit https://errors.pydantic.dev/2.5/v/missing
```
Setting `logprobs` to `None` fixes the problem.
```python 
from openai.types.chat.chat_completion import ChatCompletionMessage, Choice

msg = ChatCompletionMessage(role=""assistant"", content="""")

Choice(
    index=0,
    finish_reason=""stop"",
    message=msg,
    logprobs=None # added line
)
```

### Code snippets

```Python
see above
```


### OS

Linux

### Python version

Python 3.10.13

### Library version

openai 1.6.0",closed,2023-12-22T08:36:16Z,2023-12-23T23:25:46Z,['enhancement'],https://github.com/openai/openai-python/issues/1006,38.83,True,missing_option_violation,medium,"The 'logprobs' field in the Choice class lacks a default value, causing backward compatibility issues.",high,
1004,openai.APITimeoutError: Request timed out.,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [ ] This is an issue with the Python library

### Describe the bug

Traceback (most recent call last):
  File ""C:/Users/14499/PycharmProjects/chatgpt.py"", line 10, in <module>
    stream=True,
  File ""C:\Users\14499\AppData\Roaming\Python\Python37\site-packages\openai\_utils\_utils.py"", line 270, in wrapper
    return func(*args, **kwargs)
  File ""C:\Users\14499\AppData\Roaming\Python\Python37\site-packages\openai\resources\chat\completions.py"", line 677, in create
    stream_cls=Stream[ChatCompletionChunk],
  File ""C:\Users\14499\AppData\Roaming\Python\Python37\site-packages\openai\_base_client.py"", line 1088, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File ""C:\Users\14499\AppData\Roaming\Python\Python37\site-packages\openai\_base_client.py"", line 858, in request
    remaining_retries=remaining_retries,
  File ""C:\Users\14499\AppData\Roaming\Python\Python37\site-packages\openai\_base_client.py"", line 890, in _request
    response_headers=None,
  File ""C:\Users\14499\AppData\Roaming\Python\Python37\site-packages\openai\_base_client.py"", line 963, in _retry_request
    stream_cls=stream_cls,
  File ""C:\Users\14499\AppData\Roaming\Python\Python37\site-packages\openai\_base_client.py"", line 890, in _request
    response_headers=None,
  File ""C:\Users\14499\AppData\Roaming\Python\Python37\site-packages\openai\_base_client.py"", line 963, in _retry_request
    stream_cls=stream_cls,
  File ""C:\Users\14499\AppData\Roaming\Python\Python37\site-packages\openai\_base_client.py"", line 893, in _request
    raise APITimeoutError(request=request) from err
openai.APITimeoutError: Request timed out.

Process finished with exit code 1


### To Reproduce

Traceback (most recent call last):
  File ""C:/Users/14499/PycharmProjects/chatgpt.py"", line 10, in <module>
    stream=True,
  File ""C:\Users\14499\AppData\Roaming\Python\Python37\site-packages\openai\_utils\_utils.py"", line 270, in wrapper
    return func(*args, **kwargs)
  File ""C:\Users\14499\AppData\Roaming\Python\Python37\site-packages\openai\resources\chat\completions.py"", line 677, in create
    stream_cls=Stream[ChatCompletionChunk],
  File ""C:\Users\14499\AppData\Roaming\Python\Python37\site-packages\openai\_base_client.py"", line 1088, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File ""C:\Users\14499\AppData\Roaming\Python\Python37\site-packages\openai\_base_client.py"", line 858, in request
    remaining_retries=remaining_retries,
  File ""C:\Users\14499\AppData\Roaming\Python\Python37\site-packages\openai\_base_client.py"", line 890, in _request
    response_headers=None,
  File ""C:\Users\14499\AppData\Roaming\Python\Python37\site-packages\openai\_base_client.py"", line 963, in _retry_request
    stream_cls=stream_cls,
  File ""C:\Users\14499\AppData\Roaming\Python\Python37\site-packages\openai\_base_client.py"", line 890, in _request
    response_headers=None,
  File ""C:\Users\14499\AppData\Roaming\Python\Python37\site-packages\openai\_base_client.py"", line 963, in _retry_request
    stream_cls=stream_cls,
  File ""C:\Users\14499\AppData\Roaming\Python\Python37\site-packages\openai\_base_client.py"", line 893, in _request
    raise APITimeoutError(request=request) from err
openai.APITimeoutError: Request timed out.

Process finished with exit code 1


### Code snippets

```Python
from openai import OpenAI

client = OpenAI(api_key='sk-xxxxx')

stream = client.chat.completions.create(
    model=""gpt-3.5-turbo"",
    messages=[{""role"": ""user"", ""content"": ""你能做什么""}],
    stream=True,
)
for chunk in stream:
    print(chunk.choices[0].delta.content or """", end="""")
```


### OS

windows11

### Python version

python3.7

### Library version

1.6.0",closed,2023-12-22T03:59:55Z,2023-12-23T00:36:23Z,['bug'],https://github.com/openai/openai-python/issues/1004,20.61,True,method_order_violation,medium,"The request to the OpenAI API timed out, indicating potential issues with the order of method calls or handling of streaming responses.",high,
999,Stuck on how to output max number of characters,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

What I'm trying to do:
I'm trying to output a max number of tokens (ex:600 tokens) in the AI response from a variable input. 

Example below:
Input
500-1000 tokens
output
<600 tokens

Via documentation (https://platform.openai.com/docs/api-reference/completions/create#chat/create-max_tokens)
- max_tokens
- The maximum number of [tokens]that can be generated in the completion.
- The token count of your prompt plus max_tokens cannot exceed the model's context length. 

Question:
It looks like the only way to control the tokens used is the max_token variable however it's the total tokens (input+output). Is there a way to control the max tokens of the output? 

### To Reproduce

    response = openai.ChatCompletion.create(
        model=""gpt-4"",
        messages=[
            {""role"": ""system"", ""content"": context_template},
            {""role"": ""user"", ""content"": prompt}
        ],
        max_tokens=600
    )

### Code snippets

_No response_

### OS

macOS 

### Python version

3.9.6

### Library version

openai=1.60",closed,2023-12-21T05:46:30Z,2023-12-21T18:52:04Z,[],https://github.com/openai/openai-python/issues/999,13.09,True,input_value_violation,medium,"The user is attempting to set a max_tokens value without considering the total token limit, which could lead to unexpected behavior if the combined input and output exceed the model's context length.",high,
937,Handling of TPM limit errors for Azure (x-rate-limit-reset-tokens),"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

We've been noticing an increasing number of TPM limit errors when calling an Azure-hosted model via the library. We have a couple of retries configured but these do not help. The reason seems to be that recently the Azure API stopped returning the `Retry-After` header in case of limit errors and now return `x-rate-limit-reset-tokens`. The library currently only knows [how to handle `Retry-After` ](https://github.com/openai/openai-python/blob/e36956673d9049713c91bca6ce7aebe58638f483/src/openai/_base_client.py#L616).

### To Reproduce

* Force a token limit error on an Azure hosted model
* Observe the response headers. Example
```
[2023-12-03 18:48:27.180] DEBUG worker_pool_8 [httpcore.http11.trace:45] receive_response_headers.complete return_value=(b'HTTP/1.1', 429, b'Too Many Requests', [(b'Content-Length', b'329'), (b'Content-Type', b'application/json'), (b'x-rate-limit-reset-tokens', b'55'), (b'apim-request-id', b'<uuid>'), (b'Strict-Transport-Security', b'max-age=31536000; includeSubDomains; preload'), (b'x-content-type-options', b'nosniff'), (b'policy-id', b'DeploymentRatelimit-Token'), (b'x-ms-region', b'West US'), (b'x-ratelimit-remaining-requests', b'52'), (b'Date', b'Sun, 03 Dec 2023 18:48:27 GMT')])
```
The `retry-after` header is no longer there and instead the `x-rate-limit-reset-tokens` is returned.

### Code snippets

_No response_

### OS

macOS

### Python version

Python 3.12

### Library version

openai 1.3.6",closed,2023-12-06T10:10:01Z,2023-12-21T18:49:16Z,['bug'],https://github.com/openai/openai-python/issues/937,368.65,True,missing_option_violation,medium,"The library fails to handle the new 'x-rate-limit-reset-tokens' header, which is essential for managing rate limit errors effectively.",high,
938,"For events of the same openai job, the id returned each time the list_events method is called may be different.","### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

When I call the list_events method to get the job log, the ID of the same log returned by two repeated calls is sometimes different, which leads to the error ""Given after argument was not found in items."" when I use the get_next_page method to get the next page.
![image](https://github.com/openai/openai-python/assets/29244475/2ec782e6-e03a-42c0-a799-c387b4424f29)
## Called twice with the same parameters, some jobs have different IDs.
![image](https://github.com/openai/openai-python/assets/29244475/9f6c61cd-4fd3-4687-a88f-8bd840d54590)


### To Reproduce

```python
from openai import AzureOpenAI

client=AzureOpenAI(
  azure_ad_token=token,
  azure_endpoint=endpoint,
  api_version=api_version
)

events = client.fine_tuning.jobs.list_events(fine_tuning_job_id=job_id, limit=10)
events.get_next_page()
```

### Code snippets

```Python
from openai import AzureOpenAI

client=AzureOpenAI(
  azure_ad_token=token,
  azure_endpoint=endpoint,
  api_version=api_version
)
client.fine_tuning.jobs.list_events(fine_tuning_job_id=job_id, limit=10)
```


### OS

macOS

### Python version

Python v3.10.13

### Library version

openai v1.3.5",closed,2023-12-06T11:18:12Z,2023-12-08T22:59:36Z,['Azure'],https://github.com/openai/openai-python/issues/938,59.69,True,missing_dependency_violation,high,"The list_events method returns different IDs for the same job log across repeated calls, indicating a missing dependency in the state management of event IDs, leading to errors when calling get_next_page.",high,
727,Uploading JSON to Files API returns invalid file format,"Upload to the files endpoint with a JSON file throws an error

**Code:**
```python
from openai import OpenAI

client = OpenAI()
file = client.files.create(
    file=open(""example_1.json"", ""rb""),
    # Can either be fine-tuned or assistant
    purpose=""assistants"",
)
```

**Stacktrace:**
```
ile [~/anaconda3/lib/python3.10/site-packages/openai/resources/files.py:88](https://file+.vscode-resource.vscode-cdn.net/Users/vashishtmadhavan/Documents/playground/~/anaconda3/lib/python3.10/site-packages/openai/resources/files.py:88), in Files.create(self, file, purpose, extra_headers, extra_query, extra_body, timeout)
     82 if files:
     83     # It should be noted that the actual Content-Type header that will be
     84     # sent to the server will contain a `boundary` parameter, e.g.
     85     # multipart/form-data; boundary=---abc--
     86     extra_headers = {""Content-Type"": ""multipart/form-data"", **(extra_headers or {})}
---> 88 return self._post(
     89     ""[/files](https://file+.vscode-resource.vscode-cdn.net/files)"",
     90     body=maybe_transform(body, file_create_params.FileCreateParams),
     91     files=files,
     92     options=make_request_options(
     93         extra_headers=extra_headers, extra_query=extra_query, extra_body=extra_body, timeout=timeout
     94     ),
     95     cast_to=FileObject,
     96 )

File [~/anaconda3/lib/python3.10/site-packages/openai/_base_client.py:1055](https://file+.vscode-resource.vscode-cdn.net/Users/vashishtmadhavan/Documents/playground/~/anaconda3/lib/python3.10/site-packages/openai/_base_client.py:1055), in SyncAPIClient.post(self, path, cast_to, body, options, files, stream, stream_cls)
   1041 def post(
   1042     self,
   1043     path: str,
   (...)
   1050     stream_cls: type[_StreamT] | None = None,
   1051 ) -> ResponseT | _StreamT:
   1052     opts = FinalRequestOptions.construct(
   1053         method=""post"", url=path, json_data=body, files=to_httpx_files(files), **options
   1054     )
-> 1055     return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))

File [~/anaconda3/lib/python3.10/site-packages/openai/_base_client.py:834](https://file+.vscode-resource.vscode-cdn.net/Users/vashishtmadhavan/Documents/playground/~/anaconda3/lib/python3.10/site-packages/openai/_base_client.py:834), in SyncAPIClient.request(self, cast_to, options, remaining_retries, stream, stream_cls)
    825 def request(
    826     self,
    827     cast_to: Type[ResponseT],
   (...)
    832     stream_cls: type[_StreamT] | None = None,
    833 ) -> ResponseT | _StreamT:
--> 834     return self._request(
    835         cast_to=cast_to,
    836         options=options,
    837         stream=stream,
    838         stream_cls=stream_cls,
    839         remaining_retries=remaining_retries,
    840     )

File [~/anaconda3/lib/python3.10/site-packages/openai/_base_client.py:877](https://file+.vscode-resource.vscode-cdn.net/Users/vashishtmadhavan/Documents/playground/~/anaconda3/lib/python3.10/site-packages/openai/_base_client.py:877), in SyncAPIClient._request(self, cast_to, options, remaining_retries, stream, stream_cls)
    874     # If the response is streamed then we need to explicitly read the response
    875     # to completion before attempting to access the response text.
    876     err.response.read()
--> 877     raise self._make_status_error_from_response(err.response) from None
    878 except httpx.TimeoutException as err:
    879     if retries > 0:

BadRequestError: Error code: 400 - {'error': {'message': ""Invalid file format. Supported formats: ['c', 'cpp', 'csv', 'docx', 'html', 'java', 'json', 'md', 'pdf', 'php', 'pptx', 'py', 'rb', 'tex', 'txt', 'css', 'jpeg', 'jpg', 'js', 'gif', 'png', 'tar', 'ts', 'xlsx', 'xml', 'zip']"", 'type': 'invalid_request_error', 'param': None, 'code': None}}
```

Here is the example file: [example_1.json](https://github.com/openai/openai-python/files/13290969/example_1.json)


",closed,2023-11-08T01:27:51Z,2023-11-10T03:00:14Z,"['bug', 'API-feedback']",https://github.com/openai/openai-python/issues/727,49.54,False,,,,low,
787,UnicodeEncodeError: 'ascii' codec can't encode character '\u201d' in position 59: ordinal not in range(128),"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

I run the code from docs but receive the error 
**I find the cause of the bug is text load from .env have some special character.** 
```
from dotenv import load_dotenv
load_dotenv()
import os
openai.api_key = os.getenv(""OPENAI_API_KEY"")
```
**and solved with** 
```
client = OpenAI(
      api_key=""sk....""
)
```
Seem have something weird when load .env file
My .env file is and I still use it for other code. But dont know this version receive error

I use new .venv of python 3.10.12 only install python-dotenv

### To Reproduce

!pip install python-dotenv
!pip install openai

RUN the code snip set

I still can print  
`print(u""\u201d"")`

### Code snippets

```Python
import openai
from openai import OpenAI

from dotenv import load_dotenv
load_dotenv()
import os
openai.api_key = os.getenv(""OPENAI_API_KEY"")

client = OpenAI()
response = client.chat.completions.create(
  model=""gpt-3.5-turbo"",
    response_format={ ""type"": ""json_object"" },

  messages=[
    {""role"": ""system"", ""content"": ""You are 3translator to translate the input text to destination language the input is the text nees to translate. First: Detect the language of the input text Second: Translate the input text from the detected language in first step to the Vietnamese language Third: Return output only the translated text not add up the description""},
    {""role"": ""user"", ""content"": ""this is a cat""},
  ]
)
```


### OS

ubuntu 22.04.3

### Python version

3.10.12

### Library version

openai 1.2.3",closed,2023-11-11T17:01:49Z,2023-12-19T03:11:39Z,['bug'],https://github.com/openai/openai-python/issues/787,898.16,True,input_value_violation,high,"The presence of special characters in the input from the .env file causes a UnicodeEncodeError, indicating that the input value is unacceptable for the expected encoding.",high,
769,Constant timeouts after multiple calls with async ,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

Constant timeouts after multiple asynchronous calls. It was discovered when using the Llama_Index framework that when calls are made to this library through the openai-python client wrapped with async, constant timeouts begin. If you do this without async or with asynchrony, but on the old version like 0.28, then there are no problems.

### To Reproduce

Several calls in a row, for example, to embeddings that are wrapped with asynс.

### Code snippets

_No response_

### OS

ubuntu

### Python version

Python 3.11.4

### Library version

v1.2.0 and newer",closed,2023-11-10T09:19:15Z,2023-12-04T02:35:12Z,['bug'],https://github.com/openai/openai-python/issues/769,569.27,False,,,,low,
933,OSError: [Errno 24] Too many open files during retry ,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

Our retry logic was successful before migration to openai v1.0.0. However, now the retry fails as described below.

After a call to openai's API `chat.completions.create` fails with a Connection Error, our code will catch the exception and retry. However, the retry fails with the following error:

```
  File ""<our code>.py"", line xxx, in <our_method1>
  File ""/opt/conda/envs/pbap_prompt/lib/python3.10/site-packages/nest_asyncio.py"", line 35, in run
  File ""/opt/conda/envs/pbap_prompt/lib/python3.10/site-packages/nest_asyncio.py"", line 90, in run_until_complete
  File ""/opt/conda/envs/pbap_prompt/lib/python3.10/asyncio/futures.py"", line 201, in result
  File ""/opt/conda/envs/pbap_prompt/lib/python3.10/asyncio/tasks.py"", line 232, in __step
  File ""<our code>.py"", line xxx, in <our_method2>
  File ""/opt/conda/envs/pbap_prompt/lib/python3.10/site-packages/openai/_client.py"", line 317, in __init__
  File ""/opt/conda/envs/pbap_prompt/lib/python3.10/site-packages/openai/_base_client.py"", line 1190, in __init__
  File ""/opt/conda/envs/pbap_prompt/lib/python3.10/site-packages/httpx/_client.py"", line 1397, in __init__
  File ""/opt/conda/envs/pbap_prompt/lib/python3.10/site-packages/httpx/_client.py"", line 1445, in _init_transport
  File ""/opt/conda/envs/pbap_prompt/lib/python3.10/site-packages/httpx/_transports/default.py"", line 272, in __init__
  File ""/opt/conda/envs/pbap_prompt/lib/python3.10/site-packages/httpx/_config.py"", line 51, in create_ssl_context
  File ""/opt/conda/envs/pbap_prompt/lib/python3.10/site-packages/httpx/_config.py"", line 75, in __init__
  File ""/opt/conda/envs/pbap_prompt/lib/python3.10/site-packages/httpx/_config.py"", line 87, in load_ssl_context
  File ""/opt/conda/envs/pbap_prompt/lib/python3.10/site-packages/httpx/_config.py"", line 145, in load_ssl_context_verify
OSError: [Errno 24] Too many open files
```

Here is a snippet of our methods:

method1:
```
        try:
            outputs = asyncio.run(
                                method2
             )
```

method2:
```
          AsyncOpenAI(
            api_key=xxx,
            organization=xxx,
            max_retries=0,
        ).chat.completions.create(
                    model=model,
                    messages=input_data,
                    max_tokens=max_new_tokens,
                    temperature=temperature,
                    timeout=timeout,
                )
            )
```

### To Reproduce

1. create openai async client and call `chat.completions.create`
2. openai returns error such as `Connection Error`
3. catch error and create new openai async client and call `chat.completions.create` again
4. receive error `OSError: [Errno 24] Too many open files`

### Code snippets

```Python
See above
```


### OS

Unix Ubuntu

### Python version

Python v3.10.0

### Library version

openai 1.3.5",closed,2023-12-04T22:03:20Z,2023-12-15T00:58:48Z,['bug'],https://github.com/openai/openai-python/issues/933,242.92,True,input_value_violation,High,"The error 'OSError: [Errno 24] Too many open files' indicates that the application is exceeding the limit of file descriptors, likely due to repeatedly creating new instances of the async client without properly closing previous ones after retries.",high,
955,Example code snippet references an incorrect default timeout value,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

Example code snippet references an incorrect default timeout value of 60 seconds:

https://github.com/openai/openai-python/blob/e36956673d9049713c91bca6ce7aebe58638f483/README.md?plain=1#L362

When it's actually 10 minutes:
https://github.com/openai/openai-python/blob/e36956673d9049713c91bca6ce7aebe58638f483/src/openai/_constants.py#L8-L9

I suggest either update code snippet comment to reflect an actual value or remove comment from the code snippet entirely.

### To Reproduce

1. Note timeout on the README.md https://github.com/openai/openai-python/blob/e36956673d9049713c91bca6ce7aebe58638f483/README.md?plain=1#L362
2. Note timeout in the _constants.py https://github.com/openai/openai-python/blob/e36956673d9049713c91bca6ce7aebe58638f483/src/openai/_constants.py#L8-L9
3. See the difference

### Code snippets

```Python
> # default timeout is 10 minutes
> DEFAULT_TIMEOUT = httpx.Timeout(timeout=600.0, connect=5.0)


vs 
> # default is 60s
```


### OS

Any

### Python version

Any

### Library version

openai v1.3.7",closed,2023-12-08T16:44:53Z,2023-12-15T03:05:25Z,['bug'],https://github.com/openai/openai-python/issues/955,154.34,False,,,,low,
850,consistent seed not generating reproducible responses,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

When I call gpt-4-turbo streaming with a constant seed (1234), There is still variations in the output, espeically as the prompt and answer get longer. Is there any way to avoid this or somehow force it to be even more deterministic? 

### To Reproduce

code below, problem occurs specifically with in-debth thought provoking prompts.

### Code snippets

```Python
client = AsyncOpenAI(timeout=30)

async def send_openai_request(prompt, engine=""gpt-4-1106-preview""):
    try:
        stream = await client.chat.completions.create(
            messages=[{""role"": ""user"", ""content"": prompt}],
            stream=True,
            model=engine,
            seed=1234,
            temperature=0.0001,
        )
        collected_messages = []

        async for part in stream:
            print(part.choices[0].delta.content or """")
            collected_messages.append(part.choices[0].delta.content or """")

        all_messages = ''.join(collected_messages)
        return all_messages
```
```


### OS

macOS

### Python version

Python 3.10.12

### Library version

openai 1.3.3",closed,2023-11-20T04:37:34Z,2023-11-21T00:51:32Z,"['bug', 'API-feedback']",https://github.com/openai/openai-python/issues/850,20.23,True,input_value_violation,medium,"The use of a constant seed with a very low temperature may not guarantee deterministic outputs, leading to variations in responses.",high,
960,Issue with custom transport,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

So, I'm trying to resolve https://github.com/pyodide/pyodide/issues/4292

With these fixes in place: https://github.com/urllib3/urllib3/pull/3195, https://github.com/encode/httpx/discussions/2994, and the custom transport feature in the library the solution is more feasible.

### To Reproduce

Testing environment: https://jupyter.org/try-jupyter/lab/
```python
import micropip
await micropip.install('https://raw.githubusercontent.com/psymbio/pyodide_wheels/main/multidict/multidict-4.7.6-py3-none-any.whl', keep_going=True)
await micropip.install('https://raw.githubusercontent.com/psymbio/pyodide_wheels/main/frozenlist/frozenlist-1.4.0-py3-none-any.whl', keep_going=True)
await micropip.install('https://raw.githubusercontent.com/psymbio/pyodide_wheels/main/aiohttp/aiohttp-4.0.0a2.dev0-py3-none-any.whl', keep_going=True)
await micropip.install('https://raw.githubusercontent.com/psymbio/pyodide_wheels/main/openai/openai-1.3.7-py3-none-any.whl', keep_going=True)
await micropip.install('https://raw.githubusercontent.com/psymbio/pyodide_wheels/main/urllib3/urllib3-2.1.0-py3-none-any.whl', keep_going=True)
await micropip.install(""ssl"")
import ssl
await micropip.install(""httpx"", keep_going=True)
import httpx
await micropip.install('https://raw.githubusercontent.com/psymbio/pyodide_wheels/main/urllib3/urllib3-2.1.0-py3-none-any.whl', keep_going=True)
import urllib3
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
import json

class URLLib3Transport(httpx.BaseTransport):
    def __init__(self):
        self.pool = urllib3.PoolManager()

    def handle_request(self, request: httpx.Request):
        urllib3_response = self.pool.request('GET', str(request.url))  # Convert httpx.URL to string
        content = json.loads(urllib3_response.data.decode('utf-8'))  # Decode the data and load as JSON
        stream = httpx.ByteStream(json.dumps(content).encode(""utf-8""))  # Convert back to JSON and encode
        headers = [(b""content-type"", b""application/json"")]
        return httpx.Response(200, headers=headers, stream=stream)

client = httpx.Client(transport=URLLib3Transport())
from openai import OpenAI

openai_client = OpenAI(
    base_url=""https://api.openai.com/v1"",
    api_key=""xxx"",
    http_client=client
)
response = openai_client.chat.completions.with_raw_response.create(
    messages=[{
        ""role"": ""user"",
        ""content"": ""Say this is a test"",
    }],
    model=""gpt-3.5-turbo"",
)
completion = response.parse()
print(completion)
```

But I get this output:
```
ChatCompletion(id=None, choices=None, created=None, model=None, object=None, system_fingerprint=None, usage=None, error={'message': ""You didn't provide an API key. You need to provide your API key in an Authorization header using Bearer auth (i.e. Authorization: Bearer YOUR_KEY), or as the password field (with blank username) if you're accessing the API from your browser and are prompted for a username and password. You can obtain an API key from https://platform.openai.com/account/api-keys."", 'type': 'invalid_request_error', 'param': None, 'code': None})
```

I have provided the API key - so what am I doing wrong here?

### Code snippets

_No response_

### OS

Pyodide

### Python version

Python v3.11.3

### Library version

openai v1.3.7",closed,2023-12-09T16:24:47Z,2023-12-09T20:12:17Z,[],https://github.com/openai/openai-python/issues/960,3.79,True,missing_dependency_violation,high,"The custom transport implementation does not properly handle the API key, leading to an invalid request error.",high,
815,Support for Pyodide,"### Confirm this is a feature request for the Python library and not the underlying OpenAI API.

- [X] This is a feature request for the Python library

### Describe the feature or improvement you're requesting

```python
import micropip
await micropip.install('openai', keep_going=True)
await micropip.install(""ssl"")
import openai
from openai import OpenAI

client = OpenAI(
    api_key=""API KEY"",
)

chat_completion = client.chat.completions.create(
    messages=[
        {
            ""role"": ""user"",
            ""content"": ""Say this is a test"",
        }
    ],
    model=""gpt-3.5-turbo"",
)
```

Running this results in the following error:

```python
    OSError                                   Traceback (most recent call last)
File /lib/python3.11/site-packages/httpcore/_exceptions.py:10, in map_exceptions(map)
    9 try:
---> 10     yield
    11 except Exception as exc:  # noqa: PIE786

File /lib/python3.11/site-packages/httpcore/_backends/sync.py:206, in SyncBackend.connect_tcp(self, host, port, timeout, local_address, socket_options)
    205 with map_exceptions(exc_map):
--> 206     sock = socket.create_connection(
    207         address,
    208         timeout,
    209         source_address=source_address,
    210     )
    211     for option in socket_options:

File /lib/python311.zip/socket.py:851, in create_connection(address, timeout, source_address, all_errors)
    850 if not all_errors:
--> 851     raise exceptions[0]
    852 raise ExceptionGroup(""create_connection failed"", exceptions)

File /lib/python311.zip/socket.py:836, in create_connection(address, timeout, source_address, all_errors)
    835     sock.bind(source_address)
--> 836 sock.connect(sa)
    837 # Break explicitly a reference cycle

OSError: [Errno 23] Host is unreachable

The above exception was the direct cause of the following exception:

ConnectError                              Traceback (most recent call last)
File /lib/python3.11/site-packages/httpx/_transports/default.py:66, in map_httpcore_exceptions()
    65 try:
---> 66     yield
    67 except Exception as exc:

File /lib/python3.11/site-packages/httpx/_transports/default.py:228, in HTTPTransport.handle_request(self, request)
    227 with map_httpcore_exceptions():
--> 228     resp = self._pool.handle_request(req)
    230 assert isinstance(resp.stream, typing.Iterable)

File /lib/python3.11/site-packages/httpcore/_sync/connection_pool.py:268, in ConnectionPool.handle_request(self, request)
    267         self.response_closed(status)
--> 268     raise exc
    269 else:

File /lib/python3.11/site-packages/httpcore/_sync/connection_pool.py:251, in ConnectionPool.handle_request(self, request)
    250 try:
--> 251     response = connection.handle_request(request)
    252 except ConnectionNotAvailable:
    253     # The ConnectionNotAvailable exception is a special case, that
    254     # indicates we need to retry the request on a new connection.
(...)
    258     # might end up as an HTTP/2 connection, but which actually ends
    259     # up as HTTP/1.1.

File /lib/python3.11/site-packages/httpcore/_sync/connection.py:99, in HTTPConnection.handle_request(self, request)
    98         self._connect_failed = True
---> 99         raise exc
    100 elif not self._connection.is_available():

File /lib/python3.11/site-packages/httpcore/_sync/connection.py:76, in HTTPConnection.handle_request(self, request)
    75 try:
---> 76     stream = self._connect(request)
    78     ssl_object = stream.get_extra_info(""ssl_object"")

File /lib/python3.11/site-packages/httpcore/_sync/connection.py:124, in HTTPConnection._connect(self, request)
    123 with Trace(""connect_tcp"", logger, request, kwargs) as trace:
--> 124     stream = self._network_backend.connect_tcp(**kwargs)
    125     trace.return_value = stream

File /lib/python3.11/site-packages/httpcore/_backends/sync.py:205, in SyncBackend.connect_tcp(self, host, port, timeout, local_address, socket_options)
    200 exc_map: ExceptionMapping = {
    201     socket.timeout: ConnectTimeout,
    202     OSError: ConnectError,
    203 }
--> 205 with map_exceptions(exc_map):
    206     sock = socket.create_connection(
    207         address,
    208         timeout,
    209         source_address=source_address,
    210     )

File /lib/python311.zip/contextlib.py:155, in _GeneratorContextManager.__exit__(self, typ, value, traceback)
    154 try:
--> 155     self.gen.throw(typ, value, traceback)
    156 except StopIteration as exc:
    157     # Suppress StopIteration *unless* it's the same exception that
    158     # was passed to throw().  This prevents a StopIteration
    159     # raised inside the ""with"" statement from being suppressed.

File /lib/python3.11/site-packages/httpcore/_exceptions.py:14, in map_exceptions(map)
    13     if isinstance(exc, from_exc):
---> 14         raise to_exc(exc) from exc
    15 raise

ConnectError: [Errno 23] Host is unreachable

The above exception was the direct cause of the following exception:

ConnectError                              Traceback (most recent call last)
File /lib/python3.11/site-packages/openai/_base_client.py:858, in SyncAPIClient._request(self, cast_to, options, remaining_retries, stream, stream_cls)
    857 try:
--> 858     response = self._client.send(request, auth=self.custom_auth, stream=stream)
    859     log.debug(
    860         'HTTP Request: %s %s ""%i %s""', request.method, request.url, response.status_code, response.reason_phrase
    861     )

File /lib/python3.11/site-packages/httpx/_client.py:901, in Client.send(self, request, stream, auth, follow_redirects)
    899 auth = self._build_request_auth(request, auth)
--> 901 response = self._send_handling_auth(
    902     request,
    903     auth=auth,
    904     follow_redirects=follow_redirects,
    905     history=[],
    906 )
    907 try:

File /lib/python3.11/site-packages/httpx/_client.py:929, in Client._send_handling_auth(self, request, auth, follow_redirects, history)
    928 while True:
--> 929     response = self._send_handling_redirects(
    930         request,
    931         follow_redirects=follow_redirects,
    932         history=history,
    933     )
    934     try:

File /lib/python3.11/site-packages/httpx/_client.py:966, in Client._send_handling_redirects(self, request, follow_redirects, history)
    964     hook(request)
--> 966 response = self._send_single_request(request)
    967 try:

File /lib/python3.11/site-packages/httpx/_client.py:1002, in Client._send_single_request(self, request)
1001 with request_context(request=request):
-> 1002     response = transport.handle_request(request)
1004 assert isinstance(response.stream, SyncByteStream)

File /lib/python3.11/site-packages/httpx/_transports/default.py:227, in HTTPTransport.handle_request(self, request)
    215 req = httpcore.Request(
    216     method=request.method,
    217     url=httpcore.URL(
(...)
    225     extensions=request.extensions,
    226 )
--> 227 with map_httpcore_exceptions():
    228     resp = self._pool.handle_request(req)

File /lib/python311.zip/contextlib.py:155, in _GeneratorContextManager.__exit__(self, typ, value, traceback)
    154 try:
--> 155     self.gen.throw(typ, value, traceback)
    156 except StopIteration as exc:
    157     # Suppress StopIteration *unless* it's the same exception that
    158     # was passed to throw().  This prevents a StopIteration
    159     # raised inside the ""with"" statement from being suppressed.

File /lib/python3.11/site-packages/httpx/_transports/default.py:83, in map_httpcore_exceptions()
    82 message = str(exc)
---> 83 raise mapped_exc(message) from exc

ConnectError: [Errno 23] Host is unreachable

During handling of the above exception, another exception occurred:

OSError                                   Traceback (most recent call last)
File /lib/python3.11/site-packages/httpcore/_exceptions.py:10, in map_exceptions(map)
    9 try:
---> 10     yield
    11 except Exception as exc:  # noqa: PIE786

File /lib/python3.11/site-packages/httpcore/_backends/sync.py:206, in SyncBackend.connect_tcp(self, host, port, timeout, local_address, socket_options)
    205 with map_exceptions(exc_map):
--> 206     sock = socket.create_connection(
    207         address,
    208         timeout,
    209         source_address=source_address,
    210     )
    211     for option in socket_options:

File /lib/python311.zip/socket.py:851, in create_connection(address, timeout, source_address, all_errors)
    850 if not all_errors:
--> 851     raise exceptions[0]
    852 raise ExceptionGroup(""create_connection failed"", exceptions)

File /lib/python311.zip/socket.py:836, in create_connection(address, timeout, source_address, all_errors)
    835     sock.bind(source_address)
--> 836 sock.connect(sa)
    837 # Break explicitly a reference cycle

OSError: [Errno 23] Host is unreachable

The above exception was the direct cause of the following exception:

ConnectError                              Traceback (most recent call last)
File /lib/python3.11/site-packages/httpx/_transports/default.py:66, in map_httpcore_exceptions()
    65 try:
---> 66     yield
    67 except Exception as exc:

File /lib/python3.11/site-packages/httpx/_transports/default.py:228, in HTTPTransport.handle_request(self, request)
    227 with map_httpcore_exceptions():
--> 228     resp = self._pool.handle_request(req)
    230 assert isinstance(resp.stream, typing.Iterable)

File /lib/python3.11/site-packages/httpcore/_sync/connection_pool.py:268, in ConnectionPool.handle_request(self, request)
    267         self.response_closed(status)
--> 268     raise exc
    269 else:

File /lib/python3.11/site-packages/httpcore/_sync/connection_pool.py:251, in ConnectionPool.handle_request(self, request)
    250 try:
--> 251     response = connection.handle_request(request)
    252 except ConnectionNotAvailable:
    253     # The ConnectionNotAvailable exception is a special case, that
    254     # indicates we need to retry the request on a new connection.
(...)
    258     # might end up as an HTTP/2 connection, but which actually ends
    259     # up as HTTP/1.1.

File /lib/python3.11/site-packages/httpcore/_sync/connection.py:99, in HTTPConnection.handle_request(self, request)
    98         self._connect_failed = True
---> 99         raise exc
    100 elif not self._connection.is_available():

File /lib/python3.11/site-packages/httpcore/_sync/connection.py:76, in HTTPConnection.handle_request(self, request)
    75 try:
---> 76     stream = self._connect(request)
    78     ssl_object = stream.get_extra_info(""ssl_object"")

File /lib/python3.11/site-packages/httpcore/_sync/connection.py:124, in HTTPConnection._connect(self, request)
    123 with Trace(""connect_tcp"", logger, request, kwargs) as trace:
--> 124     stream = self._network_backend.connect_tcp(**kwargs)
    125     trace.return_value = stream

File /lib/python3.11/site-packages/httpcore/_backends/sync.py:205, in SyncBackend.connect_tcp(self, host, port, timeout, local_address, socket_options)
    200 exc_map: ExceptionMapping = {
    201     socket.timeout: ConnectTimeout,
    202     OSError: ConnectError,
    203 }
--> 205 with map_exceptions(exc_map):
    206     sock = socket.create_connection(
    207         address,
    208         timeout,
    209         source_address=source_address,
    210     )

File /lib/python311.zip/contextlib.py:155, in _GeneratorContextManager.__exit__(self, typ, value, traceback)
    154 try:
--> 155     self.gen.throw(typ, value, traceback)
    156 except StopIteration as exc:
    157     # Suppress StopIteration *unless* it's the same exception that
    158     # was passed to throw().  This prevents a StopIteration
    159     # raised inside the ""with"" statement from being suppressed.

File /lib/python3.11/site-packages/httpcore/_exceptions.py:14, in map_exceptions(map)
    13     if isinstance(exc, from_exc):
---> 14         raise to_exc(exc) from exc
    15 raise

ConnectError: [Errno 23] Host is unreachable

The above exception was the direct cause of the following exception:

ConnectError                              Traceback (most recent call last)
File /lib/python3.11/site-packages/openai/_base_client.py:858, in SyncAPIClient._request(self, cast_to, options, remaining_retries, stream, stream_cls)
    857 try:
--> 858     response = self._client.send(request, auth=self.custom_auth, stream=stream)
    859     log.debug(
    860         'HTTP Request: %s %s ""%i %s""', request.method, request.url, response.status_code, response.reason_phrase
    861     )

File /lib/python3.11/site-packages/httpx/_client.py:901, in Client.send(self, request, stream, auth, follow_redirects)
    899 auth = self._build_request_auth(request, auth)
--> 901 response = self._send_handling_auth(
    902     request,
    903     auth=auth,
    904     follow_redirects=follow_redirects,
    905     history=[],
    906 )
    907 try:

File /lib/python3.11/site-packages/httpx/_client.py:929, in Client._send_handling_auth(self, request, auth, follow_redirects, history)
    928 while True:
--> 929     response = self._send_handling_redirects(
    930         request,
    931         follow_redirects=follow_redirects,
    932         history=history,
    933     )
    934     try:

File /lib/python3.11/site-packages/httpx/_client.py:966, in Client._send_handling_redirects(self, request, follow_redirects, history)
    964     hook(request)
--> 966 response = self._send_single_request(request)
    967 try:

File /lib/python3.11/site-packages/httpx/_client.py:1002, in Client._send_single_request(self, request)
1001 with request_context(request=request):
-> 1002     response = transport.handle_request(request)
1004 assert isinstance(response.stream, SyncByteStream)

File /lib/python3.11/site-packages/httpx/_transports/default.py:227, in HTTPTransport.handle_request(self, request)
    215 req = httpcore.Request(
    216     method=request.method,
    217     url=httpcore.URL(
(...)
    225     extensions=request.extensions,
    226 )
--> 227 with map_httpcore_exceptions():
    228     resp = self._pool.handle_request(req)

File /lib/python311.zip/contextlib.py:155, in _GeneratorContextManager.__exit__(self, typ, value, traceback)
    154 try:
--> 155     self.gen.throw(typ, value, traceback)
    156 except StopIteration as exc:
    157     # Suppress StopIteration *unless* it's the same exception that
    158     # was passed to throw().  This prevents a StopIteration
    159     # raised inside the ""with"" statement from being suppressed.

File /lib/python3.11/site-packages/httpx/_transports/default.py:83, in map_httpcore_exceptions()
    82 message = str(exc)
---> 83 raise mapped_exc(message) from exc

ConnectError: [Errno 23] Host is unreachable

During handling of the above exception, another exception occurred:

OSError                                   Traceback (most recent call last)
File /lib/python3.11/site-packages/httpcore/_exceptions.py:10, in map_exceptions(map)
    9 try:
---> 10     yield
    11 except Exception as exc:  # noqa: PIE786

File /lib/python3.11/site-packages/httpcore/_backends/sync.py:206, in SyncBackend.connect_tcp(self, host, port, timeout, local_address, socket_options)
    205 with map_exceptions(exc_map):
--> 206     sock = socket.create_connection(
    207         address,
    208         timeout,
    209         source_address=source_address,
    210     )
    211     for option in socket_options:

File /lib/python311.zip/socket.py:851, in create_connection(address, timeout, source_address, all_errors)
    850 if not all_errors:
--> 851     raise exceptions[0]
    852 raise ExceptionGroup(""create_connection failed"", exceptions)

File /lib/python311.zip/socket.py:836, in create_connection(address, timeout, source_address, all_errors)
    835     sock.bind(source_address)
--> 836 sock.connect(sa)
    837 # Break explicitly a reference cycle

OSError: [Errno 23] Host is unreachable

The above exception was the direct cause of the following exception:

ConnectError                              Traceback (most recent call last)
File /lib/python3.11/site-packages/httpx/_transports/default.py:66, in map_httpcore_exceptions()
    65 try:
---> 66     yield
    67 except Exception as exc:

File /lib/python3.11/site-packages/httpx/_transports/default.py:228, in HTTPTransport.handle_request(self, request)
    227 with map_httpcore_exceptions():
--> 228     resp = self._pool.handle_request(req)
    230 assert isinstance(resp.stream, typing.Iterable)

File /lib/python3.11/site-packages/httpcore/_sync/connection_pool.py:268, in ConnectionPool.handle_request(self, request)
    267         self.response_closed(status)
--> 268     raise exc
    269 else:

File /lib/python3.11/site-packages/httpcore/_sync/connection_pool.py:251, in ConnectionPool.handle_request(self, request)
    250 try:
--> 251     response = connection.handle_request(request)
    252 except ConnectionNotAvailable:
    253     # The ConnectionNotAvailable exception is a special case, that
    254     # indicates we need to retry the request on a new connection.
(...)
    258     # might end up as an HTTP/2 connection, but which actually ends
    259     # up as HTTP/1.1.

File /lib/python3.11/site-packages/httpcore/_sync/connection.py:99, in HTTPConnection.handle_request(self, request)
    98         self._connect_failed = True
---> 99         raise exc
    100 elif not self._connection.is_available():

File /lib/python3.11/site-packages/httpcore/_sync/connection.py:76, in HTTPConnection.handle_request(self, request)
    75 try:
---> 76     stream = self._connect(request)
    78     ssl_object = stream.get_extra_info(""ssl_object"")

File /lib/python3.11/site-packages/httpcore/_sync/connection.py:124, in HTTPConnection._connect(self, request)
    123 with Trace(""connect_tcp"", logger, request, kwargs) as trace:
--> 124     stream = self._network_backend.connect_tcp(**kwargs)
    125     trace.return_value = stream

File /lib/python3.11/site-packages/httpcore/_backends/sync.py:205, in SyncBackend.connect_tcp(self, host, port, timeout, local_address, socket_options)
    200 exc_map: ExceptionMapping = {
    201     socket.timeout: ConnectTimeout,
    202     OSError: ConnectError,
    203 }
--> 205 with map_exceptions(exc_map):
    206     sock = socket.create_connection(
    207         address,
    208         timeout,
    209         source_address=source_address,
    210     )

File /lib/python311.zip/contextlib.py:155, in _GeneratorContextManager.__exit__(self, typ, value, traceback)
    154 try:
--> 155     self.gen.throw(typ, value, traceback)
    156 except StopIteration as exc:
    157     # Suppress StopIteration *unless* it's the same exception that
    158     # was passed to throw().  This prevents a StopIteration
    159     # raised inside the ""with"" statement from being suppressed.

File /lib/python3.11/site-packages/httpcore/_exceptions.py:14, in map_exceptions(map)
    13     if isinstance(exc, from_exc):
---> 14         raise to_exc(exc) from exc
    15 raise

ConnectError: [Errno 23] Host is unreachable

The above exception was the direct cause of the following exception:

ConnectError                              Traceback (most recent call last)
File /lib/python3.11/site-packages/openai/_base_client.py:858, in SyncAPIClient._request(self, cast_to, options, remaining_retries, stream, stream_cls)
    857 try:
--> 858     response = self._client.send(request, auth=self.custom_auth, stream=stream)
    859     log.debug(
    860         'HTTP Request: %s %s ""%i %s""', request.method, request.url, response.status_code, response.reason_phrase
    861     )

File /lib/python3.11/site-packages/httpx/_client.py:901, in Client.send(self, request, stream, auth, follow_redirects)
    899 auth = self._build_request_auth(request, auth)
--> 901 response = self._send_handling_auth(
    902     request,
    903     auth=auth,
    904     follow_redirects=follow_redirects,
    905     history=[],
    906 )
    907 try:

File /lib/python3.11/site-packages/httpx/_client.py:929, in Client._send_handling_auth(self, request, auth, follow_redirects, history)
    928 while True:
--> 929     response = self._send_handling_redirects(
    930         request,
    931         follow_redirects=follow_redirects,
    932         history=history,
    933     )
    934     try:

File /lib/python3.11/site-packages/httpx/_client.py:966, in Client._send_handling_redirects(self, request, follow_redirects, history)
    964     hook(request)
--> 966 response = self._send_single_request(request)
    967 try:

File /lib/python3.11/site-packages/httpx/_client.py:1002, in Client._send_single_request(self, request)
1001 with request_context(request=request):
-> 1002     response = transport.handle_request(request)
1004 assert isinstance(response.stream, SyncByteStream)

File /lib/python3.11/site-packages/httpx/_transports/default.py:227, in HTTPTransport.handle_request(self, request)
    215 req = httpcore.Request(
    216     method=request.method,
    217     url=httpcore.URL(
(...)
    225     extensions=request.extensions,
    226 )
--> 227 with map_httpcore_exceptions():
    228     resp = self._pool.handle_request(req)

File /lib/python311.zip/contextlib.py:155, in _GeneratorContextManager.__exit__(self, typ, value, traceback)
    154 try:
--> 155     self.gen.throw(typ, value, traceback)
    156 except StopIteration as exc:
    157     # Suppress StopIteration *unless* it's the same exception that
    158     # was passed to throw().  This prevents a StopIteration
    159     # raised inside the ""with"" statement from being suppressed.

File /lib/python3.11/site-packages/httpx/_transports/default.py:83, in map_httpcore_exceptions()
    82 message = str(exc)
---> 83 raise mapped_exc(message) from exc

ConnectError: [Errno 23] Host is unreachable

The above exception was the direct cause of the following exception:

APIConnectionError                        Traceback (most recent call last)
Cell In[7], line 11
    5 from openai import OpenAI
    7 client = OpenAI(
    8     api_key=""API KEY"",
    9 )
---> 11 chat_completion = client.chat.completions.create(
    12     messages=[
    13         {
    14             ""role"": ""user"",
    15             ""content"": ""Say this is a test"",
    16         }
    17     ],
    18     model=""gpt-3.5-turbo"",
    19 )

File /lib/python3.11/site-packages/openai/_utils/_utils.py:299, in required_args.<locals>.inner.<locals>.wrapper(*args, **kwargs)
    297             msg = f""Missing required argument: {quote(missing[0])}""
    298     raise TypeError(msg)
--> 299 return func(*args, **kwargs)

File /lib/python3.11/site-packages/openai/resources/chat/completions.py:594, in Completions.create(self, messages, model, frequency_penalty, function_call, functions, logit_bias, max_tokens, n, presence_penalty, response_format, seed, stop, stream, temperature, tool_choice, tools, top_p, user, extra_headers, extra_query, extra_body, timeout)
    548 @required_args([""messages"", ""model""], [""messages"", ""model"", ""stream""])
    549 def create(
    550     self,
(...)
    592     timeout: float | httpx.Timeout | None | NotGiven = NOT_GIVEN,
    593 ) -> ChatCompletion | Stream[ChatCompletionChunk]:
--> 594     return self._post(
    595         ""/chat/completions"",
    596         body=maybe_transform(
    597             {
    598                 ""messages"": messages,
    599                 ""model"": model,
    600                 ""frequency_penalty"": frequency_penalty,
    601                 ""function_call"": function_call,
    602                 ""functions"": functions,
    603                 ""logit_bias"": logit_bias,
    604                 ""max_tokens"": max_tokens,
    605                 ""n"": n,
    606                 ""presence_penalty"": presence_penalty,
    607                 ""response_format"": response_format,
    608                 ""seed"": seed,
    609                 ""stop"": stop,
    610                 ""stream"": stream,
    611                 ""temperature"": temperature,
    612                 ""tool_choice"": tool_choice,
    613                 ""tools"": tools,
    614                 ""top_p"": top_p,
    615                 ""user"": user,
    616             },
    617             completion_create_params.CompletionCreateParams,
    618         ),
    619         options=make_request_options(
    620             extra_headers=extra_headers, extra_query=extra_query, extra_body=extra_body, timeout=timeout
    621         ),
    622         cast_to=ChatCompletion,
    623         stream=stream or False,
    624         stream_cls=Stream[ChatCompletionChunk],
    625     )

File /lib/python3.11/site-packages/openai/_base_client.py:1055, in SyncAPIClient.post(self, path, cast_to, body, options, files, stream, stream_cls)
1041 def post(
1042     self,
1043     path: str,
(...)
1050     stream_cls: type[_StreamT] | None = None,
1051 ) -> ResponseT | _StreamT:
1052     opts = FinalRequestOptions.construct(
1053         method=""post"", url=path, json_data=body, files=to_httpx_files(files), **options
1054     )
-> 1055     return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))

File /lib/python3.11/site-packages/openai/_base_client.py:834, in SyncAPIClient.request(self, cast_to, options, remaining_retries, stream, stream_cls)
    825 def request(
    826     self,
    827     cast_to: Type[ResponseT],
(...)
    832     stream_cls: type[_StreamT] | None = None,
    833 ) -> ResponseT | _StreamT:
--> 834     return self._request(
    835         cast_to=cast_to,
    836         options=options,
    837         stream=stream,
    838         stream_cls=stream_cls,
    839         remaining_retries=remaining_retries,
    840     )

File /lib/python3.11/site-packages/openai/_base_client.py:890, in SyncAPIClient._request(self, cast_to, options, remaining_retries, stream, stream_cls)
    888 except Exception as err:
    889     if retries > 0:
--> 890         return self._retry_request(
    891             options,
    892             cast_to,
    893             retries,
    894             stream=stream,
    895             stream_cls=stream_cls,
    896         )
    897     raise APIConnectionError(request=request) from err
    899 return self._process_response(
    900     cast_to=cast_to,
    901     options=options,
(...)
    904     stream_cls=stream_cls,
    905 )

File /lib/python3.11/site-packages/openai/_base_client.py:925, in SyncAPIClient._retry_request(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)
    921 # In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a
    922 # different thread if necessary.
    923 time.sleep(timeout)
--> 925 return self._request(
    926     options=options,
    927     cast_to=cast_to,
    928     remaining_retries=remaining,
    929     stream=stream,
    930     stream_cls=stream_cls,
    931 )

File /lib/python3.11/site-packages/openai/_base_client.py:890, in SyncAPIClient._request(self, cast_to, options, remaining_retries, stream, stream_cls)
    888 except Exception as err:
    889     if retries > 0:
--> 890         return self._retry_request(
    891             options,
    892             cast_to,
    893             retries,
    894             stream=stream,
    895             stream_cls=stream_cls,
    896         )
    897     raise APIConnectionError(request=request) from err
    899 return self._process_response(
    900     cast_to=cast_to,
    901     options=options,
(...)
    904     stream_cls=stream_cls,
    905 )

File /lib/python3.11/site-packages/openai/_base_client.py:925, in SyncAPIClient._retry_request(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)
    921 # In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a
    922 # different thread if necessary.
    923 time.sleep(timeout)
--> 925 return self._request(
    926     options=options,
    927     cast_to=cast_to,
    928     remaining_retries=remaining,
    929     stream=stream,
    930     stream_cls=stream_cls,
    931 )

File /lib/python3.11/site-packages/openai/_base_client.py:897, in SyncAPIClient._request(self, cast_to, options, remaining_retries, stream, stream_cls)
    889     if retries > 0:
    890         return self._retry_request(
    891             options,
    892             cast_to,
(...)
    895             stream_cls=stream_cls,
    896         )
--> 897     raise APIConnectionError(request=request) from err
    899 return self._process_response(
    900     cast_to=cast_to,
    901     options=options,
(...)
    904     stream_cls=stream_cls,
    905 )

APIConnectionError: Connection error.
```

Is there any possible way to support openai to run in Pyodide?

Also, mentioning https://github.com/pyodide/pyodide/issues/4292 for other possible solutions to work with the library on Pyodide.

### Additional context

_No response_",closed,2023-11-14T04:53:32Z,2023-12-09T16:30:37Z,[],https://github.com/openai/openai-python/issues/815,611.62,True,missing_dependency_violation,High,"The API call to create chat completions fails due to a missing network connection, indicating a dependency on network availability that is not met in the Pyodide environment.",high,
953,"response_format={""type"": ""json_object""} leads to abnormal output","### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

The expected model returns a list-type JSON task. After adding the following parameter, the square brackets disappeared: response_format={""type"": ""json_object""}

### To Reproduce

1. The expected model was supposed to return a list-type JSON, for example: [{""A""},{""B""}]
2. Add the parameter response_format={""type"": ""json_object""} in completions.
3. Unexpected changes occurred in the model output, where the square brackets disappeared and it became like this: {""A""}

### Code snippets

```Python
response = client.chat.completions.with_raw_response.create(
            model=model,
            response_format={""type"": ""json_object""},
            max_tokens=4096,
            temperature=0,
            messages=messages
        )
```


### OS

win11

### Python version

Python v3.11.6

### Library version

openai v1.3.7",closed,2023-12-08T13:48:05Z,2023-12-08T14:49:39Z,[],https://github.com/openai/openai-python/issues/953,1.03,True,missing_option_violation,high,"The use of response_format={'type': 'json_object'} is not aligned with the expected output format, leading to incorrect model behavior.",high,
958,Broken link on docx,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

https://platform.openai.com/docs/guides/text-generation/managing-tokens 


![image](https://github.com/openai/openai-python/assets/136206577/20b6a6a1-efef-46a2-9e85-7069e22f33bb)

The hyper link is broke. 404.



### To Reproduce

Click The [ChatML documentation](https://github.com/openai/openai-python/blob/main/chatml.md) explains how messages are converted into tokens by the OpenAI API, and may be useful for writing your own function.

### Code snippets

_No response_

### OS

Windows

### Python version

NA

### Library version

NA",closed,2023-12-08T19:30:39Z,2023-12-09T00:06:24Z,['documentation'],https://github.com/openai/openai-python/issues/958,4.6,False,,,,,
865,Memory leak when using `with_options()` to configure options per request,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

Using `with_options()` to configure options per request introduces a memory leak. 

### How to determine it's a memory leak?
The following code calls the completion API 10 times. On each iteration, it takes a snapshot of traces of memory blocks using `tracemalloc`, compares it to the snapshot of the previous iteration, and prints the top 2 differences. The memory required by the `openai` library increases on every iteration. In particular, the following lines have the most increases (using sync client):
```
.../env/lib/python3.11/site-packages/openai/_response.py:227: size=321 KiB (+26.0 KiB), count=2681 (+175), average=123 B
.../env/lib/python3.11/site-packages/openai/_response.py:226: size=214 KiB (+19.4 KiB), count=1830 (+165), average=120 B
```

```
import os
import tracemalloc
from openai import OpenAI

tracemalloc.start()

client = OpenAI(api_key=os.getenv(""OPENAI_API_KEY""))

snapshot1 = tracemalloc.take_snapshot()
for _ in range(10):
    client.with_options(max_retries=5).chat.completions.create(
        messages=[
            {
                ""role"": ""user"",
                ""content"": ""How can I get the name of the current day in Node.js?"",
            }
        ],
        model=""gpt-3.5-turbo"",
    )

    snapshot2 = tracemalloc.take_snapshot()
    top_stats = snapshot2.compare_to(snapshot1, 'lineno')
    print(""[ Top 2 differences ]"")
    for stat in top_stats[:2]:
        print(stat)
    
    snapshot1 = snapshot2
```

### Potential cause
Looking at the library code, when using `with_options()`, a new client is created on every request ([sync client](https://github.com/openai/openai-python/blob/main/src/openai/_client.py#L154-L205) and [async client](https://github.com/openai/openai-python/blob/main/src/openai/_client.py#L363-L414)).


### To Reproduce

Use `with_options()` and make multiple API calls.

### Code snippets

_No response_

### OS

macOS

### Python version

Python v3.11.5

### Library version

openai v1.3.3",closed,2023-11-22T23:10:29Z,2023-12-08T18:57:53Z,['bug'],https://github.com/openai/openai-python/issues/865,379.79,True,memory_out_of_bound,High,"Memory leak caused by the repeated creation of new clients with each call to with_options(), leading to increased memory usage.",high,
942,Weird interaction change with OpenAI Client and files,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

I had a webapp running which was using the openai==0.27.7 I recently was working on revamping it and switched to the new openai client. I thought it'll be just plug and play with the new Interface like how GPT works. I tried changing my approach a few times but kept getting weird file format errors everytime. Not an issue on the file end since the same one works on the older api for the same. 
Totally an issue on the new client



### To Reproduce

` def transcribe(self):
        # transcript_text = openai.Audio.transcribe(""whisper-1"", self.audio_file)[""text""]
        transcript_text = client.audio.transcriptions.create(
            model = ""whisper-1"", 
            file = self.audio_file
        )[""text""]
        self.transcript = transcript_text

        return transcript_text`
`@app.post(""/transcribe_audio"")
async def transcribe_audio(audio_file: UploadFile = File(...)):
    if audio_file.filename.endswith(("".mp3"", "".wav"", ""m4a"", ""mp4"", ""mpeg"", ""mpga"", ""oga"", ""ogg"", ""webm"")):
        audio_query = AudioQuery(audio_file)  
        transcript_text = audio_query.transcribe()
        return {""transcript"": transcript_text}
    else:
        raise HTTPException(status_code=400, detail=""Invalid file type. Please upload a file in a supported format"")
`


### Code snippets

```Python
@app.post(""/transcribe_audio"")
async def transcribe_audio(audio_file: UploadFile = File(...)):
    if audio_file.filename.endswith(("".mp3"", "".wav"")):
        file_name = os.path.splitext(os.path.basename(audio_file.filename))[0]
        audio_query = AudioQuery(audio_file, file_name)
        
        transcript_text = audio_query.transcribe()
        return {""transcript"": transcript_text}
    else:
        raise HTTPException(status_code=400, detail=""Invalid file type. Please upload a .mp3 or .wav file"")


    def transcribe(self):
        transcript_text = fetch_transcription(self.file_name)

        if transcript_text:
            self.transcript = transcript_text
        else:
            openai.api_key = API_KEY
            transcript_text = openai.Audio.transcribe(""whisper-1"", self.audio_file)[""text""]
            cache_transcription(self.file_name, transcript_text)
            self.transcript = transcript_text
```


### OS

Windows

### Python version

Python v3.11.4

### Library version

openai v1.0.1",closed,2023-12-07T14:20:07Z,2023-12-08T04:54:55Z,[],https://github.com/openai/openai-python/issues/942,14.58,True,input_type_violation,high,"The new OpenAI client is producing file format errors, indicating that the input file type may not be compatible with the expected input type for the transcribe method.",high,
948,`body` in `APIError` is never assigned,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

In the [`APIError`](https://github.com/openai/openai-python/blame/main/src/openai/_exceptions.py#L32) class, there is a `body` class variable. Based on the comment:
```
    """"The API response body.

    If the API responded with a valid JSON structure then this property will be the
    decoded result.

    If it isn't a valid JSON structure then this will be the raw response.

    If there was no response associated with this error then it will be `None`.
    """"""
```

This class variable should contain the body of the response. However in the `__init__` function, the `body` variable is not assigned and kept.

### To Reproduce

Simply `error.body` will result in:
>  AttributeError: 'XxxxxxError' object has no attribute 'body'

### Code snippets

_No response_

### OS

OS agnostic

### Python version

Python version agnostic

### Library version

Lib version agnostic",closed,2023-12-07T23:20:46Z,2023-12-08T10:03:16Z,['bug'],https://github.com/openai/openai-python/issues/948,10.71,True,missing_dependency_violation,high,"The 'body' attribute in the APIError class is not assigned in the __init__ function, leading to an AttributeError when accessed.",high,
918,404 here.. ,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

https://github.com/openai/openai-python/blob/main/chatml.md.  404

### To Reproduce

https://github.com/openai/openai-python/blob/main/chatml.md

### Code snippets

```Python
ddd
```


### OS

Mac 

### Python version

na

### Library version

na",closed,2023-12-01T17:43:23Z,2023-12-01T23:47:32Z,[],https://github.com/openai/openai-python/issues/918,6.07,False,,,,low,
940,typing issues,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

typing errors occur such as:
```
Type of ""response"" is partially unknown
  Type of ""response"" is ""Unknown | ChatCompletion""Pylance[reportUnknownVariableType](https://github.com/microsoft/pyright/blob/main/docs/configuration.md#reportUnknownVariableType)
```



### To Reproduce

```python
from openai import AsyncOpenAI
from openai.types.chat import ChatCompletion

...

        response: ChatCompletion = await client.chat.completions.create(
            model=""gpt-4-1106-preview"",
            messages=[
                {
                    ""role"": ""user"",
                    ""content"": data,
                },
            ],
            functions=check_charges_functions,
            function_call=""auto"",
            request_timeout=30,
        )
        response_message = response.choices[0].message
```

### Code snippets

_No response_

### OS

macOS

### Python version

v3.12

### Library version

v1.3.7",closed,2023-12-07T10:15:41Z,2023-12-07T10:22:35Z,['question'],https://github.com/openai/openai-python/issues/940,0.12,True,input_type_violation,unknown,"The type of the 'response' variable is partially unknown, indicating a potential mismatch or lack of clarity in expected types.",high,
925,Easy way to get a context window for a model,"### Confirm this is a feature request for the Python library and not the underlying OpenAI API.

- [X] This is a feature request for the Python library

### Describe the feature or improvement you're requesting

The current version of `openai-python` (v1.3.7) does not contain any method to get the size of context window for a model used for a chat completion. It might cause some overload or underutilization of a model. 

Definitely, some model names are clear enough (like gpt-4-**32k**) but the majority of them must be manually checked using the correspondent tables at https://platform.openai.com/docs/models.

It would be great to fix this inconsistency by any or all of these measures:

1. Include the size of the context window in the name of each model.
2. Improve the model object (https://platform.openai.com/docs/api-reference/models/object) by adding the size of context window.

### Additional context

_No response_",closed,2023-12-04T07:23:06Z,2023-12-05T22:44:37Z,[],https://github.com/openai/openai-python/issues/925,39.36,False,,,,,
930,Delete thread message... It seems the API supports delete message but python library doesn't have it implemented ,"### Confirm this is a feature request for the Python library and not the underlying OpenAI API.

- [X] This is a feature request for the Python library

### Describe the feature or improvement you're requesting

The need for this feature came because with the newer model where input is 120k tokens, when the conversation  gets longer and longer, one message might cost up to $1.2+ So users might want to cleanup the thread conversation and leave only desired number of messages in history...
I've already implemented this feature in my fork below, and would be very happy if we also have it in the official repo as well.
Link to my fork commit: https://github.com/openai/openai-python/commit/6c408960577d22604f1086edc913cc6500752521

### Additional context

Also some related discussion here: https://community.openai.com/t/how-to-limit-the-number-of-messages-or-tokens-that-are-persisted-in-a-thread-to-maintain-context-in-open-ai-assistants/531814",closed,2023-12-04T12:17:11Z,2023-12-05T22:43:25Z,[],https://github.com/openai/openai-python/issues/930,34.44,True,missing_option_violation,medium,"The Python library does not implement a feature that is supported by the underlying API, leading to a lack of flexibility for users to manage message history.",high,
887,response_format error,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

```
completion = client.chat.completions.create(
    messages=[
        {
            ""role"": ""user"",
            ""content"": ""Can you generate an example json object describing a fruit?"",
        }
    ],
    model=""gpt-3.5-turbo"",
    response_format={""type"": ""json_object""},
)

---------------------------------------------------------------------------
BadRequestError                           Traceback (most recent call last)
Cell In[5], line 1
----> 1 completion = client.chat.completions.create(
      2     messages=[
      3         {
      4             ""role"": ""user"",
      5             ""content"": ""Can you generate an example json object describing a fruit?"",
      6         }
      7     ],
      8     model=""gpt-3.5-turbo"",
      9     response_format={""type"": ""json_object""},
     10 )

File ~/anaconda3/envs/llama/lib/python3.10/site-packages/openai/_utils/_utils.py:299, in required_args.<locals>.inner.<locals>.wrapper(*args, **kwargs)
    297             msg = f""Missing required argument: {quote(missing[0])}""
    298     raise TypeError(msg)
--> 299 return func(*args, **kwargs)

File ~/anaconda3/envs/llama/lib/python3.10/site-packages/openai/resources/chat/completions.py:598, in Completions.create(self, messages, model, frequency_penalty, function_call, functions, logit_bias, max_tokens, n, presence_penalty, response_format, seed, stop, stream, temperature, tool_choice, tools, top_p, user, extra_headers, extra_query, extra_body, timeout)
    551 @required_args([""messages"", ""model""], [""messages"", ""model"", ""stream""])
    552 def create(
    553     self,
   (...)
    596     timeout: float | httpx.Timeout | None | NotGiven = NOT_GIVEN,
    597 ) -> ChatCompletion | Stream[ChatCompletionChunk]:
--> 598     return self._post(
    599         ""/chat/completions"",
    600         body=maybe_transform(
    601             {
    602                 ""messages"": messages,
    603                 ""model"": model,
    604                 ""frequency_penalty"": frequency_penalty,
    605                 ""function_call"": function_call,
    606                 ""functions"": functions,
    607                 ""logit_bias"": logit_bias,
    608                 ""max_tokens"": max_tokens,
    609                 ""n"": n,
    610                 ""presence_penalty"": presence_penalty,
    611                 ""response_format"": response_format,
    612                 ""seed"": seed,
    613                 ""stop"": stop,
    614                 ""stream"": stream,
    615                 ""temperature"": temperature,
    616                 ""tool_choice"": tool_choice,
    617                 ""tools"": tools,
    618                 ""top_p"": top_p,
    619                 ""user"": user,
    620             },
    621             completion_create_params.CompletionCreateParams,
    622         ),
    623         options=make_request_options(
    624             extra_headers=extra_headers, extra_query=extra_query, extra_body=extra_body, timeout=timeout
    625         ),
    626         cast_to=ChatCompletion,
    627         stream=stream or False,
    628         stream_cls=Stream[ChatCompletionChunk],
    629     )

File ~/anaconda3/envs/llama/lib/python3.10/site-packages/openai/_base_client.py:1063, in SyncAPIClient.post(self, path, cast_to, body, options, files, stream, stream_cls)
   1049 def post(
   1050     self,
   1051     path: str,
   (...)
   1058     stream_cls: type[_StreamT] | None = None,
   1059 ) -> ResponseT | _StreamT:
   1060     opts = FinalRequestOptions.construct(
   1061         method=""post"", url=path, json_data=body, files=to_httpx_files(files), **options
   1062     )
-> 1063     return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))

File ~/anaconda3/envs/llama/lib/python3.10/site-packages/openai/_base_client.py:842, in SyncAPIClient.request(self, cast_to, options, remaining_retries, stream, stream_cls)
    833 def request(
    834     self,
    835     cast_to: Type[ResponseT],
   (...)
    840     stream_cls: type[_StreamT] | None = None,
    841 ) -> ResponseT | _StreamT:
--> 842     return self._request(
    843         cast_to=cast_to,
    844         options=options,
    845         stream=stream,
    846         stream_cls=stream_cls,
    847         remaining_retries=remaining_retries,
    848     )

File ~/anaconda3/envs/llama/lib/python3.10/site-packages/openai/_base_client.py:885, in SyncAPIClient._request(self, cast_to, options, remaining_retries, stream, stream_cls)
    882     # If the response is streamed then we need to explicitly read the response
    883     # to completion before attempting to access the response text.
    884     err.response.read()
--> 885     raise self._make_status_error_from_response(err.response) from None
    886 except httpx.TimeoutException as err:
    887     if retries > 0:

BadRequestError: Error code: 400 - {'error': {'message': ""Invalid parameter: 'response_format' of type 'json_object' is not supported with this model."", 'type': 'invalid_request_error', 'param': 'response_format', 'code': None}}
```

### To Reproduce

1. run the code block 
2. error

### Code snippets

```Python
completion = client.chat.completions.create(
    messages=[
        {
            ""role"": ""user"",
            ""content"": ""Can you generate an example json object describing a fruit?"",
        }
    ],
    model=""gpt-3.5-turbo"",
    response_format={""type"": ""json_object""},
)
```


### OS

22.04.1-Ubuntu

### Python version

3.10.13 

### Library version

 1.3.5",closed,2023-11-27T22:32:26Z,2023-11-28T04:53:24Z,[],https://github.com/openai/openai-python/issues/887,6.35,True,input_value_violation,high,"The response_format parameter provided is not supported by the specified model, leading to a BadRequestError.",high,
921,Anyio>4 support,"### Confirm this is a feature request for the Python library and not the underlying OpenAI API.

- [X] This is a feature request for the Python library

### Describe the feature or improvement you're requesting

The project depends on anyio>=3.5.0,<4, however I would like to use it in a project that requires anyio==4.1.0

### Additional context

https://pypi.org/project/anyio/#history
https://github.com/openai/openai-python/blob/e36956673d9049713c91bca6ce7aebe58638f483/pyproject.toml#L14",closed,2023-12-02T22:37:30Z,2023-12-04T10:38:08Z,['enhancement'],https://github.com/openai/openai-python/issues/921,36.01,True,missing_dependency_violation,unknown,The request to support anyio==4.1.0 indicates a missing dependency on the versioning constraints that may affect compatibility with the existing library.,medium,
284,Connection failed.,"### Describe the bug

Request timed out: HTTPSConnectionPool(host='api.openai.com', port=443): Max retries exceeded with url: /v1/completions (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x000002CE434DAC50>, 'Connection to api.openai.com timed out. (connect timeout=600)'))

### To Reproduce

Request timed out: HTTPSConnectionPool(host='api.openai.com', port=443): Max retries exceeded with url: /v1/completions (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x000002CE434DAC50>, 'Connection to api.openai.com timed out. (connect timeout=600)'))

### Code snippets

_No response_

### OS

win

### Python version

3.11

### Library version

last",closed,2023-03-08T07:59:33Z,2023-04-08T15:27:02Z,['bug'],https://github.com/openai/openai-python/issues/284,751.46,False,,,,,
896,tab completion in ipython crashes interpreter,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

When using tab completion on `openai` in interactive mode, my interpreter shows a big traceback and crashes ipython. 

```
Traceback (most recent call last):
  File ""/path/lib/python3.10/site-packages/IPython/core/completer.py"", line 3171, in _complete
    result = matcher(context)
  File ""/path/lib/python3.10/site-packages/IPython/core/completer.py"", line 2707, in custom_completer_matcher
    matches = self.dispatch_custom_completer(context.token) or []
  File ""/path/lib/python3.10/site-packages/IPython/core/completer.py"", line 2747, in dispatch_custom_completer
    res = c(event)
  File ""/path/lib/python3.10/site-packages/IPython/core/completerlib.py"", line 272, in module_completer
    return module_completion(event.line)
  File ""/path/lib/python3.10/site-packages/IPython/core/completerlib.py"", line 249, in module_completion
    completion_list = try_import('.'.join(mod[:-1]), True)
  File ""/path/lib/python3.10/site-packages/IPython/core/completerlib.py"", line 183, in try_import
    completions.extend( [attr for attr in dir(m) if
  File ""/path/lib/python3.10/site-packages/IPython/core/completerlib.py"", line 184, in <listcomp>
    is_importable(m, attr, only_modules)])
  File ""/path/lib/python3.10/site-packages/IPython/core/completerlib.py"", line 153, in is_importable
    return inspect.ismodule(getattr(module, attr))
  File ""/.pyenv/versions/3.10.8/lib/python3.10/inspect.py"", line 189, in ismodule
    return isinstance(object, types.ModuleType)
  File ""/path/lib/python3.10/site-packages/openai/_utils/_proxy.py"", line 39, in __class__
    return self.__get_proxied__().__class__
  File ""/path/lib/python3.10/site-packages/openai/_utils/_proxy.py"", line 43, in __get_proxied__
    return self.__load__()
  File ""/path/lib/python3.10/site-packages/openai/lib/_old_api.py"", line 33, in __load__
    raise APIRemovedInV1(symbol=self._symbol)
openai.lib._old_api.APIRemovedInV1: 

You tried to access openai.Audio, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.

You can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. 

Alternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`

A detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742
``` 

### To Reproduce

Open an interactive ipython session.

type `from openai` and press your tab key.

### Code snippets

_No response_

### OS

macOS

### Python version

v3.10.8

### Library version

openai v1.3.6",closed,2023-11-29T09:08:05Z,2023-12-03T02:41:16Z,['bug'],https://github.com/openai/openai-python/issues/896,89.55,True,input_value_violation,high,The attempt to access a removed API feature (openai.Audio) leads to a crash in the interpreter.,high,
506,ChatML update for th gpt-3.5-turbo-0613 and gpt-4,"### Describe the feature or improvement you're requesting

The chatml.md document said every message is represented as 

```
<|im_start|>{role}\n{text}<|im_end|>\n
```

That would add 5 more tokens beside the text itself. 

This is correct for the gpt-3.5-turbo-0301 model. As the gpt-3.5-turbo-0613 and gpt-4 models, it seems that every message is represneted as the following form

``` 
<|im_start|>{role}\n{text}<|im_end|>
```

For eample

```python
>>> openai.ChatCompletion.create(
... model=""gpt-3.5-turbo-0613"",
... messages=[{""role"":""user"", ""content"":""hello""}])
<OpenAIObject chat.completion id=chatcmpl-7WK2PVLogo1vxUXDAgApf9JWbXsET at 0x1da6e756210> JSON: {
  ""id"": ""chatcmpl-7WK2PVLogo1vxUXDAgApf9JWbXsET"",
  ""object"": ""chat.completion"",
  ""created"": 1687937877,
  ""model"": ""gpt-3.5-turbo-0613"",
  ""choices"": [
    {
      ""index"": 0,
      ""message"": {
        ""role"": ""assistant"",
        ""content"": ""Hello! How can I assist you today?""
      },
      ""finish_reason"": ""stop""
    }
  ],
  ""usage"": {
    ""prompt_tokens"": 8,
    ""completion_tokens"": 9,
    ""total_tokens"": 17
  }
}

>>> encoder.encode(""hello"")
[15339]
```

Accorsing to the response, the prompt_tokens is 8, if we follow the rule described in the chatml.md, it's represened as  

```
<|im_start|>user\nhello<|im_end|>\n<|im_start|>assistant\n
```

That would be 9 tokens. My guess is that there's no need for the '\n' after  the <|im_end|> token.

How can I make sure for that? Thanks.

### Additional context

_No response_",closed,2023-06-28T07:47:29Z,2023-12-02T21:38:23Z,[],https://github.com/openai/openai-python/issues/506,3781.85,True,input_value_violation,medium,"The discrepancy in token representation suggests that the input format may not adhere to the expected structure, potentially leading to incorrect token counting and response generation.",high,
827,Adding license classifier,"### Confirm this is a feature request for the Python library and not the underlying OpenAI API.

- [X] This is a feature request for the Python library

### Describe the feature or improvement you're requesting

See https://github.com/openai/openai-python/pull/826

It is important that Python packages have proper classifiers added. That way we can make use of PyPI's metadata to detect the license of the package easily.

### Additional context

_No response_",closed,2023-11-15T14:35:41Z,2023-12-02T09:13:37Z,[],https://github.com/openai/openai-python/issues/827,402.63,False,,,,,
913, base_url of OpenAI Client Instance Cannot Be Modified Due to Missing Update in Setter Method,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

I encountered an issue where the `base_url` of an OpenAI client instance couldn't be modified. This is due to the setter method of `base_url` not updating `self._base_url`.

### Possible Solution
I suggest modifying the setter method for base_url to ensure self._base_url is also updated. Additionally, incorporate handling for the trailing slash of the URL, as done in [the __init__ method](https://github.com/openai/openai-python/blob/17ac6779958b2b74999c634c4ea4c7b74906027a/src/openai/_base_client.py#L346C1-L346C69):
~~~python
@base_url.setter
def base_url(self, url: URL | str) -> None:
    url = url if isinstance(url, URL) else URL(url)
    url = self._enforce_trailing_slash(url)
    self._base_url = url
    self._client.base_url = url
~~~



### To Reproduce

1. Set up an OpenAI client instance with initial parameters including base_url.
2. Attempt to modifybase_url.
3. Observe that self._base_url within the client instance remains unchanged, indicating that the modification has not been applied.

![image](https://github.com/openai/openai-python/assets/11143269/d7260044-eff1-48a7-97a6-348c1ff437e0)

### Code snippets

```Python
import os

from openai import OpenAI

client = OpenAI(
    api_key=os.getenv(""OPENAI_API_KEY""),
    base_url=""https://api.openai.com/v1"",
)
print(client.base_url)
client.base_url = ""https://newurl.example.com/v1""
print(client.base_url)
```


### OS

Windows11

### Python version

Python 3.12.0

### Library version

1.3.6",closed,2023-12-01T06:57:12Z,2023-12-01T18:08:54Z,['bug'],https://github.com/openai/openai-python/issues/913,11.2,True,missing_dependency_violation,high,"The setter method for base_url fails to update the internal state of the client instance, leading to incorrect behavior when attempting to modify the base_url.",high,
903,Fix code example for Streaming section in Readme,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

There are a couple of errors in the examples provided for the streaming section.

1. First Issue   
This is the example code, the last line should be **print(chunk.choices[0].delta.content)**    
```python
from openai import OpenAI

client = OpenAI()

stream = client.chat.completions.create(
    model=""gpt-4"",
    messages=[{""role"": ""user"", ""content"": ""Say this is a test""}],
    stream=True,
)
for chunk in stream:
    if chunk.choices[0].delta.content is not None:
        print(part.choices[0].delta.content)
```
2. Second Issue    
This is the example code, the last line should be **print(chunk.choices[0].delta.content)**    
```python
from openai import AsyncOpenAI

client = AsyncOpenAI()

stream = await client.chat.completions.create(
    prompt=""Say this is a test"",
    messages=[{""role"": ""user"", ""content"": ""Say this is a test""}],
    stream=True,
)
async for chunk in stream:
    if chunk.choices[0].delta.content is not None:
        print(part.choices[0].delta.content)
```


### To Reproduce

If you run the snippet provided, there are **None** values as result
```python
from openai import OpenAI

client = OpenAI()

stream = client.chat.completions.create(
    model=""gpt-4"",
    messages=[{""role"": ""user"", ""content"": ""Say this is a test""}],
    stream=True,
)
for chunk in stream:
    if chunk.choices[0].delta.content is not None:
        print(part.choices[0].delta.content)+
```

Result:
None
None
None
None
None
None

### Code snippets

_No response_

### OS

linux

### Python version

Python 3.10.12

### Library version

openai v1.3.3",closed,2023-11-30T00:07:20Z,2023-11-30T02:24:58Z,['bug'],https://github.com/openai/openai-python/issues/903,2.29,False,,,,low,
895,Azure OpenAI methods for embeddings and chat completions don't have a deployment parameter,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

The Azure `AsyncAzureOpenAI` class has a single parameter to set the deployment name `azure_deployment`.

A common situation is to have an embeddings model deployment and a chat model deployment. In the calls for `embeddings.create()` and `chat.completions.create()` there is no parameter to set the deployment name.

What this means is you need to create multiple clients for each deployment you want to use. 

### To Reproduce

```python
openai_client = AsyncAzureOpenAI(
            api_version=""2023-07-01-preview"",
            azure_endpoint=f""https://{AZURE_OPENAI_SERVICE}.openai.azure.com"",
            azure_ad_token_provider=token_provider,
            organization=OPENAI_ORGANIZATION,
            azure_deployment='?', # A single parameter for the whole client?
        )

# No parameter for the chat deployment name?
await openai_client.chat.completions.create(
            model='gpt-35-turbo',
            messages=messages,
            temperature=0.0,
            max_tokens=100,
            n=1,
            functions=functions,
            function_call=""auto"",
        )
# No deployment name parameter?
await openai_client.embeddings.create(model=embedding_model, input=query_text)
```

Without the deployment name being correct, calls to the embeddings API give an error message

### Code snippets

_No response_

### OS

macOS

### Python version

Python 3.11.1

### Library version

1.3.6",closed,2023-11-29T05:59:21Z,2023-11-30T00:13:37Z,['bug'],https://github.com/openai/openai-python/issues/895,18.24,True,missing_dependency_violation,high,"The absence of a deployment parameter for embeddings and chat completions methods leads to incorrect API usage, requiring multiple clients for different deployments, which can cause errors.",high,
901,@rattrayalex  @RobertCraigie  - hey there - Keiji from Azure OpenAI here!,"@rattrayalex  @RobertCraigie  - hey there - Keiji from Azure OpenAI here! 

We have written a more extended Azure OpenAI migration guide in our documentation: https://aka.ms/oai/v1-python-migration

Would it be possible to add it to the main body of this Guide?

_Originally posted by @gojira in https://github.com/openai/openai-python/discussions/742#discussioncomment-7525272_",closed,2023-11-29T18:57:42Z,2023-11-29T19:09:20Z,[],https://github.com/openai/openai-python/issues/901,0.19,False,,,,,
888,Calling OpenAI async no longer works after latest update,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

Trying to use OpenAI asyncrhonously via aiohttp.  This worked in the last version of the API (like 2 or 3 weeks ago), why isn't it working now?

import openai
from aiohttp import ClientSession
self.session = ClientSession()
openai.aiosession.set(self.session)

Exception: AttributeError: module 'openai' has no attribute 'aiosession'


### To Reproduce

Run the code above

### Code snippets

```Python
import openai
from aiohttp import ClientSession
self.session = ClientSession()
openai.aiosession.set(self.session)
```


### OS

Windows 11

### Python version

Python 3.9

### Library version

openai v1.3.5",closed,2023-11-28T00:13:28Z,2023-11-28T00:35:22Z,[],https://github.com/openai/openai-python/issues/888,0.36,True,missing_dependency_violation,unknown,"The 'aiosession' attribute is missing from the 'openai' module, indicating a potential missing dependency or change in the API that was not documented.",high,
885,Assistant API tokens usage,"### Confirm this is a feature request for the Python library and not the underlying OpenAI API.

- [X] This is a feature request for the Python library

### Describe the feature or improvement you're requesting

There is not usage when retrieving run/message/assistant using AssistantAPI, not like ChatCompletion.
Wanted to know maybe there is a way to get the cost/usage or you are working on that.

Thank you very much! :)

### Additional context

_No response_",closed,2023-11-27T08:02:59Z,2023-11-27T14:22:46Z,[],https://github.com/openai/openai-python/issues/885,6.33,False,,,,,
882,Enhanced Chat History Search,"### Confirm this is a feature request for the Python library and not the underlying OpenAI API.

- [X] This is a feature request for the Python library

### Describe the feature or improvement you're requesting

Problem:

Currently, users are unable to search through their chat history, making it difficult to locate specific information. This necessitates scrolling through the entire conversation history, which can be time-consuming and inefficient.

Proposed Solution:

Implement a search functionality within the chat history, similar to that of WhatsApp. This feature would allow users to quickly and easily find the information they need by simply entering keywords or phrases.

Benefits:

Enhanced User Experience: The ability to search chat history would significantly improve the user experience by making it easier to navigate and locate specific information.

Improved Efficiency: By eliminating the need to manually scroll through entire chat histories, users would save time and effort, leading to increased efficiency.

Reduced Frustration: The ability to quickly find the desired information would reduce frustration and improve overall user satisfaction.

Implementation Details:

The search functionality should be integrated seamlessly into the existing chat interface. Users should be able to access the search bar from any point within the chat history. The search results should be displayed in a clear and organized manner, highlighting the most relevant matches.


If this seems a relevant feature, shall I give this a try?

### Additional context

_No response_",closed,2023-11-25T18:44:23Z,2023-11-25T22:01:33Z,[],https://github.com/openai/openai-python/issues/882,3.29,False,,,,,
879,[Security][PIP] [Github] Setup  dependabot in the repo,"### Confirm this is a feature request for the Python library and not the underlying OpenAI API.

- [X] This is a feature request for the Python library

### Describe the feature or improvement you're requesting

Setup Dependentbot in the repo, to have a daily scan for possible outdated dependencies, which represents a vulnerability risk. 
Assignee: @omonimus1
PR presented:

### Additional context

_No response_",closed,2023-11-25T16:38:02Z,2023-11-25T22:04:53Z,[],https://github.com/openai/openai-python/issues/879,5.45,False,,,,,
624,please add OPENAI_API_BASE to documentation,"### Describe the feature or improvement you're requesting

please add OPENAI_API_BASE ""https://api.openai.com/v1 to the readme

### Additional context

_No response_",closed,2023-09-26T11:01:01Z,2023-11-10T03:40:33Z,[],https://github.com/openai/openai-python/issues/624,1072.66,False,,,,,
881,CI pipeline is not running Unit tests,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

CI pipeline is not running Unit tests.

### To Reproduce

1 . Open a PULL request against the main branch. 
2. See that tests do not run => Is clear when observing the ci.yml file

### Code snippets

_No response_

### OS

ANY

### Python version

ANY

### Library version

ANY",closed,2023-11-25T16:50:57Z,2023-11-25T22:02:40Z,[],https://github.com/openai/openai-python/issues/881,5.2,False,,,,low,
848,403 error on post request,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

i have error 403 on my discord bot, using library 
i dont use VPN or proxy

```
2023-11-18 17:38:35.096 INFO: HTTP Request: POST https://api.openai.com/v1/chat/completions ""HTTP/1.1 403 Forbidden""
2023-11-18 17:38:35.098 ERROR: Ignoring exception in on_message
Traceback (most recent call last):
  File ""/usr/local/lib/python3.11/dist-packages/discord/client.py"", line 409, in _run_event
    await coro(*args, **kwargs)
  File ""/root/gpt-discord/gpt-discord.py"", line 85, in on_message
    async for part in await openai_client.chat.completions.create(model=os.environ[""GPT_MODEL""], messages=msgs, max_tokens=MAX_COMPLETION_TOKENS, stream=True):
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.11/dist-packages/openai/resources/chat/completions.py"", line 1191, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.11/dist-packages/openai/_base_client.py"", line 1480, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.11/dist-packages/openai/_base_client.py"", line 1275, in request
    return await self._request(
           ^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.11/dist-packages/openai/_base_client.py"", line 1318, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.PermissionDeniedError: <!DOCTYPE html>
<!--[if lt IE 7]> <html class=""no-js ie6 oldie"" lang=""en-US""> <![endif]-->
<!--[if IE 7]>    <html class=""no-js ie7 oldie"" lang=""en-US""> <![endif]-->
<!--[if IE 8]>    <html class=""no-js ie8 oldie"" lang=""en-US""> <![endif]-->
<!--[if gt IE 8]><!--> <html class=""no-js"" lang=""en-US""> <!--<![endif]-->
<head>
<title>Attention Required! | Cloudflare</title>
<meta charset=""UTF-8"" />
<meta http-equiv=""Content-Type"" content=""text/html; charset=UTF-8"" />
<meta http-equiv=""X-UA-Compatible"" content=""IE=Edge"" />
<meta name=""robots"" content=""noindex, nofollow"" />
<meta name=""viewport"" content=""width=device-width,initial-scale=1"" />
<link rel=""stylesheet"" id=""cf_styles-css"" href=""/cdn-cgi/styles/cf.errors.css"" />
<!--[if lt IE 9]><link rel=""stylesheet"" id='cf_styles-ie-css' href=""/cdn-cgi/styles/cf.errors.ie.css"" /><![endif]-->
<style>body{margin:0;padding:0}</style>


<!--[if gte IE 10]><!-->
<script>
  if (!navigator.cookieEnabled) {
    window.addEventListener('DOMContentLoaded', function () {
      var cookieEl = document.getElementById('cookie-alert');
      cookieEl.style.display = 'block';
    })
  }
</script>
<!--<![endif]-->


</head>
<body>
  <div id=""cf-wrapper"">
    <div class=""cf-alert cf-alert-error cf-cookie-error"" id=""cookie-alert"" data-translate=""enable_cookies"">Please enable cookies.</div>    <div id=""cf-error-details"" class=""cf-error-details-wrapper"">
      <div class=""cf-wrapper cf-header cf-error-overview"">
        <h1 data-translate=""block_headline"">Sorry, you have been blocked</h1>
        <h2 class=""cf-subheadline""><span data-translate=""unable_to_access"">You are unable to access</span> api.openai.com</h2>
      </div><!-- /.header -->

      <div class=""cf-section cf-highlight"">
        <div class=""cf-wrapper"">
          <div class=""cf-screenshot-container cf-screenshot-full"">
            
              <span class=""cf-no-screenshot error""></span>
            
          </div>
        </div>
      </div><!-- /.captcha-container -->

      <div class=""cf-section cf-wrapper"">
        <div class=""cf-columns two"">
          <div class=""cf-column"">
            <h2 data-translate=""blocked_why_headline"">Why have I been blocked?</h2>

            <p data-translate=""blocked_why_detail"">This website is using a security service to protect itself from online attacks. The action you just performed triggered the security solution. There are several actions that could trigger this block including submitting a certain word or phrase, a SQL command or malformed data.</p>
          </div>

          <div class=""cf-column"">
            <h2 data-translate=""blocked_resolve_headline"">What can I do to resolve this?</h2>

            <p data-translate=""blocked_resolve_detail"">You can email the site owner to let them know you were blocked. Please include what you were doing when this page came up and the Cloudflare Ray ID found at the bottom of this page.</p>
          </div>
        </div>
      </div><!-- /.section -->

      <div class=""cf-error-footer cf-wrapper w-240 lg:w-full py-10 sm:py-4 sm:px-8 mx-auto text-center sm:text-left border-solid border-0 border-t border-gray-300"">
  <p class=""text-13"">
    <span class=""cf-footer-item sm:block sm:mb-1"">Cloudflare Ray ID: <strong class=""font-semibold"">8280f53d0de016a3</strong></span>
    <span class=""cf-footer-separator sm:hidden"">&bull;</span>
    <span id=""cf-footer-item-ip"" class=""cf-footer-item hidden sm:block sm:mb-1"">
      Your IP:
      <button type=""button"" id=""cf-footer-ip-reveal"" class=""cf-footer-ip-reveal-btn"">Click to reveal</button>
      <span class=""hidden"" id=""cf-footer-ip"">194.87.252.3</span>
      <span class=""cf-footer-separator sm:hidden"">&bull;</span>
    </span>
    <span class=""cf-footer-item sm:block sm:mb-1""><span>Performance &amp; security by</span> <a rel=""noopener noreferrer"" href=""https://www.cloudflare.com/5xx-error-landing"" id=""brand_link"" target=""_blank"">Cloudflare</a></span>
    
  </p>
  <script>(function(){function d(){var b=a.getElementById(""cf-footer-item-ip""),c=a.getElementById(""cf-footer-ip-reveal"");b&&""classList""in b&&(b.classList.remove(""hidden""),c.addEventListener(""click"",function(){c.classList.add(""hidden"");a.getElementById(""cf-footer-ip"").classList.remove(""hidden"")}))}var a=document;document.addEventListener&&a.addEventListener(""DOMContentLoaded"",d)})();</script>
</div><!-- /.error-footer -->


    </div><!-- /#cf-error-details -->
  </div><!-- /#cf-wrapper -->

  <script>
  window._cf_translation = {};
  
  
</script>

</body>
</html>
```

### To Reproduce

1. start bot
2. send request (post)
3. i have a error

### Code snippets

_No response_

### OS

ubuntu 22.04

### Python version

3.11.6

### Library version

1.2.0",closed,2023-11-18T14:51:30Z,2023-11-18T23:40:19Z,[],https://github.com/openai/openai-python/issues/848,8.81,True,missing_dependency_violation,High,"The 403 Forbidden error indicates that the request was blocked due to missing authentication or permissions, suggesting a missing dependency on required API keys or user permissions for the OpenAI API.",high,
872,"In openai V0.27.0 version, there is a field called response_ms in the API call return, but it cannot be found after 1.0.0. Is there an alternative way to obtain the call duration?","### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

In openai V0.27.0 version, there is a field called response_ms in the API call return, but it cannot be found after 1.0.0. Is there an alternative way to obtain the call duration?

ERROR messageAttributeError: 'ChatCompletion' object has no attribute 'response_ms'


Thank you very much


### To Reproduce

    client = openai.AzureOpenAI(
        azure_endpoint=endpoint,
        api_key=api_key,
        api_version=""2023-09-01-preview""
    )

        completion = client.chat.completions.create(
            model=deployment,
            messages=messages,
            max_tokens=4096,
            temperature=0,
        )

    response_time = completion .response_ms


### Code snippets

```Python
client = openai.AzureOpenAI(
        azure_endpoint=endpoint,
        api_key=api_key,
        api_version=""2023-09-01-preview""
    )

        completion = client.chat.completions.create(
            model=deployment,
            messages=messages,
            max_tokens=4096,
            temperature=0,
        )

    response_time = completion .response_ms

ERROR messageAttributeError: 'ChatCompletion' object has no attribute 'response_ms'
```


### OS

win11

### Python version

Python v3.11.6

### Library version

openai v1.3.5",closed,2023-11-23T14:41:29Z,2023-11-23T17:09:04Z,['enhancement'],https://github.com/openai/openai-python/issues/872,2.46,True,missing_option_violation,medium,The API user is attempting to access a field 'response_ms' that was removed in version 1.0.0 without seeking an alternative method to obtain the call duration.,high,
825,use openai.proxies to set proxy doesn't work,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

before v1 I can use openai.proxy to set proxy, but after v1 I can't use it any more. At first I  thought it would be good again after change openai.proxy to openai.proxies, but not working either.

### To Reproduce

openai.proxies = {...proxy setting}

### Code snippets

_No response_

### OS

ubuntu20.04

### Python version

python 3.9.17

### Library version

1.2.4",closed,2023-11-15T13:09:28Z,2023-11-16T03:23:33Z,['bug'],https://github.com/openai/openai-python/issues/825,14.23,True,missing_option_violation,medium,The user is attempting to set a proxy using an outdated method (openai.proxy) and is not following the updated contract for proxy settings (openai.proxies).,high,
868,"Embeddings with openAI ""text-embedding-ada-002"" are not deterministic","### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

I'm not sure if this is an issue with the python library or part of the should be reported as part of the developer community.
Reproducing the embeddings from the same input produce different embeddings.

### To Reproduce

```
>>> from openai import OpenAI
>>> client = OpenAI(api_key=""..."")
>>> for i in range(50):
...     client.embeddings.create(model=""text-embedding-ada-002"", input=""hello"").data[0].embedding[:5]
... 
[-0.025058425962924957, -0.019388560205698013, -0.027781018987298012, -0.030979404225945473, -0.024688364937901497]
[-0.025058425962924957, -0.019388560205698013, -0.027781018987298012, -0.030979404225945473, -0.024688364937901497]
[-0.025058425962924957, -0.019388560205698013, -0.027781018987298012, -0.030979404225945473, -0.024688364937901497]
[-0.025058425962924957, -0.019388560205698013, -0.027781018987298012, -0.030979404225945473, -0.024688364937901497]
[-0.025058425962924957, -0.019388560205698013, -0.027781018987298012, -0.030979404225945473, -0.024688364937901497]
[-0.025058425962924957, -0.019388560205698013, -0.027781018987298012, -0.030979404225945473, -0.024688364937901497]
[-0.025058425962924957, -0.019388560205698013, -0.027781018987298012, -0.030979404225945473, -0.024688364937901497]
[-0.025058425962924957, -0.019388560205698013, -0.027781018987298012, -0.030979404225945473, -0.024688364937901497]
[-0.025058425962924957, -0.019388560205698013, -0.027781018987298012, -0.030979404225945473, -0.024688364937901497]
[-0.025058425962924957, -0.019388560205698013, -0.027781018987298012, -0.030979404225945473, -0.024688364937901497]
[-0.025058425962924957, -0.019388560205698013, -0.027781018987298012, -0.030979404225945473, -0.024688364937901497]
[-0.025058425962924957, -0.019388560205698013, -0.027781018987298012, -0.030979404225945473, -0.024688364937901497]
[-0.025058425962924957, -0.019388560205698013, -0.027781018987298012, -0.030979404225945473, -0.024688364937901497]
[-0.024994507431983948, -0.019366780295968056, -0.027768738567829132, -0.031097816303372383, -0.02462460845708847]
[-0.025058425962924957, -0.019388560205698013, -0.027781018987298012, -0.030979404225945473, -0.024688364937901497]
[-0.0250103659927845, -0.01939525455236435, -0.027798103168606758, -0.030995413661003113, -0.024706488475203514]
[-0.025058425962924957, -0.019388560205698013, -0.027781018987298012, -0.030979404225945473, -0.024688364937901497]
[-0.025058425962924957, -0.019388560205698013, -0.027781018987298012, -0.030979404225945473, -0.024688364937901497]
[-0.025058425962924957, -0.019388560205698013, -0.027781018987298012, -0.030979404225945473, -0.024688364937901497]
[-0.025058425962924957, -0.019388560205698013, -0.027781018987298012, -0.030979404225945473, -0.024688364937901497]
[-0.025058425962924957, -0.019388560205698013, -0.027781018987298012, -0.030979404225945473, -0.024688364937901497]
[-0.0250103659927845, -0.01939525455236435, -0.027798103168606758, -0.030995413661003113, -0.024706488475203514]
[-0.025058425962924957, -0.019388560205698013, -0.027781018987298012, -0.030979404225945473, -0.024688364937901497]
[-0.025058425962924957, -0.019388560205698013, -0.027781018987298012, -0.030979404225945473, -0.024688364937901497]
[-0.025058425962924957, -0.019388560205698013, -0.027781018987298012, -0.030979404225945473, -0.024688364937901497]
[-0.025058425962924957, -0.019388560205698013, -0.027781018987298012, -0.030979404225945473, -0.024688364937901497]
[-0.025058425962924957, -0.019388560205698013, -0.027781018987298012, -0.030979404225945473, -0.024688364937901497]
[-0.025048330426216125, -0.019377758726477623, -0.027810918167233467, -0.0310361385345459, -0.02466500550508499]
[-0.025058425962924957, -0.019388560205698013, -0.027781018987298012, -0.030979404225945473, -0.024688364937901497]
[-0.025058425962924957, -0.019388560205698013, -0.027781018987298012, -0.030979404225945473, -0.024688364937901497]
[-0.025058425962924957, -0.019388560205698013, -0.027781018987298012, -0.030979404225945473, -0.024688364937901497]
[-0.025058425962924957, -0.019388560205698013, -0.027781018987298012, -0.030979404225945473, -0.024688364937901497]
[-0.025058425962924957, -0.019388560205698013, -0.027781018987298012, -0.030979404225945473, -0.024688364937901497]
[-0.025058425962924957, -0.019388560205698013, -0.027781018987298012, -0.030979404225945473, -0.024688364937901497]
[-0.025058425962924957, -0.019388560205698013, -0.027781018987298012, -0.030979404225945473, -0.024688364937901497]
[-0.025058425962924957, -0.019388560205698013, -0.027781018987298012, -0.030979404225945473, -0.024688364937901497]
[-0.025058425962924957, -0.019388560205698013, -0.027781018987298012, -0.030979404225945473, -0.024688364937901497]
[-0.025058425962924957, -0.019388560205698013, -0.027781018987298012, -0.030979404225945473, -0.024688364937901497]
[-0.025058425962924957, -0.019388560205698013, -0.027781018987298012, -0.030979404225945473, -0.024688364937901497]
[-0.025058425962924957, -0.019388560205698013, -0.027781018987298012, -0.030979404225945473, -0.024688364937901497]
[-0.025058425962924957, -0.019388560205698013, -0.027781018987298012, -0.030979404225945473, -0.024688364937901497]
[-0.02502652257680893, -0.019331468269228935, -0.027801373973488808, -0.031051915138959885, -0.02469618245959282]
[-0.025058425962924957, -0.019388560205698013, -0.027781018987298012, -0.030979404225945473, -0.024688364937901497]
[-0.025058425962924957, -0.019388560205698013, -0.027781018987298012, -0.030979404225945473, -0.024688364937901497]
[-0.025058425962924957, -0.019388560205698013, -0.027781018987298012, -0.030979404225945473, -0.024688364937901497]
[-0.025058425962924957, -0.019388560205698013, -0.027781018987298012, -0.030979404225945473, -0.024688364937901497]
[-0.025058425962924957, -0.019388560205698013, -0.027781018987298012, -0.030979404225945473, -0.024688364937901497]
[-0.025058425962924957, -0.019388560205698013, -0.027781018987298012, -0.030979404225945473, -0.024688364937901497]
[-0.025058425962924957, -0.019388560205698013, -0.027781018987298012, -0.030979404225945473, -0.024688364937901497]
[-0.025058425962924957, -0.019388560205698013, -0.027781018987298012, -0.030979404225945473, -0.024688364937901497]
```

### Code snippets

_No response_

### OS

MacOS

### Python version

Python 3.10.10

### Library version

openai v1.3.5",closed,2023-11-23T04:44:53Z,2023-11-23T20:39:19Z,['bug'],https://github.com/openai/openai-python/issues/868,15.91,True,incorrect_functionality,High,"The embeddings generated from the same input are not deterministic, leading to inconsistent outputs.",high,
870,Pylint complains when using Stream[ChatCompletionChunk]: E1133 not-an-iterable,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

I´m constructing a chat completion response:
```
query_response: Stream[ChatCompletionChunk] = client.chat.completions.create(
            ...
            stream=True,
        )
```
Then, when iterating over the chunks, it works but Pylint raises an error:
```
        for chunk in query_response:
            choice = chunk.choices[0]
```
Error:
**Non-iterable value query_response is used in an iterating context**


### To Reproduce

1. Create a chat completion with streaming enabled
2. Read chunks by iterating over the Stream
3. Run pylint

### Code snippets

_No response_

### OS

macOS

### Python version

3.11.5

### Library version

1.3.5",closed,2023-11-23T10:40:43Z,2023-11-23T11:47:54Z,['question'],https://github.com/openai/openai-python/issues/870,1.12,True,input_type_violation,medium,"The API expects an iterable for the 'query_response', but Pylint indicates that it is not recognized as such, leading to potential runtime errors.",high,
69,"OpenAI fine-tune shows ""That model doesn't exist""","I have already trained a fine-tuned model. I am able to retrieve the details of the model using Open AI API. However, when I am using it in a Completion.create() it gives ""This model doesn't exist"".

Any reason as to why this happened ?

I have already been charged for the fine-tune training process. It also shows up on my account.",closed,2022-01-29T19:33:22Z,2022-01-29T20:47:46Z,[],https://github.com/openai/openai-python/issues/69,1.24,True,missing_dependency_violation,High,"The issue indicates that the Completion.create() method is unable to recognize the fine-tuned model, suggesting a missing dependency between the model training and its subsequent usage in the API.",high,
863,"Why, my msg can be used, but the tools case cannot be passed","### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

(openai) /mnt/workspace/project/Eagent> python 1.py 
Traceback (most recent call last):
  File ""/mnt/workspace/project/Eagent/1.py"", line 94, in <module>
    print(run_conversation())
  File ""/mnt/workspace/project/Eagent/1.py"", line 55, in run_conversation
    response = client.chat.completions.create(
  File ""/home/pai/lib/python3.9/site-packages/openai/_utils/_utils.py"", line 299, in wrapper
    return func(*args, **kwargs)
  File ""/home/pai/lib/python3.9/site-packages/openai/resources/chat/completions.py"", line 598, in create
    return self._post(
  File ""/home/pai/lib/python3.9/site-packages/openai/_base_client.py"", line 1063, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File ""/home/pai/lib/python3.9/site-packages/openai/_base_client.py"", line 842, in request
    return self._request(
  File ""/home/pai/lib/python3.9/site-packages/openai/_base_client.py"", line 885, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.NotFoundError: Error code: 404 - {'error': {'message': 'Unrecognized request argument supplied: tools (request id: 202311220623247350366364AkoGmo5)', 'type': 'invalid_request_error', 'param': '', 'code': None}}

### To Reproduce

Fill in ak and run the code

### Code snippets

```Python
from openai import OpenAI
from openai import api_version
import json
import os 
api_base = ""https://vp01.glyph.cyou:8443/v1""
api_key = ""xxxx""
model_name = ""gpt-3.5-turbo-1106""
# model_name = ""gpt-4""
# gpt-3.5-turbo、 gpt-3.5-turbo-16k、gpt-4、gpt-4-32k、gpt-3.5-turbo-1106、gpt-4-1106-preview

os.environ[""OPENAI_API_BASE""] = api_base
os.environ[""OPENAI_API_KEY""] = api_key
client = OpenAI(
    api_key=api_key,
    base_url=api_base
)
# client = OpenAI()

# Example dummy function hard coded to return the same weather
# In production, this could be your backend API or an external API
def get_current_weather(location, unit=""fahrenheit""):
    """"""Get the current weather in a given location""""""
    if ""tokyo"" in location.lower():
        return json.dumps({""location"": ""Tokyo"", ""temperature"": ""10"", ""unit"": ""celsius""})
    elif ""san francisco"" in location.lower():
        return json.dumps({""location"": ""San Francisco"", ""temperature"": ""72"", ""unit"": ""fahrenheit""})
    elif ""paris"" in location.lower():
        return json.dumps({""location"": ""Paris"", ""temperature"": ""22"", ""unit"": ""celsius""})
    else:
        return json.dumps({""location"": location, ""temperature"": ""unknown""})

def run_conversation():
    # Step 1: send the conversation and available functions to the model
    messages = [{""role"": ""user"", ""content"": ""What's the weather like in San Francisco, Tokyo, and Paris?""}]
    tools = [
        {
            ""type"": ""function"",
            ""function"": {
                ""name"": ""get_current_weather"",
                ""description"": ""Get the current weather in a given location"",
                ""parameters"": {
                    ""type"": ""object"",
                    ""properties"": {
                        ""location"": {
                            ""type"": ""string"",
                            ""description"": ""The city and state, e.g. San Francisco, CA"",
                        },
                        ""unit"": {""type"": ""string"", ""enum"": [""celsius"", ""fahrenheit""]},
                    },
                    ""required"": [""location""],
                },
            },
        }
    ]
    response = client.chat.completions.create(
        model=model_name,
        messages=messages,
        tools=tools,
        # tool_choice=""auto"",  # auto is default, but we'll be explicit
    )
    response_message = response.choices[0].message
    print(response_message.model_dump_json(indent=4))
    tool_calls = response_message.tool_calls
    # Step 2: check if the model wanted to call a function
    if tool_calls:
        # Step 3: call the function
        # Note: the JSON response may not always be valid; be sure to handle errors
        available_functions = {
            ""get_current_weather"": get_current_weather,
        }  # only one function in this example, but you can have multiple
        messages.append(response_message)  # extend conversation with assistant's reply
        # Step 4: send the info for each function call and function response to the model
        for tool_call in tool_calls:
            function_name = tool_call.function.name
            function_to_call = available_functions[function_name]
            function_args = json.loads(tool_call.function.arguments)
            function_response = function_to_call(
                location=function_args.get(""location""),
                unit=function_args.get(""unit""),
            )
            messages.append(
                {
                    ""tool_call_id"": tool_call.id,
                    ""role"": ""tool"",
                    ""name"": function_name,
                    ""content"": function_response,
                }
            )  # extend conversation with function response
        second_response = client.chat.completions.create(
            model=""gpt-3.5-turbo-1106"",
            messages=messages,
        )  # get a new response from the model where it can see the function response
        return second_response
print(run_conversation())
```


### OS

macos

### Python version

v3.10

### Library version

openai v1.34.",closed,2023-11-22T06:26:12Z,2023-11-22T14:49:10Z,['API-feedback'],https://github.com/openai/openai-python/issues/863,8.38,False,,,,low,
645,"Cannot install openai because ""Failed building wheel for aiohttp""","### Describe the bug

I yesterday freshly install Python, and now i want to install openai. The installation always fails at building the wheel for aiohttp. Im not  a big programmer, so i dont know what is wrong here (I did remove my user name for this post)

### To Reproduce

1. Install Python (newest version)
2. Install openai

Expected: Succesfully Installing openai

### Code snippets

```Python
pip3.12 install --upgrade openai
Collecting openai
  Obtaining dependency information for openai from https://files.pythonhosted.org/packages/1e/9f/385c25502f437686e4aa715969e5eaf5c2cb5e5ffa7c5cdd52f3c6ae967a/openai-0.28.1-py3-none-any.whl.metadata
  Using cached openai-0.28.1-py3-none-any.whl.metadata (11 kB)
Requirement already satisfied: requests>=2.20 in c:\users\\appdata\local\programs\python\python312\lib\site-packages (from openai) (2.31.0)
Collecting tqdm (from openai)
  Obtaining dependency information for tqdm from https://files.pythonhosted.org/packages/00/e5/f12a80907d0884e6dff9c16d0c0114d81b8cd07dc3ae54c5e962cc83037e/tqdm-4.66.1-py3-none-any.whl.metadata
  Using cached tqdm-4.66.1-py3-none-any.whl.metadata (57 kB)
Collecting aiohttp (from openai)
  Using cached aiohttp-3.8.6.tar.gz (7.4 MB)
  Installing build dependencies ... done
  Getting requirements to build wheel ... done
  Installing backend dependencies ... done
  Preparing metadata (pyproject.toml) ... done
Requirement already satisfied: charset-normalizer<4,>=2 in c:\users\\appdata\local\programs\python\python312\lib\site-packages (from requests>=2.20->openai) (3.3.0)
Requirement already satisfied: idna<4,>=2.5 in c:\users\\appdata\local\programs\python\python312\lib\site-packages (from requests>=2.20->openai) (3.4)
Requirement already satisfied: urllib3<3,>=1.21.1 in c:\users\\appdata\local\programs\python\python312\lib\site-packages (from requests>=2.20->openai) (2.0.6)
Requirement already satisfied: certifi>=2017.4.17 in c:\users\\appdata\local\programs\python\python312\lib\site-packages (from requests>=2.20->openai) (2023.7.22)
Collecting attrs>=17.3.0 (from aiohttp->openai)
  Using cached attrs-23.1.0-py3-none-any.whl (61 kB)
Collecting multidict<7.0,>=4.5 (from aiohttp->openai)
  Using cached multidict-6.0.4-cp312-cp312-win_amd64.whl
Collecting async-timeout<5.0,>=4.0.0a3 (from aiohttp->openai)
  Obtaining dependency information for async-timeout<5.0,>=4.0.0a3 from https://files.pythonhosted.org/packages/a7/fa/e01228c2938de91d47b307831c62ab9e4001e747789d0b05baf779a6488c/async_timeout-4.0.3-py3-none-any.whl.metadata 
  Using cached async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)
Collecting yarl<2.0,>=1.0 (from aiohttp->openai)
  Using cached yarl-1.9.2-cp312-cp312-win_amd64.whl
Collecting frozenlist>=1.1.1 (from aiohttp->openai)
  Using cached frozenlist-1.4.0-cp312-cp312-win_amd64.whl
Collecting aiosignal>=1.1.2 (from aiohttp->openai)
  Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)
Requirement already satisfied: colorama in c:\users\\appdata\local\programs\python\python312\lib\site-packages (from tqdm->openai) (0.4.6)
Using cached openai-0.28.1-py3-none-any.whl (76 kB)
Using cached tqdm-4.66.1-py3-none-any.whl (78 kB)
Using cached async_timeout-4.0.3-py3-none-any.whl (5.7 kB)
Building wheels for collected packages: aiohttp
  Building wheel for aiohttp (pyproject.toml) ... error
  error: subprocess-exited-with-error

  × Building wheel for aiohttp (pyproject.toml) did not run successfully.
  │ exit code: 1
  ╰─> [110 lines of output]
      *********************
      * Accelerated build *
      *********************
      running bdist_wheel
      running build
      running build_py
      creating build
      creating build\lib.win-amd64-cpython-312
      creating build\lib.win-amd64-cpython-312\aiohttp
      copying aiohttp\abc.py -> build\lib.win-amd64-cpython-312\aiohttp
      copying aiohttp\base_protocol.py -> build\lib.win-amd64-cpython-312\aiohttp
      copying aiohttp\client.py -> build\lib.win-amd64-cpython-312\aiohttp
      copying aiohttp\client_exceptions.py -> build\lib.win-amd64-cpython-312\aiohttp
      copying aiohttp\client_proto.py -> build\lib.win-amd64-cpython-312\aiohttp
      copying aiohttp\client_reqrep.py -> build\lib.win-amd64-cpython-312\aiohttp
      copying aiohttp\client_ws.py -> build\lib.win-amd64-cpython-312\aiohttp
      copying aiohttp\connector.py -> build\lib.win-amd64-cpython-312\aiohttp
      copying aiohttp\cookiejar.py -> build\lib.win-amd64-cpython-312\aiohttp
      copying aiohttp\formdata.py -> build\lib.win-amd64-cpython-312\aiohttp
      copying aiohttp\hdrs.py -> build\lib.win-amd64-cpython-312\aiohttp
      copying aiohttp\helpers.py -> build\lib.win-amd64-cpython-312\aiohttp
      copying aiohttp\http.py -> build\lib.win-amd64-cpython-312\aiohttp
      copying aiohttp\http_exceptions.py -> build\lib.win-amd64-cpython-312\aiohttp
      copying aiohttp\http_parser.py -> build\lib.win-amd64-cpython-312\aiohttp
      copying aiohttp\http_websocket.py -> build\lib.win-amd64-cpython-312\aiohttp
      copying aiohttp\http_writer.py -> build\lib.win-amd64-cpython-312\aiohttp
      copying aiohttp\locks.py -> build\lib.win-amd64-cpython-312\aiohttp
      copying aiohttp\log.py -> build\lib.win-amd64-cpython-312\aiohttp
      copying aiohttp\multipart.py -> build\lib.win-amd64-cpython-312\aiohttp
      copying aiohttp\payload.py -> build\lib.win-amd64-cpython-312\aiohttp
      copying aiohttp\payload_streamer.py -> build\lib.win-amd64-cpython-312\aiohttp
      copying aiohttp\pytest_plugin.py -> build\lib.win-amd64-cpython-312\aiohttp
      copying aiohttp\resolver.py -> build\lib.win-amd64-cpython-312\aiohttp
      copying aiohttp\streams.py -> build\lib.win-amd64-cpython-312\aiohttp
      copying aiohttp\tcp_helpers.py -> build\lib.win-amd64-cpython-312\aiohttp
      copying aiohttp\test_utils.py -> build\lib.win-amd64-cpython-312\aiohttp
      copying aiohttp\tracing.py -> build\lib.win-amd64-cpython-312\aiohttp
      copying aiohttp\typedefs.py -> build\lib.win-amd64-cpython-312\aiohttp
      copying aiohttp\web.py -> build\lib.win-amd64-cpython-312\aiohttp
      copying aiohttp\web_app.py -> build\lib.win-amd64-cpython-312\aiohttp
      copying aiohttp\web_exceptions.py -> build\lib.win-amd64-cpython-312\aiohttp
      copying aiohttp\web_fileresponse.py -> build\lib.win-amd64-cpython-312\aiohttp
      copying aiohttp\web_log.py -> build\lib.win-amd64-cpython-312\aiohttp
      copying aiohttp\web_middlewares.py -> build\lib.win-amd64-cpython-312\aiohttp
      copying aiohttp\web_protocol.py -> build\lib.win-amd64-cpython-312\aiohttp
      copying aiohttp\web_request.py -> build\lib.win-amd64-cpython-312\aiohttp
      copying aiohttp\web_response.py -> build\lib.win-amd64-cpython-312\aiohttp
      copying aiohttp\web_routedef.py -> build\lib.win-amd64-cpython-312\aiohttp
      copying aiohttp\web_runner.py -> build\lib.win-amd64-cpython-312\aiohttp
      copying aiohttp\web_server.py -> build\lib.win-amd64-cpython-312\aiohttp
      copying aiohttp\web_urldispatcher.py -> build\lib.win-amd64-cpython-312\aiohttp
      copying aiohttp\web_ws.py -> build\lib.win-amd64-cpython-312\aiohttp
      copying aiohttp\worker.py -> build\lib.win-amd64-cpython-312\aiohttp
      copying aiohttp\__init__.py -> build\lib.win-amd64-cpython-312\aiohttp
      running egg_info
      writing aiohttp.egg-info\PKG-INFO
      writing dependency_links to aiohttp.egg-info\dependency_links.txt
      writing requirements to aiohttp.egg-info\requires.txt
      writing top-level names to aiohttp.egg-info\top_level.txt
      reading manifest file 'aiohttp.egg-info\SOURCES.txt'
      reading manifest template 'MANIFEST.in'
      warning: no files found matching 'aiohttp' anywhere in distribution
      warning: no previously-included files matching '*.pyc' found anywhere in distribution
      warning: no previously-included files matching '*.pyd' found anywhere in distribution
      warning: no previously-included files matching '*.so' found anywhere in distribution
      warning: no previously-included files matching '*.lib' found anywhere in distribution
      warning: no previously-included files matching '*.dll' found anywhere in distribution
      warning: no previously-included files matching '*.a' found anywhere in distribution
      warning: no previously-included files matching '*.obj' found anywhere in distribution
      warning: no previously-included files found matching 'aiohttp\*.html'
      no previously-included directories found matching 'docs\_build'
      adding license file 'LICENSE.txt'
      writing manifest file 'aiohttp.egg-info\SOURCES.txt'
      copying aiohttp\_cparser.pxd -> build\lib.win-amd64-cpython-312\aiohttp
      copying aiohttp\_find_header.pxd -> build\lib.win-amd64-cpython-312\aiohttp
      copying aiohttp\_headers.pxi -> build\lib.win-amd64-cpython-312\aiohttp
      copying aiohttp\_helpers.pyi -> build\lib.win-amd64-cpython-312\aiohttp
      copying aiohttp\_helpers.pyx -> build\lib.win-amd64-cpython-312\aiohttp
      copying aiohttp\_http_parser.pyx -> build\lib.win-amd64-cpython-312\aiohttp
      copying aiohttp\_http_writer.pyx -> build\lib.win-amd64-cpython-312\aiohttp
      copying aiohttp\_websocket.pyx -> build\lib.win-amd64-cpython-312\aiohttp
      copying aiohttp\py.typed -> build\lib.win-amd64-cpython-312\aiohttp
      creating build\lib.win-amd64-cpython-312\aiohttp\.hash
      copying aiohttp\.hash\_cparser.pxd.hash -> build\lib.win-amd64-cpython-312\aiohttp\.hash
      copying aiohttp\.hash\_find_header.pxd.hash -> build\lib.win-amd64-cpython-312\aiohttp\.hash
      copying aiohttp\.hash\_helpers.pyi.hash -> build\lib.win-amd64-cpython-312\aiohttp\.hash
      copying aiohttp\.hash\_helpers.pyx.hash -> build\lib.win-amd64-cpython-312\aiohttp\.hash
      copying aiohttp\.hash\_http_parser.pyx.hash -> build\lib.win-amd64-cpython-312\aiohttp\.hash
      copying aiohttp\.hash\_http_writer.pyx.hash -> build\lib.win-amd64-cpython-312\aiohttp\.hash
      copying aiohttp\.hash\_websocket.pyx.hash -> build\lib.win-amd64-cpython-312\aiohttp\.hash
      copying aiohttp\.hash\hdrs.py.hash -> build\lib.win-amd64-cpython-312\aiohttp\.hash
      running build_ext
      building 'aiohttp._websocket' extension
      creating build\temp.win-amd64-cpython-312
      creating build\temp.win-amd64-cpython-312\Release
      creating build\temp.win-amd64-cpython-312\Release\aiohttp
      ""C:\Program Files (x86)\Microsoft Visual Studio\2022\BuildTools\VC\Tools\MSVC\14.37.32822\bin\HostX86\x64\cl.exe"" /c /nologo /O2 /W3 /GL /DNDEBUG /MD -IC:\Users\\AppData\Local\Programs\Python\Python312\include -IC:\Users\\AppData\Local\Programs\Python\Python312\Include ""-IC:\Program Files (x86)\Microsoft Visual Studio\2022\BuildTools\VC\Tools\MSVC\14.37.32822\include"" ""-IC:\Program Files (x86)\Microsoft Visual Studio\2022\BuildTools\VC\Auxiliary\VS\include"" ""-IC:\Program Files (x86)\Windows Kits\10\include\10.0.22621.0\ucrt"" ""-IC:\Program Files (x86)\Windows Kits\10\\include\10.0.22621.0\\um"" ""-IC:\Program Files (x86)\Windows Kits\10\\include\10.0.22621.0\\shared"" ""-IC:\Program Files (x86)\Windows Kits\10\\include\10.0.22621.0\\winrt"" ""-IC:\Program Files (x86)\Windows Kits\10\\include\10.0.22621.0\\cppwinrt"" /Tcaiohttp/_websocket.c /Fobuild\temp.win-amd64-cpython-312\Release\aiohttp/_websocket.obj
      _websocket.c
      aiohttp/_websocket.c(1475): warning C4996: 'Py_OptimizeFlag': deprecated in 3.12
      aiohttp/_websocket.c(3042): error C2039: 'ob_digit': is not a member of '_longobject'
      C:\Users\\AppData\Local\Programs\Python\Python312\include\cpython/longintrepr.h(87): note: see declaration of '_longobject'
      aiohttp/_websocket.c(3097): error C2039: 'ob_digit': is not a member of '_longobject'
      C:\Users\\AppData\Local\Programs\Python\Python312\include\cpython/longintrepr.h(87): note: see declaration of '_longobject'
      aiohttp/_websocket.c(3238): error C2039: 'ob_digit': is not a member of '_longobject'
      C:\Users\\AppData\Local\Programs\Python\Python312\include\cpython/longintrepr.h(87): note: see declaration of '_longobject'
      aiohttp/_websocket.c(3293): error C2039: 'ob_digit': is not a member of '_longobject'
      C:\Users\\AppData\Local\Programs\Python\Python312\include\cpython/longintrepr.h(87): note: see declaration of '_longobject'
      aiohttp/_websocket.c(3744): error C2039: 'ob_digit': is not a member of '_longobject'
      C:\Users\\AppData\Local\Programs\Python\Python312\include\cpython/longintrepr.h(87): note: see declaration of '_longobject'
      error: command 'C:\\Program Files (x86)\\Microsoft Visual Studio\\2022\\BuildTools\\VC\\Tools\\MSVC\\14.37.32822\\bin\\HostX86\\x64\\cl.exe' failed with exit code 2
      [end of output]

  note: This error originates from a subprocess, and is likely not a problem with pip.
  ERROR: Failed building wheel for aiohttp
Failed to build aiohttp
ERROR: Could not build wheels for aiohttp, which is required to install pyproject.toml-based projects
```


### OS

Windows 11

### Python version

Python v3.12

### Library version

Newest",closed,2023-10-10T14:46:10Z,2023-10-11T11:11:13Z,['fixed in v1'],https://github.com/openai/openai-python/issues/645,20.42,True,missing_dependency_violation,High,"The installation process fails due to a missing or incompatible dependency (aiohttp) required for the openai library, which prevents successful installation.",high,
857,"json format causes infinite ""\n                       \n                       \n                       \n"" in response","### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

I am trying to use json format to get json response, it worked well when I give same short example, but when I use production data to test it, prompt_token = 2966, then it start to response with all ""\n                       \n                       \n                       \n"", till max token. Then, I use the same prompt without response_format, it worked well though it's not a json object. 

(test with a another shorter user message still failed, is that the ""#"" matter?)

### To Reproduce

```
tsg = """"""
# TSG for debugging openai
## 1. install openai
### 1.1. install openai with pip
### 1.2. install openai with conda
## 2. import openai
### 2.1. import openai in python
### 2.2. import openai in jupyter notebook
## 3. use openai
### 3.1. use openai in python
### 3.2. use openai in jupyter notebook
""""""
messages = [
                {
                    ""role"": ""system"",
                    ""content"": 
                        """"""
                        You are an expert of reading troubeshooting guidance.
                        The users will provide you a troubleshooting guide in markdown format, which consists of several steps.
                        You need to break down the document into steps based on the text semantics and markdown structure and return JSON format
                        Note that: (1) Ignore the document title if it does not indicate a step. (2) Only do text slicing from front to back, can't lose any content of the step. (3) Maintain the original text in task_description without any summarization or abbreviation. (4) Don't lose the prefix and serial number of the title displayed in the document. (5) If the step itself has a title in document, the task_title should use the original content.
                        You will respond with the list of steps as a JSON object. Here's an example of your output format: 
                        [{
                            ""task_title"": """",
                            ""task_description"": """",
                        },
                        {
                            ""task_title"": """",
                            ""task_description"": """",
                        }].
                        Here is an example of the input markdown document:
                            # Troubleshooting guide for buying a puppy
                            ## 1. know what puppy you want
                            ### 1.1. you could surf the internet to find the puppy you want
                            ### 1.2. visit friends who have puppies to see if you like them
                            ## 2. buy healthy puppies
                            ### 2.1. you could go to puppy selling websites to find healthy puppies, if you prefer buying puppies online, please go to step 3 for more information
                            ### 2.2. you could go to pet stores to find healthy puppies
                            ## 3. buy puppies online
                            here is a list of puppy selling websites: www.happydog.com, www.puppy.com, www.puppylove.com
                        Here is an example of the output json object:
                        [{
                            ""task_title"": ""1. know what puppy you want"",
                            ""task_description"": ""### 1.1. you could surf the internet to find the puppy you want\n### 1.2. visit friends who have puppies to see if you like them""
                        },
                        {
                            ""task_title"": ""2. buy healthy puppies"",
                            ""task_description"": ""### 2.1. you could go to puppy selling websites to find healthy puppies, if you prefer buying puppies online, please go to step 3 for more information\n### 2.2. you could go to pet stores to find healthy puppies""
                        },
                        {
                            ""task_title"": ""3. buy puppies online"",
                            ""task_description"": ""here is a list of puppy selling websites: www.happydog.com, www.puppy.com, www.puppylove.com""
                        }
                        ]
                        """"""
                },
                {
                    ""role"": ""user"", 
                    ""content"": tsg
                }
            ]

response = llm.client.chat.completions.create(
            model = llm.engine,
            messages = messages,
            response_format={""type"": ""json_object""},
            temperature = 0,
        )
```

### Code snippets

_No response_

### OS

win11

### Python version

3.10.12

### Library version

1.3.3",closed,2023-11-21T13:43:05Z,2023-11-21T14:11:14Z,['bug'],https://github.com/openai/openai-python/issues/857,0.47,True,input_value_violation,performance_degradation,"The prompt_token value of 2966 exceeds the expected token range, causing the API to return excessive newline characters in the response.",high,
861,New bug in Thread,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

I had great code working last night (Nov 20, 2023). It had been working well for a week. Around 11:00 pm it stopped returning the expected result and would only return the prompt I typed in.  I then ran your example: examples/assistant.py

I was using openai==1.3.2 when the error appeared.  I upgraded to openai==1.3.4, but this did not resolve anything.

Your code in examples/assistant.py will now not complete.  Openai never completes. Please advise and let us know what has changed in the last 12 hours.

### To Reproduce

python assistant.py

checking assistant status.
in progress...
in progress...
in progress...
in progress...
in progress...
in progress...
in progress...
in progress...
in progress...
in progress...
in progress...
in progress...
in progress...
in progress...
in progress...
in progress...
in progress...
in progress...
in progress...
in progress...
in progress...
in progress...
in progress...
in progress...
in progress...
in progress...
in progress...
in progress...
in progress...
in progress...
in progress...
in progress...
in progress...
in progress...
in progress...
in progress...
in progress...
in progress...
in progress...
in progress...
in progress...
in progress...
in progress...
in progress...
in progress...
in progress...
in progress...
in progress...
in progress...
in progress...
in progress...
in progress...
in progress...
in progress...
in progress...
in progress...
in progress...
in progress...
in progress...
in progress...
in progress...
in progress...
in progress...
in progress...
in progress...
in progress...
in progress...
in progress...
in progress...
in progress...
in progress...
in progress...
in progress...
in progress...
in progress...
in progress...
in progress...
in progress...
in progress...
in progress...
in progress...
in progress...
in progress...
in progress...
in progress...
in progress...
in progress...
in progress...
in progress...
in progress...
in progress...
in progress...
in progress...
in progress...
in progress...
in progress...
in progress...
in progress...
in progress...
in progress...
in progress...
in progress...
in progress...
in progress...
in progress...
in progress...
in progress...
in progress...
in progress...
in progress...
in progress...
in progress...
in progress...
in progress...
in progress...
in progress...
in progress...
in progress...
in progress...
in progress...

### Code snippets

_No response_

### OS

Windows 10

### Python version

Python 3.12.0

### Library version

openai==1.3.2",closed,2023-11-21T20:04:48Z,2023-11-22T00:03:51Z,['bug'],https://github.com/openai/openai-python/issues/861,3.98,True,method_order_violation,hang,"The API method calls are not completing as expected, leading to repeated 'in progress...' messages without resolution.",high,
862,test,"### Confirm this is a feature request for the Python library and not the underlying OpenAI API.

- [X] This is a feature request for the Python library

### Describe the feature or improvement you're requesting

na

### Additional context

_No response_",closed,2023-11-21T22:51:08Z,2023-11-21T22:52:15Z,[],https://github.com/openai/openai-python/issues/862,0.02,True,missing_option_violation,low,"The issue lacks a clear description of the feature or improvement being requested, which may lead to ambiguity in understanding user needs.",medium,
511,Error when importing embeddings_utils,"### Describe the bug

![image](https://github.com/openai/openai-python/assets/31582215/54a67736-3276-4644-a02e-22da54a5d579)

```
ImportError: cannot import name 'ARRAY_FUNCTIONS' from 'numpy.core.overrides' (/home/ygg/.cache/pypoetry/virtualenvs/stakeholderstudy-shsA1FjX-py3.10/lib/python3.10/site-packages/numpy/core/overrides.py)
```
```
openai==0.27.8                                                                                                                             
numpy==1.25.0
```

### To Reproduce

poetry add openai

from openai import embeddings_utils

### Code snippets

_No response_

### OS

Pop!_OS 22.04 LTS x86_64

### Python version

Python 3.10.6

### Library version

openai==0.27.8",closed,2023-06-30T00:21:37Z,2023-11-06T16:38:21Z,"['bug', 'fixed in v1']",https://github.com/openai/openai-python/issues/511,3112.28,True,missing_dependency_violation,high,"The error indicates a missing dependency on the expected version of numpy, which is required for the openai library to function correctly.",high,
845,Keep Conversation History,"### Confirm this is a feature request for the Python library and not the underlying OpenAI API.

- [X] This is a feature request for the Python library

### Describe the feature or improvement you're requesting

When using the API, it should be the case that conversation history is kept. While one can do this manually (by providing the history in the prompt), this costs additional API tokens...

### Additional context

_No response_",closed,2023-11-17T09:10:45Z,2023-11-18T22:58:08Z,[],https://github.com/openai/openai-python/issues/845,37.79,True,missing_dependency_violation,medium,"The request implies a missing dependency in the API's ability to manage conversation history automatically, which is essential for maintaining context in interactions.",high,
338,openai tools fine_tunes.prepare_data have error missing `pandas` ,"### Describe the bug

pip install --upgrade openai

and

when ""openai tools fine_tunes.prepare_data"" have error 

```
openai.datalib.MissingDependencyError: 

OpenAI error: 

    missing `pandas` 
```

### To Reproduce

step 1:
执行：
`openai tools fine_tunes.prepare_data --file dataSet-10latS7FRxjUjk9FgbMwhZ.jsonl --quiet`
报错：
```
Analyzing...
Traceback (most recent call last):
  File ""/opt/homebrew/bin/openai"", line 8, in <module>
    sys.exit(main())
  File ""/opt/homebrew/lib/python3.8/site-packages/openai/_openai_scripts.py"", line 63, in main
    args.func(args)
  File ""/opt/homebrew/lib/python3.8/site-packages/openai/cli.py"", line 586, in prepare_data
    df, remediation = read_any_format(fname)
  File ""/opt/homebrew/lib/python3.8/site-packages/openai/validators.py"", line 477, in read_any_format
    assert_has_pandas()
  File ""/opt/homebrew/lib/python3.8/site-packages/openai/datalib.py"", line 56, in assert_has_pandas
    raise MissingDependencyError(PANDAS_INSTRUCTIONS)
openai.datalib.MissingDependencyError: 

OpenAI error: 

    missing `pandas` 

This feature requires additional dependencies:

    $ pip install openai[datalib]
```

step2:
执行：
`pip install openai[datalib]`
报错：
`zsh: no matches found: openai[datalib]`

step3:
执行：
pip install openai""[datalib]""
结果：安装成功

step4:
与step1内容一致

step5:
执行：
`pip3 install pandas`
结果：
```
Requirement already satisfied: pandas in /Users/jetsommax/opt/anaconda3/envs/openai/lib/python3.10/site-packages (1.5.3)
Requirement already satisfied: numpy>=1.21.0 in /Users/jetsommax/opt/anaconda3/envs/openai/lib/python3.10/site-packages (from pandas) (1.24.2)
Requirement already satisfied: pytz>=2020.1 in /Users/jetsommax/opt/anaconda3/envs/openai/lib/python3.10/site-packages (from pandas) (2022.7.1)
Requirement already satisfied: python-dateutil>=2.8.1 in /Users/jetsommax/opt/anaconda3/envs/openai/lib/python3.10/site-packages (from pandas) (2.8.2)
Requirement already satisfied: six>=1.5 in /Users/jetsommax/opt/anaconda3/envs/openai/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)
```



### Code snippets

_No response_

### OS

Apple M1 Max;  MacOS ventura 13.2;

### Python version

Python 3.10.10

### Library version

openai 0.27.2",closed,2023-03-24T04:38:37Z,2023-03-29T23:12:28Z,"['bug', 'wontfix']",https://github.com/openai/openai-python/issues/338,138.56,True,missing_dependency_violation,high,"The API method 'fine_tunes.prepare_data' fails due to a missing dependency on 'pandas', which is required for its operation.",high,
837,Client objects are not pickle-able for multiprocess programs,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

I am developing a program that employs multiprocessing to enhance efficiency through parallel execution. Python relies on the `pickle` module for object serialization and inter-process communication. It would be nice if we let all classes in the `openai` package be pickle-able。

### To Reproduce

1. Create an `OpenAI` or `AsyncOpenAI` client.
2. Serialize the client using the `pickle` module.

### Code snippets

```Python
import pickle

from openai import OpenAI, AsyncOpenAI


client = OpenAI(api_key='sk-...')
async_client = AsyncOpenAI(api_key='sk-...')

pickle.dumps(client)  # TypeError: cannot pickle '_thread.RLock' object
pickle.dumps(async_client)  # TypeError: cannot pickle '_thread.RLock' object
```


### OS

macOS

### Python version

Python 3.11.6

### Library version

openai v1.3.0",closed,2023-11-16T09:51:30Z,2023-11-16T20:29:49Z,[],https://github.com/openai/openai-python/issues/837,10.64,False,,,,low,
818,How to get usage in streaming response?,"### Confirm this is a feature request for the Python library and not the underlying OpenAI API.

- [X] This is a feature request for the Python library

### Describe the feature or improvement you're requesting

We can use `print(dict(completion).get('usage'))` to get the token usage of the request, but in streaming mode, there is no way to get the token usage. It would be great if there was a way to retrieve token usage in streaming mode as well.

### Additional context

_No response_",closed,2023-11-14T11:06:17Z,2023-11-16T03:40:29Z,[],https://github.com/openai/openai-python/issues/818,40.57,True,missing_option_violation,medium,"The feature request indicates a missing option for retrieving token usage in streaming mode, which is not currently supported by the API.",high,
828,Can't import openai,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

openai module is installed, yet it gives the following error

![image](https://github.com/openai/openai-python/assets/70210166/1b9d6836-64ee-48ac-b89a-77e2858a44ff)


### To Reproduce

1. pip install openai
2. Start a REPL session, try to `import openai`

### Code snippets

_No response_

### OS

Ubuntu 22.04.3 LTS

### Python version

Python 3.10.12

### Library version

openai 1.2.4",closed,2023-11-15T15:20:24Z,2023-11-15T16:40:33Z,['question'],https://github.com/openai/openai-python/issues/828,1.34,True,missing_dependency_violation,unknown,"The issue suggests a potential missing dependency or misconfiguration in the environment that prevents the openai module from being imported, despite being installed.",medium,
819,Assistant API - Code Interpreter Image File Creation (files.retrieve_content),"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

when using the API to write and download a local file using files.retrieve_content it fails to create the file correctly.

To reproduce this error, try asking an assistant to create a graph for you. It will use the tool Code Interpreter and will create the file which will be returned in the thread messages. This file_id can then be used to view the file via the Playground. However, if you want to render this file in the application you are building, you can not.

### To Reproduce

Run the below code on google colab

### Code snippets

```Python
!pip install --upgrade openai
!openai migrate

from openai import OpenAI

client = OpenAI(api_key='sk-...')
file = client.files.create(
    file=open(""balancesheet.pdf"", ""rb""),
    purpose='assistants'
)


assistant = client.beta.assistants.create(
  name=""Data Visualizer"",
  description=""You are great at creating beautiful data visualizations. You analyze data present in .pdf files, understand trends, and come up with data visualizations relevant to those trends. You also share a brief text summary of the trends observed."",
  model=""gpt-4-1106-preview"",
  tools=[{""type"": ""code_interpreter""},{""type"": ""retrieval""}],
  file_ids=[file.id]
)

thread = client.beta.threads.create()
client.beta.threads.messages.create(
    thread_id=thread.id,
    role=""user"",
    content=""Can you please give me a simple example of a scatter plot""
)

run = client.beta.threads.runs.create(
  thread_id=thread.id,
  assistant_id=assistant.id
)

messages = client.beta.threads.messages.list(
  thread_id=thread.id
)

messages


content = client.files.retrieve_content(file.id)


Error message:
BadRequestError                           Traceback (most recent call last)
<ipython-input-17-23ff13466060> in <cell line: 1>()
----> 1 content = client.files.retrieve_content(file.id)

4 frames
/usr/local/lib/python3.10/dist-packages/openai/_base_client.py in _request(self, cast_to, options, remaining_retries, stream, stream_cls)
    875             # to completion before attempting to access the response text.
    876             err.response.read()
--> 877             raise self._make_status_error_from_response(err.response) from None
    878         except httpx.TimeoutException as err:
    879             if retries > 0:

BadRequestError: Error code: 400 - {'error': {'message': 'Not allowed to download files of purpose: assistants', 'type': 'invalid_request_error', 'param': None, 'code': None}}
```


### OS

Google Colab

### Python version

Python 3.10.12

### Library version

openai 1.2.4",closed,2023-11-14T21:16:01Z,2023-11-14T21:52:53Z,"['enhancement', 'API-feedback']",https://github.com/openai/openai-python/issues/819,0.61,True,missing_dependency_violation,high,"The API call to retrieve content from a file fails due to a missing dependency on the file's purpose, which is set to 'assistants', making it not allowed for download.",high,
786,Better (Azure OpenAI’s content management policy) exception management,"### Confirm this is a feature request for the Python library and not the underlying OpenAI API.

- [X] This is a feature request for the Python library

### Describe the feature or improvement you're requesting

Something between a feature request and better Python module exception management. 

When using an OpenAI (chat) create completion API endpoint through an Azure deployment sometime I got an UNEXPECTED exception raised by the Azure content filtering component.

As far as I understand, Azure ""prepend"" ANY  OpenAI completion model deployment by a content filter on top. 

```
┌────────────────────────────────────────────────────────────┐
│                                                            │
│ Completion API Request with prompt containing the sentence │
│                                                            │
│ “vorrei vedere la cosa più bella ad Ercolano. Qual’è?”     │
│                                                            │
└─────────────────────────────┬──────────────────────────────┘
                              │
                              │            InvalidRequestError
                              │                   ▲
                              │                   │
                              │                   │
                              │                   │
               ┌──────────────┴──────────────┐    │
               │                             │    │
               │   (Python) OpenAI module    │    │
               │                             │    │
               └──────────────┬──────────────┘    │
                              │                   │
                              │                   │
               ┌──────────────▼──────────────┐    │
               │                             │    │
               │    Azure Content Filter     │    │
               │                             │    │
               └──────────────┬──────────────┘    │
                              │                   │
               exception      ▼                   │
               prompt triggering Azure OpenAI’s   │
               content management policy          │
                                                  │
                              │                   │
               ┌──────────────▼──────────────┐    │
               │                             │    │
               │                             │    │
               │   Azure OpenAI Deployment   │    │
               │    (Any Model Behind)       │    │
               │                             │    │
               │                             │    │
               │                             │    │
               └──────────────┬──────────────┘    │
                              │                   │
                              │                   │
                              └───────────────────┘
```

When testing a chatbot (made with Azure OpenAI completions, e.g. a deployment using a GPT3.5-Turbo policy) in Italian language, I got an exception 

```python
              except openai.error.InvalidRequestError as e:
                  # Handle invalid request error, e.g. validate parameters or log
                  # WARNING: The response was filtered due to the prompt triggering Azure OpenAI’s content management policy.
                  sys.stderr.write(f""OpenAI API request was invalid: {e}"")
                  return ""Non ho capito bene. Cosa intendi?""
```

_OpenAI API request was invalid: The response was filtered due to the prompt triggering Azure OpenAI’s content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766_

I experienced that sometime the Azure content filter component UNEXPECTEDLY trigger an exception with a content category management violation.

As rightly stated in the closed issue https://github.com/openai/openai-python/issues/331#issuecomment-1805017174 that problem is apparently NOT an OpenAI model issue but a maybe WRONG triggering due to a not correct behavior of the Azure filter management. So I already opened an issue to Azure. 

Nevertheless, when concerning the Python module engineering I'm a bit perplexed about the fact openAI python SDK raises an `InvalidRequestError` for an exception NOT related to the OpenAI but raised by an external component (in this case the Azure Filtering component).

I didn't inspected the code, but I think the specific error due to the Azure filter management component could reserve a specific error  `AzureContentManagementPolicy` instead of a generic and misleading `InvalidRequestError` (sse documentation: reading documentation here: https://help.openai.com/en/articles/6897213-openai-library-error-types-guidance).
 
Thanks
Giorgio


### Additional context

```
$ py --version
Python 3.11.6

$ pip show openai | grep Version
Version: 0.28.1
```",closed,2023-11-11T10:26:55Z,2023-11-14T06:17:19Z,"['enhancement', 'Azure']",https://github.com/openai/openai-python/issues/786,67.84,True,missing_dependency_violation,medium,"The OpenAI Python SDK raises a generic InvalidRequestError for an exception triggered by the Azure content filtering component, indicating a missing dependency in error handling related to Azure's content management policy.",high,
792,Streaming Completions for Assistant Thread Output,"### Confirm this is a feature request for the Python library and not the underlying OpenAI API.

- [X] This is a feature request for the Python library

### Describe the feature or improvement you're requesting

Add streaming functionality to assistant messages.

### Additional context

_No response_",closed,2023-11-12T06:46:09Z,2023-11-14T03:58:59Z,[],https://github.com/openai/openai-python/issues/792,45.21,False,,,,,
91,openai.error.APIConnectionError: Error communicating with OpenAI,"Hello, we are getting this issue in our production environment, but seems to be working fine locally. Do you know what the issue might be?

```
Traceback (most recent call last):
  File ""/env/lib/python3.9/site-packages/openai/api_requestor.py"", line 279, in request_raw
    result = _thread_context.session.request(
  File ""/env/lib/python3.9/site-packages/requests/sessions.py"", line 529, in request
    resp = self.send(prep, **send_kwargs)
  File ""/env/lib/python3.9/site-packages/requests/sessions.py"", line 645, in send
    r = adapter.send(request, **kwargs)
  File ""/env/lib/python3.9/site-packages/requests/adapters.py"", line 519, in send
    raise ConnectionError(e, request=request)
requests.exceptions.ConnectionError: HTTPSConnectionPool(host='api.openai.com', port=443): Max retries exceeded with url: /v1/engines/content-filter-alpha/completions (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f08403ee970>: Failed to establish a new connection: [Errno 110] Connection timed out'))


The above exception was the direct cause of the following exception:
Traceback (most recent call last):
  File ""/app/beni_services/usecase.py"", line 33, in execute
    response_parameters = self.do_execute()
  File ""/app/product_catalog/use_cases/get_dynamic_filters.py"", line 150, in do_execute
    product_details_using_gpt3 = self._get_product_details_using_gpt3(product_details=product_details)
  File ""/app/product_catalog/use_cases/get_dynamic_filters.py"", line 276, in _get_product_details_using_gpt3
    if self._safe_to_use_openai(openai_prompt):
  File ""/app/product_catalog/use_cases/get_dynamic_filters.py"", line 302, in _safe_to_use_openai
    response = openai.Completion.create(
  File ""/env/lib/python3.9/site-packages/openai/api_resources/completion.py"", line 31, in create
    return super().create(*args, **kwargs)
  File ""/env/lib/python3.9/site-packages/openai/api_resources/abstract/engine_api_resource.py"", line 90, in create
    response, _, api_key = requestor.request(
  File ""/env/lib/python3.9/site-packages/openai/api_requestor.py"", line 100, in request
    result = self.request_raw(
  File ""/env/lib/python3.9/site-packages/openai/api_requestor.py"", line 289, in request_raw
    raise error.APIConnectionError(""Error communicating with OpenAI"") from e
openai.error.APIConnectionError: Error communicating with OpenAI
```

Thank you!",closed,2022-05-17T17:57:12Z,2023-11-10T04:15:52Z,['fixed in v1'],https://github.com/openai/openai-python/issues/91,12994.31,False,,,,low,
797,Tool invocation via API,"### Confirm this is a feature request for the Python library and not the underlying OpenAI API.

- [X] This is a feature request for the Python library

### Describe the feature or improvement you're requesting

Can we have tools invoked by the API so we can have control over access:

# Example #
# Assuming you've already uploaded your files and have their IDs
```
file_ids = [""file-123abc"", ""file-456def""]

response = openai.Tool(""retriever"").create(
    file_ids=file_ids,
    query=""Find information about GPT-3 capabilities""
)

# For Code Interpreter
code_snippet = """"""
with open('my_uploaded_file.txt', 'r') as file:
    data = file.read()
    # Process data...
""""""
response = openai.Tool(""code-interpreter"").create(
    code=code_snippet,
    file_id=""file-123abc""
)
```

This would remove the need for local sandboxed environments to run the tools. In a work around we can instruct through inference to run a tool but its not ideal as context is misused for data parameterization.



### Additional context

_No response_",closed,2023-11-12T17:54:45Z,2023-11-14T03:56:15Z,[],https://github.com/openai/openai-python/issues/797,34.02,True,missing_dependency_violation,medium,"The API method calls for tool invocation may lack necessary context or metadata, which could lead to incorrect or suboptimal behavior when processing requests.",high,
799,ChatCompletion Not Accessible via openai.types in init.py,"### Confirm this is a feature request for the Python library and not the underlying OpenAI API.

- [ ] This is a feature request for the Python library

### Describe the feature or improvement you're requesting

Environment:

openai-python version: Most recent version
Python version: 3.11.2
Operating System: Win 11

Description:
I have encountered an issue where ChatCompletion cannot be accessed via openai.types as one would expect from the module's __init__.py. This is causing type hinting and direct import issues for developers trying to use type hints with the openai client for chat-based operations.

Steps to Reproduce:

1. Install the latest version of openai-python client.
2. Attempt to import ChatCompletion using the following code snippet:
`from openai.types import ChatCompletion`
3. Observe the ImportError that occurs.

Expected behavior:
I expected to be able to access all types relevant to chat completions directly through openai.types for consistency and convenience when using type hints.

Actual behavior:
An ImportError is raised when attempting to import ChatCompletion from openai.types, suggesting that it is not exported properly in __init__.py.

Possible Fix:
Ensure that ChatCompletion and other relevant types are accessible through openai.types by adjusting the __init__.py file accordingly.

### Additional context

Access to consistent and correct type hints is crucial for developing robust applications with proper static code analysis. The current inability to import ChatCompletion as expected hinders this process and could lead to less type-safe code.

Thank you for looking into this matter. I am looking forward to the fix.",closed,2023-11-13T00:22:10Z,2023-11-14T03:51:27Z,[],https://github.com/openai/openai-python/issues/799,27.49,True,missing_dependency_violation,medium,"ChatCompletion is not accessible via openai.types, leading to issues with type hinting and direct imports.",high,
801,Add copy thread API,"### Confirm this is a feature request for the Python library and not the underlying OpenAI API.

- [X] This is a feature request for the Python library

### Describe the feature or improvement you're requesting

It seems that there is not copy thread API. I think it's a common scenario to use it.

A possible design is as follows:

```python
from openai import OpenAI

original_thread_id = ""xxx""
client = OpenAI()
thread = client.beta.threads.copy(thread_id=original_thread_id)
```

I want to hear your thoughts.

### Additional context

_No response_",closed,2023-11-13T10:28:46Z,2023-11-14T03:48:45Z,[],https://github.com/openai/openai-python/issues/801,17.33,False,,,,,
687,gpt-4-1106-preview only has 4096 tokens?,"I got this error while trying to generate with gpt-4-1106-preview:

Generating with model: gpt-4-1106-preview ...
Error while generating: Error code: 400 - {'error': {'message': 'max_tokens is too large: 5500. This model supports at most 4096 completion tokens, whereas you provided 5500.', 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': None}}

This seems unexpected, since max_tokens is supposed to have a much higher ceiling than that.

Anyone now what could be happening? Thanks. ",closed,2023-11-06T21:12:11Z,2023-11-06T22:25:15Z,['question'],https://github.com/openai/openai-python/issues/687,1.22,True,input_value_violation,high,The provided max_tokens value of 5500 exceeds the model's maximum limit of 4096 tokens.,high,
785,Redirects aren't working anymore on version 1.x (with private api),"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

So, I am running a private API, that is/was compatible with the openai python library.
In one of my endpoints, I am redirecting to another endpoint, to give model specific rate limits with slowapi.
`return RedirectResponse(url=""/v1/chat/completions/llama-70b-chat"")` <- this works just fine with version 0.28 and you get no response status code failure, but on the newest version, its raising an error saying response code was 307.



### To Reproduce

1. Make a openai library compatble fastapi server, with one endpoint redirecting to another.
2. call the first endpoint with library version 0.28 and with the newest version 1.x
3. you can see that the endpoint works on older versions, as they seem to allow redirects. But on newer versions it throws you the response code failure

### Code snippets

```Python
# FASTAPI EXAMPLE
import time

import fastapi
from starlette.responses import RedirectResponse

app = fastapi.FastAPI()

@app.post(""/v1/chat/completions"")
def chat_compl():
    return RedirectResponse(url=""/v1/chat/completions/new"")

@app.post(""/v1/chat/completions/new"")
def chat_compl():
    return {""id"": f""chatcmpl-fag4646utg49a"",
            'object': 'chat.completion',
            'created': int(time.time()),
            'model': f'test',
            'choices': [
                {'index': 0, 'message':
                    {'role': 'assistant',
                     'content': ""This is a test.""},
                 'finish_reason': 'stop'}],
            'usage': {'prompt_tokens': 17, 'completion_tokens': 16, 'total_tokens': 33}}


if __name__ == ""__main__"":
    import uvicorn
    uvicorn.run(app, host=""0.0.0.0"", port=8000)


# CLIENT EXAMPLE VERSION 0.28 (working)
import openai

openai.api_base = ""http://localhost:8000/v1""
openai.api_key = ""Some random text""

response = openai.ChatCompletion.create(
    model=""test"",
    messages=[
        {""role"": ""system"", ""content"": ""Doesnt matter what you write here""},
        {""role"": ""user"", ""content"": ""Doesnt matter what you write here""}
    ])

print(response)


# CLIENT EXAMPLE VERSION 1.x (Not working)
import os
import openai
from openai import OpenAI

client = OpenAI(base_url=""http://localhost:8000/v1"", api_key=""some random text"")

response = client.chat.completions.create(
    model=""test"",
    messages=[
        {""role"": ""system"", ""content"": ""Doesnt matter what you write here""},
        {""role"": ""user"", ""content"": ""Doesnt matter what you write here""}
    ]
)

print(response)
```


### OS

Arch Linux

### Python version

Python 3.11.5

### Library version

openai v1.x",closed,2023-11-11T09:59:20Z,2023-11-13T21:58:05Z,['bug'],https://github.com/openai/openai-python/issues/785,59.98,True,method_order_violation,medium,"The new version of the library does not handle redirects as expected, indicating a potential change in the required method call sequence for handling API responses.",high,
798,Dynamic function call callback possibility,"### Confirm this is a feature request for the Python library and not the underlying OpenAI API.

- [X] This is a feature request for the Python library

### Describe the feature or improvement you're requesting

I have a use case where the list of functions that could be used by is dynamic (I get the list programmatically from an external API).  
I can populate the list of functions list easily with the API response JSON, no problem. 
The problem is with the actual ""callback"" functions: I cannot provide them ""dynamically"". In my use case, if a function is identified and should be called, then I would need to call a generic API that I created that implements the function logic. 

I can call this API with the name of the function and the parameters. So it's not a local Python function but something on an external server. But the list of these functions should be always dynamic, and be provided by an external API.

I could use something like the following:

def func_call_proxy(**data):
    # function name
    func_name = inspect.currentframe().f_code.co_name
    # params and values
    for key, value in data.items():
        print(""{} is {}"".format(key,value))

But the actual function name here is of course always the same. Maybe I am not that adept with Python, I thought about using lambda functions, but there does not seem to be a way to realize my use case of dynamic functions. If this is the case, then it would be nice to be able to support it via e.g. providing a proxy function that receivers the name of the function first and arguments.

Thanks!

### Additional context

_No response_",closed,2023-11-12T21:04:55Z,2023-11-12T22:53:11Z,[],https://github.com/openai/openai-python/issues/798,1.8,True,missing_dependency_violation,medium,"The request lacks a clear specification on how to handle dynamic function callbacks, which may lead to missing dependencies between the function names and their corresponding implementations.",high,
378,ImportError: cannot import name 'OpenAI' from 'openai',"### Describe the bug

Please help me solve the problem...
suggested, char JPT, query connection option in Django - doesn't work:
//------------------------------------------------ ------------------------------------
from openai import OpenAI
import requests
# Get the API key
api_key=""""

# Initialize OpenAI
openai = OpenAI(api_key)

# Function to get response from ChatGPT
def get_response(query):
     # Form a request to the API
     response = openai.completion.create(
         engine=""davinci"",
         prompt=query
         max_tokens=100
     )

     # Get response
     response_json = response.json()
     reply = response_json[""choices""][0][""text""]

     return reply

# Function for processing the dialog form
def process_form(query):
     # Get response from ChatGPT
     response = get_response(query)

     # Send response to user
     return response
//------------------------------------------------ ------------------------------------
I get an error - it is not possible to load OpenAI!
the package is installed, for example, this version of the error does not cause an error at startup:

//------------------------------------------------ ------------------------------------

api_key = ""here is my key""
openai.api_key = api_key
# Function to get response from ChatGPT
def get_response(query):
     # Form a request to the API
     response = openai.completion.create(
         engine=""davinci"",
         prompt=query
         max_tokens=200
     )

     # Get response
     response_json = response.json()
     reply = response_json[""choices""][0][""text""]
     return reply
//------------------------------------------------ ------------------------------------

but a runtime error occurs when calling: response.json()
some argument error

### To Reproduce


//------------------------------------------------ ------------------------------------
from openai import OpenAI
import requests
# Get the API key
api_key=""""

# Initialize OpenAI
openai = OpenAI(api_key)

# Function to get response from ChatGPT
def get_response(query):
     # Form a request to the API
     response = openai.completion.create(
         engine=""davinci"",
         prompt=query
         max_tokens=100
     )

     # Get response
     response_json = response.json()
     reply = response_json[""choices""][0][""text""]

     return reply

# Function for processing the dialog form
def process_form(query):
     # Get response from ChatGPT
     response = get_response(query)

     # Send response to user
     return response
//------------------------------------------------ ------------------------------------
I get an error - it is not possible to load OpenAI!
ImportError: cannot import name 'OpenAI' from 'openai'

the package is installed, for example, this version of code the error does not cause an error at startup:

//------------------------------------------------ ------------------------------------

api_key = ""here is my key""
openai.api_key = api_key
# Function to get response from ChatGPT
def get_response(query):
     # Form a request to the API
     response = openai.completion.create(
         engine=""davinci"",
         prompt=query
         max_tokens=200
     )

     # Get response
     response_json = response.json()
     reply = response_json[""choices""][0][""text""]
     return reply
//------------------------------------------------ ------------------------------------

but a runtime error occurs when calling: response.json()
some argument error

Exception Type: | AttributeError
json
/home/murd/buf/ai_chat/.ai_env/lib/python3.8/site-packages/openai/openai_object.py, line 61, in __getattr__

response_json = response.json()
raise AttributeError(*err.args)

k    | 'json'
self | {'choices': [{'finish_reason': 'length',               'index': 0,               'logprobs': None,               'text': '\n'                       '\n'


 

### Code snippets

_No response_

### OS

Ubuntu

### Python version

Python 3.8.10

### Library version

Name: openai Version: 0.27.3 Summary: Python client library for the OpenAI API Home-page: https://github.com/openai/openai-python Author: OpenAI Author-email: support@openai.com License: None Location: /home/murd/buf/ai_chat/.ai_env/lib/python3.8/site-packages Requires: aiohttp, requests, tqdm",closed,2023-04-04T16:49:05Z,2023-04-05T18:55:02Z,['bug'],https://github.com/openai/openai-python/issues/378,26.1,False,,,,low,
784,AttributeError: module 'openai' has no attribute 'api_base'.,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

Hello, I'm encountering an issue with my program [EasyTranslator](https://github.com/artwalker/EasyTranslator).   
```python
if len(self.api_proxy) == 0:
   print(""-"" * 3)
   print(f""\033[1;32mOpenAI API proxy not detected, currently using the api address: {openai.api_base}\033[0m"") 
else:
   self.api_proxy_url = self.api_proxy + ""/v1""
   openai.api_base = os.environ.get(""OPENAI_API_BASE"", self.api_proxy_url)
   print(""-"" * 3)
   print(f""\033[1;32mUsing OpenAI API proxy, the proxy address is: {openai.api_base}\033[0m"")
```
The program consistently throws an error:  
**AttributeError: module 'openai' has no attribute 'api_base'. Did you mean: 'api_type'?**  
  
I also noticed that this attribute is indeed missing, as shown in the image below:  
![image](https://github.com/openai/openai-python/assets/44759507/8aaa1b49-c4b3-4897-9007-3734ab97aa38)    
  
Could you please clarify if there's another attribute used in place of 'api_base,' or if it's deprecated?  
Additionally, I'm using Azure to call the openai API's openai.api_base.  
How should I address this problem? Thank you very much!

### To Reproduce

1. git clone https://github.com/artwalker/EasyTranslator.git
2. cd EasyTranslator
3. pip install -r preconditions.txt
4. python easy_translator.py ./book/profile.txt

### Code snippets

_No response_

### OS

win 11

### Python version

Python 3.12.0

### Library version

openai v1.2.3",closed,2023-11-11T08:11:22Z,2023-11-11T09:38:40Z,['question'],https://github.com/openai/openai-python/issues/784,1.46,True,input_type_violation,unknown,"The code attempts to access an attribute 'api_base' from the 'openai' module, which does not exist, leading to an AttributeError. This indicates a potential mismatch between the expected API contract and the actual implementation of the library.",high,
703,The official example for Function Calling doesn't work with SDK version 1.1.1,"## Expected behavior

The ""Example with one function called in parallel"" code from the documentation should correctly show how to use the function call feature. 

## Actual behavior

I get `BadRequestError: Error code: 400 - {'error': {'message': ""'content' is a required property - 'messages.1'"", 'type': 'invalid_request_error', 'param': None, 'code': None}}` while running the code.

Stack trace:

```
BadRequestError                           Traceback (most recent call last)
[<ipython-input-73-3a60881757d5>](https://localhost:8080/#) in <cell line: 77>()
     75         )  # get a new response from the model where it can see the function response
     76         return second_response
---> 77 print(run_conversation())

5 frames
[<ipython-input-73-3a60881757d5>](https://localhost:8080/#) in run_conversation()
     70                 }
     71             )  # extend conversation with function response
---> 72         second_response = openai.chat.completions.create(
     73             model=""gpt-3.5-turbo-1106"",
     74             messages=messages,

[/usr/local/lib/python3.10/dist-packages/openai/_utils/_utils.py](https://localhost:8080/#) in wrapper(*args, **kwargs)
    297                         msg = f""Missing required argument: {quote(missing[0])}""
    298                 raise TypeError(msg)
--> 299             return func(*args, **kwargs)
    300 
    301         return wrapper  # type: ignore

[/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py](https://localhost:8080/#) in create(self, messages, model, frequency_penalty, function_call, functions, logit_bias, max_tokens, n, presence_penalty, response_format, seed, stop, stream, temperature, tool_choice, tools, top_p, user, extra_headers, extra_query, extra_body, timeout)
    554         timeout: float | httpx.Timeout | None | NotGiven = NOT_GIVEN,
    555     ) -> ChatCompletion | Stream[ChatCompletionChunk]:
--> 556         return self._post(
    557             ""/chat/completions"",
    558             body=maybe_transform(

[/usr/local/lib/python3.10/dist-packages/openai/_base_client.py](https://localhost:8080/#) in post(self, path, cast_to, body, options, files, stream, stream_cls)
   1053             method=""post"", url=path, json_data=body, files=to_httpx_files(files), **options
   1054         )
-> 1055         return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
   1056 
   1057     def patch(

[/usr/local/lib/python3.10/dist-packages/openai/_base_client.py](https://localhost:8080/#) in request(self, cast_to, options, remaining_retries, stream, stream_cls)
    832         stream_cls: type[_StreamT] | None = None,
    833     ) -> ResponseT | _StreamT:
--> 834         return self._request(
    835             cast_to=cast_to,
    836             options=options,

[/usr/local/lib/python3.10/dist-packages/openai/_base_client.py](https://localhost:8080/#) in _request(self, cast_to, options, remaining_retries, stream, stream_cls)
    875             # to completion before attempting to access the response text.
    876             err.response.read()
--> 877             raise self._make_status_error_from_response(err.response) from None
    878         except httpx.TimeoutException as err:
    879             if retries > 0:
```

## Versions

OpenAI SDK: 1.1.1
Python: 3.10.12",closed,2023-11-07T06:35:31Z,2023-11-11T02:24:43Z,[],https://github.com/openai/openai-python/issues/703,91.82,True,missing_dependency_violation,High,"'content' is a required property in the 'messages' argument, indicating a missing dependency in the API call.",high,
770,Response Format `verbose_json` in Audio Transcription with V1 Library,"### Confirm this is a feature request for the Python library and not the underlying OpenAI API.

- [X] This is a feature request for the Python library

### Describe the feature or improvement you're requesting

There is a few response formats documented in the [OpenAI documentation](https://platform.openai.com/docs/api-reference/audio/createTranscription#audio-createtranscription-response_format), but it seems that the V1 library only supports for the response of response format `json`.

It is fine when using the V0 library as it will return an entire response body. However, since V1 library return a Pydantic model, and the [`Transcription`](https://github.com/openai/openai-python/blob/v1.2.2/src/openai/types/audio/transcription.py#L8) model only has a `text` property. Even if I request the transcript in `verbose_json` format, the extra properties (e.g. `duration`, `language`) are dumped after parsing to a Pydantic model.

It should be able to add the extra properties to the Pydantic model. Can the team work on it, please?

### Additional context

_No response_",closed,2023-11-10T09:23:51Z,2023-11-11T02:26:02Z,['enhancement'],https://github.com/openai/openai-python/issues/770,17.04,True,missing_option_violation,medium,"The V1 library does not support the 'verbose_json' response format as documented, leading to incomplete response handling.",high,
299,"You exceeded your current quota, please check your plan and billing details","### Describe the bug

python code:

`import openai
openai.api_key = ""sk-m0DqPqTSAWoZ2XXXXXXXXXBZ4C""  # api_key hidden

completion = openai.ChatCompletion.create(model=""gpt-3.5-turbo"", messages=[{""role"": ""user"", ""content"": ""Hello world!""}])
print(completion.choices[0].message.content)`

openai.error.RateLimitError: You exceeded your current quota, please check your plan and billing details.

personal accout, google about this problem. Seems like i need to bind a credit card first before i can use ths API?

### To Reproduce

pip install openai
run the code above

### Code snippets

```Python
import openai
openai.api_key = ""sk-m0DqPqTSAWoZ2XXXXXXXXXBZ4C""  # api_key hidden

completion = openai.ChatCompletion.create(model=""gpt-3.5-turbo"", messages=[{""role"": ""user"", ""content"": ""Hello world!""}])
print(completion.choices[0].message.content)
```


### OS

Windows 10 Home Version 22H2

### Python version

Python 3.10.2 (tags/v3.10.2:a58ebcc, Jan 17 2022, 14:12:15) [MSC v.1929 64 bit (AMD64)] on win32

### Library version

openai.version.VERSION '0.27.1'",closed,2023-03-12T08:54:19Z,2023-03-13T16:07:18Z,['bug'],https://github.com/openai/openai-python/issues/299,31.22,True,missing_dependency_violation,unknown,"The user exceeded their API quota, indicating a missing dependency on account billing or plan configuration that is necessary to use the API.",high,
763,The connection is not returned to the httpx pool when using a stream,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [x] This is an issue with the Python library

### Describe the bug

The connections is not returned to the pool when using a stream

In this case (see ""To reproduce""), the error occurs in venv/lib/python3.11/site-packages/httpcore/_async/connection_pool.py in the wait_for_connection() method of the RequestStatus() class

And if after each end of reading the stream you call AsyncStream.response.aclose(), then everything will work correctly

### To Reproduce

1. Initialize an AsyncOpenAI object with a maximum number of connections equal to 2
2. Send 2 requests (chat.completions) with stream=True and read the entire stream
3. Send a third request
4. We will receive an APITimeoutError error

### Code snippets

```Python
import asyncio

import httpx
from openai import AsyncOpenAI


async def stream(client: AsyncOpenAI):
    response = await client.chat.completions.create(
        messages=[
            {'role': 'system', 'content': 'You are helpful assistant'},
            {'role': 'user', 'content': 'Hello'}
        ],
        model='gpt-4',
        stream=True,
        timeout=30,
    )
    result = ''
    async for message in response:
        result += message.choices[0].delta.content or """"


async def main():
    client = AsyncOpenAI(
        api_key='',
        http_client=httpx.AsyncClient(
            limits=httpx.Limits(
                max_connections=2,
            )
        )
    )
    for _ in range(3):
        await stream(client)


if __name__ == '__main__':
    asyncio.run(main())
```


### OS

macOS, Ubuntu 22.04

### Python version

Python v3.11.6

### Library version

openai 1.2.0",closed,2023-11-09T22:55:18Z,2023-11-10T22:46:00Z,['bug'],https://github.com/openai/openai-python/issues/763,23.84,True,missing_dependency_violation,medium,"The connection is not returned to the pool after reading the stream, leading to an APITimeoutError when exceeding the maximum number of connections.",high,
399,Stream error after 5 minutes,"### Describe the bug

Streaming output from gpt4 consistently fails with `Connection broken: InvalidChunkLength(got length b'', 0 bytes read)` for prompts that trigger long-form responses. I've been able to reproduce this by prompting gpt to rewrite a long document.

A few observations
1) This failure occurs _exactly_ at the 5 minute mark every time, which leads me to believe there's a timeout somewhere.
2) The python implementation of stream reader assumes that an empty string will never be returned by the server.
3) I sent the same request using a non-streaming client with a timeout at 10min. The timeout wasn't triggered until the 10min mark,so this problem seems isolated to streaming

Code snippet from `urllib3/response.py`:
```
def _update_chunk_length(self):
        # First, we'll figure out length of a chunk and then
        # we'll try to read it from socket.
        if self.chunk_left is not None:
            return
        line = self._fp.fp.readline()  # self._fp.fp.readline() returns b''
        line = line.split(b"";"", 1)[0]
        try:
            self.chunk_left = int(line, 16)  # this raises a value error
        except ValueError:
            # Invalid chunked protocol response, abort.
            self.close()
            raise InvalidChunkLength(self, line)
```

It seems like a problem on the server side but Im not confident

### To Reproduce

1. Create gpt4 streaming client
2. Prompt it to rewrite a very long text. For example, verses 1:1 -> 4:10 in [the book of genesis](https://www.vatican.va/archive/bible/genesis/documents/bible_genesis_en.html)


### Code snippets

_No response_

### OS

macOs

### Python version

Python 3.10.9

### Library version

v.0.26.4",closed,2023-04-14T18:19:17Z,2023-11-10T04:16:22Z,"['bug', 'fixed in v1']",https://github.com/openai/openai-python/issues/399,5025.95,True,input_value_violation,performance_degradation,"The streaming output fails after a specific duration (5 minutes), indicating a potential timeout issue that affects performance during long-form responses.",high,
660,openai.error.InvalidRequestError: Must provide an 'engine' or 'deployment_id' parameter to create a <class 'openai.api_resources.chat_completion.ChatCompletion'>,"### Describe the bug

I seem unable to consume my Azure OpenAI API. See the code below.

I'm getting the following error message:

_""openai.error.InvalidRequestError: Must provide an 'engine' or 'deployment_id' parameter to create a <class 'openai.api_resources.chat_completion.ChatCompletion'>""_

### To Reproduce

1. Deploy a resource on Azure OpenAI Studio
2. Copy the API keys etc.
3. Copy&Paste the code snippet

### Code snippets

```Python
import openai
import os
import load_dotenv

'''
AZURE_OPENAI_RESOURCE = 'AI_Sandbox'
AZURE_OPENAI_MODEL = 'gpt-3.5-turbo'
AZURE_OPENAI_KEY = ''
AZURE_OPENAI_ENDPOINT = 'https://{resource}.openai.azure.com/'
AZURE_OPENAI_MODEL_NAME = 'gpt-35-turbo'
AZURE_DEPLOYMENT_NAME = 'GPT-35-TURBO'
'''

# AOAI Integration Settings
AZURE_OPENAI_RESOURCE = os.environ.get(""AZURE_OPENAI_RESOURCE"")
AZURE_OPENAI_MODEL = os.environ.get(""AZURE_OPENAI_MODEL"")
AZURE_OPENAI_MODEL_NAME = os.environ.get(""AZURE_OPENAI_MODEL_NAME"")
AZURE_OPENAI_ENDPOINT = os.environ.get(""AZURE_OPENAI_ENDPOINT"")
AZURE_OPENAI_KEY = os.environ.get(""AZURE_OPENAI_KEY"")
AZURE_DEPLOYMENT_NAME = os.environ.get(""AZURE_DEPLOYMENT_NAME"")

base_url = AZURE_OPENAI_ENDPOINT if AZURE_OPENAI_ENDPOINT else f""https://{AZURE_OPENAI_RESOURCE}.openai.azure.com/""
        
class OpenAIService:
    def __init__(self):
        openai.api_key = AZURE_OPENAI_KEY
        openai.api_type = ""azure""
        openai.api_base = base_url
        openai.api_version = ""2023-03-15-preview""

    def list_models(self):
        return openai.Model.list()

    def prompt(self, prompt):
        return openai.ChatCompletion.create(
            engine=AZURE_OPENAI_MODEL_NAME,
            deployment_id=AZURE_DEPLOYMENT_NAME,
            messages=[{""role"": ""user"", ""content"": prompt}],
            temperature=0,
            timeout=60,
        )
```


### OS

macOS

### Python version

Python 3.11

### Library version

openai",closed,2023-10-17T14:00:38Z,2023-11-10T03:39:16Z,['bug'],https://github.com/openai/openai-python/issues/660,565.64,True,missing_dependency_violation,high,"The API call to create a ChatCompletion requires either an 'engine' or 'deployment_id' parameter, which is not provided in the context of the error.",high,
750,Implement retrieving information about all threads without providing a thread id,"### Confirm this is a feature request for the Python library and not the underlying OpenAI API.

- [X] This is a feature request for the Python library

### Describe the feature or improvement you're requesting

Please implement fetching all the thread ids available from the python library.

That would be similar to calling the following, which works as of 2023-11-09 9:16 UTC:
```python
r = httpx.get(
    ""https://api.openai.com/v1/threads"",
    headers={
        ""Authorization"": ""Bearer "" + api_key,
        ""Content-Type"": ""application/json"",
        ""OpenAI-Beta"": ""assistants=v1"",
    },
)
``` 

The existing ```OpenAI.beta.threads.retrieve``` requires a specific thread id, and calling it with an empty id produces #749.

*At this time this endpoint does not seem to be documented. See https://platform.openai.com/docs/api-reference/threads/getThread*

### Additional context

See issue #749 for an actual bug triggered by trying to retrieve information about all the threads using the existing 
```OpenAI.beta.threads.retrieve```.",closed,2023-11-09T09:35:27Z,2023-11-10T03:06:46Z,[],https://github.com/openai/openai-python/issues/750,17.52,True,input_value_violation,high,"Calling the method with an empty thread id leads to an error, indicating unacceptable input value.",high,
708,Not able to achieve `Reproducible outputs` with the seed parameter (SDK version 1.1.1),"Hi :wave:,

I've noticed the exciting new capability for achieving [Reproducible outputs](https://platform.openai.com/docs/guides/text-generation/reproducible-outputs).
Nevertheless, I'm encountering varying results when using identical input parameters and getting the same system fingerprint.

![Screenshot from 2023-11-07 13-22-23](https://github.com/openai/openai-python/assets/79843522/e39b715b-0a47-425c-99a4-b32b9be22050)

Am I missing something?

<details>
     <summary>Python code to reproduce this issue (might need a few runs)</summary>


    import openai
    from openai.types.chat import ChatCompletionUserMessageParam

    print(openai.__version__)
    print()

    for i in range(5):
        message = ChatCompletionUserMessageParam(
            content=(
                ""Imagine something completely random and crazy! (approximately 100 words)""
            ),
            role=""user"",
        )
        response = openai.chat.completions.create(
            messages=[message], model=""gpt-3.5-turbo-1106"", temperature=0, seed=42
        )
        print(response.system_fingerprint, response.choices[0].message.content)

</details>

### Versions
OpenAI SDK 1.1.1
Python version 3.11.5",closed,2023-11-07T12:29:59Z,2023-11-10T10:41:46Z,[],https://github.com/openai/openai-python/issues/708,70.2,False,,,,low,
767,"Hope to cancel ""required_args"" mecanism. Upgraded openai-python (v0->v1) refuses argumment ""repetition_penalty""","### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

I notice that about 2 weeks ago  [vllm](https://github.com/vllm-project/vllm)  has merged pr about adding an argument `repetition_penalty` aligned with huggingface. (refer to [pr 1424](https://github.com/vllm-project/vllm/pull/1424))

However, openai-python also graded to v1 several days ago. it refuses the unknown argument repetition_penalty. The following is the detail error.
` xxx/examples/openai_chatcompletion_client.py"", line 72, in <module>
    chat_completion = client.chat.completions.create(
  File ""/opt/conda/lib/python3.10/site-packages/openai/_utils/_utils.py"", line 299, in wrapper
    return func(*args, **kwargs)
TypeError: Completions.create() got an unexpected keyword argument 'repetition_penalty'`

This conflict makes that we can not depend openai library for open-source models and self-defined arguments (much more than repetition_penalty). This library has been factually used in many API-based model deployment library like fastchat and vllm. 

I really hope that the `required_args` can be removed. At least, we can modify its behaviors without modifying the source code of openai-python.

### To Reproduce

1. git clone vllm and setup it.
2. Make some request using vllm and openai, add arguments `repetition_penalty` 
3. The error happens.

### Code snippets

_No response_

### OS

Linux

### Python version

Python v3.10

### Library version

openai v1.2.2",closed,2023-11-10T03:23:47Z,2023-11-10T08:27:56Z,['question'],https://github.com/openai/openai-python/issues/767,5.07,True,missing_option_violation,medium,"The openai-python library does not accept the 'repetition_penalty' argument, which is expected by users familiar with other libraries like vllm and huggingface.",high,
500,M1 issue,"### Describe the bug

Same issue closed here, but I can't resolved in any way:
https://github.com/openai/openai-python/issues/338


### To Reproduce

pip install openai[datalib]

zsh: no matches found: openai[datalib]
michele@MacBook-Pro-di-michele model % pip install openai""[datalib]""

Defaulting to user installation because normal site-packages is not writeable
Requirement already satisfied: openai[datalib] in /Users/michele/Library/Python/3.8/lib/python/site-packages (0.27.8)
Requirement already satisfied: requests>=2.20 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from openai[datalib]) (2.30.0)
Requirement already satisfied: tqdm in /Users/michele/Library/Python/3.8/lib/python/site-packages (from openai[datalib]) (4.65.0)
Requirement already satisfied: aiohttp in /Users/michele/Library/Python/3.8/lib/python/site-packages (from openai[datalib]) (3.8.4)
Requirement already satisfied: numpy in /Users/michele/Library/Python/3.8/lib/python/site-packages (from openai[datalib]) (1.24.3)
Requirement already satisfied: pandas>=1.2.3 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from openai[datalib]) (2.0.2)
Requirement already satisfied: pandas-stubs>=1.1.0.11 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from openai[datalib]) (2.0.2.230605)
Requirement already satisfied: openpyxl>=3.0.7 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from openai[datalib]) (3.1.2)
Requirement already satisfied: et-xmlfile in /Users/michele/Library/Python/3.8/lib/python/site-packages (from openpyxl>=3.0.7->openai[datalib]) (1.1.0)
Requirement already satisfied: python-dateutil>=2.8.2 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from pandas>=1.2.3->openai[datalib]) (2.8.2)
Requirement already satisfied: pytz>=2020.1 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from pandas>=1.2.3->openai[datalib]) (2023.3)
Requirement already satisfied: tzdata>=2022.1 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from pandas>=1.2.3->openai[datalib]) (2023.3)
Requirement already satisfied: types-pytz>=2022.1.1 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from pandas-stubs>=1.1.0.11->openai[datalib]) (2023.3.0.0)
Requirement already satisfied: charset-normalizer<4,>=2 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from requests>=2.20->openai[datalib]) (3.1.0)
Requirement already satisfied: idna<4,>=2.5 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from requests>=2.20->openai[datalib]) (3.4)
Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from requests>=2.20->openai[datalib]) (2.0.2)
Requirement already satisfied: certifi>=2017.4.17 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from requests>=2.20->openai[datalib]) (2023.5.7)
Requirement already satisfied: attrs>=17.3.0 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from aiohttp->openai[datalib]) (23.1.0)
Requirement already satisfied: multidict<7.0,>=4.5 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from aiohttp->openai[datalib]) (6.0.4)
Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from aiohttp->openai[datalib]) (4.0.2)
Requirement already satisfied: yarl<2.0,>=1.0 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from aiohttp->openai[datalib]) (1.9.2)
Requirement already satisfied: frozenlist>=1.1.1 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from aiohttp->openai[datalib]) (1.3.3)
Requirement already satisfied: aiosignal>=1.1.2 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from aiohttp->openai[datalib]) (1.3.1)
Requirement already satisfied: six>=1.5 in /Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.8/lib/python3.8/site-packages (from python-dateutil>=2.8.2->pandas>=1.2.3->openai[datalib]) (1.15.0)
michele@MacBook-Pro-di-michele model % pip3 install pandas

Defaulting to user installation because normal site-packages is not writeable
Requirement already satisfied: pandas in /Users/michele/Library/Python/3.8/lib/python/site-packages (2.0.2)
Requirement already satisfied: python-dateutil>=2.8.2 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from pandas) (2.8.2)
Requirement already satisfied: pytz>=2020.1 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from pandas) (2023.3)
Requirement already satisfied: tzdata>=2022.1 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from pandas) (2023.3)
Requirement already satisfied: numpy>=1.20.3 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from pandas) (1.24.3)
Requirement already satisfied: six>=1.5 in /Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.8/lib/python3.8/site-packages (from python-dateutil>=2.8.2->pandas) (1.15.0)
michele@MacBook-Pro-di-michele model % openai tools fine_tunes.prepare_data -f training_data.jsonl
Analyzing...
Traceback (most recent call last):
  File ""/opt/homebrew/bin/openai"", line 8, in <module>
    sys.exit(main())
  File ""/opt/homebrew/lib/python3.9/site-packages/openai/_openai_scripts.py"", line 78, in main
    args.func(args)
  File ""/opt/homebrew/lib/python3.9/site-packages/openai/cli.py"", line 594, in prepare_data
    df, remediation = read_any_format(fname)
  File ""/opt/homebrew/lib/python3.9/site-packages/openai/validators.py"", line 481, in read_any_format
    assert_has_pandas()
  File ""/opt/homebrew/lib/python3.9/site-packages/openai/datalib/pandas_helper.py"", line 15, in assert_has_pandas
    raise MissingDependencyError(PANDAS_INSTRUCTIONS)
openai.datalib.common.MissingDependencyError: 

OpenAI error:

    missing `pandas`

This feature requires additional dependencies:

    $ pip install openai[datalib]


michele@MacBook-Pro-di-michele model % 

michele@MacBook-Pro-di-michele model % pip3 install openai \[datalib\]
Defaulting to user installation because normal site-packages is not writeable
ERROR: Invalid requirement: '[datalib]'


michele@MacBook-Pro-di-michele model % pip3 install openai \[datalib\]
michele@MacBook-Pro-di-michele model % 
michele@MacBook-Pro-di-michele model % pip install openai\[datalib\]
Defaulting to user installation because normal site-packages is not writeable
Requirement already satisfied: openai[datalib] in /Users/michele/Library/Python/3.8/lib/python/site-packages (0.27.8)
Requirement already satisfied: requests>=2.20 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from openai[datalib]) (2.30.0)
Requirement already satisfied: tqdm in /Users/michele/Library/Python/3.8/lib/python/site-packages (from openai[datalib]) (4.65.0)
Requirement already satisfied: aiohttp in /Users/michele/Library/Python/3.8/lib/python/site-packages (from openai[datalib]) (3.8.4)
Requirement already satisfied: numpy in /Users/michele/Library/Python/3.8/lib/python/site-packages (from openai[datalib]) (1.24.3)
Requirement already satisfied: pandas>=1.2.3 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from openai[datalib]) (2.0.2)
Requirement already satisfied: pandas-stubs>=1.1.0.11 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from openai[datalib]) (2.0.2.230605)
Requirement already satisfied: openpyxl>=3.0.7 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from openai[datalib]) (3.1.2)
Requirement already satisfied: et-xmlfile in /Users/michele/Library/Python/3.8/lib/python/site-packages (from openpyxl>=3.0.7->openai[datalib]) (1.1.0)
Requirement already satisfied: python-dateutil>=2.8.2 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from pandas>=1.2.3->openai[datalib]) (2.8.2)
Requirement already satisfied: pytz>=2020.1 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from pandas>=1.2.3->openai[datalib]) (2023.3)
Requirement already satisfied: tzdata>=2022.1 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from pandas>=1.2.3->openai[datalib]) (2023.3)
Requirement already satisfied: types-pytz>=2022.1.1 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from pandas-stubs>=1.1.0.11->openai[datalib]) (2023.3.0.0)
Requirement already satisfied: charset-normalizer<4,>=2 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from requests>=2.20->openai[datalib]) (3.1.0)
Requirement already satisfied: idna<4,>=2.5 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from requests>=2.20->openai[datalib]) (3.4)
Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from requests>=2.20->openai[datalib]) (2.0.2)
Requirement already satisfied: certifi>=2017.4.17 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from requests>=2.20->openai[datalib]) (2023.5.7)
Requirement already satisfied: attrs>=17.3.0 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from aiohttp->openai[datalib]) (23.1.0)
Requirement already satisfied: multidict<7.0,>=4.5 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from aiohttp->openai[datalib]) (6.0.4)
Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from aiohttp->openai[datalib]) (4.0.2)
Requirement already satisfied: yarl<2.0,>=1.0 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from aiohttp->openai[datalib]) (1.9.2)
Requirement already satisfied: frozenlist>=1.1.1 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from aiohttp->openai[datalib]) (1.3.3)
Requirement already satisfied: aiosignal>=1.1.2 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from aiohttp->openai[datalib]) (1.3.1)
Requirement already satisfied: six>=1.5 in /Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.8/lib/python3.8/site-packages (from python-dateutil>=2.8.2->pandas>=1.2.3->openai[datalib]) (1.15.0)
michele@MacBook-Pro-di-michele model % pip3.8 install openai\[datalib\]
Defaulting to user installation because normal site-packages is not writeable
Requirement already satisfied: openai[datalib] in /Users/michele/Library/Python/3.8/lib/python/site-packages (0.27.8)
Requirement already satisfied: requests>=2.20 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from openai[datalib]) (2.30.0)
Requirement already satisfied: tqdm in /Users/michele/Library/Python/3.8/lib/python/site-packages (from openai[datalib]) (4.65.0)
Requirement already satisfied: aiohttp in /Users/michele/Library/Python/3.8/lib/python/site-packages (from openai[datalib]) (3.8.4)
Requirement already satisfied: numpy in /Users/michele/Library/Python/3.8/lib/python/site-packages (from openai[datalib]) (1.24.3)
Requirement already satisfied: pandas>=1.2.3 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from openai[datalib]) (2.0.2)
Requirement already satisfied: pandas-stubs>=1.1.0.11 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from openai[datalib]) (2.0.2.230605)
Requirement already satisfied: openpyxl>=3.0.7 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from openai[datalib]) (3.1.2)
Requirement already satisfied: et-xmlfile in /Users/michele/Library/Python/3.8/lib/python/site-packages (from openpyxl>=3.0.7->openai[datalib]) (1.1.0)
Requirement already satisfied: python-dateutil>=2.8.2 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from pandas>=1.2.3->openai[datalib]) (2.8.2)
Requirement already satisfied: pytz>=2020.1 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from pandas>=1.2.3->openai[datalib]) (2023.3)
Requirement already satisfied: tzdata>=2022.1 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from pandas>=1.2.3->openai[datalib]) (2023.3)
Requirement already satisfied: types-pytz>=2022.1.1 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from pandas-stubs>=1.1.0.11->openai[datalib]) (2023.3.0.0)
Requirement already satisfied: charset-normalizer<4,>=2 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from requests>=2.20->openai[datalib]) (3.1.0)
Requirement already satisfied: idna<4,>=2.5 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from requests>=2.20->openai[datalib]) (3.4)
Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from requests>=2.20->openai[datalib]) (2.0.2)
Requirement already satisfied: certifi>=2017.4.17 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from requests>=2.20->openai[datalib]) (2023.5.7)
Requirement already satisfied: attrs>=17.3.0 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from aiohttp->openai[datalib]) (23.1.0)
Requirement already satisfied: multidict<7.0,>=4.5 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from aiohttp->openai[datalib]) (6.0.4)
Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from aiohttp->openai[datalib]) (4.0.2)
Requirement already satisfied: yarl<2.0,>=1.0 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from aiohttp->openai[datalib]) (1.9.2)
Requirement already satisfied: frozenlist>=1.1.1 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from aiohttp->openai[datalib]) (1.3.3)
Requirement already satisfied: aiosignal>=1.1.2 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from aiohttp->openai[datalib]) (1.3.1)
Requirement already satisfied: six>=1.5 in /Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.8/lib/python3.8/site-packages (from python-dateutil>=2.8.2->pandas>=1.2.3->openai[datalib]) (1.15.0)
michele@MacBook-Pro-di-michele model % openai tools fine_tunes.prepare_data -f training_data.jsonl
Analyzing...
Traceback (most recent call last):
  File ""/opt/homebrew/bin/openai"", line 8, in <module>
    sys.exit(main())
  File ""/opt/homebrew/lib/python3.9/site-packages/openai/_openai_scripts.py"", line 78, in main
    args.func(args)
  File ""/opt/homebrew/lib/python3.9/site-packages/openai/cli.py"", line 594, in prepare_data
    df, remediation = read_any_format(fname)
  File ""/opt/homebrew/lib/python3.9/site-packages/openai/validators.py"", line 481, in read_any_format
    assert_has_pandas()
  File ""/opt/homebrew/lib/python3.9/site-packages/openai/datalib/pandas_helper.py"", line 15, in assert_has_pandas
    raise MissingDependencyError(PANDAS_INSTRUCTIONS)
openai.datalib.common.MissingDependencyError: 

OpenAI error:

    missing `pandas`

This feature requires additional dependencies:

    $ pip install openai[datalib]


michele@MacBook-Pro-di-michele model % 


### Code snippets

```Python
pip install openai[datalib]

zsh: no matches found: openai[datalib]
michele@MacBook-Pro-di-michele model % pip install openai""[datalib]""

Defaulting to user installation because normal site-packages is not writeable
Requirement already satisfied: openai[datalib] in /Users/michele/Library/Python/3.8/lib/python/site-packages (0.27.8)
Requirement already satisfied: requests>=2.20 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from openai[datalib]) (2.30.0)
Requirement already satisfied: tqdm in /Users/michele/Library/Python/3.8/lib/python/site-packages (from openai[datalib]) (4.65.0)
Requirement already satisfied: aiohttp in /Users/michele/Library/Python/3.8/lib/python/site-packages (from openai[datalib]) (3.8.4)
Requirement already satisfied: numpy in /Users/michele/Library/Python/3.8/lib/python/site-packages (from openai[datalib]) (1.24.3)
Requirement already satisfied: pandas>=1.2.3 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from openai[datalib]) (2.0.2)
Requirement already satisfied: pandas-stubs>=1.1.0.11 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from openai[datalib]) (2.0.2.230605)
Requirement already satisfied: openpyxl>=3.0.7 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from openai[datalib]) (3.1.2)
Requirement already satisfied: et-xmlfile in /Users/michele/Library/Python/3.8/lib/python/site-packages (from openpyxl>=3.0.7->openai[datalib]) (1.1.0)
Requirement already satisfied: python-dateutil>=2.8.2 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from pandas>=1.2.3->openai[datalib]) (2.8.2)
Requirement already satisfied: pytz>=2020.1 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from pandas>=1.2.3->openai[datalib]) (2023.3)
Requirement already satisfied: tzdata>=2022.1 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from pandas>=1.2.3->openai[datalib]) (2023.3)
Requirement already satisfied: types-pytz>=2022.1.1 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from pandas-stubs>=1.1.0.11->openai[datalib]) (2023.3.0.0)
Requirement already satisfied: charset-normalizer<4,>=2 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from requests>=2.20->openai[datalib]) (3.1.0)
Requirement already satisfied: idna<4,>=2.5 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from requests>=2.20->openai[datalib]) (3.4)
Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from requests>=2.20->openai[datalib]) (2.0.2)
Requirement already satisfied: certifi>=2017.4.17 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from requests>=2.20->openai[datalib]) (2023.5.7)
Requirement already satisfied: attrs>=17.3.0 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from aiohttp->openai[datalib]) (23.1.0)
Requirement already satisfied: multidict<7.0,>=4.5 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from aiohttp->openai[datalib]) (6.0.4)
Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from aiohttp->openai[datalib]) (4.0.2)
Requirement already satisfied: yarl<2.0,>=1.0 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from aiohttp->openai[datalib]) (1.9.2)
Requirement already satisfied: frozenlist>=1.1.1 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from aiohttp->openai[datalib]) (1.3.3)
Requirement already satisfied: aiosignal>=1.1.2 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from aiohttp->openai[datalib]) (1.3.1)
Requirement already satisfied: six>=1.5 in /Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.8/lib/python3.8/site-packages (from python-dateutil>=2.8.2->pandas>=1.2.3->openai[datalib]) (1.15.0)
michele@MacBook-Pro-di-michele model % pip3 install pandas

Defaulting to user installation because normal site-packages is not writeable
Requirement already satisfied: pandas in /Users/michele/Library/Python/3.8/lib/python/site-packages (2.0.2)
Requirement already satisfied: python-dateutil>=2.8.2 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from pandas) (2.8.2)
Requirement already satisfied: pytz>=2020.1 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from pandas) (2023.3)
Requirement already satisfied: tzdata>=2022.1 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from pandas) (2023.3)
Requirement already satisfied: numpy>=1.20.3 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from pandas) (1.24.3)
Requirement already satisfied: six>=1.5 in /Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.8/lib/python3.8/site-packages (from python-dateutil>=2.8.2->pandas) (1.15.0)
michele@MacBook-Pro-di-michele model % openai tools fine_tunes.prepare_data -f training_data.jsonl
Analyzing...
Traceback (most recent call last):
  File ""/opt/homebrew/bin/openai"", line 8, in <module>
    sys.exit(main())
  File ""/opt/homebrew/lib/python3.9/site-packages/openai/_openai_scripts.py"", line 78, in main
    args.func(args)
  File ""/opt/homebrew/lib/python3.9/site-packages/openai/cli.py"", line 594, in prepare_data
    df, remediation = read_any_format(fname)
  File ""/opt/homebrew/lib/python3.9/site-packages/openai/validators.py"", line 481, in read_any_format
    assert_has_pandas()
  File ""/opt/homebrew/lib/python3.9/site-packages/openai/datalib/pandas_helper.py"", line 15, in assert_has_pandas
    raise MissingDependencyError(PANDAS_INSTRUCTIONS)
openai.datalib.common.MissingDependencyError: 

OpenAI error:

    missing `pandas`

This feature requires additional dependencies:

    $ pip install openai[datalib]


michele@MacBook-Pro-di-michele model % 

michele@MacBook-Pro-di-michele model % pip3 install openai \[datalib\]
Defaulting to user installation because normal site-packages is not writeable
ERROR: Invalid requirement: '[datalib]'


michele@MacBook-Pro-di-michele model % pip3 install openai \[datalib\]
michele@MacBook-Pro-di-michele model % 
michele@MacBook-Pro-di-michele model % pip install openai\[datalib\]
Defaulting to user installation because normal site-packages is not writeable
Requirement already satisfied: openai[datalib] in /Users/michele/Library/Python/3.8/lib/python/site-packages (0.27.8)
Requirement already satisfied: requests>=2.20 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from openai[datalib]) (2.30.0)
Requirement already satisfied: tqdm in /Users/michele/Library/Python/3.8/lib/python/site-packages (from openai[datalib]) (4.65.0)
Requirement already satisfied: aiohttp in /Users/michele/Library/Python/3.8/lib/python/site-packages (from openai[datalib]) (3.8.4)
Requirement already satisfied: numpy in /Users/michele/Library/Python/3.8/lib/python/site-packages (from openai[datalib]) (1.24.3)
Requirement already satisfied: pandas>=1.2.3 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from openai[datalib]) (2.0.2)
Requirement already satisfied: pandas-stubs>=1.1.0.11 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from openai[datalib]) (2.0.2.230605)
Requirement already satisfied: openpyxl>=3.0.7 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from openai[datalib]) (3.1.2)
Requirement already satisfied: et-xmlfile in /Users/michele/Library/Python/3.8/lib/python/site-packages (from openpyxl>=3.0.7->openai[datalib]) (1.1.0)
Requirement already satisfied: python-dateutil>=2.8.2 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from pandas>=1.2.3->openai[datalib]) (2.8.2)
Requirement already satisfied: pytz>=2020.1 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from pandas>=1.2.3->openai[datalib]) (2023.3)
Requirement already satisfied: tzdata>=2022.1 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from pandas>=1.2.3->openai[datalib]) (2023.3)
Requirement already satisfied: types-pytz>=2022.1.1 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from pandas-stubs>=1.1.0.11->openai[datalib]) (2023.3.0.0)
Requirement already satisfied: charset-normalizer<4,>=2 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from requests>=2.20->openai[datalib]) (3.1.0)
Requirement already satisfied: idna<4,>=2.5 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from requests>=2.20->openai[datalib]) (3.4)
Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from requests>=2.20->openai[datalib]) (2.0.2)
Requirement already satisfied: certifi>=2017.4.17 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from requests>=2.20->openai[datalib]) (2023.5.7)
Requirement already satisfied: attrs>=17.3.0 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from aiohttp->openai[datalib]) (23.1.0)
Requirement already satisfied: multidict<7.0,>=4.5 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from aiohttp->openai[datalib]) (6.0.4)
Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from aiohttp->openai[datalib]) (4.0.2)
Requirement already satisfied: yarl<2.0,>=1.0 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from aiohttp->openai[datalib]) (1.9.2)
Requirement already satisfied: frozenlist>=1.1.1 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from aiohttp->openai[datalib]) (1.3.3)
Requirement already satisfied: aiosignal>=1.1.2 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from aiohttp->openai[datalib]) (1.3.1)
Requirement already satisfied: six>=1.5 in /Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.8/lib/python3.8/site-packages (from python-dateutil>=2.8.2->pandas>=1.2.3->openai[datalib]) (1.15.0)
michele@MacBook-Pro-di-michele model % pip3.8 install openai\[datalib\]
Defaulting to user installation because normal site-packages is not writeable
Requirement already satisfied: openai[datalib] in /Users/michele/Library/Python/3.8/lib/python/site-packages (0.27.8)
Requirement already satisfied: requests>=2.20 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from openai[datalib]) (2.30.0)
Requirement already satisfied: tqdm in /Users/michele/Library/Python/3.8/lib/python/site-packages (from openai[datalib]) (4.65.0)
Requirement already satisfied: aiohttp in /Users/michele/Library/Python/3.8/lib/python/site-packages (from openai[datalib]) (3.8.4)
Requirement already satisfied: numpy in /Users/michele/Library/Python/3.8/lib/python/site-packages (from openai[datalib]) (1.24.3)
Requirement already satisfied: pandas>=1.2.3 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from openai[datalib]) (2.0.2)
Requirement already satisfied: pandas-stubs>=1.1.0.11 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from openai[datalib]) (2.0.2.230605)
Requirement already satisfied: openpyxl>=3.0.7 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from openai[datalib]) (3.1.2)
Requirement already satisfied: et-xmlfile in /Users/michele/Library/Python/3.8/lib/python/site-packages (from openpyxl>=3.0.7->openai[datalib]) (1.1.0)
Requirement already satisfied: python-dateutil>=2.8.2 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from pandas>=1.2.3->openai[datalib]) (2.8.2)
Requirement already satisfied: pytz>=2020.1 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from pandas>=1.2.3->openai[datalib]) (2023.3)
Requirement already satisfied: tzdata>=2022.1 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from pandas>=1.2.3->openai[datalib]) (2023.3)
Requirement already satisfied: types-pytz>=2022.1.1 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from pandas-stubs>=1.1.0.11->openai[datalib]) (2023.3.0.0)
Requirement already satisfied: charset-normalizer<4,>=2 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from requests>=2.20->openai[datalib]) (3.1.0)
Requirement already satisfied: idna<4,>=2.5 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from requests>=2.20->openai[datalib]) (3.4)
Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from requests>=2.20->openai[datalib]) (2.0.2)
Requirement already satisfied: certifi>=2017.4.17 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from requests>=2.20->openai[datalib]) (2023.5.7)
Requirement already satisfied: attrs>=17.3.0 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from aiohttp->openai[datalib]) (23.1.0)
Requirement already satisfied: multidict<7.0,>=4.5 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from aiohttp->openai[datalib]) (6.0.4)
Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from aiohttp->openai[datalib]) (4.0.2)
Requirement already satisfied: yarl<2.0,>=1.0 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from aiohttp->openai[datalib]) (1.9.2)
Requirement already satisfied: frozenlist>=1.1.1 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from aiohttp->openai[datalib]) (1.3.3)
Requirement already satisfied: aiosignal>=1.1.2 in /Users/michele/Library/Python/3.8/lib/python/site-packages (from aiohttp->openai[datalib]) (1.3.1)
Requirement already satisfied: six>=1.5 in /Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.8/lib/python3.8/site-packages (from python-dateutil>=2.8.2->pandas>=1.2.3->openai[datalib]) (1.15.0)
michele@MacBook-Pro-di-michele model % openai tools fine_tunes.prepare_data -f training_data.jsonl
Analyzing...
Traceback (most recent call last):
  File ""/opt/homebrew/bin/openai"", line 8, in <module>
    sys.exit(main())
  File ""/opt/homebrew/lib/python3.9/site-packages/openai/_openai_scripts.py"", line 78, in main
    args.func(args)
  File ""/opt/homebrew/lib/python3.9/site-packages/openai/cli.py"", line 594, in prepare_data
    df, remediation = read_any_format(fname)
  File ""/opt/homebrew/lib/python3.9/site-packages/openai/validators.py"", line 481, in read_any_format
    assert_has_pandas()
  File ""/opt/homebrew/lib/python3.9/site-packages/openai/datalib/pandas_helper.py"", line 15, in assert_has_pandas
    raise MissingDependencyError(PANDAS_INSTRUCTIONS)
openai.datalib.common.MissingDependencyError: 

OpenAI error:

    missing `pandas`

This feature requires additional dependencies:

    $ pip install openai[datalib]


michele@MacBook-Pro-di-michele model %
```


### OS

MacOS Big Sur 11.7.7 (20G1345)

### Python version

Python 3.8

### Library version

openai 0.27.2",closed,2023-06-24T13:00:48Z,2023-11-10T04:00:35Z,"['bug', 'fixed in v1']",https://github.com/openai/openai-python/issues/500,3327.0,False,,,,low,
535,Space start validator crashes on empty completion,"### Describe the bug

When running `openai tools fine_tunes.prepare_data`, a completion that is empty, or that becomes empty after a remediation is applied, causes `openai.validators.completions_space_start_validator` to raise an unhandled `IndexError`. @tuliren has contributed a fix in #406. I'm opening this issue to give information about the bug and its impact. Merging #406 would fix this bug.

#125 added the test that finds this bug, `test_long_examples_validator`. The bug is in the code under test (not in the test itself). The bug precedes that pull request.

Since #153, `numpy` and `pandas` are not installed by default, and the test is skipped in their absence. Installing with `pip install -e '.[dev]'` and running `pytest` does not reveal the bug for this reason, whereas `pip install -e '.[dev,datalib]'` does show the failure.

I haven't found a commit in the history of the main branch in which the test has been present but able to pass. It looks like it may have originally been run with modified data tailored to the problem in #121 (which #125 fixes), and then stopped failing for many developers once #153 was merged, due to being skipped.

When actually using `openai tools fine_tunes.prepare_data`, I think it is unusual to include an empty completion or one that would become empty due to a remediation, so I think the user impact of this bug is low. But it seems to me that the ongoing situation where not all tests are able to pass may be confusing to new contributors. (I personally find it confusing.) Fortunately, the fix in #406 is straightforward.

### To Reproduce

1. On GNU/Linux or macOS, prepare an environment using Python 3.7, 3.8, 3.9, 3.10, or 3.11. (Although I've manually verified that the bug affects Windows, and `test_long_examples_validator` does not pass on Windows, other issues obscure this one.) I tested this locally on Ubuntu 22.04 with all those versions of Python, and on CI with `ubuntu-latest` and `macos-latest` using [a workflow](https://github.com/EliahKagan/openai-python/actions/runs/5525296227/jobs/10078739427) derived from [tuliren's](https://github.com/tuliren/openai-python/pull/1).

2. In that environment, create an editable install, including at least the `datalib` extra, by running:
   `pip install '.[dev,datalib]'`

3. Run: `pytest`
   Or, for the whole traceback, run: `pytest -vv`
   Or, for the whole traceback and just that one test, run: `pytest -vvk test_long_examples_validator`

### Code snippets

`x[0]` raises `IndexError` when `x` is empty in:

```python
lambda x: ("""" if x[0] == "" "" else "" "") + x
```

This appears in the `add_space_start` local function, in `completions_space_start_validator`, in `openai/validators.py`.

### OS

All (tested mainly in Ubuntu 22.04.2 and macOS 12.6.7).

### Python version

Python 3.7.17, 3.8.17, 3.9.17, 3.10.12, and 3.11.4.

### Library version

Tested on b82a3f7 (main branch), though a range of versions are affected.",closed,2023-07-12T03:11:18Z,2023-11-10T08:20:11Z,['bug'],https://github.com/openai/openai-python/issues/535,2909.15,True,input_value_violation,high,"The API method raises an unhandled IndexError when an empty completion is provided, indicating that unacceptable input values are being supplied.",high,
594,"Function call: return a ""content"" containing function_call instead of a ""function_call""","### Describe the bug

Hello,

While using function call, we met a bug that occurs occasionally. Even in GPT4.


Instead of returning a json with key ""function_call"", which it should be, GPT returns a json with key ""content"". This ""content"" contains ""function_call"" as a string.

case 1:

<img width=""970"" alt=""MicrosoftTeams-image"" src=""https://github.com/openai/openai-python/assets/13915357/1e1f7506-13ae-41d6-881f-73d2f9fbdb2a"">

case 2:

![Snipaste_2023-08-30_16-35-49](https://github.com/openai/openai-python/assets/13915357/4666900e-53a7-4b42-8c4d-0513164b9602)

GPT_MODEL = ""gpt-4""
API_VERSION = ""2023-07-01-preview""
openai.api_type = ""azure""

### To Reproduce

It occurs occasionally. We cannot guarantee it can be reproduced.

### Code snippets

_No response_

### OS

Linux

### Python version

3.8.10

### Library version

0.27.8",closed,2023-08-30T08:39:47Z,2023-11-10T04:27:37Z,"['bug', 'Azure']",https://github.com/openai/openai-python/issues/594,1723.8,True,incorrect_functionality,High,"The API is returning an incorrect JSON structure, providing 'content' instead of the expected 'function_call', leading to incorrect system behavior.",medium,
630,Incorrect error displaying when context size is larger than organization TPM,"### Describe the bug

When you make a request to the OpenAI API and the context length exceeds your organizations allowed tokens per minute for the specific model, you will receive the following error:

```
Rate limit reached for default-gpt-4-large in organization org-xxxxxxxxxxxx on tokens per min. 

Limit: 150000 / min. Current: 0 / min. Contact us through our help center at help.openai.com if you continue to have issues.
```

This is confusing because it makes the issue appear to be at the organization level, when in fact a bug in your code is just causing your context to be too large. Instead, the following error message should appear:

```
This model's maximum context length is 4097 tokens, however you requested 150678 tokens (150000 in your prompt; 678 for the completion). Please reduce your prompt; or completion length.""
```

### To Reproduce

1. Use the OpenAI python client
2. Use any open ai chat completions endpoint with far too large a context length
3. Observe error 

### Code snippets

_No response_

### OS

macOS

### Python version

^3.9

### Library version

^0.27.8",closed,2023-09-29T19:57:11Z,2023-11-10T03:35:41Z,['bug'],https://github.com/openai/openai-python/issues/630,991.64,True,input_value_violation,performance_degradation,"The error message does not accurately reflect the issue of exceeding the maximum context length, leading to confusion about the source of the problem.",high,
513,Feature Request: return dollar amount for OpenAI request response,"### Describe the feature or improvement you're requesting

Add the dollar amount to the response, or a separate function to calculate the same.

### Additional context

The openai library returns the number of tokens with each response.
However, the pricing keeps changing. Is there a way you can add the dollar amount to that response, or a separate function for the same?

Thanks",closed,2023-06-30T08:00:03Z,2023-11-10T04:39:50Z,[],https://github.com/openai/openai-python/issues/513,3188.66,False,,,,low,
536,Tests with temporary files or subprocesses are broken on Windows,"### Describe the bug

Several unit tests rely inadvertently on behavior that varies by operating system, in ways that cause them not to work on Windows. These tests work on other platforms, just not Windows. This is a bug in the tests, not the code under test. I've opened #537 to fix them.

The affected tests are:

| File | Test |
|--------|--------|
| [`test_file_cli.py`](https://github.com/openai/openai-python/blob/main/openai/tests/test_file_cli.py) | `test_file_cli` |
| [`test_long_examples_validator.py`](https://github.com/openai/openai-python/blob/main/openai/tests/test_long_examples_validator.py) | `test_long_examples_validator` |
| [`test_util.py`](https://github.com/openai/openai-python/blob/main/openai/tests/test_util.py) | `test_openai_api_key_path` |
| [`test_util.py`](https://github.com/openai/openai-python/blob/main/openai/tests/test_util.py) | `test_openai_api_key_path_with_malformed_key` | 

The causes of incompatibility with Windows (some applying to multiple tests) are that, *on Windows*:

1. Attempting to open a file created by [`NamedTemporaryFile`](https://docs.python.org/3/library/tempfile.html#tempfile.NamedTemporaryFile) while it is already open raises `PermissionError`. (This is the incompatibility that affects the most tests.)
2. Giving `subprocess.run` a multiple-argument command to run using a shell, when done by passing a one-element list whose element is the whole command, gives the `cmd.exe` shell a command it cannot parse correctly. This shell fails with ""The filename, directory name, or volume label syntax is incorrect."" (Doing it in this specific way was probably unintentional, and the other ways of passing commands through `subprocess.run` do work.)
3. The system default encoding is not UTF-8. It is usually [CP-1252](https://en.wikipedia.org/wiki/Windows-1252) (Windows-1252), at least on English-language versions of Windows. This can often be addressed by passing `encoding=""utf-8""` where applicable, but as an argument to `subprocess.run` that can cause (rather than avoid) a `UnicodeDecodeError`. When the child process is another Python process, its standard streams almost always use the system default encoding.

### To Reproduce

1. Set up a Python virtual environment on Windows for any version of Python this library supports.
2. In it, install the project by running `pip install -e '.[dev,datalib]'`. (The `datalib` extra is needed for `test_long_examples_validator`, which is skipped if `numpy` and `pandas` are unavailable. For the others, it would be sufficient to run `pip install -e '.[dev]'`.)
3. Run `pytest` to run the tests. (To verify exactly what's going on, it may help to run `pytest -vv`.)

This shows the first two kinds of problems described above; the third is only observable once these problems are fixed (and a fix or workaround has been applied for #535).

Developers without access to a Windows system who want to partially verify the details of this bug can examine [this CI output](https://github.com/EliahKagan/openai-python/actions/runs/5525296227/jobs/10078740888) (from a workflow based on [tuliren's](https://github.com/tuliren/openai-python/pull/1)).

### Code snippets

As one example of the first problem, [`test_util.py`](https://github.com/openai/openai-python/blob/main/openai/tests/test_util.py) uses `NamedTemporaryFile` in a fixture, `api_key_file`, that allows it to be used this way:

```python
def test_openai_api_key_path(api_key_file) -> None:
    print(""sk-foo"", file=api_key_file)
    api_key_file.flush()
    assert util.default_api_key() == ""sk-foo""
```

However, the code under test separately attempts to open that file to read an API key. On Windows, this raises `PermissionError`.

### OS

Windows

### Python version

Python 3.7.9, 3.8.10, 3.9.13, 3.10.11, and 3.11.4.

### Library version

Tested on b82a3f7 (main branch), though the problem was not introduced recently.",closed,2023-07-13T07:19:04Z,2023-11-10T04:32:10Z,['bug'],https://github.com/openai/openai-python/issues/536,2877.22,True,input_value_violation,performance_degradation,"Tests rely on behavior that varies by operating system, causing them to fail on Windows due to incompatible file handling and subprocess execution.",high,
457,openai.Deployment.Create - deployment name setting not working,"### Deployment name not setting with python api

I need to create a model deployment in my Azure Open AI workspace, specifically using openai.Deployment.create() python API. There is no clear spec on what needs to be passed in here, so I have not been able to get deployment name to set correctly. 

I've tried setting the following params for setting it: **name, deployment_name, deployment_id, openai_id, deployment, id**

None seem to work - all set a generic default - I'm not sure if this functionality is broken or there is another param name. 

`openai.Deployment.create(model=completion_params[""model_name""], name=completion_params[""deployment_id""], scale_settings=scale_settings)`

Currently, its been setting a random default Id:

<img width=""1039"" alt=""image"" src=""https://github.com/openai/openai-python/assets/25394373/2014da27-1469-4398-a999-a1822387b244"">

Please help here with guidance.

### To Reproduce

Run: 
```
scale_settings = {
                ""scale_type"": ""standard"",
            }
openai.Deployment.create(model=""text-chat-davinci-002"", name=""test-deployment-name"", scale_settings=scale_settings)
```

Check AOAI workspace to see deployment name , and its not set to what I want

### Code snippets

_No response_

### OS

Windows

### Python version

3.8.16

### Library version

openai 0.27.6",closed,2023-05-19T15:50:44Z,2023-11-10T04:30:53Z,['bug'],https://github.com/openai/openai-python/issues/457,4188.67,True,missing_option_violation,unknown,"The user is unable to set the deployment name correctly due to unclear specifications on required parameters, leading to potential misuse of the API.",high,
605,n=X Parameter counts as scalar for total token count towards rate limit,"### Describe the bug

It appears as if using n>1 works as a multiplier on the total token number of prompt and longest response when the TPM rate is calculated. 

I had 5-7 requests with 1k total tokens each and n=5 and got a rate limit error saying my TPM was 76k

### To Reproduce

set n = 5  and create multiple requests having larger prompts and smaller responses within short time without violating the size(prompt)+max_tokens*n limit,
it will result in a rate limit error anyway.
e.g. Summarize this page in a sentence

### Code snippets

_No response_

### OS

-

### Python version

-

### Library version

-",closed,2023-09-07T23:15:43Z,2023-11-10T04:18:42Z,['bug'],https://github.com/openai/openai-python/issues/605,1517.05,True,input_value_violation,medium,"The use of n>1 as a multiplier for token count leads to unexpected rate limit errors, indicating that the input value for n may be exceeding acceptable limits for the API's rate limiting mechanism.",high,
606,Show current line of training_file for FineTuningJob event/event file,"### Describe the feature or improvement you're requesting

I hope that through the FineTuningJob event, we can pinpoint certain rows of data in the training set where there are abnormal losses during fine-tuning.

### Additional context

_No response_",closed,2023-09-08T07:33:27Z,2023-11-10T04:17:24Z,[],https://github.com/openai/openai-python/issues/606,1508.73,True,missing_dependency_violation,medium,"The request lacks necessary context or metadata about the training data, which is essential for identifying abnormal losses during fine-tuning.",high,
569,Must provide an 'engine' or 'deployment_id' parameter,"### Describe the bug

I am running the github code in local using VSCode start.cmd file. All the necessary packages were installed and the website is running now. When I prompt a quesestoin, I got the below error
Must provide an 'engine' or 'deployment_id' parameter

**Error**
Must provide an 'engine' or 'deployment_id' parameter to create a <class 'openai.api_resources.chat_completion.ChatCompletion'>

### To Reproduce

Clone the github report and changed few environment variables as given in the instructions. 

### Code snippets

_No response_

### OS

Windows

### Python version

Python v3.11.4

### Library version

openai 0.27.7",closed,2023-08-10T15:41:15Z,2023-11-10T04:16:56Z,"['bug', 'Azure']",https://github.com/openai/openai-python/issues/569,2196.59,True,missing_dependency_violation,high,"The API call requires either an 'engine' or 'deployment_id' parameter, which is missing.",high,
609,Azure OpenAI KeyError on 500,"### Describe the bug

Currently, 500 errors from Azure OpenAI do not return a dictionary with key `""error""`. This leads to a generic APIError/KeyError when it could attempt to return the message from the instead, which is more useful for end users

### To Reproduce

Any request to Azure OpenAI that fails because of a non-specific Server Error

Example response:

```json
{
    ""activityId"": ""123-123-123"",
    ""message"": ""Internal server error"",
    ""statusCode"": 500
}
```

### Code snippets

_No response_

### OS

Linux

### Python version

3.11.3

### Library version

v0.28.0",closed,2023-09-11T16:13:08Z,2023-11-10T04:14:39Z,"['bug', 'fixed in v1']",https://github.com/openai/openai-python/issues/609,1428.03,True,missing_option_violation,medium,"The API does not return a structured error response with the expected 'error' key, leading to unhandled exceptions.",high,
541,support 3rd party gateway,"### Describe the feature or improvement you're requesting

Ability to route the calls via 3rd party API gateway's such as APIGEE, MuleSoft etc.
Client application doesn't need to store the actual API-key rather it'll be set at the API Gateway.

Right now it seem to support direct calls to OpenAI or Microsoft gateway (limited) 

### Additional context

_No response_",closed,2023-07-15T00:12:19Z,2023-11-10T04:12:39Z,['enhancement'],https://github.com/openai/openai-python/issues/541,2836.01,True,missing_option_violation,medium,The request to support 3rd party API gateways does not follow the existing API contract which currently supports only direct calls to OpenAI or Microsoft gateways.,high,
309,ChatML: name property should support '@' character,"### Describe the bug

Lots of existing usernames contain @ as namespace and prefixing. Complete understand why this is defined narrowly, but you are pushing a bunch of work on to developers and this surprised me.
  
File ""/Users/dmp/Library/Python/3.9/lib/python/site-packages/openai/api_requestor.py"", line 679, in _interpret_response_line
    raise self.handle_error_response(
openai.error.InvalidRequestError: '@username' does not match '^[a-zA-Z0-9_-]{1,64}$' - 'messages.6.name'

### To Reproduce

<img width=""693"" alt=""image"" src=""https://user-images.githubusercontent.com/332509/225145831-7c1a91dd-91f9-4d54-b074-775a66c31dd5.png"">

`#add the user + message in the third argument
whoiam = getpass.getuser()
whoiam = ""@"" + whoiam
prompt = {""role"": ""user"", ""content"": sys.argv[3], ""name"": whoiam}

#talk to chatGPT
completion = openai.ChatCompletion.create(
  model = ""gpt-3.5-turbo"", 
  messages = chatml + [prompt]
)`

### Code snippets

```Python
#add the user + message in the third argument
whoiam = getpass.getuser()
whoiam = ""@"" + whoiam
prompt = {""role"": ""user"", ""content"": sys.argv[3], ""name"": whoiam}

#talk to chatGPT
completion = openai.ChatCompletion.create(
  model = ""gpt-3.5-turbo"", 
  messages = chatml + [prompt]
)
```


### OS

macOS

### Python version

Python

### Library version

openai-python",closed,2023-03-14T21:47:12Z,2023-11-10T04:08:21Z,['bug'],https://github.com/openai/openai-python/issues/309,5766.35,True,input_value_violation,high,"The '@' character in the username violates the expected format defined by the API, leading to an InvalidRequestError.",high,
494,openai.error.AuthenticationError: Incorrect API key provided: cmdftp@e******.com if .netrc file is present,"### Describe the bug

I am trying the example code from https://github.com/openai/openai-python
```python
openai.api_key = os.getenv(""OPENAI_API_KEY"")
        # list models
        models = openai.Model.list()
        
        # print the first model's id
        print(models.data[0].id)
        
        # create a chat completion
        chat_completion = openai.ChatCompletion.create(model=""gpt-3.5-turbo"", messages=[{""role"": ""user"", ""content"": ""Hello world""}])
        
        # print the chat completion
        print(chat_completion.choices[0].message.content)
```

I have checked my API key with:
```bash
curl https://api.openai.com/v1/chat/completions \
    -H ""Content-Type: application/json"" \
    -H ""Authorization: Bearer $OPENAI_API_KEY"" \
    -d '{
       ""model"": ""gpt-3.5-turbo"",
       ""messages"": [{""role"": ""user"", ""content"": ""Say this is a test!""}],
       ""temperature"": 0.7
     }'
```
which returns:
```json
{
  ""id"": ""chatcmpl-7TsXvCYItiGrF46Ljo26yS9BgsZWt"",
  ""object"": ""chat.completion"",
  ""created"": 1687355543,
  ""model"": ""gpt-3.5-turbo-0301"",
  ""choices"": [
    {
      ""index"": 0,
      ""message"": {
        ""role"": ""assistant"",
        ""content"": ""This is a test!""
      },
      ""finish_reason"": ""stop""
    }
  ],
  ""usage"": {
    ""prompt_tokens"": 14,
    ""completion_tokens"": 5,
    ""total_tokens"": 19
  }
}
```



### To Reproduce
```bash
macOS Big Sur 11.6.2
Darwin Kernel Version 20.6.0
python --version
Python 3.10.8
```

create a .netrc file
```bash
default
login anonymous
password cmdftp@example.com
```

save the codesnippet below as testopenai and run
```bash
./testopenai
```

which gives the error message:
```bash
openai.error.AuthenticationError: Incorrect API key provided: cmdftp@e******.com. You can find your API key at https://platform.openai.com/account/api-keys.
```

The payment plan has been setup, the key has been created after setting the plan and the curl command works - so why doesn't the python one? See also https://stackoverflow.com/a/76523513/1497139

### Code snippets

```Python
#!/bin/bash
# WF 2023-06-21
if [ ""$OPENAI_API_KEY"" == """" ]
then
  echo ""OPENAI_API_KEY env variable needs to be set to a key see https://platform.openai.com/account/api-keys""
  echo ""export OPENAI_API_KEY=""
  exit 1
fi
#if [ ""$OPENAI_API_ORG"" == """" ]
# then
#  echo ""OPENAI_API_ORG env variable needs to be set  see https://platform.openai.com/account/org-settings""
#  echo ""export OPENAI_API_ORG=""
#  exit 1
#fi

#
# test via curl
#
viacurl() {
  curl https://api.openai.com/v1/chat/completions \
    -H ""Content-Type: application/json"" \
    -H ""Authorization: Bearer $OPENAI_API_KEY"" \
    -d '{
       ""model"": ""gpt-3.5-turbo"",
       ""messages"": [{""role"": ""user"", ""content"": ""Say this is a test!""}],
       ""temperature"": 0.7
     }'
}

#
# test via python
#
viapython() {
  code=""/tmp/testopenai.py""
cat << EOF > $code
import openai
import os
openai.api_key = os.getenv('OPENAI_API_KEY')

# list models
models = openai.Model.list()

# print the first model's id
print(models.data[0].id)

# create a chat completion
chat_completion = openai.ChatCompletion.create(model=""gpt-3.5-turbo"", messages=[{""role"": ""user"", ""content"": ""Hello world""}])

# print the chat completion
print(chat_completion.choices[0].message.content)
EOF
python $code
}

viacurl
viapython
```


### OS

macOS

### Python version

Python 3.10

### Library version

openai-python 0.27.8",closed,2023-06-21T13:54:02Z,2023-11-10T04:01:20Z,"['bug', 'fixed in v1']",https://github.com/openai/openai-python/issues/494,3398.12,True,input_value_violation,high,"The provided API key is incorrect, leading to an authentication error when attempting to access the OpenAI API.",high,
423,AttributeError: module 'openai' has no attribute 'ChatCompletion/Completion',"### Describe the bug

The feature is not working at all for me, so I created a new venv and it still does not work

### To Reproduce

```
import openai

openai.api_key = ""key""

completion = openai.Completion.create(
         model=""gpt-3.5-turbo"", 
         messages=[{""role"": ""user"", ""content"": ""Hello world!""}]
)
```

### Code snippets

```Python
Additionally, I tried to run this cURL command in my terminal given by the openai documentation:

curl https://api.openai.com/v1/completions \
  -H ""Authorization: Bearer $OPENAI_API_KEY"" \
  -H ""Content-Type: application/json"" \
  -d '{""prompt"": YOUR_PROMPT, ""model"": FINE_TUNED_MODEL}'

and replaced it with my api key which in return printed:
    ""error"": {
        ""message"": ""Incorrect API key provided: -*****************************************. You can find your API key at https://platform.openai.com/account/api-keys."",
        ""type"": ""invalid_request_error"",
        ""param"": null,
        ""code"": ""invalid_api_key""
    }

I've used 3 api keys at this point, I am not sure if that's what is causing the issue since both my python and openai are up to date.
```


### OS

macOS

### Python version

Python v3.11.3

### Library version

openai-python v0.27.5",closed,2023-04-30T17:38:09Z,2023-11-10T03:57:09Z,['bug'],https://github.com/openai/openai-python/issues/423,4642.32,True,input_type_violation,high,"The user is attempting to call 'openai.Completion.create' with a model that is not compatible with the expected input type for the method, as 'gpt-3.5-turbo' should be used with 'openai.ChatCompletion.create' instead.",high,
576,Embedding.create() fails when it receives a list of strings that contains an empty string,"### Describe the bug

According to https://platform.openai.com/docs/api-reference/embeddings/create, you can send an input of type `str` or `list of str`. You receive a response if you send an empty string ("""") as input. But, if you send a list of str that contains an empty string like [""a"", ""b"", """", ""d""], you receive an error of type `InvalidRequestError` with this message `'$.input' is invalid. Please check the API reference: https://platform.openai.com/docs/api-reference.`

### To Reproduce

res = openai.Embedding.create(input="""", engine=EMBEDDING_MODEL)
print(len(res['data'][0]['embedding']))
res2 = openai.Embedding.create(input=[""a"", """"], engine=EMBEDDING_MODEL)
print(res2['data'][0]['embedding'])

------------------------
Output

1536

---------------------------------------------------------------------------
InvalidRequestError                       Traceback (most recent call last)
Cell In[74], line 3
      1 res = openai.Embedding.create(input="""", engine=EMBEDDING_MODEL)
      2 print(len(res['data'][0]['embedding']))
----> 3 res2 = openai.Embedding.create(input=[""a"", """"], engine=EMBEDDING_MODEL)
      4 print(res2['data'][0]['embedding'])

File [~/workspace/platzi/semantic-search/.venv/lib/python3.10/site-packages/openai/api_resources/embedding.py:33](https://vscode-remote+wsl-002bubuntu.vscode-resource.vscode-cdn.net/home/juanwtf/workspace/platzi/semantic-search/~/workspace/platzi/semantic-search/.venv/lib/python3.10/site-packages/openai/api_resources/embedding.py:33), in Embedding.create(cls, *args, **kwargs)
     31 while True:
     32     try:
---> 33         response = super().create(*args, **kwargs)
     35         # If a user specifies base64, we'll just return the encoded string.
     36         # This is only for the default case.
     37         if not user_provided_encoding_format:

File [~/workspace/platzi/semantic-search/.venv/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py:153](https://vscode-remote+wsl-002bubuntu.vscode-resource.vscode-cdn.net/home/juanwtf/workspace/platzi/semantic-search/~/workspace/platzi/semantic-search/.venv/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py:153), in EngineAPIResource.create(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)
    127 @classmethod
    128 def create(
    129     cls,
   (...)
    136     **params,
    137 ):
    138     (
    139         deployment_id,
...
    764         rbody, rcode, resp.data, rheaders, stream_error=stream_error
    765     )
    766 return resp

InvalidRequestError: '$.input' is invalid. Please check the API reference: https://platform.openai.com/docs/api-reference.tps://platform.openai.com/docs/api-reference.

### Code snippets

_No response_

### OS

Ubuntu

### Python version

Python v3.10.12

### Library version

openai-python v0.27.8",closed,2023-08-17T22:56:41Z,2023-11-10T03:48:37Z,['API-feedback'],https://github.com/openai/openai-python/issues/576,2020.87,True,input_value_violation,high,"The API fails when a list of strings contains an empty string, which is an unacceptable input value.",high,
572,Suppressing token 100265 `<|im_end|>` using logit_bias results in InvalidRequestError,"### Describe the bug

When trying to ban the sampling of token 100265 using `logit_bias` by mapping it to -100, I get:
```
{'error': {'message': ""Invalid key in 'logit_bias': 100265. Maximum value is 100257."", 'type': 'invalid_request_error', 'param': 'logit_bias', 'code': None}}
```
As you are aware 100257 is `<|endoftext|>` and it can be suppressed currently to get full length (of `max_tokens`) completions on the Completions API.

However its equivalent on chat completions API i.e `100265` → `<|im_end|>` can’t be used on chat completions API.


I am curious is intentional (safety reasons) to limit tokens till 100257 OR it is inherited by reusing the old completion API spec.


### To Reproduce

Simply pass a bias: `{100265: -100}` to the `logit_bias` param of chat completion request.

This results in:
```console
Exception has occurred: InvalidRequestError
Invalid key in 'logit_bias': 100265. Maximum value is 100257.
  File ""dir/tokenext.py"", line 12, in <module>
    reply = openai.ChatCompletion.create(model=""gpt-3.5-turbo"",
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
openai.error.InvalidRequestError: Invalid key in 'logit_bias': 100265. Maximum value is 100257.
```
### Code snippets

```Python
import openai
openai.api_key = ""API_KEY""  # supply your API key however you choose

conversation = [{
    ""role"": ""user"",
    ""content"": input(""Input:"")
}]

bias = {100265: -100}

print(""GPT: "")
reply = openai.ChatCompletion.create(model=""gpt-3.5-turbo"",
                                     messages=conversation,
                                     max_tokens=1000,
                                     logit_bias=bias)
print(reply)
```


### OS

macOS

### Python version

Python v3.11.2 64-bit

### Library version

openai-python v0.27.7",closed,2023-08-13T05:22:43Z,2023-11-10T03:45:28Z,['bug'],https://github.com/openai/openai-python/issues/572,2134.38,True,input_value_violation,high,"The provided token 100265 exceeds the maximum allowable value of 100257 for the logit_bias parameter, resulting in an InvalidRequestError.",high,
628,No SSL certificate exception not handled,"### Describe the bug

Hey guys, first time I'm reporting a bug so I hope I'm doing it the right way :)

Yesterday, I tried to use the openai-python library in async, just as written in the documentation (see code below)

`completion = await openai.ChatCompletion.acreate(
        model=""gpt-4"", messages=test_messages
    )`

I was already using the sync method (and it worked) and wanted to do it in async, but it didn't work and the exception was absolutely not clear. It just said : Error communicating with OpenAI

I didn't find anything useful, neither on Google nor with chatGPT, so after like 2 hours trying to see if I missed a comma, I decided to give up and do it with aiohttp, so without the openai-python library, and doing this gave me a ClientConnectorCertificateError that told me that I actually didn't install the ssl certificates. 

Therefore I installed those certificates by running this command in my terminal: /Applications/Python 3.11/Install Certificates.command

After doing so, I tried the acreate method again and it finally worked! I'm therefore suggesting that you try to improve the exceptions handling in that case, because it was really unclear if that was caused by my api key, my internet connection, a mistake in my code or something else, and I'm sure other people might have the same issue :)

### To Reproduce

1. Make sure you don't have the ssl certificates installed in Python (not sure how to do so :/)
2. Just make a request to the API using the openai-python library in async (with acreate)

### Code snippets

_No response_

### OS

macOS

### Python version

Python v3.11

### Library version

openai-python 0.28.1",closed,2023-09-29T07:57:29Z,2023-11-10T03:43:40Z,['bug'],https://github.com/openai/openai-python/issues/628,1003.77,True,missing_dependency_violation,medium,"The API method does not handle the missing SSL certificate dependency, leading to unclear error messages for the user.",high,
617,Announcing An OpenAI Microservice Quickstart for AWS,"### Describe the feature or improvement you're requesting

Just a heads up that I've released v0.1.0 of [https://github.com/FullStackWithLawrence/aws-openai](https://github.com/FullStackWithLawrence/aws-openai), a REST API implementing each of the [30 example applications](https://platform.openai.com/examples) from the official [OpenAI API Documentation](https://platform.openai.com/docs/api-reference/making-requests?lang=python) using a modularized Terraform approach. Implemented as a serverless microservice using AWS [API Gateway](https://aws.amazon.com/api-gateway/), [Lambda](https://aws.amazon.com/lambda/) and the [OpenAI Python Library](https://pypi.org/project/openai/). Leverages OpenAI's suite of AI models, including [GPT-3.5](https://platform.openai.com/docs/models/gpt-3-5), [GPT-4](https://platform.openai.com/docs/models/gpt-4), [DALL·E](https://platform.openai.com/docs/models/dall-e), [Whisper](https://platform.openai.com/docs/models/whisper), [Embeddings](https://platform.openai.com/docs/models/embeddings), and [Moderation](https://platform.openai.com/docs/models/moderation).

### Additional context

This is a non-commercial community-oriented project. Contributors are welcome. Next steps: add some of the ""cookbook"" ideas.",closed,2023-09-14T18:24:23Z,2023-11-10T03:41:39Z,[],https://github.com/openai/openai-python/issues/617,1353.29,False,,,,,
574,Error: Response payload is not completed,"### Describe the bug

When using the `acreate` interface and attempting to utilize streaming output, I encountered ""aiohttp.client_exceptions.ClientPayloadError: Response payload is not completed""

- **Time**: (UTC) 2023-08-17 06:08:24,030 ~ 2023-08-17 06:13:24,180
- **Request ID**: 9c46544d6517b7b8e9df1e9e61fd7f53

### To Reproduce

1. using acreate interface
2. stream = True

### Code snippets

```Python
async for chunk in completion_stream:
File ""/usr/local/lib/python3.9/site-packages/openai/api_resources/abstract/engine_api_resource.py"", line 230, in <genexpr>
return (
File ""/usr/local/lib/python3.9/site-packages/openai/api_requestor.py"", line 319, in wrap_resp
async for r in resp:
File ""/usr/local/lib/python3.9/site-packages/openai/api_requestor.py"", line 633, in <genexpr>
return (
File ""/usr/local/lib/python3.9/site-packages/openai/api_requestor.py"", line 114, in parse_stream_async
async for line in rbody:
File ""/usr/local/lib/python3.9/site-packages/aiohttp/streams.py"", line 35, in __anext__
rv = await self.read_func()
File ""/usr/local/lib/python3.9/site-packages/aiohttp/streams.py"", line 311, in readline
return await self.readuntil()
File ""/usr/local/lib/python3.9/site-packages/aiohttp/streams.py"", line 343, in readuntil
await self._wait(""readuntil"")
File ""/usr/local/lib/python3.9/site-packages/aiohttp/streams.py"", line 304, in _wait
await waiter
aiohttp.client_exceptions.ClientPayloadError: Response payload is not completed
```


### OS

Linux

### Python version

Python v3.9

### Library version

openai-python v0.27.2",closed,2023-08-17T07:39:16Z,2023-11-10T03:40:57Z,"['bug', 'Azure', 'fixed in v1']",https://github.com/openai/openai-python/issues/574,2036.03,True,method_order_violation,medium,"The error indicates that the response payload was not completed, suggesting that the streaming output was not properly initialized or that the method calls required for streaming were not executed in the correct order.",high,
435,Pandas UDF Failure,"### Describe the bug

When trying to create a PANDAS UDF to run the latest GPT3.5Turbo API via Azure openAI completion function it gives an error regarding missing `key`. 

### To Reproduce

### Code snippets

```
#!pip install openai

from pyspark.sql import SparkSession
import openai
from pyspark.sql.functions import pandas_udf  
from pyspark.sql.types import StringType
import pandas as pd
import os

# Bootstrap Spark Session
spark = SparkSession.builder.getOrCreate()

service_name = ""oai-eus-ww-int-hqfin""
deployment_name = ""GPT35Model""

os.environ[""OPENAI_API_KEY""] = """"

# Setup for OpenAI API for GPT3.5Turbo
openai.api_type = ""azure""
openai.api_base = """"
openai.api_version = ""2023-03-15-preview""
openai.api_key = os.environ.get(""OPENAI_API_KEY"")

# Create a Spark DataFrame with a prompt column
data = [  
    (""Python code for Hello World""),  
    (""What is GZCL?""),
    (""Who is Sandman?"")  
]  
df = spark.createDataFrame(pd.DataFrame(data, columns=[""prompt""]))

# Define the pandas UDF function  
@pandas_udf(returnType=StringType())
def generate_text(input: pd.Series) -> pd.Series:  
    results = []
    prompt = """"
    for n in input:  
        message = prompt + n  
          
        responseGPT = openai.ChatCompletion.create(  
            engine=""GPT35Model"",  
            messages=[{  
                ""role"": ""user"",  
                ""content"": message  
            }],  
            temperature=0.1,  
            max_tokens=800,  
            top_p=0.95,  
            frequency_penalty=0,  
            presence_penalty=0,  
            stop=None  
        )  
          
        results.append(responseGPT[""choices""][0][""message""][""content""])  
      
    return pd.Series(results)

df = df.withColumn(""results"", generate_text(""prompt"")).cache()
display(df)
```

### OS

linux

### Python version

Python v3.10

### Library version

openai-python v0.27.6",closed,2023-05-04T16:29:56Z,2023-11-10T03:37:31Z,['bug'],https://github.com/openai/openai-python/issues/435,4547.13,True,missing_dependency_violation,high,"The API call to OpenAI's GPT-3.5 Turbo fails due to a missing API key, which is a required dependency for the API to function correctly.",high,
650,JSONDecodeError when streaming chunks for ChatCompletion,"### Describe the bug

`openai.ChatCompletion.acreate(..., stream=True)` seems to fail with a JSONDecodeError when it's called such that:
- the prompt alone doesn't exceed the rate limit
- the generated output + prompt together exceeds the rate limit

Example:
- Suppose my rate limit is 10000 tokens/min, and my prompt is 8000 tokens.
- If the generated output is 2000+ tokens, then I hit my rate limit before my response is finished streaming
- The first 2000 tokens stream successfully, but after I hit my rate limit, the OpenAI server streams a rate limit exception message to me
- This rate limit exception message is parsed incorrectly, triggering JSONDecodeError

I believe this is parsed incorrectly because the OpenAI Python library parses each line in the streamed output separately and assumes each line is JSON. However, the rate limit exception message is pretty-printed JSON and includes internal newlines, so [this call](https://github.com/openai/openai-python/blob/284c1799070c723c6a553337134148a7ab088dd8/openai/api_requestor.py#L765) to `JSON.loads` fails.

This is the line in the streamed output that triggered the error: `""{\""rate_limit_usage\"": {\`

Error below:
```
HTTP code 200 from API (""{\""rate_limit_usage\"": {\)
Traceback (most recent call last):
  File "".venv/lib/python3.10/site-packages/openai/api_requestor.py"", line 765, in _interpret_response_line
    data = json.loads(rbody)
  File ""/Users/xxx/.pyenv/versions/3.10.11/lib/python3.10/json/__init__.py"", line 346, in loads
    return _default_decoder.decode(s)
  File ""/Users/xxx/.pyenv/versions/3.10.11/lib/python3.10/json/decoder.py"", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File ""/Users/xxx/.pyenv/versions/3.10.11/lib/python3.10/json/decoder.py"", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
json.decoder.JSONDecodeError: Unterminated string starting at: line 1 column 1 (char 0)

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  ...
  File "".venv/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py"", line 232, in <genexpr>
    return (
  File "".venv/lib/python3.10/site-packages/openai/api_requestor.py"", line 396, in wrap_resp
    async for r in resp:
  File "".venv/lib/python3.10/site-packages/openai/api_requestor.py"", line 725, in <genexpr>
    self._interpret_response_line(
  File "".venv/lib/python3.10/site-packages/openai/api_requestor.py"", line 767, in _interpret_response_line
    raise error.APIError(
openai.error.APIError: HTTP code 200 from API (""{\""rate_limit_usage\"": {\)
```

### To Reproduce

Call `openai.ChatCompletion.acreate(..., stream=True)` with a prompt that doesn't exceed the rate limit, but whose generated output + prompt exceeds the rate limit.

### Code snippets

_No response_

### OS

macOS

### Python version

Python v3.10.11

### Library version

openai-python v0.28.1",closed,2023-10-12T02:35:29Z,2023-11-10T03:36:11Z,"['bug', 'fixed in v1']",https://github.com/openai/openai-python/issues/650,697.01,False,,,,low,
621,Order ,"### Describe the bug

When the number of prompt tokens exceeds the maximum context length, an `InvalidRequestError` is raised. However, when the number of prompt tokens also exceeds the remaining (or total) token rate limit per minute, a `RateLimitError` is raised first. 

This messes with potential retry logic, as upon an `InvalidRequestError`, every attempt to retry the request is futile, while this is not the case for `RateLimitError`. It only makes sense to retry a request if the request itself is valid.

### To Reproduce

1. Send a request with the number of prompt tokens exceeding the max context length of the requested model without exceeding the token rate limit per minute. 
2. Send a request with the number of prompt tokens exceeding both the max context length of the requested model and the token rate limit per minute. 
3. Observe the exceptions.

### Code snippets

_No response_

### OS

Win10

### Python version

Python v3.11.3

### Library version

openai-python-v0.28.0",closed,2023-09-20T13:54:28Z,2023-11-10T03:33:50Z,['API-feedback'],https://github.com/openai/openai-python/issues/621,1213.66,True,input_value_violation,high,"The API raises an InvalidRequestError when the number of prompt tokens exceeds the maximum context length, indicating that the input value is out of acceptable bounds.",high,
666,Invalid URL 'gpt-3.5-turbo/fine_tuning/jobs': No scheme supplied. Perhaps you meant https://gpt-3.5-turbo/fine_tuning/jobs?,"### Describe the bug

When using fine tuning I uploaded a file with success and when creating a job using the resulting job id I get this error:
`Error communicating with OpenAI: Invalid URL 'gpt-3.5-turbo/fine_tuning/jobs': No scheme supplied. Perhaps you meant https://gpt-3.5-turbo/fine_tuning/jobs?`



### To Reproduce

This is the python code I'm running:
`response = openai.FineTuningJob.create(training_file_id, model)`

for model I'm using ""gpt-3.5-turbo""

I'm using openai library version 0.28.1

### Code snippets

_No response_

### OS

linux

### Python version

3.11

### Library version

0.28.1",closed,2023-10-26T09:47:09Z,2023-11-10T03:29:25Z,['bug'],https://github.com/openai/openai-python/issues/666,353.7,True,input_value_violation,high,"The URL provided is missing a scheme (e.g., http or https), which is required for proper communication with the API.",high,
656,openai.error.APIError: Invalid response object from API？,"### Describe the bug

```
openai.api_base = ""http://localhost:10860/v1/embeddings""

def test_embedding():
    embedding = openai.Embedding.create(model =model,input=""hello"")
    print(embedding)
    print(embedding[""data""][0][""embedding""].shape)
    print(embedding[""data""][0][""embedding""])
```

### To Reproduce

Traceback (most recent call last):
  File ""C:\Users\loong\.conda\envs\nlp\lib\site-packages\openai\api_requestor.py"", line 403, in handle_error_response
    error_data = resp[""error""]
KeyError: 'error'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""D:\llm\NLP\aquila_openai_api_vllm.py"", line 54, in <module>
    test_embedding()
  File ""D:\llm\NLP\aquila_openai_api_vllm.py"", line 25, in test_embedding
    embedding = openai.Embedding.create(model =model,input=""你好"")
  File ""C:\Users\loong\.conda\envs\nlp\lib\site-packages\openai\api_resources\embedding.py"", line 33, in create
    response = super().create(*args, **kwargs)
  File ""C:\Users\loong\.conda\envs\nlp\lib\site-packages\openai\api_resources\abstract\engine_api_resource.py"", line 153, in create
    response, _, api_key = requestor.request(
  File ""C:\Users\loong\.conda\envs\nlp\lib\site-packages\openai\api_requestor.py"", line 298, in request
    resp, got_stream = self._interpret_response(result, stream)
  File ""C:\Users\loong\.conda\envs\nlp\lib\site-packages\openai\api_requestor.py"", line 707, in _interpret_response
    self._interpret_response_line(
  File ""C:\Users\loong\.conda\envs\nlp\lib\site-packages\openai\api_requestor.py"", line 772, in _interpret_response_line
    raise self.handle_error_response(
  File ""C:\Users\loong\.conda\envs\nlp\lib\site-packages\openai\api_requestor.py"", line 405, in handle_error_response
    raise error.APIError(
openai.error.APIError: Invalid response object from API: '{""detail"":""Not Found""}' (HTTP response code was 404)

### Code snippets

_No response_

### OS

centos

### Python version

3.10

### Library version

0.28.0",closed,2023-10-14T13:48:54Z,2023-11-10T03:28:05Z,['bug'],https://github.com/openai/openai-python/issues/656,637.65,True,input_value_violation,unknown,"The API returned a 404 error indicating that the requested resource was not found, potentially due to an invalid model name or incorrect input.",high,
349,Exception is thrown during parsing of response to a request which triggered Azure's content management,"### Describe the bug

When the Azure content management system flags a request, the library fails to handle the response, causing an exception to be thrown.

Example stack trace:
```
Traceback (most recent call last):
  File ""/Users/WSkinner/.pyenv/versions/3.10.7/envs/ml-generative/lib/python3.10/site-packages/openai/api_requestor.py"", line 331, in handle_error_response
    error_data = resp[""error""]
TypeError: string indices must be integers

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/Users/WSkinner/work/ripcord/ml-generative/bug.py"", line 9, in <module>
    response = openai.Completion.create(
  File ""/Users/WSkinner/.pyenv/versions/3.10.7/envs/ml-generative/lib/python3.10/site-packages/openai/api_resources/completion.py"", line 25, in create
    return super().create(*args, **kwargs)
  File ""/Users/WSkinner/.pyenv/versions/3.10.7/envs/ml-generative/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py"", line 153, in create
    response, _, api_key = requestor.request(
  File ""/Users/WSkinner/.pyenv/versions/3.10.7/envs/ml-generative/lib/python3.10/site-packages/openai/api_requestor.py"", line 226, in request
    resp, got_stream = self._interpret_response(result, stream)
  File ""/Users/WSkinner/.pyenv/versions/3.10.7/envs/ml-generative/lib/python3.10/site-packages/openai/api_requestor.py"", line 619, in _interpret_response
    self._interpret_response_line(
  File ""/Users/WSkinner/.pyenv/versions/3.10.7/envs/ml-generative/lib/python3.10/site-packages/openai/api_requestor.py"", line 682, in _interpret_response_line
    raise self.handle_error_response(
  File ""/Users/WSkinner/.pyenv/versions/3.10.7/envs/ml-generative/lib/python3.10/site-packages/openai/api_requestor.py"", line 333, in handle_error_response
    raise error.APIError(
  File ""/Users/WSkinner/.pyenv/versions/3.10.7/envs/ml-generative/lib/python3.10/site-packages/openai/error.py"", line 32, in __init__
    self.error = self.construct_error_object()
  File ""/Users/WSkinner/.pyenv/versions/3.10.7/envs/ml-generative/lib/python3.10/site-packages/openai/error.py"", line 62, in construct_error_object
    or not isinstance(self.json_body[""error""], dict)
TypeError: string indices must be integers
```

There is a [related issue](https://github.com/openai/openai-python/issues/331), from which I have copied the prompt which triggers the content management policy. However, that issue appears to be treating the rejection of a request due to content management, resulting in a 400 status code, as a bug. This new issue is specifically regarding the `openai-python` library's treatment of that response, not the fact that the response was returned.

### To Reproduce

To reproduce the issue, run the following code.
```python
import os
import openai

openai.api_type = ""azure""
openai.api_version = ""2023-03-15-preview""
openai.api_base = ""https://rc-ai.openai.azure.com/""
openai.api_key = os.getenv(""AZURE_OPENAI_API_KEY"")

response = openai.Completion.create(
    engine=""gpt-35-turbo"",
    prompt=""SUBREDDIT: r/AskReddit TITLE: Cock blocked by a friend (Who's a girl). POST: So for the past week there's ""
        ""been this girl in one of my classes I've been talking to, she's pretty cute (dyed red hair, fair skin, ""
        ""a few freckles, not ginger), she loves star wars and I suspect she's a redditor. I was going to ask her for ""
        ""her number today, but a girl i met about a year ago came and sat right where the red head had been sitting, ""
        ""effectively cock-blocking me and driving the girl I was interested in away. Now it seems like the red head ""
        ""thinks I'm uninterested in her and has since found some other guy to talk to. Has anybody been in a similar ""
        ""scenario? Advice? \nTL;DR: Got cock blocked by a friend who's a girl.""
)
```

### Code snippets

_No response_

### OS

macOS

### Python version

3.10.7

### Library version

0.27.2",closed,2023-03-27T19:09:28Z,2023-11-10T03:27:01Z,"['bug', 'Azure', 'fixed in v1']",https://github.com/openai/openai-python/issues/349,5456.29,True,input_type_violation,high,"The library fails to handle the response correctly when a request is flagged by Azure's content management, resulting in a TypeError due to unexpected response format.",high,
671,Assistant not using function response,"### Describe the bug

When my assistant gets the output from a function with long text it doesn't use that output to response to the user. Instead it makes reference to it like if the final user could also see the response of the function when it is actually internal. Here's an example:
```

{
    {
      ""role"": ""assistant"",
      ""function_call"": {
        ""name"": ""answer_question"",
        ""arguments"": {
          ""question"": ""what happened in catalonia during ww2""
        }
      },
      ""content"": null
    },
    {
      ""role"": ""function"",
      ""name"": ""answer_question"",
      ""content"": ""During World War II, Catalonia was part of Spain under the rule of Francisco Franco. Although Spain declared neutrality, Catalonia was affected by the conflict. Italy and Germany had some interest in Catalonia before the war, but their attempts to establish a fascist movement failed. Barcelona was bombed by Italian planes supporting the Nationalist side in the Spanish Civil War. Catalan individuals, such as Joan Pujol and Josep Trueta, played roles in the Allied side, with Pujol acting as a double agent and Trueta organizing medical services. Some Catalans also fought on the Soviet side in the Eastern Front.""
    },
    {
      ""role"": ""assistant"",
      ""content"": ""Thank you for your question about Catalonia during WWII. Now, can you tell me the time period during which World War II took place?""
    }
}
```

### To Reproduce

Run the script copied below

### Code snippets

```Python
import openai
import json
​
openai.api_key = ""YOUR_API_KEY""
​
​
# Example dummy function hard coded to return the same answer to the example question
def answer_question(question):
    """"""Get the answer to the student's question""""""
    answer = ""During World War II, Catalonia was part of Spain under the rule of Francisco Franco. Although Spain declared neutrality, Catalonia was affected by the conflict. Italy and Germany had some interest in Catalonia before the war, but their attempts to establish a fascist movement failed. Barcelona was bombed by Italian planes supporting the Nationalist side in the Spanish Civil War. Catalan individuals, such as Joan Pujol and Josep Trueta, played roles in the Allied side, with Pujol acting as a double agent and Trueta organizing medical services. Some Catalans also fought on the Soviet side in the Eastern Front.""
    return answer
​
​
def run_conversation():
    messages = [
        {
            ""role"": ""system"",
            ""content"": ""I want you to act as a history teacher who has to ask some questions to his students.\nQuestions: \n- When did WWII take place?\n- Which countries were the principal belligerents?\nYour primary focus is asking the provided questions but if the student asks any history questions, you will try to answer them using the provided functions.\n""
        },
        {
            ""role"": ""assistant"",
            ""content"": ""Can I ask you some history questions?""
        },
        {
            ""role"": ""user"",
            ""content"": ""what happened in Catalonia during WW2""
        }
    ]
    functions = [
       {
           ""name"": ""answer_question"",
           ""description"": ""Answers a question from the student that is outside the context of the agent's prompt"",
           ""parameters"": {
               ""type"": ""object"",
               ""properties"": {
                   ""question"": {
                       ""description"": ""The question the student is asking"",
                       ""type"": ""string""
                   }
               },
               ""required"": [""question""]
           }
       }
   ]
    response = openai.ChatCompletion.create(
        model=""gpt-4"",
        messages=messages,
        functions=functions,
        function_call=""auto"",
    )
    response_message = response[""choices""][0][""message""]
​
    if response_message.get(""function_call""):
        available_functions = {
            ""answer_question"": answer_question,
        }
        function_name = response_message[""function_call""][""name""]
        function_to_call = available_functions[function_name]
        function_args = json.loads(response_message[""function_call""][""arguments""])
        function_response = function_to_call(
            question=function_args.get(""question""),
        )
​
        messages.append(response_message)
        messages.append(
            {
                ""role"": ""function"",
                ""name"": function_name,
                ""content"": function_response,
            }
        )
        second_response = openai.ChatCompletion.create(
            model=""gpt-4"",
            messages=messages,
        )
        return second_response
​
​
print(run_conversation())
```
```


### OS

macOS

### Python version

Python v3.9.*

### Library version

openai-python v0.27.*",closed,2023-10-31T08:34:12Z,2023-11-10T03:24:58Z,"['bug', 'API-feedback']",https://github.com/openai/openai-python/issues/671,234.85,True,missing_dependency_violation,high,"The assistant fails to use the function response in its output, indicating a missing dependency on the function's output for generating a coherent response.",high,
288,Why do we get a `openai.error.InvalidRequestError` if we pass `None` to `logit_bias` even if it defaults to `null`?,"### Describe the bug

According to [the docs](https://platform.openai.com/docs/api-reference/completions/create#completions/create-logit_bias), `logit_bias` defaults to `null`, but if we pass `None` to this parameter we get `openai.error.InvalidRequestError`

> openai.error.InvalidRequestError: None is not of type 'object' - 'logit_bias'

So, I suppose this is a bug in this package, as `None` should correspond to what the docs refer to as `null`, or the documentation is wrong.

### To Reproduce

Use the script below

### Code snippets

```Python
import openai

completions = openai.Completion.create(model=""text-davinci-003"",
                                       prompt=""hello"",
                                       logit_bias=None)
print(completions)
```


### OS

macos monterey (12.5.1)

### Python version

Python 3.8.13

### Library version

0.27.0",closed,2023-03-08T14:41:19Z,2023-11-10T03:20:35Z,"['bug', 'API-feedback']",https://github.com/openai/openai-python/issues/288,5916.65,True,input_type_violation,high,"Passing 'None' to 'logit_bias' does not match the expected input type, which should be an object, leading to an InvalidRequestError.",high,
366,Whisper Timestamp support.,"### Describe the feature or improvement you're requesting

- output returned by Whisper should contain sentence-level ad word-level timestamps.

### Additional context

_No response_",closed,2023-04-03T11:25:35Z,2023-11-10T03:17:41Z,[],https://github.com/openai/openai-python/issues/366,5295.87,False,,,,low,
333,[Whisper] Audio format errors on valid file,"### Describe the bug

Hello

I am trying to integrate the whisper API into my Flask app. However I get the following error when I input the received file from the flask endpoint, I get the following error:

```
openai.error.InvalidRequestError: Invalid file format. Supported formats: ['m4a', 'mp3', 'webm', 'mp4', 'mpga', 'wav', 'mpeg']
```

However, loading the file in the interactive console works fine.
```
In [16]: r = openai.Audio.transcribe('whisper-1',open('../Downloads/sample.mp3','rb'))

In [17]: r
Out[17]:
<OpenAIObject at 0x192993c6750> JSON: {
  ""text"": ""This episode is actually a co-production with another podcast called Digital Folklore, which is hosted by Mason Amadeus and Perry Carpenter. We've been doing a lot of our research together and our brainstorming sessions have been so thought-provoking, I wanted to bring them on so we could discuss the genre of analog horror together. So, why don't you guys introduce yourselves so we know who's who? Yeah, this is Perry Carpenter and I'm one of the hosts of Digital Folklore. And I'm Mason Amadeus and I'm the other host of Digital Folklore. And tell me, what is Digital Folklore? Yeah, so Digital Folklore is the evolution of folklore, you know, the way that we typically think about it. And folklore really is the product of basically anything that humans create that doesn't have a centralized canon. But when we talk about digital folklore, we're talking about...""
}
```


### To Reproduce

1.  Create a Flask App.
2. Add an end point that receives an valid audio file.
3. pass the bytes data of the file to `openai.Audio.transcribe` method through 'request.files[fileName].stream.read()`.

### Code snippets

```Python
The end point code:


with tempfile.TemporaryFile() as temp_file:
    temp_file.write(audio_file)
    transcript_read = openai.Audio.transcribe(""whisper-1"", temp_file)
return transcript_read
```


the FFprobe info of the file:
```
ffprobe version 4.4.1-full_build-www.gyan.dev Copyright (c) 2007-2021 the FFmpeg developers
  built with gcc 11.2.0 (Rev1, Built by MSYS2 project)
  configuration: --enable-gpl --enable-version3 --enable-static --disable-w32threads --disable-autodetect --enable-fontconfig --enable-iconv --enable-gnutls --enable-libxml2 --enable-gmp --enable-lzma --enable-libsnappy --enable-zlib --enable-librist --enable-libsrt --enable-libssh --enable-libzmq --enable-avisynth --enable-libbluray --enable-libcaca --enable-sdl2 --enable-libdav1d --enable-libzvbi --enable-librav1e --enable-libsvtav1 --enable-libwebp --enable-libx264 --enable-libx265 --enable-libxvid --enable-libaom --enable-libopenjpeg --enable-libvpx --enable-libass --enable-frei0r --enable-libfreetype --enable-libfribidi --enable-libvidstab --enable-libvmaf --enable-libzimg --enable-amf --enable-cuda-llvm --enable-cuvid --enable-ffnvcodec --enable-nvdec --enable-nvenc --enable-d3d11va --enable-dxva2 --enable-libmfx --enable-libglslang --enable-vulkan --enable-opencl --enable-libcdio --enable-libgme --enable-libmodplug --enable-libopenmpt --enable-libopencore-amrwb --enable-libmp3lame --enable-libshine --enable-libtheora --enable-libtwolame --enable-libvo-amrwbenc --enable-libilbc --enable-libgsm --enable-libopencore-amrnb --enable-libopus --enable-libspeex --enable-libvorbis --enable-ladspa --enable-libbs2b --enable-libflite --enable-libmysofa --enable-librubberband --enable-libsoxr --enable-chromaprint
  libavutil      56. 70.100 / 56. 70.100
  libavcodec     58.134.100 / 58.134.100
  libavformat    58. 76.100 / 58. 76.100
  libavdevice    58. 13.100 / 58. 13.100
  libavfilter     7.110.100 /  7.110.100
  libswscale      5.  9.100 /  5.  9.100
  libswresample   3.  9.100 /  3.  9.100
  libpostproc    55.  9.100 / 55.  9.100
Input #0, mp3, from '.\Downloads\sample.mp3':
  Metadata:
    title           : Monsters in the Static
    comment         : We look at the subgenre of analog horror, where something sinister might be lurking in the horizontal lines and vertical holds of those old VHS tapes.
    lyrics-ENG      : <p>In the subgenre of analog horror, there’s something sinister or supernatural lurking in the horizontal lines and vertical holds in those old VHS tapes. Filmmaker <a href=""https://wnuf.bigcartel.com/"">Chris LaMartina</a> explains why he wanted his mov
    album           : Imaginary Worlds
    genre           : Podcast
    date            : 2020
    encoder         : Lavf58.76.100
  Duration: 00:00:50.05, start: 0.025057, bitrate: 128 kb/s
  Stream #0:0: Audio: mp3, 44100 Hz, stereo, fltp, 128 kb/s
    Metadata:
      encoder         : Lavc58.13
```


### OS

Windows 11

### Python version

Python v10.5

### Library version

0.27.2",closed,2023-03-22T12:15:09Z,2023-11-10T03:15:32Z,['bug'],https://github.com/openai/openai-python/issues/333,5583.01,True,input_type_violation,high,"The API method received an input file format that is not supported, despite the file being valid in other contexts.",high,
607,Why is Azure openai api throwing an exception when attempting to handle a quota throttling error?,"### Describe the bug

In the example below, the Azure openai quota is exceeded and generates an expected exception. However, while handling that exception, the openai api generates another exception. It looks as though the ""error_data"" variable is a string instead of a dictionary. The ""error_data"" string contains ""Quota has been exceeded"" instead of a dictionary like {""message"": ""Quota has been exceeded""}

Running miniconda and openai 0.27.4

Ref: [Why is Azure openai api throwing an exception when attempting to handle a quota throttling error?](https://learn.microsoft.com/en-us/answers/questions/1362313/why-is-azure-openai-api-throwing-an-exception-when)
Can you please help me.

### To Reproduce

In the example below, the Azure openai quota is exceeded and generates an expected exception. However, while handling that exception, the openai api generates another exception. It looks as though the ""error_data"" variable is a string instead of a dictionary. The ""error_data"" string contains ""Quota has been exceeded"" instead of a dictionary like {""message"": ""Quota has been exceeded""}

Running miniconda and openai 0.27.4

```
import os
import openai

openai.api_type = ""azure""
openai.api_base = os.getenv(""AZURE_OPENAI_ENDPOINT"")
openai.api_version = ""2023-05-15""
openai.api_key = os.getenv(""AZURE_OPENAI_KEY"")

# The intent here is to make 2 calls in rapid succession to generate
# an error that we have exceeded the quota.
response = openai.Completion.create(
engine=""gpt-35-turbo""
messages=[
{""role"": ""system"", ""content"": ""You are a helpful assistant.""},
{""role"": ""user"", ""content"": ""Tell me a story about an elephant?""}
]
)
# Somewhat brittle test that may need to be done a few times to exceed
# the quota
response = openai.Completion.create(
engine=""gpt-35-turbo""
messages=[
{""role"": ""system"", ""content"": ""You are a helpful assistant.""},
{""role"": ""user"", ""content"": ""Tell me a story about a fox?""}
]
)

AttributeError                            Traceback (most recent call last)
.
.
.
File ~\Projects\chatGPT\env_conda\lib\site-packages\openai\api_requestor.py:346, in APIRequestor.handle_error_response(self, rbody, rcode, resp, rheaders, stream_error)
       341 if ""internal_message"" in error_data:
       342     error_data[""message""] += ""\n\n"" + error_data[""internal_message""]
       344 util.log_info(
       345     ""OpenAI API error received"",
-->    346     error_code=error_data.get(""code""),
       347     error_type=error_data.get(""type""),
       348     error_message=error_data.get(""message""),
       349     error_param=error_data.get(""param""),
       350     stream_error=stream_error,
       351 )
       353 # Rate limits were previously coded as 400's with code 'rate_limit'
       354 if rcode == 429:
AttributeError: 'str' object has no attribute 'get'
```
Ref: [Why is Azure openai api throwing an exception when attempting to handle a quota throttling error?](https://learn.microsoft.com/en-us/answers/questions/1362313/why-is-azure-openai-api-throwing-an-exception-when)
Can you please help me.

### Code snippets

```Python
import os
import openai

openai.api_type = ""azure""
openai.api_base = os.getenv(""AZURE_OPENAI_ENDPOINT"")
openai.api_version = ""2023-05-15""
openai.api_key = os.getenv(""AZURE_OPENAI_KEY"")

# The intent here is to make 2 calls in rapid succession to generate
# an error that we have exceeded the quota.
response = openai.Completion.create(
engine=""gpt-35-turbo""
messages=[
{""role"": ""system"", ""content"": ""You are a helpful assistant.""},
{""role"": ""user"", ""content"": ""Tell me a story about an elephant?""}
]
)
# Somewhat brittle test that may need to be done a few times to exceed
# the quota
response = openai.Completion.create(
engine=""gpt-35-turbo""
messages=[
{""role"": ""system"", ""content"": ""You are a helpful assistant.""},
{""role"": ""user"", ""content"": ""Tell me a story about a fox?""}
]
)
```


### OS

windows

### Python version

Python v3.7.1

### Library version

v0.27.4",closed,2023-09-08T21:39:27Z,2023-11-10T03:11:11Z,['bug'],https://github.com/openai/openai-python/issues/607,1493.53,True,input_type_violation,high,"The 'error_data' variable is expected to be a dictionary but is a string, leading to an AttributeError when attempting to access its attributes.",high,
675,"timeout param is not handled correctly, resulting in no timeout being set","### Describe the bug

Passing a timeout to `openai.ChatCompletion.create` doesn't do anything. Likely due to he incorrect logic here: https://github.com/openai/openai-python/blob/main/openai/api_resources/abstract/engine_api_resource.py#L101
If pass 3, according to this I will have no timeout.

### To Reproduce

call a completion api with a small timeout

### Code snippets

_No response_

### OS

any

### Python version

any

### Library version

v0.28.1 ",closed,2023-11-04T19:17:47Z,2023-11-06T16:37:51Z,"['bug', 'fixed in v1']",https://github.com/openai/openai-python/issues/675,45.33,True,input_value_violation,performance_degradation,"The timeout parameter is not being handled correctly, leading to a situation where no timeout is effectively set, which can degrade performance.",high,
699,"client.files.retrieve_content only returns strings, and not bytes/binary data","When I try to download/retrieve a binary file (eg. PNG image) created by an assistant, it get automatically [cast to a string](https://github.com/openai/openai-python/blob/e0aafc6c1a45334ac889fe3e54957d309c3af93f/src/openai/resources/files.py#L229), so it can't be correctly parsed/displayed.

Eg:

``` py
ret_file = client.files.retrieve_content('file-XXX')
ret_file[:10]
# '�PNG\r\n\x1a\n\x00\x00'
```

There doesn't seem to be a clean way in the API to retrieve a file as raw bytes from what I can see.

This is important for code interpreter scenarios where the agent returns binary files that need to be rendered, like Images.

For others having this issue, you can just request the files directly using `requests` like:

``` py
import requests
from io import BytesIO

file_id = 'file-XXXX'
headers = {
    'Authorization': f""Bearer {os.environ['OPENAI_API_KEY']}""
}
response = requests.get(f'https://api.openai.com/v1/files/{file_id}/content', headers=headers)
Image.open(BytesIO(response.content))
```",closed,2023-11-07T02:14:25Z,2023-11-09T19:07:59Z,[],https://github.com/openai/openai-python/issues/699,64.89,True,input_type_violation,high,"The API method client.files.retrieve_content is expected to return binary data but instead returns a string, leading to incorrect data parsing and display.",high,
757,Get request headers data,"### Confirm this is a feature request for the Python library and not the underlying OpenAI API.

- [X] This is a feature request for the Python library

### Describe the feature or improvement you're requesting

When sending a request to openAI through this library in removes a lot of valuable metadata that comes back via headers, keeping those headers as part of the response would be great.  

### Additional context

_No response_",closed,2023-11-09T17:21:07Z,2023-11-09T17:27:44Z,[],https://github.com/openai/openai-python/issues/757,0.11,True,missing_option_violation,medium,"The feature request suggests a missing option to retain response headers, which could lead to suboptimal usage of the API by losing valuable metadata.",high,
740,Passing headers with the Azure OpenAI client in 1.1.1,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [x] This is an issue with the Python library

### Describe the bug

To be clear, I’m not certain this is a bug, but I’m uncertain how to do this now.

For extra security layers, I am exposing Azure OpenAI via the Azure API Management service. When I make a call this way, I pass a bearer JWT token and Apim subscription key as headers. I had no problem doing this on lower versions like 0.28.1, but now I don’t know how to do this.

### To Reproduce

Attempting to call Azure OpenAI with 1.1.1. The call does not successfully connect. After glancing through the source Python code, I thought maybe sticking the headers as `default_headers` in the AzureOpenAI client would work, but it does not.

```py
# Setting the auth headers
auth_headers = {
    ‘Ocp-Apim-Subscription-Key’: my_apim_sub_key,
    ‘Authorization’: f’Bearer {my_jwt_token}’
}

# Setting the Azure OpenAI client
azure_openai_client = AzureOpenAI(
   api_key = ‘null’,
   api_version = ‘2023-07-01-preview’,
   azure_endpoint = my_special_url,
   default_headers = auth_headers
)

# Calling Azure OpenAI
openai_response = azure_openai_client.chat.completions.create(
    model = ‘gpt-35-turbo-16k’,
    messages = my_messages
)
```

Here’s the way I’m doing it on 0.28.1, and this still works just fine. As you can see, I’m passing the auth headers at the time of the actual API call here.


```py
import openai

# Setting the auth headers
auth_headers = {
    ‘Ocp-Apim-Subscription-Key’: my_apim_sub_key,
    ‘Authorization’: f’Bearer {my_jwt_token}’
}

# Setting config information on OpenAI
openai.api_url = my_special_url
openai.api_version = ‘2023-07-01-preview’
openai.api_key = ‘null’

# Calling Azure OpenAI
openai_response = openai.ChatCompletion.create(
    engine = ‘gpt-35-turbo-16k’,
    messages = my_messages,
    Headers = auth_headers
)
```

What might I be doing wrong?

### Code snippets

_No response_

### OS

macOS

### Python version

Python v3.10

### Library version

1.1.1",closed,2023-11-08T19:38:46Z,2023-11-09T13:56:23Z,['question'],https://github.com/openai/openai-python/issues/740,18.29,True,missing_dependency_violation,medium,"The Azure OpenAI client in version 1.1.1 does not support passing headers in the same manner as version 0.28.1, indicating a missing dependency in the API contract regarding header management during API calls.",high,
752,AzureOpenAI error :(,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

I'm getting this error:
ImportError: cannot import name 'AzureOpenAI' from 'openai' (/usr/local/lib/python3.10/dist-packages/openai/__init__.py)
 while import ing lbrary, 0.28.1 version of openai is being used.
Whay should i do?

### To Reproduce

1. imported library openai and AzureOpenAI
2. didn't run gave error.

### Code snippets

_No response_

### OS

macos

### Python version

3.10

### Library version

0.28.1",closed,2023-11-09T14:17:50Z,2023-11-09T14:48:15Z,['question'],https://github.com/openai/openai-python/issues/752,0.51,True,input_type_violation,unknown,"The user is attempting to import a class 'AzureOpenAI' that may not exist in the specified version of the library, indicating a potential type mismatch or missing dependency in the API contract.",high,
710,Type incompatibility between ToolAssistantToolsFunctionFunction and Function,"`Function` from `types/chat/completion_create_params.py` is not compatible with `ToolAssistantToolsFunctionFunction` from `types/beta/assistant_create_param` because `description` is not a required field. 

I have existing code that's emitting a `Function` schema and using the type. Now that I'm transitioning some work over to assistants, I'd like to use the same function schema generation I've had before. For the moment I'll probably switch to using `ToolAssistantToolsFunctionFunction`, however these types may need some alignment.",closed,2023-11-07T12:39:18Z,2023-11-09T01:29:42Z,['enhancement'],https://github.com/openai/openai-python/issues/710,36.84,True,input_type_violation,high,Type incompatibility between Function and ToolAssistantToolsFunctionFunction due to the optional 'description' field.,high,
744,Pydantic serializer warnings when dumping a completion response with logprobs,"### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

Dumping a completion response from a request with `logprobs` set raises this warning:

```
python3.11/site-packages/pydantic/main.py:308: UserWarning: Pydantic serializer warnings:
  Expected `int` but got `float` - serialized value may not be as expected
  Expected `int` but got `float` - serialized value may not be as expected
  Expected `int` but got `float` - serialized value may not be as expected
  return self.__pydantic_serializer__.to_python(
```

I confirmed that changing [this type annotation](https://github.com/openai/openai-python/blob/ba424470504a470d1b0902847bcfae7149deae33/src/openai/types/completion_choice.py#L18) to

```python
    top_logprobs: Optional[List[Dict[str, float]]] = None
```

removes the warnings.

### To Reproduce

1. Install `openai>=1.0.0`
2. In a `completions` API call, set `logprobs>=1`
3. Dump the response using `.model_dump()`, as advised in the [migration guide](https://github.com/openai/openai-python/discussions/742)

### Code snippets

```Python
import openai

client = openai.OpenAI()

# Set logprobs >= 1
response = client.completions.create(
    model=""curie"", prompt=""test"", max_tokens=3, logprobs=1
)

response_as_dict = response.model_dump()
# raises a UserWarning
```

To ignore the warnings, the user currently has to do this:

```Python
import warnings

with warnings.catch_warnings():
    warnings.filterwarnings(
        ""ignore"",
        category=UserWarning,
        message=""Pydantic serializer warnings"",
    )
    response_as_dict = response.model_dump()
```

### OS

macOS

### Python version

Python v3.11.5

### Library version

openai v1.1.2",closed,2023-11-08T21:06:40Z,2023-11-09T01:27:42Z,['bug'],https://github.com/openai/openai-python/issues/744,4.35,True,input_type_violation,high,"The API expects an integer type for logprobs but receives a float, leading to potential incorrect serialization of the response.",high,
726,Support for generate function's json object,"For using func_call, hoping there is a parse tool to genenrate a json object of funtion's description. Like that:

1. Write an utils function with docstring

  ```python
  def foo(a: str, b: int = 0) -> None:
      """"""""
      Description for funcion
  
      Args:
          a(str): argument a
          b(int): argument b
      """"""""
  ```

2. Parse the `foo`

  ```python
  from openai.xxx import parse
  
  parse(foo)
  ```
  will get the output
  ```json
  {
    ""type"": ""function"",
    ""function"": {
       ""name"": ""foo"",
       ""description"": ""Description for funcion"",
       ""parameters"": {
          ""type"": ""object"",
          ""properties"": {
             ""a"": {""type"": ""str"", ""description"": ""argument a""},
             ""b"": {""type"": ""int"", ""description"": ""argument b""},
           },
           ""required"": [""a""],
        },
      },
  }
  ```

Maybe can use the `inspect` to get the argmument name and annotions, use `func.__doc__` and `re` to extract the descriptions of function and argmuents, or even call GPT to do that.
If this is ok, mayby I could try to do it.",closed,2023-11-08T00:54:53Z,2023-11-09T00:06:51Z,[],https://github.com/openai/openai-python/issues/726,23.2,False,,,,,
735,client.audio.transcriptions.create takes one positional argument but 3 were given,"I was just trying to update my code with latest openai. But i get this response. I just pass model, file and prompt as i did in previous versions. ",closed,2023-11-08T16:55:33Z,2023-11-08T18:40:48Z,['question'],https://github.com/openai/openai-python/issues/735,1.75,True,input_type_violation,high,"The API method client.audio.transcriptions.create expects one positional argument but received three, indicating a mismatch in the expected input type.",high,
78,AttributeError: module 'openai' has no attribute 'Embedding',"I've installed openai via: 

`pip install openai`
`pip install --upgrade openai` 

and yet when I try to run an example like:

```
import openai
print(openai.Embedding)
```

 I get the error 

```
Traceback (most recent call last):
  File ""/Users/travisbarton/.conda/envs/work3.6/lib/python3.6/site-packages/IPython/core/interactiveshell.py"", line 3343, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File ""<ipython-input-15-c5f5985add60>"", line 1, in <module>
    openai.Embedding
AttributeError: module 'openai' has no attribute 'Embedding'
```

the version I have is `0.8.0`

this happens for full examples, but this is just the most simple case.
any idea why?",closed,2022-03-10T00:05:11Z,2022-03-11T07:51:43Z,[],https://github.com/openai/openai-python/issues/78,31.78,True,input_type_violation,unknown,"The user is attempting to access an attribute 'Embedding' that does not exist in the 'openai' module, indicating a potential mismatch between expected and actual API structure.",high,
731,AzureChatOpenAI  missing ?,"I have started seeing this error since Monday when using the latest openai package:

ValidationError: 1 validation error for AzureChatOpenAI __root__ `openai` has no `ChatCompletion` attribute, this is likely due to an old version of the openai package. Try upgrading it with `pip install --upgrade openai`. (type=value_error)

I got a similar error when using ChatOpenAI as well.


But if I revert back to older version, e.g. 0.28.1, it all works fine.
Can someone look into this ?",closed,2023-11-08T13:20:38Z,2023-11-08T13:45:23Z,[],https://github.com/openai/openai-python/issues/731,0.41,True,missing_dependency_violation,high,"The error indicates a missing attribute 'ChatCompletion' in the 'openai' package, suggesting a dependency on a specific version of the package that is not met.",high,
730,Ai,,closed,2023-11-08T12:46:09Z,2023-11-08T12:53:33Z,[],https://github.com/openai/openai-python/issues/730,0.12,True,missing_dependency_violation,high,"The issue lacks any body content, indicating a missing dependency on required information for proper context or understanding.",high,
701,Cannot import OpenAI from openai,"The pypi page shows this example
```
from openai import OpenAI

client = OpenAI(
    # defaults to os.environ.get(""OPENAI_API_KEY"")
    api_key=""My API Key"",
)

chat_completion = client.chat.completions.create(
    messages=[
        {
            ""role"": ""user"",
            ""content"": ""Say this is a test"",
        }
    ],
    model=""gpt-3.5-turbo"",
)
```

Similar examples using openai.OpenAI can be found in official docs for Dalle-3:

```
from openai import OpenAI
client = OpenAI()

response = client.images.generate(
  model=""dall-e-3"",
  prompt=""a white siamese cat"",
  size=""1024x1024"",
  quality=""standard"",
  n=1,
)

image_url = response.data[0].url
```

However, this code will cause the following error (with openai==1.1.1):
ImportError: cannot import name 'OpenAI' from 'openai'

Tested this on both local Windows and a Databricks notebook.",closed,2023-11-07T02:21:25Z,2023-11-07T02:35:31Z,[],https://github.com/openai/openai-python/issues/701,0.23,True,input_type_violation,unknown,"The code attempts to import 'OpenAI' from the 'openai' module, which results in an ImportError, indicating that the expected type or structure is not available in the current version of the library.",high,
542,"Tried querying the ""text-davinci-003"" model via an API key and got this error APIConnectionError","### Describe the bug

APIConnectionError: Error communicating with OpenAI: HTTPSConnectionPool(host='api.openai.com', port=443): Max retries exceeded with url: /v1/engines/text-davinci-003/completions (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x14ea861b14e0>: Failed to establish a new connection: [Errno -2] Name or service not known'))

### To Reproduce

import openai

# Use the API key for authentication
openai.api_key = ""sk-XXXXXXXXXXXXXXXX""

# Define the model to use
model_engine = ""text-davinci-003""

# Define the prompt to use as input
prompt = ""Are all Indians into tech?""

# Request a completion from the model
completions = openai.Completion.create(
    engine=model_engine,
    prompt=prompt,
    max_tokens=1024,
    n=1,
    stop=None,
    temperature=0.5,
)

# Get the first response from the completions
message = completions.choices[0].text

# Print the response
print(message)

### Code snippets

_No response_

### OS

macOS 

### Python version

Python v3.7.1 

### Library version

openai-python v0.26.4",closed,2023-07-17T11:48:05Z,2023-09-06T12:16:49Z,['bug'],https://github.com/openai/openai-python/issues/542,1224.48,False,,,,low,
723,TypeError: object ImagesResponse can't be used in 'await' expression,"With `AsyncOpenAI`, `openai.images.generate` does not work correctly when it is used with await, but it works without await. This seems to work perfectly in NodeJS, but not here in Python.

Example code:

```
response = await openai.images.generate(
    model=""dall-e-3"",
    prompt=prompt,
    n=1,
    quality=""hd"",
    response_format=""b64_json"",
    size=(""792x1024""
)
```

And here is the traceback:

```
Traceback (most recent call last):
  File ""app.py"", line 83, in dall_e
    response = await openai.images.generate(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: object ImagesResponse can't be used in 'await' expression
```

By removing `await`, it works correctly, but it is obviously not running asynchronously.
",closed,2023-11-07T21:31:15Z,2023-11-07T21:54:14Z,[],https://github.com/openai/openai-python/issues/723,0.38,True,input_type_violation,unknown,The 'await' expression is incorrectly applied to an object that does not support asynchronous execution.,high,
721,async client Azure Support,will the async client be updated to support azure?,closed,2023-11-07T19:49:22Z,2023-11-07T19:59:08Z,[],https://github.com/openai/openai-python/issues/721,0.16,False,,,,low,
722,Error: module 'openai' has no attribute 'completion',"It was working before the update. Are there any alternatives to initialize the process?

```py
def analyze_text_with_gpt4(text):
    response = openai.ChatCompletion.create(
        model=""gpt-4-1106-preview"",
        messages=[
            {""role"": ""system"", ""content"": ""You are a helpful research assistant.""},
            {""role"": ""user"", ""content"": f""....: {text}""}
        ],
    )
    return response['choices'][0]['message']['content']
```

Except for switching back to the v0.28.",closed,2023-11-07T19:53:27Z,2023-11-07T19:58:08Z,[],https://github.com/openai/openai-python/issues/722,0.08,True,missing_option_violation,medium,"The error indicates that the 'openai' module no longer has the 'completion' attribute, suggesting that the API has changed and the user did not follow the updated usage guidelines.",high,
716,disregard,disregard,closed,2023-11-07T16:26:43Z,2023-11-07T16:31:39Z,[],https://github.com/openai/openai-python/issues/716,0.08,True,input_value_violation,unknown,"The issue body contains a vague term 'disregard' which does not provide meaningful input or context for the API, potentially leading to misinterpretation.",medium,
714,AttributeError: 'httpcore' module has no attribute 'UnsupportedProtocol' when importing openai,"### Issue description
When attempting to import the `openai` Python package, an `AttributeError` is raised, indicating that the `httpcore` module lacks the attribute 'UnsupportedProtocol'.

### Environment
* Python 3.10.12
* httpcore 1.0.1
* Databricks Runtime 13.1
* Spark 3.4.0

### Trouble Shooting
* Checked for updates on both `openai` and `httpcore` packages.
* Attempted to reinstall both packages.",closed,2023-11-07T14:52:55Z,2023-11-07T16:00:07Z,['question'],https://github.com/openai/openai-python/issues/714,1.12,True,input_type_violation,unknown,"The import error suggests a mismatch between the expected and actual attributes in the httpcore module, indicating a potential type issue with the library versions.",high,
707,AsssitantDeleted Typo in spec -> generated file,"While working with the types from `openai.types.beta` I noticed that there's one too many `s` in AssistantDeleted:

https://github.com/openai/openai-python/blob/e0aafc6c1a45334ac889fe3e54957d309c3af93f/src/openai/types/beta/asssitant_deleted.py#L10C11-L10C11

I'd make a PR, but this looks like its autogenerated from an OpenAPI Schema.",closed,2023-11-07T12:28:43Z,2023-11-07T15:36:14Z,['bug'],https://github.com/openai/openai-python/issues/707,3.13,False,,,,,
712,How can I use Async with Azure?,"Just now I'm updating from 0.28.1 to the latest version and migrating. I am currently using ```
await openai.ChatCompletion.acreate
```. I understand in migrating that I need to instantiate a Client, however there doesn't appear to be an Async client for Azure, only the standard `AzureOpenAI()` that doesn't appear to support Async.

Any guidance would be much appreciated - if this is a case of it not being implemented yet then I would just keep using 0.28.1 until there is Async support for Azure.",closed,2023-11-07T13:57:37Z,2023-11-07T15:30:53Z,['question'],https://github.com/openai/openai-python/issues/712,1.55,True,missing_dependency_violation,unknown,"The user is attempting to use an async method without the necessary async client implementation for Azure, indicating a missing dependency in the API contract.",medium,
700,Using Azure OpenAI with AAD example broken link in OpenAI v1 Release Notes Discussion,"https://github.com/openai/openai-python/discussions/631 links to https://github.com/openai/openai-python/blob/v1/examples/azure_ad.py

Fix: Should be https://github.com/openai/openai-python/blob/main/examples/azure_ad.py",closed,2023-11-07T02:20:42Z,2023-11-07T09:35:02Z,"['bug', 'documentation']",https://github.com/openai/openai-python/issues/700,7.24,False,,,,,
705,"chat-create-response_format - response_format={""type"": ""json_object""} does not follow other default types for that param","Most are `Typing.Literal`, not `Dict`. ~This is especially confusing given that the default param is a `str` and  the documentation seems to imply the [type is str](https://platform.openai.com/docs/api-reference/chat/create#chat-create-response_format)~. Instead it requires ` response_format={""type"": ""json_object""} ` and `{""type"": ""text""}`
```
openai.chat.completions.create(
            model=model_type,
            messages=prompts,
            response_format=""json_object"" 
        )
```
produces:
```
openai.BadRequestError: Error code: 400 - 
{'error': {'message': 
""'json_object' is not of type 'object' - 'response_format'"", 
'type': 'invalid_request_error', 'param': None, 'code': None}}
```

I haven't dug deeper; perhaps this has a reason I am missing, such as it is detected as dict, so the api ""knows"" the type. ",closed,2023-11-07T08:58:06Z,2023-11-07T09:25:15Z,[],https://github.com/openai/openai-python/issues/705,0.45,False,,,,low,
680,Getting ChatCompletion error via Azure OpenAI,"  `openai` has no `ChatCompletion` attribute, this is likely due to an old version of the openai package. Try upgrading it with `pip install --upgrade openai`. (type=value_error)
  

  What is that what should i do ? (FYI upgrading openai doesn't work out)",closed,2023-11-06T17:57:21Z,2023-11-06T18:02:06Z,['question'],https://github.com/openai/openai-python/issues/680,0.08,True,input_type_violation,unknown,"The user is attempting to access a non-existent attribute 'ChatCompletion' in the 'openai' package, indicating a potential mismatch between expected and actual API structure.",high,
704,ChatCompletion with Azure ,"  `openai` has no `ChatCompletion` attribute, this is likely due to an old version of the openai package. Try upgrading it with `pip install --upgrade openai`. (type=value_error)

Keep getting this error, although yesterday downgraded openai version it worked today it showes the same error with downgraded and upgraded model as well!
What should I do ?",closed,2023-11-07T06:36:28Z,2023-11-07T07:42:31Z,['question'],https://github.com/openai/openai-python/issues/704,1.1,True,input_type_violation,unknown,"The error indicates a mismatch between the expected API method and the provided package version, suggesting an input type issue with the API call.",high,
686,Assistant API url Invalid,"Code 

![image](https://github.com/openai/openai-python/assets/76844310/57d7616c-bde9-4864-ac72-485ffc216f6f)

Error

![image](https://github.com/openai/openai-python/assets/76844310/91920b85-1fa1-4391-8169-06e225bd1a16)

openai version - 1.1.0

",closed,2023-11-06T21:04:21Z,2023-11-07T05:32:27Z,[],https://github.com/openai/openai-python/issues/686,8.47,True,input_value_violation,High,"The API URL provided is invalid, which indicates that an unacceptable or out-of-range input value was supplied to the API method.",high,
691,Official Documentation Example for Function Calling Does Not Work Post Major Update,"## Issue Description
Following the recent major update to the OpenAI library, the function calling example provided in the official documentation no longer works.

### Steps to Reproduce
1. Visit the official OpenAI documentation on function calling with Chat models [here](https://platform.openai.com/docs/guides/function-calling).
2. Attempt to run the provided example.

### Expected Behavior
The function calling example should execute without errors, demonstrating how to utilize the feature with the latest library version.

### Actual Behavior
When attempting to run the function calling example from the official documentation, the following error occurs: [insert error message]
",closed,2023-11-06T22:29:53Z,2023-11-06T22:42:06Z,[],https://github.com/openai/openai-python/issues/691,0.2,False,,,,low,
670,GPT4V - API,"### Describe the feature or improvement you're requesting

I am requesting GPT4V API support. The ability to provide image input with the purpose of multimodality.

This feature has been out for a little bit now with ChatGPT Plus mobile and web. Even if this is not on the V1 release roadmap, it would be nice to understand timeline.

Any information on this functionality would be great. Thanks
",closed,2023-10-29T22:32:22Z,2023-11-06T22:05:58Z,[],https://github.com/openai/openai-python/issues/670,191.56,False,,,,low,
627,Support text and image pair query?,"### Describe the feature or improvement you're requesting

Since the new gpt4v avaible on mobile phones, will be available on python as well?

### Additional context

We really need text and image pairs query nowadays",closed,2023-09-28T07:15:16Z,2023-11-06T22:05:40Z,[],https://github.com/openai/openai-python/issues/627,950.84,True,missing_option_violation,medium,"The request for text and image pair queries suggests that the API may not currently support this combination, which is a missing option in the API's contract for handling multimodal inputs.",high,
685,Missing OpenAI object,"# Problem
```
from django.conf import settings
from openai import OpenAI
client = OpenAI()

client.models.list()
```

# Error
```
---------------------------------------------------------------------------
ImportError                               Traceback (most recent call last)
Cell In[6], line 2
      1 from django.conf import settings
----> 2 from openai import OpenAI
      3 client = OpenAI()
      5 client.models.list()

ImportError: cannot import name 'OpenAI' from 'openai' (/usr/local/lib/python3.11/site-packages/openai/__init__.py)
```

# Docs
https://platform.openai.com/docs/api-reference/models/list


# Version
```
pip freeze | grep openai
openai==1.1.0
```",closed,2023-11-06T20:56:18Z,2023-11-06T21:38:52Z,['question'],https://github.com/openai/openai-python/issues/685,0.71,True,missing_dependency_violation,high,"The OpenAI object is missing, leading to an ImportError when trying to instantiate the client.",high,
688,OpenAI client lacks Completions in version 1.1.0,"With version 1.1.0 installed to pyenv virtualenv and python 3.11.2 :

```
>>> from langchain.llms import OpenAI
>>> llm = OpenAI()
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/Users/lifehedging/.pyenv/versions/myenv/lib/python3.11/site-packages/langchain/load/serializable.py"", line 97, in __init__
    super().__init__(**kwargs)
  File ""/Users/lifehedging/.pyenv/versions/myenv/lib/python3.11/site-packages/pydantic/v1/main.py"", line 339, in __init__
    values, fields_set, validation_error = validate_model(__pydantic_self__.__class__, data)
                                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/lifehedging/.pyenv/versions/myenv/lib/python3.11/site-packages/pydantic/v1/main.py"", line 1102, in validate_model
    values = validator(cls_, values)
             ^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/lifehedging/.pyenv/versions/myenv/lib/python3.11/site-packages/langchain/llms/openai.py"", line 266, in validate_environment
    values[""client""] = openai.Completion
                       ^^^^^^^^^^^^^^^^^
AttributeError: module 'openai' has no attribute 'Completion'. Did you mean: 'completions'?
```


Or this issue is for langchain if openai has changed the interface.",closed,2023-11-06T21:39:32Z,2023-11-06T21:48:30Z,[],https://github.com/openai/openai-python/issues/688,0.15,True,input_type_violation,high,"The OpenAI client is attempting to access an attribute 'Completion' that does not exist in the 'openai' module, indicating a mismatch between expected and actual API structure.",high,
684,openai 1.0.1 is missing model and other things seems behind the docs,"Hey,

openai 1.0.1 is missing model and other things seems behind    

```
response = self.client.images.generate(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: Images.generate() got an unexpected keyword argument 'model'
```

Main docs page mentions model can be used
https://platform.openai.com/docs/guides/images/generations

![image](https://github.com/openai/openai-python/assets/11944251/d8d4ecb8-3644-4dd7-bf91-5a21104ed6a5)

",closed,2023-11-06T20:19:12Z,2023-11-06T20:59:34Z,[],https://github.com/openai/openai-python/issues/684,0.67,True,missing_option_violation,high,"The API method 'Images.generate()' does not recognize the 'model' argument, indicating a discrepancy between the documentation and the actual API implementation.",high,
681,Missing `client.audio.speech`?,"The example at https://platform.openai.com/docs/guides/text-to-speech/quick-start doesn't work. Not sure if you forgot to push an update to this repo?

```
dave@mbp openai-test-1 % python3 main.py
Traceback (most recent call last):
  File ""/Users/dave/Work/openai-test-1/main.py"", line 10, in <module>
    response = client.audio.speech.create(
               ^^^^^^^^^^^^^^^^^^^
AttributeError: 'Audio' object has no attribute 'speech'
```",closed,2023-11-06T18:32:18Z,2023-11-06T18:56:52Z,[],https://github.com/openai/openai-python/issues/681,0.41,True,missing_dependency_violation,unknown,"The 'Audio' object is expected to have a 'speech' attribute, which is missing, indicating a potential missing update or dependency.",high,
683,AttributeError: 'Audio' object has no attribute 'speech' in 1.0.1,"AttributeError: 'Audio' object has no attribute 'speech'

Traceback (most recent call last):
  File ""/Users/xxx/src/jshare-llm-demo/openai-demo/tts.py"", line 12, in <module>
    response = client.audio.speech.create(
AttributeError: 'Audio' object has no attribute 'speech'

```
from pathlib import Path
from openai import OpenAI

client = OpenAI(
    # defaults to os.environ.get(""OPENAI_API_KEY"")
    api_key=""xxxxxx"",
)

speech_file_path = Path(__file__).parent / ""speech.mp3""
response = client.audio.speech.create(
  model=""tts-1"",
  voice=""alloy"",
  input=""Today is a wonderful day to build something people love!""
)

response.stream_to_file(speech_file_path)
```
=============================

> OS: mac OS
> python: 3.9.*
> openai: 1.0.1",closed,2023-11-06T19:57:55Z,2023-11-06T20:10:13Z,[],https://github.com/openai/openai-python/issues/683,0.2,True,missing_dependency_violation,unknown,"The 'Audio' object does not have the expected 'speech' attribute, indicating a potential missing dependency or incorrect API usage.",high,
674,Images too different,"### Describe the bug

Im using dall-e 3 api but images with same prompt on chatgpt generate too much different images.

Prompt: ""cartoon style + king davi from bible""

From Python:
![e48d86e9840af5871a27df0db9eb6909](https://github.com/openai/openai-python/assets/395096/1b2c0041-52c2-4e15-b4c7-01bd117e3d9e)

From ChatGPT:
<img width=""844"" alt=""image"" src=""https://github.com/openai/openai-python/assets/395096/eec851da-80da-4267-a75b-47a40266365f"">

What im doing wrong?

### To Reproduce

In python call api with prompt ""cartoon style + king davi from bible"".

In chatgpt use prompt ""cartoon style + king davi from bible"".

### Code snippets

_No response_

### OS

macOS

### Python version

3.9

### Library version

latest",closed,2023-11-03T23:48:06Z,2023-11-06T19:04:06Z,['API-feedback'],https://github.com/openai/openai-python/issues/674,67.27,True,missing_option_violation,medium,"The user is not following the recommended prompt structure or context management strategies, leading to inconsistent image outputs.",high,
444,Moderation endpoint is missing a timeout/request_timeout parameter,"### Describe the feature or improvement you're requesting

Just like any other endpoint, a timeout is very much needed for the moderation endpoint.

### Additional context

_No response_",closed,2023-05-09T21:47:08Z,2023-11-06T17:42:53Z,"['bug', 'fixed in v1']",https://github.com/openai/openai-python/issues/444,4339.93,True,missing_dependency_violation,medium,"The moderation endpoint lacks a timeout parameter, which is essential for managing request durations and preventing hangs.",high,
402,openai cli `--stream` error,"### Describe the bug

![image](https://user-images.githubusercontent.com/13300261/232494925-353ab498-3b3a-4406-b780-955dc6afa1e7.png)

When adding the `--stream` parameter to the OpenAI command, an error occurred.

### To Reproduce

When adding the `--stream` parameter to the OpenAI command, an error occurred.

```
> openai api chat_completions.create -m gpt-3.5-turbo -g user ""Hello"" --stream
Traceback (most recent call last):
  File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python310\lib\runpy.py"", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python310\lib\runpy.py"", line 86, in _run_code
    exec(code, run_globals)
  File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python310\Scripts\openai.exe\__main__.py"", line 7, in <module>
  File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\_openai_scripts.py"", line 70, in main
    args.func(args)
  File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python310\lib\site-packages\openai\cli.py"", line 144, in create
    sys.stdout.write(c[""message""][""content""])
KeyError: 'message'
```

### Code snippets

_No response_

### OS

Windows

### Python version

Python v3.10.6

### Library version

openai v0.27.4",closed,2023-04-17T13:18:18Z,2023-11-06T17:37:59Z,"['bug', 'fixed in v1']",https://github.com/openai/openai-python/issues/402,4876.33,True,missing_option_violation,high,"The error occurs when the `--stream` option is used without the required context or setup, leading to a KeyError due to missing expected response structure.",high,
397,"Embeddings ""/v1/embeddings""endpoint support.","### Describe the feature or improvement you're requesting

Embeddings use api url as ""/engines/text-embedding-ada-002/embeddings"", can we support the ""/v1/embeddings"" with **model** in Request body? 
I 'm new to openai api and only found new entrypoint described in https://platform.openai.com/docs/api-reference/embeddings/create

### Additional context

_No response_",closed,2023-04-14T05:08:18Z,2023-11-06T17:36:17Z,[],https://github.com/openai/openai-python/issues/397,4956.47,True,missing_option_violation,medium,The request for supporting a new endpoint '/v1/embeddings' with a model in the request body may not adhere to existing API contract options for embeddings.,medium,
422,`get_embedding` does not update.,"### Describe the bug

I've noticed that using `get_embedding` repeatedly does not actually embeds the tokens but returns the previous result. Side by side comparison of `from openai.embeddings_utils import get_embedding` and `tiktoken` with just a length check:
```
Generating Text.
1536 1320
1536 1707
1536 1986
1536 1901
1536 2239
1536 2429
1536 2406
1536 2364
Final Length: 1328 (which is normal)
```

### To Reproduce

It should be simple enough to count tokens for different files with both `get_embedding` and `tiktoken`.

### Code snippets

_No response_

### OS

macOs

### Python version

Python 3

### Library version

openai-python",closed,2023-04-30T15:15:47Z,2023-11-06T17:33:53Z,['question'],https://github.com/openai/openai-python/issues/422,4562.3,True,missing_dependency_violation,incorrect_functionality,"The `get_embedding` method does not update the embeddings as expected, indicating a potential missing dependency on input changes or context updates.",high,
463,Questions about using openai libraries and network proxies together,"### Describe the bug

I have successfully installed the openai-python library. When I copy the following code from README.md, I get an error when trying to run the program.

Code content：
```python
import openai
with open('.\config\config.json', 'r', encoding='utf-8') as f:
    config = json.load(f)
openai.api_key = config['api-key']
chat_completion = openai.ChatCompletion.create(model=""gpt-3.5-turbo"", messages=[{""role"": ""user"", ""content"": ""Hello world""}])
print(chat_completion)
``` 
Note: Due to relevant confidentiality requirements, I save my api-key in my local json file, and after testing, my code can normally read the api-key stored in the json file

Error content：
```
ERROR: APIConnectionError
Error communicating with OpenAI: HTTPSConnectionPool(host='api.openai.com', port=443): Max retries exceeded with url: /v1/chat/completions (Caused by ProxyError('Unable to connect to proxy', SSLError(SSLEOFError(8, 'EOF occurred in violation of protocol (_ssl.c:1129)'))))
urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='api.openai.com', port=443): Max retries exceeded with url: /v1/chat/completions (Caused by ProxyError('Unable to connect to proxy', SSLError(SSLEOFError(8, 'EOF occurred in violation of protocol (_ssl.c:1129)'))))

During handling of the above exception, another exception occurred:

requests.exceptions.ProxyError: HTTPSConnectionPool(host='api.openai.com', port=443): Max retries exceeded with url: /v1/chat/completions (Caused by ProxyError('Unable to connect to proxy', SSLError(SSLEOFError(8, 'EOF occurred in violation of protocol (_ssl.c:1129)'))))

The above exception was the direct cause of the following exception:

  File ""C:\Users\xupei\Desktop\ChatBot(GPT3.5turbo)\main.py"", line 32, in <module>
    chat_completion = openai.ChatCompletion.create(model=""gpt-3.5-turbo"", messages=[{""role"": ""user"", ""content"": ""Hello world""}])
openai.error.APIConnectionError: Error communicating with OpenAI: HTTPSConnectionPool(host='api.openai.com', port=443): Max retries exceeded with url: /v1/chat/completions (Caused by ProxyError('Unable to connect to proxy', SSLError(SSLEOFError(8, 'EOF occurred in violation of protocol (_ssl.c:1129)'))))
``` 

After repeated attempts, I found that it was because I was using a network proxy. So I tried shutting down the network agent and rerun the code. But after a long waiting period, the error was still reported.

Error content：
```
Error communicating with OpenAI: HTTPSConnectionPool(host='api.openai.com', port=443): Max retries exceeded with url: /v1/chat/completions (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x000002ABD10FB130>: Failed to establish a new connection: [WinError 10060] 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。'))
TimeoutError: [WinError 10060] 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。

The above exception was the direct cause of the following exception:

urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x000002ABD10FB130>: Failed to establish a new connection: [WinError 10060] 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。

The above exception was the direct cause of the following exception:

urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='api.openai.com', port=443): Max retries exceeded with url: /v1/chat/completions (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x000002ABD10FB130>: Failed to establish a new connection: [WinError 10060] 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。'))

During handling of the above exception, another exception occurred:

requests.exceptions.ConnectionError: HTTPSConnectionPool(host='api.openai.com', port=443): Max retries exceeded with url: /v1/chat/completions (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x000002ABD10FB130>: Failed to establish a new connection: [WinError 10060] 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。'))

The above exception was the direct cause of the following exception:

  File ""C:\Users\xupei\Desktop\ChatBot(GPT3.5turbo)\main.py"", line 32, in <module>
    chat_completion = openai.ChatCompletion.create(model=""gpt-3.5-turbo"", messages=[{""role"": ""user"", ""content"": ""Hello world""}])
openai.error.APIConnectionError: Error communicating with OpenAI: HTTPSConnectionPool(host='api.openai.com', port=443): Max retries exceeded with url: /v1/chat/completions (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x000002ABD10FB130>: Failed to establish a new connection: [WinError 10060] 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。'))
``` 

The connection to the host timed out or failed to connect. This is due to the fact that my country and region cannot connect to 
openai's server without network proxy.

I really hope this problem can be solved and I am looking forward to your reply.

### To Reproduce

1.edit code
code content:
```python
import openai
with open('.\config\config.json', 'r', encoding='utf-8') as f:
    config = json.load(f)
openai.api_key = config['api-key']
chat_completion = openai.ChatCompletion.create(model=""gpt-3.5-turbo"", messages=[{""role"": ""user"", ""content"": ""Hello world""}])
print(chat_completion)
``` 
Note: Due to relevant confidentiality requirements, I save my api-key in my local json file, and after testing, my code can normally read the api-key stored in the json file
2.run
3.see error

### Code snippets

_No response_

### OS

Windows11

### Python version

Python 3.9.10

### Library version

openai-python v0.27.6",closed,2023-05-24T14:12:04Z,2023-11-06T17:25:33Z,['question'],https://github.com/openai/openai-python/issues/463,3987.22,True,missing_dependency_violation,High,"The API call to openai.ChatCompletion.create requires a valid network connection and proper proxy configuration, which are not met in the user's environment, leading to connection errors.",high,
418,Why does newline negatively impact embedding performance?,"### Describe the bug

While reading the code of the `embeddings_utils` I have stumbled upon this:

https://github.com/openai/openai-python/blob/db3f3526f16ea518bcd16308508e035017f9259e/openai/embeddings_utils.py#L20-L21

Could you please provide more context on:

> replace newlines, which can negatively affect performance. 

Are there any references/papers/numbers behind that negative impact?

### To Reproduce

```py
get_embedding(""foo bar\nbaz"")
```

### Code snippets

_No response_

### OS

macOS

### Python version

3.10

### Library version

0.27.4",closed,2023-04-27T15:43:32Z,2023-11-06T17:18:43Z,['question'],https://github.com/openai/openai-python/issues/418,4633.59,True,input_value_violation,performance_degradation,"The input value contains a newline character, which is stated to negatively impact embedding performance.",high,
538,ChatCompletion does not handle functions = None,"### Describe the bug

Firstly I noticed how you have integrated the chat_completion_request function into the Python OpenAI package instead of writing it out in code. I have used prior version and the new version but am finding the newer integrated version giving me issues.

Specificaly it seems that it does not handle the case where functions = None very well. There does not appear to be a way to tell 'ChatCompletion' to not run any functions other than giving it a fake function and then setting the function_call flag to 'none'. 

Example error:
error_code=None error_message=""None is not of type 'array' - 'functions'"" error_param=None error_type=invalid_request_error message='OpenAI API error received' stream_error=False

It would be good if it could proces requests when functions is set to none and function_call is set to 'none'.

### To Reproduce

Pass in functions=Non and function_call='none' to the code below.

### Code snippets

```Python
def chat_completion_request(messages, functions=None, function_call=""auto"", model=GPT_MODEL):
    if functions is None: 
        function_call =""none""
   
    r= openai.ChatCompletion.create(
        model=GPT_MODEL,
        messages=messages,
        functions=functions,
        function_call=function_call,  # auto is default, but we'll be explicit
    )
    return r
```


### OS

Windows 11

### Python version

Latest

### Library version

Latest",closed,2023-07-13T16:18:13Z,2023-11-06T16:55:28Z,"['bug', 'fixed in v1']",https://github.com/openai/openai-python/issues/538,2784.62,True,input_type_violation,high,"The API does not handle the case where 'functions' is None, leading to an invalid request error when it expects an array.",high,
649, Streaming chunk generator: incomplete JSON ,"### Describe the bug

![Screenshot_20231011_185746_Termux](https://github.com/openai/openai-python/assets/288394/6f6323f9-9af1-4f06-8f31-3a62f38bf81e)

When iterating through chunks in a chat completion stream response, the generator is crashing due to an incomplete JSON expression: 

File ""/data/data/com.termux/files/usr/lib/python3.11/site-packages/openai/api_requestor.py"", line 765, in _interpret_response_line
    data = json.loads(rbody)
           ^^^^^^^^^^^^^^^^^

(Pdb) p rbody
'""{\\""rate_limit_usage\\"": {\\'

It looks like it should potentially collect more responses and concatenate them before trying to do a json.loads decode. Or alternatively the server should provide the entire JSON object in the same response. This issue appears to have come into existence today as I had no issues yesterday.


### To Reproduce

import openai, os
openai.api_key = os.environ['OPENAI_API_KEY']

params = {'model': 'gpt-4', 'messages': [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': 'tell me three arbitrary words'}], 'stream': True}

response = openai.ChatCompletion.create(**params)

for chunk in response:
    print(chunk)


### Code snippets

```Python
import openai, os
openai.api_key = os.environ['OPENAI_API_KEY']

params = {'model': 'gpt-4', 'messages': [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': 'tell me three arbitrary words'}], 'stream': True}

response = openai.ChatCompletion.create(**params)

for chunk in response:
    print(chunk)
```


### OS

android/termux

### Python version

python 3.11.5

### Library version

openai 0.28.1",closed,2023-10-12T02:26:16Z,2023-10-13T18:55:14Z,['bug'],https://github.com/openai/openai-python/issues/649,40.48,True,input_value_violation,high,"The generator crashes due to an incomplete JSON expression, indicating that the input value (streamed response) is not valid JSON, leading to potential system crashes.",high,
278,acreate method fail when using async ,"### Describe the bug

![image](https://user-images.githubusercontent.com/61367724/223109301-718e8e33-36d8-4ad3-8146-f26eca0bc295.png)

![image](https://user-images.githubusercontent.com/61367724/223112509-f2a783fe-30bb-4402-849a-403cfbdb4697.png)


when using async  acreate , the API timeout 


### To Reproduce

async def chat(query, retry_count=0):
response = await openai.ChatCompletion.acreate(
         model=""gpt-3.5-turbo"",  
         messages=query,
         temperature=0.7,  
         max_tokens=1200,  
         top_p=1,
         frequency_penalty=0.0,  
         presence_penalty=0.0,  
     )

     out = response.choices[0]['message']['content']

@gpt_app.post(""/gpt_go"")
async def gpt_go(request: Request):
    json_data = await request.form()
    
    role = json_data[""role""]
    content = json_data[""content""]
    output = await chat([{""role"": ""system"", ""content"":""hello""}])
    return output

### Code snippets

_No response_

### OS

linux

### Python version

Python 3.7

### Library version

openai 0.27.0",closed,2023-03-06T12:42:03Z,2023-11-03T22:31:05Z,"['bug', 'fixed in v1']",https://github.com/openai/openai-python/issues/278,5817.82,True,method_order_violation,hang,"The async acreate method is timing out, indicating that the method calls may not be occurring in the correct order or that required dependencies are not being met.",high,
616,new sessions evert 3 minutes,"### Describe the bug

https://github.com/openai/openai-python/blob/e389823ba013a24b4c32ce38fa0bd87e6bccae94/openai/api_requestor.py#L590C2-L590C2

This `MAX_SESSION_LIFETIME_SECS` setting kills sessions after 3 minutes. I believe this breaks long running tasks. It would also be difficult to troubleshoot downstream issues if the client is killing sessions at an arbitrary 3 minutes. Can someone verify the PR the brought this change is 100% good to go? It seems to me like this shouldn't be there, or at least be configurable, and that it would mess with users passing in sessions.
```
elif (
            time.time() - getattr(_thread_context, ""session_create_time"", 0)
            >= MAX_SESSION_LIFETIME_SECS
        ):
            _thread_context.session.close()
            _thread_context.session = _make_session()
            _thread_context.session_create_time = time.time()
```

@jhallard 

### To Reproduce

Run a long running llm request, and a new session is created after 3 minutes during.

### Code snippets

_No response_

### OS

macOS

### Python version

3.8

### Library version

openai-python v0.28.0",closed,2023-09-13T10:18:31Z,2023-11-03T22:29:25Z,"['bug', 'fixed in v1']",https://github.com/openai/openai-python/issues/616,1236.18,True,method_order_violation,medium,"The automatic session termination after 3 minutes disrupts long-running tasks, indicating a violation of expected session management order.",high,
336,"Additional ClientTimeout parameter support on async API calls - sock_read, sock_connect","### Describe the feature or improvement you're requesting

Want a logic to retry API if sock_read on stream is not working in <2 seconds. Sometimes this happens on a web-server due to various issues with the requesting library (requests.request / aiohttp). Currently only
```
            timeout = aiohttp.ClientTimeout(
                connect=request_timeout[0],
                total=request_timeout[1],
            )
```

is supported. Perhaps accepting additional lengths on request_timeout, or adding a new kwarg for timeout object, or accepting a dictionary instead a tuple on request_timeout would be helpful. 

### Additional context

_No response_",closed,2023-03-23T17:51:36Z,2023-11-03T22:29:19Z,['fixed in v1'],https://github.com/openai/openai-python/issues/336,5404.63,False,,,,,
563,"Consider using a class constructor for configuration, not module globals","### Describe the feature or improvement you're requesting

This is the current way to configure the credentials for the openai library:

```
    openai.api_base = f""https://{AZURE_OPENAI_SERVICE}.openai.azure.com""
    openai.api_version = ""2023-05-15""
    openai.api_type = ""azure_ad""
    openai_token = await azure_credential.get_token(
        ""https://cognitiveservices.azure.com/.default""
    )
    openai.api_key = openai_token.token
```

(Or simpler than that, if using keys instead of Azure credentials)


Here's how Python handles it, as far as I understand:

* When a file imports `openai`, Python stores it in the sysmodules dict
* Then when that file sets attributes of the module, Python updates the attributes of the module that's stored in the sysmodules dict
* When another file later imports openai, Python finds the module in the sysmodules dict and that hopefully has the correct attributes set 

This approach of configuring via module-level globals seems fairly brittle to me, since it's dependent on how Python caches modules, what order things are imported, and how things are imported. It also seems likely to be incompatible with multiprocessing or asynchronous workers in a large application.

I recommend instead something like what the NodeJS SDK uses, a class that can be instantiated with the configuration:

https://github.com/openai/openai-node

That also makes it possible to use different credentials in different parts of a codebase, by passing the credential'd object around. 

I realize that would be a significantly large change, but I think it would make for much more robust code overall.



### Additional context

_No response_",closed,2023-08-04T18:42:35Z,2023-11-03T22:29:12Z,['fixed in v1'],https://github.com/openai/openai-python/issues/563,2187.78,True,method_order_violation,medium,"The current configuration method relies on module-level globals, which can lead to issues with the order of imports and may not work correctly in asynchronous or multiprocessing contexts.",high,
518,ClientError vs ServerError,"### Describe the feature or improvement you're requesting

Generally, it's a good idea to separate error responsibilities between client and server, just as HTTP status code represents with 400-499 (client error) vs 500-599 (server error).

This library has many error types, but they are all subclass of `OpenAIError` which makes it hard to handle exceptions at a higher level.

For instance, we'd swallow `ServiceUnavailableError` on the UI and let the user retry so that devs won't be bothered with the error notification on this. But we'd want to get notified upon `InvalidRequestError` because it's a bug in our codebase.

If we had `ServerError` and `ClientError`, we'd write some code like this:

```py
try:
    response = openai.ChatCompletion.create(**params)
except openai.error.ClientError as e:
    # This is our bug, notify and fix it!
    raise e
except openai.error.ServerError:
    # OpenAI seems to be busy
    # Do not raise error and prompt the user to retry later
```

I also see `tenacity.RetryError` for embeddings from time to time, so it would be great if it's handled in the same semantics.

### Additional context

The semantics here is:

- If we face a client error (403, 404, etc.), we want to raise the exception and report the error (often via tools like [Sentry](https://sentry.io/)) so that devs can fix it. Do NOT retry with the same set of parameters because it will fail until you fix the bug.
- If we face a server error (502, 503, etc.), the issue is usually temporary and retry would fix by itself.

Group those errors into one of the two parent classes, `ServerError` and `ClientError`.

```py
class OpenAIError(Exception):
class APIError(OpenAIError):
class TryAgain(OpenAIError):
class Timeout(OpenAIError):
class APIConnectionError(OpenAIError):
class InvalidRequestError(OpenAIError):
class AuthenticationError(OpenAIError):
class PermissionError(OpenAIError):
class RateLimitError(OpenAIError):
class ServiceUnavailableError(OpenAIError):
class InvalidAPIType(OpenAIError):
class SignatureVerificationError(OpenAIError):
```
",closed,2023-07-04T07:20:15Z,2023-11-03T22:29:02Z,['fixed in v1'],https://github.com/openai/openai-python/issues/518,2943.15,True,missing_dependency_violation,medium,"The current error handling does not differentiate between client and server errors, leading to potential mismanagement of error responses.",high,
371,Connection reset from long-running or stale API connections,"### Describe the bug

As we've used the openai.ChatCompletion.create (with gpt-3.5-turbo), we've had intermittent 

```
requests.exceptions.ConnectionError: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))
```

without a clear reproduction. At first I thought it was https://github.com/openai/openai-python/issues/91 and due to too many open connections to the OpenAI servers. Now I think it looks more like https://github.com/openai/openai-python/issues/368 instead, but I have some hypotheses about it. I'm opening a new issue separate from https://github.com/openai/openai-python/issues/368 in case they're different. If this is a duplicate, we can feel free to tack on my details there. 

My hypothesis is that if you have a long running process (like a web server), and it calls out to OpenAI, that periods of inactivity cause the server side to terminate the connection and it takes a long time for the client to reestablish the connection. I dug into related issues on the requests side (like this one, https://github.com/psf/requests/issues/4937) that hinted at the root cause. Essentially, what I think is happening is that, 

* First connection is made to OpenAI, returns a result, requests maintains a connection under the hood with default keep-alive
* some time passes, in my experience, around 10 minutes should do
* New connection is made to OpenAI, but the client throws a ConnectionResetError
  * A new call after this succeeds

I believe that the OpenAI servers are terminating the connection after a brief time (perhaps minutes) but the client still tries to keep it alive. 

The reason why I think this is a bug worth reporting is that I think you could modify the client code so it responds more gracefully to these server-side settings. Changing some of the [keep-alive settings](https://man7.org/linux/man-pages/man7/tcp.7.html) from the default ones would help out several folks using this. 

### To Reproduce

1. Write a long-running program. In our case, we have a Python web server running FastAPI
2. As part of a route for the server, call OpenAI to do some work. In our case, we're calling openai.ChatCompletion.create with gpt-3.5-turbo to manipulate some input language and respond back with it
3. Run the server and call the endpoint once
4. Wait 10 minutes
5. Call the endpoint again
6. You'll likely get a Connection reset by peer issue on the second call

### Code snippets

_No response_

### OS

Linux

### Python version

Python v3.8

### Library version

openai-python 0.27.2",closed,2023-04-04T00:01:07Z,2023-11-03T22:28:34Z,"['bug', 'fixed in v1']",https://github.com/openai/openai-python/issues/371,5134.46,True,method_order_violation,medium,"The issue suggests that the client does not handle the server's connection termination gracefully, leading to errors when attempting to reuse a stale connection after a period of inactivity.",high,
233,Proper encapsulation of the API keys,"### Describe the feature or improvement you're requesting

In the Node.js API we have the API accepting keys upon creation of the `OpenAIApi` object:

```javascript
import { Configuration, OpenAIApi } from ""openai"";
const configuration = new Configuration({
    organization: ""org-1qRDHAPS0UN4vVhAii7VYY7y"",
    apiKey: process.env.OPENAI_API_KEY,
});
const openai = new OpenAIApi(configuration);
```

In Python, however, the key is set globally, which poses a problem when dealing with multiple keys/organizations in requests. It would be nice to have proper encapsulation of the key into a Python class that is responsible for the API like it is usually done for many Python APIs, instead of using global objects for keys.

### Additional context

_No response_",closed,2023-02-28T16:35:23Z,2023-11-03T22:28:26Z,['fixed in v1'],https://github.com/openai/openai-python/issues/233,5957.88,True,missing_dependency_violation,medium,"The Python API's global key management lacks encapsulation, leading to potential issues with handling multiple keys/organizations, which may undermine the API's ability to function correctly in varied contexts.",high,
270,Linting issues with Pyright,"### Describe the bug

Given `reply = response[""choices""][0][""message""][""content""]`, Pyright throws linting errors:

```
 chatbot.py    49  27 error    reportGeneralTypeIssues ""__getitem__"" method not defined on type ""Generator[Unknown | list[Unknown] | dict[Unknown, Unknown], None, None]"" (lsp)
 chatbot.py    49  27 error    reportGeneralTypeIssues Argument of type ""Literal['usage']"" cannot be assigned to parameter ""__s"" of type ""slice"" in function ""__getitem__""
   ""Literal['usage']"" is incompatible with ""slice"" (lsp)
```

The code works but something wonky appears to be afoot.

### To Reproduce

1. Use Pyright LSP (likely also affects Pylance)
2. View errors when using any ChatGPT API example where a response is fetched

### Code snippets

```py
import openai

messages = []
system_msg = input(""What type of chatbot would you like to create? "")
messages.append({""role"": ""system"", ""content"": system_msg})

print(""Say hello to your new assistant!"")
while input != ""quit()"": 
    message = input()
    messages.append({""role"": ""user"", ""content"": message})
    response = openai.ChatCompletion.create(
        model=""gpt-3.5-turbo"",
        messages=messages)
    reply = response[""choices""][0][""message""][""content""]
    messages.append({""role"": ""assistant"", ""content"": reply})
    print(""\n"" + reply + ""\n"")
```

### OS

Linux

### Python version

Python 3.11.1

### Library version

openai 0.27.0",closed,2023-03-04T08:22:30Z,2023-11-03T22:28:12Z,"['bug', 'fixed in v1']",https://github.com/openai/openai-python/issues/270,5870.1,True,input_type_violation,high,"The code attempts to access elements from a response object using an incompatible type, leading to potential runtime errors.",high,
551,Type hinting / stub files,"### Describe the feature or improvement you're requesting

I'm new to the openai-python library, and I'm really excited to start working on some cool projects with it! 🚀 However, I noticed that the library's type hinting could be improved to make it even better.

I'd like to kindly request some enhancements in the type hinting for the library's methods. Having more comprehensive type annotations would greatly help developers like me avoid bugs and make our code more readable and maintainable.

If possible, it would be awesome if the maintainers could consider adding these improvements. I believe it would enhance the overall developer experience and make the library even more delightful to use.

Additionally, if anyone in the community has already created some useful stub files, sharing them would be incredibly helpful! It's like having a secret guide to navigate the code with ease.

Thank you for your attention, and I really appreciate all the effort put into making this library fantastic.

### Additional context

_No response_",closed,2023-07-28T00:58:27Z,2023-11-03T22:27:55Z,['fixed in v1'],https://github.com/openai/openai-python/issues/551,2373.49,False,,,,,
668,Chat GPT Eduaction (Moodle),"### Describe the feature or improvement you're requesting

Moodle, Quality Matters, and other online templates are nowadays provided to create MOOCs, the problem is that generating the template involves time and advanced IT expertise (Yaml, JSON), in addition to domain expertise.
It would be very useful to have an implementation in Python or similar that allows through chatgpt and raising the theme of the course, generate the template to load in moodle, which meets the highest international quality standards, which serves as a basis for the teacher to worry only to load the template and begin to customize the content.
This would help to accelerate the speed with which knowledge is shared, going one step further.

### Additional context

https://docs.moodle.org/403/en/Course_restore
https://www.qualitymatters.org/qa-resources/rubric-standards
",closed,2023-10-27T01:07:59Z,2023-11-03T22:25:23Z,['enhancement'],https://github.com/openai/openai-python/issues/668,189.29,True,missing_dependency_violation,medium,"The request lacks specific details on required inputs or configurations needed for generating the Moodle template, which may lead to incomplete or incorrect outputs.",high,
667,Connection Not Closing Properly When Using GPT API with Stream,"### Describe the bug

I am experiencing a connection issue when utilizing OpenAI's GPT API with the streaming feature in my application. When I close the stream using the method I've implemented, the connection to OpenAI does not terminate correctly. After a certain duration, a timeout error is thrown by the discarded thread. The error message I receive is:

```
CRITICAL - Request timed out: HTTPSConnectionPool(host='api.openai.com', port=443): Read timed out. (read timeout=600)
...
openai.error.Timeout: Request timed out: HTTPSConnectionPool(host='api.openai.com', port=443): Read timed out. (read timeout=600)
```


### To Reproduce

1. Initiate the GPT API with streaming using the code snippet provided.
```
def gpt(prompt: str):
    for chunk in openai.ChatCompletion.create(
        model=""gpt-3.5-turbo"",
        messages=[{""role"": 'user', ""content"": prompt}],
        stream=True
    ):
        if (text_chunk := chunk['choices'][0]['delta'].get('content'))  is not None:
            yield text_chunk
```
2. In specific scenarios, discard the API response and close the stream.
```
gpt_stream = write(prompt)
for text_chunk in gpt_stream:
	if discarded:
		gpt_stream.close()
		return;
	... continue
```
3. Wait for a certain period.
4. Observe the aforementioned timeout error.

### Code snippets

_No response_

### OS

ubuntu 22.04

### Python version

3.9

### Library version

0.27.8",closed,2023-10-26T19:42:28Z,2023-11-03T22:24:34Z,"['bug', 'fixed in v1']",https://github.com/openai/openai-python/issues/667,194.7,True,method_order_violation,medium,"The stream is not properly closed before discarding the response, leading to a timeout error.",high,
673,Not able to build the wheel for v1,"### Describe the bug

I'm not able to build the wheel for v1 because of the error: `project.license` must be valid exactly by one definition.

### To Reproduce

Clone and checkout to the new version.
```
git clone https://github.com/openai/openai-python.git
cd openai-python/
git checkout origin/v1
```

Create a setup.py file:
```
vim setup.py
```

Paste the following:
```
from setuptools import setup

setup()
```

Build the wheel
```
python3 setup.py bdist_wheel
```

Error:
```
configuration error: `project.license` must be valid exactly by one definition (2 matches found):

    - keys:
        'file': {type: string}
      required: ['file']
    - keys:
        'text': {type: string}
      required: ['text']

DESCRIPTION:
    `Project license <https://peps.python.org/pep-0621/#license>`_.

GIVEN VALUE:
    ""Apache-2.0""

OFFENDING RULE: 'oneOf'

DEFINITION:
    {
        ""oneOf"": [
            {
                ""properties"": {
                    ""file"": {
                        ""type"": ""string"",
                        ""$$description"": [
                            ""Relative path to the file (UTF-8) which contains the license for the"",
                            ""project.""
                        ]
                    }
                },
                ""required"": [
                    ""file""
                ]
            },
            {
                ""properties"": {
                    ""text"": {
                        ""type"": ""string"",
                        ""$$description"": [
                            ""The license of the project whose meaning is that of the"",
                            ""`License field from the core metadata"",
                            ""<https://packaging.python.org/specifications/core-metadata/#license>`_.""
                        ]
                    }
                },
                ""required"": [
                    ""text""
                ]
            }
        ]
    }
Traceback (most recent call last):
  File ""/home/user_name/projects/openai-python/setup.py"", line 3, in <module>
    setup()
  File ""/home/user_name/anaconda3/lib/python3.9/site-packages/setuptools/__init__.py"", line 107, in setup
    return distutils.core.setup(**attrs)
  File ""/home/user_name/anaconda3/lib/python3.9/site-packages/setuptools/_distutils/core.py"", line 159, in setup
    dist.parse_config_files()
  File ""/home/user_name/anaconda3/lib/python3.9/site-packages/setuptools/dist.py"", line 908, in parse_config_files
    pyprojecttoml.apply_configuration(self, filename, ignore_option_errors)
  File ""/home/user_name/anaconda3/lib/python3.9/site-packages/setuptools/config/pyprojecttoml.py"", line 66, in apply_configuration
    config = read_configuration(filepath, True, ignore_option_errors, dist)
  File ""/home/user_name/anaconda3/lib/python3.9/site-packages/setuptools/config/pyprojecttoml.py"", line 129, in read_configuration
    validate(subset, filepath)
  File ""/home/user_name/anaconda3/lib/python3.9/site-packages/setuptools/config/pyprojecttoml.py"", line 55, in validate
    raise ValueError(f""{error}\n{summary}"") from None
ValueError: invalid pyproject.toml config: `project.license`.
configuration error: `project.license` must be valid exactly by one definition (2 matches found):

    - keys:
        'file': {type: string}
      required: ['file']
    - keys:
        'text': {type: string}
      required: ['text']
```

### Code snippets

_No response_

### OS

WSL2 Ubuntu

### Python version

Python v3.9.7

### Library version

openai-python v1.0.0",closed,2023-11-03T13:58:56Z,2023-11-03T20:44:42Z,[],https://github.com/openai/openai-python/issues/673,6.76,True,input_value_violation,high,"The project.license field in the configuration must adhere to a specific format, and the provided value 'Apache-2.0' does not meet the requirement of being defined by exactly one of the acceptable options (either 'file' or 'text').",high,
665,Consider switching to httpx as the HTTP client library,"### Describe the feature or improvement you're requesting

Currently, this project uses `requests` and `aiohttp` to provide synchronous and asynchronous interfaces, respectively. The `httpx` library, in contrast, is an actively maintained project that supports both with a consistent interface. On top of that, it works with the alternative `trio` async event loop implementation.

### Additional context

_No response_",closed,2023-10-25T13:48:57Z,2023-11-02T14:58:11Z,['fixed in v1'],https://github.com/openai/openai-python/issues/665,193.15,False,,,,,
261,[Audio.transcribe] Prompt parameter appears to have no effect,"### Describe the bug

I am attempting to transcribe a file with an accompanying prompt, however it appears to have no effect at all on the output.

This is a sample of the code:

`prompt = ""This is a recording of a conversation between 4 people, please present it in the following format: 'Person 1: Thanks for coming today Person 2: No problem', etc.""`
`audio_file = open(""/path/to/file.WAV"", ""rb"")`
`transcript = openai.Audio.transcribe(""whisper-1"", audio_file, prompt=prompt)`
`print(transcript)`

The returned text is exactly the same with or without the prompt parameter.






### To Reproduce

Send a transcription request with a prompt parameter

### Code snippets

_No response_

### OS

macOs

### Python version

Python v3.7.1

### Library version

OpenAI Python v0.27.0",closed,2023-03-02T19:15:00Z,2023-03-03T18:59:24Z,['question'],https://github.com/openai/openai-python/issues/261,23.74,False,,,,low,
663,JSONDecodeError when trying to stream chat completion,"### Describe the bug

After sending a request to chat completion API using `stream=True`, sometimes, the api returns a single response with two chunks inside of it. For example:
```
b'{""id"": ""chatcmpl-8BnB0NvhQxY0szkmjMdM5px67189w"", ""object"": ""chat.completion.chunk"", ""created"": ""2023-10-20T17:02:14+00:00"", ""model"": ""gpt-4"", ""choices"": [{""delta"": {""content"": ""\'s""}, ""index"": 0}]}{""id"": ""chatcmpl-8BnB0NvhQxY0szkmjMdM5px67189w"", ""object"": ""chat.completion.chunk"", ""created"": ""2023-10-20T17:02:14+00:00"", ""model"": ""gpt-4"", ""choices"": [{""delta"": {""content"": "" language""}, ""index"": 0}]}'
```

My expectation was to get one chunk per response:
```
b'{""id"": ""chatcmpl-8BnB0NvhQxY0szkmjMdM5px67189w"", ""object"": ""chat.completion.chunk"", ""created"": ""2023-10-20T17:02:14+00:00"", ""model"": ""gpt-4"", ""choices"": [{""delta"": {""content"": ""\'s""}, ""index"": 0}]}'
```

```
b'{""id"": ""chatcmpl-8BnB0NvhQxY0szkmjMdM5px67189w"", ""object"": ""chat.completion.chunk"", ""created"": ""2023-10-20T17:02:14+00:00"", ""model"": ""gpt-4"", ""choices"": [{""delta"": {""content"": "" language""}, ""index"": 0}]}'
```

### To Reproduce

1. Send a request to chat completion with `stream=True`
2. Consume each chunk of the stream
3. Check the contents of each chunk
4. One of the chunks will have two payloads inside of it

### Code snippets

_No response_

### OS

Linux

### Python version

3.10.11

### Library version

openai v0.27.10",closed,2023-10-20T17:26:07Z,2023-10-25T20:47:23Z,"['bug', 'fixed in v1']",https://github.com/openai/openai-python/issues/663,123.35,True,input_value_violation,incorrect_functionality,"The API is returning multiple payloads in a single chunk when it is expected to return one chunk per response, leading to JSON decoding errors.",high,
3,AttributeError: partially initialized module 'openai' has no attribute 'Completion',"When importing `openai` and running the following code:
```py
import openai
openai.api_key = ""api key omitted 😉 ""
openai.Completion.create(
  engine=""davinci"",
  prompt=""Once upon a time"",
  max_tokens=5
)
```

I get the the following error message:
```
Traceback (most recent call last):
  File ""c:\Users\Oleg\Documents\Programming\OpenAI\random.py"", line 1, in <module>
    import openai
  File ""C:\Users\Oleg\Documents\Programming\OpenAI\venv\lib\site-packages\openai\__init__.py"", line 30, in <module>
    from openai.api_resources import *  # noqa
  File ""C:\Users\Oleg\Documents\Programming\OpenAI\venv\lib\site-packages\openai\api_resources\__init__.py"", line 1, in <module>
    from openai.api_resources.branch import Branch
  File ""C:\Users\Oleg\Documents\Programming\OpenAI\venv\lib\site-packages\openai\api_resources\branch.py"", line 2, in <module>
    from openai.api_resources.abstract.engine_api_resource import EngineAPIResource
  File ""C:\Users\Oleg\Documents\Programming\OpenAI\venv\lib\site-packages\openai\api_resources\abstract\__init__.py"", line 5, in <module>
    from openai.api_resources.abstract.api_resource import APIResource
  File ""C:\Users\Oleg\Documents\Programming\OpenAI\venv\lib\site-packages\openai\api_resources\abstract\api_resource.py"", line 3, in <module>
    from openai import api_requestor, error, six, util
  File ""C:\Users\Oleg\Documents\Programming\OpenAI\venv\lib\site-packages\openai\api_requestor.py"", line 15, in <module>
    from openai import error, http_client, version, util, six
  File ""C:\Users\Oleg\Documents\Programming\OpenAI\venv\lib\site-packages\openai\http_client.py"", line 8, in <module>
    import random
  File ""c:\Users\Oleg\Documents\Programming\OpenAI\random.py"", line 13, in <module>
    response = openai.Completion.create(
AttributeError: partially initialized module 'openai' has no attribute 'Completion' (most likely due to a circular import)
```

I've tried re-installing it via `pip install --upgrade openai` but I still get the same error, however VSCode seems to know that `Completion` is a class within the `openai` module.",closed,2021-02-05T16:31:19Z,2021-03-19T00:03:04Z,[],https://github.com/openai/openai-python/issues/3,991.53,True,input_type_violation,unknown,"The error indicates a potential circular import issue, which may stem from the naming conflict with the script file 'random.py' that shadows the built-in 'random' module, leading to an incorrect module initialization.",high,
658,Error in fine tune,"### Describe the bug

when I do `!openai tools fine_tunes.prepare_data --file data/prepared_data.csv --quiet`, it works successfully.
**But** when I do `!openai api fine_tunes.create --training_file data/prepared_data_prepared.jsonl --model curie --suffix ""ultraman""`, It report an error:
```

Found potentially duplicated files with name 'prepared_data_prepared.jsonl', purpose 'fine-tune' and size 446199 bytes
file-hK8jbDzaub5ORlsodyrgQUo8
Enter file ID to reuse an already uploaded file, or an empty string to upload this file anyway: file-hK8jbDzaub5ORlsodyrgQUo8
Reusing already uploaded file: file-hK8jbDzaub5ORlsodyrgQUo8
Created fine-tune: ft-dUygZsOEXNv50BJuvDHA3vKE
Streaming events until fine-tuning is complete...

(Ctrl-C will interrupt the stream, but not cancel the fine-tune)
[2023-10-17 09:08:41] Created fine-tune: ft-dUygZsOEXNv50BJuvDHA3vKE

Stream interrupted (client disconnected).
To resume the stream, run:

  openai api fine_tunes.follow -i ft-dUygZsOEXNv50BJuvDHA3vKE
```

then I `!openai api fine_tunes.follow -i ft-dUygZsOEXNv50BJuvDHA3vKE`, 
it returns
```
[2023-10-17 09:08:41] Created fine-tune: ft-dUygZsOEXNv50BJuvDHA3vKE
[2023-10-17 09:13:37] Fine-tune costs $3.67
[2023-10-17 09:13:37] Fine-tune enqueued. Queue number: 0

Stream interrupted (client disconnected).
To resume the stream, run:

  openai api fine_tunes.follow -i ft-dUygZsOEXNv50BJuvDHA3vKE
```

### To Reproduce
Stream interrupted (client disconnected).
To resume the stream, run:

  openai api fine_tunes.follow -i ft-dUygZsOEXNv50BJuvDHA3vKE

### Code snippets

_No response_

### OS

colab 

### Python version

Python 3.10

### Library version

openai 0.28.1",closed,2023-10-17T02:59:11Z,2023-10-23T01:32:24Z,['bug'],https://github.com/openai/openai-python/issues/658,142.55,True,missing_dependency_violation,medium,"The error indicates a potential missing dependency in the fine-tuning process, specifically regarding the reuse of an already uploaded file without confirming its compatibility or correctness.",high,
199,Why do I can't install openai,"I had this problem,`error: subprocess-exited-with-error`, when I installing.
How do I install openai?

My python is 3.8
pip is 22.3.1

log:
```
Collecting openai
  Using cached openai-0.26.2.tar.gz (55 kB)
  Installing build dependencies ... done
  Getting requirements to build wheel ... error
  error: subprocess-exited-with-error

  × Getting requirements to build wheel did not run successfully.
  │ exit code: 1
  ╰─> [17 lines of output]
      Traceback (most recent call last):
        File ""D:\Python\Python38\virtual_envs\open_ai\lib\site-packages\pip\_vendor\pep517\in_process\_in_process.py"", line 351, in <module>
          main()
        File ""D:\Python\Python38\virtual_envs\open_ai\lib\site-packages\pip\_vendor\pep517\in_process\_in_process.py"", line 333, in main
          json_out['return_val'] = hook(**hook_input['kwargs'])
        File ""D:\Python\Python38\virtual_envs\open_ai\lib\site-packages\pip\_vendor\pep517\in_process\_in_process.py"", line 118, in get_requires_for_build_wheel
          return hook(config_settings)
        File ""C:\Users\user\AppData\Local\Temp\pip-build-env-qcki_ch_\overlay\Lib\site-packages\setuptools\build_meta.py"", line 338, in get_requires_for_build_wheel
          return self._get_build_requires(config_settings, requirements=['wheel'])
        File ""C:\Users\user\AppData\Local\Temp\pip-build-env-qcki_ch_\overlay\Lib\site-packages\setuptools\build_meta.py"", line 320, in _get_build_requires
          self.run_setup()
        File ""C:\Users\user\AppData\Local\Temp\pip-build-env-qcki_ch_\overlay\Lib\site-packages\setuptools\build_meta.py"", line 484, in run_setup
          super(_BuildMetaLegacyBackend,
        File ""C:\Users\user\AppData\Local\Temp\pip-build-env-qcki_ch_\overlay\Lib\site-packages\setuptools\build_meta.py"", line 335, in run_setup
          exec(code, locals())
        File ""<string>"", line 13, in <module>
      UnicodeDecodeError: 'cp950' codec can't decode byte 0xe2 in position 1030: illegal multibyte sequence
      [end of output]

  note: This error originates from a subprocess, and is likely not a problem with pip.
error: subprocess-exited-with-error

× Getting requirements to build wheel did not run successfully.
│ exit code: 1
╰─> See above for output.

note: This error originates from a subprocess, and is likely not a problem with pip.

```
",closed,2023-01-25T14:03:27Z,2023-03-11T03:57:38Z,[],https://github.com/openai/openai-python/issues/199,1069.9,False,,,,low,
242,Can't access gpt-3.5-turbo - AttributeError: module 'openai' has no attribute 'ChatCompletion',"### Describe the bug

Can't access gpt-3.5-turbo with python. getting an error when i try to use openai.ChatCompletion.create

### To Reproduce

1. pip install openai
2. import openai + key
3. enter code snippet into jupyter notebook
4. get error

### Code snippets

```Python
# Note: you need to be using OpenAI Python v0.27.0 for the code below to work
import openai

openai.ChatCompletion.create(
  model=""gpt-3.5-turbo"",
  messages=[
        {""role"": ""system"", ""content"": ""You are a helpful assistant.""},
        {""role"": ""user"", ""content"": ""Who won the world series in 2020?""},
        {""role"": ""assistant"", ""content"": ""The Los Angeles Dodgers won the World Series in 2020.""},
        {""role"": ""user"", ""content"": ""Where was it played?""}
    ]
)
```


### OS

windows11

### Python version

python 3.9.7

### Library version

openai-python v0.27.0",closed,2023-03-01T20:47:05Z,2023-03-02T00:10:21Z,['bug'],https://github.com/openai/openai-python/issues/242,3.39,True,input_type_violation,unknown,"The error indicates that the 'ChatCompletion' attribute is not found in the 'openai' module, suggesting a potential mismatch between the expected API structure and the installed library version.",high,
454,`FineTune.list_events()` breaks when `stream=True`,"### Describe the bug

When I tried to list the events for my fine tune job through the Python API with streaming I get a correct response without error. However when I specify `stream=True`, this results in an exception being raised and crashing my application, even though the response was clearly received. Looking at the traceback, I can see that the response was received but failed to be parsed into JSON by `openai`.

Here's a traceback for the exception that I see:
```shell
Traceback (most recent call last):
  File ""venv_py3105/lib/python3.10/site-packages/openai/api_requestor.py"", line 677, in _interpret_response_line
    data = json.loads(rbody)
  File ""/Users/yun.kim/.pyenv/versions/3.10.5/lib/python3.10/json/__init__.py"", line 346, in loads
    return _default_decoder.decode(s)
  File ""/Users/yun.kim/.pyenv/versions/3.10.5/lib/python3.10/json/decoder.py"", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File ""/Users/yun.kim/.pyenv/versions/3.10.5/lib/python3.10/json/decoder.py"", line 355, in raw_decode
    raise JSONDecodeError(""Expecting value"", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

The above exception was the direct cause of the following exception:

File ""openai_demo.py"", line 216, in fine_tune_list_events
    resp = openai.FineTune.list_events(
  File ""venv_py3105/lib/python3.10/site-packages/openai/api_resources/abstract/nested_resource_class_methods.py"", line 123, in list_nested_resources
    return getattr(cls, resource_request_method)(""get"", url, **params)
  File ""venv_py3105/lib/python3.10/site-packages/openai/api_resources/abstract/nested_resource_class_methods.py"", line 44, in nested_resource_request
    response, _, api_key = requestor.request(
  File ""venv_py3105/lib/python3.10/site-packages/openai/api_requestor.py"", line 230, in request
    resp, got_stream = self._interpret_response(result, stream)
  File ""venv_py3105/lib/python3.10/site-packages/openai/api_requestor.py"", line 624, in _interpret_response
    self._interpret_response_line(
  File ""venv_py3105/lib/python3.10/site-packages/openai/api_requestor.py"", line 680, in _interpret_response_line
    raise error.APIError(
openai.error.APIError: HTTP code 200 from API (data: {""object"": ""fine-tune-event"", ""level"": ""info"", ""message"": ""Created fine-tune: ft-XXXXXXX"", ""created_at"": 1684155377}

data: {""object"": ""fine-tune-event"", ""level"": ""info"", ""message"": ""Fine-tune costs $X.XX"", ""created_at"": 1684155451}

data: {""object"": ""fine-tune-event"", ""level"": ""info"", ""message"": ""Fine-tune enqueued. Queue number: 1"", ""created_at"": 1684155452}

data: {""object"": ""fine-tune-event"", ""level"": ""info"", ""message"": ""Fine-tune is in the queue. Queue number: 0"", ""created_at"": 1684155452}

data: {""object"": ""fine-tune-event"", ""level"": ""info"", ""message"": ""Fine-tune started"", ""created_at"": 1684155503}

data: {""object"": ""fine-tune-event"", ""level"": ""info"", ""message"": ""Completed epoch 1/4"", ""created_at"": 1684155598}

data: {""object"": ""fine-tune-event"", ""level"": ""info"", ""message"": ""Completed epoch 2/4"", ""created_at"": 1684155603}

data: {""object"": ""fine-tune-event"", ""level"": ""info"", ""message"": ""Completed epoch 3/4"", ""created_at"": 1684155607}

data: {""object"": ""fine-tune-event"", ""level"": ""info"", ""message"": ""Completed epoch 4/4"", ""created_at"": 1684155615}

data: {""object"": ""fine-tune-event"", ""level"": ""info"", ""message"": ""Uploaded model: davinci:ft-XXXXXXX-2023-05-15-13-00-56"", ""created_at"": 1684155656}

data: {""object"": ""fine-tune-event"", ""level"": ""info"", ""message"": ""Uploaded result file: file-XXXXXXX"", ""created_at"": 1684155658}

data: {""object"": ""fine-tune-event"", ""level"": ""info"", ""message"": ""Fine-tune succeeded"", ""created_at"": 1684155658}

data: [DONE]

)

```

### To Reproduce

I am using Python 3.10.5 with `openai==0.27.6`.

Here is my application code:
```python
resp = openai.FineTune.list_events(
  ""ft-XXXXX"",  # fine tune job ID
  api_key=os.getenv(""OPENAI_API_KEY""),
  stream=True
)
```

### Code snippets

_No response_

### OS

macOS

### Python version

Python 3.10.5

### Library version

v0.27.6",closed,2023-05-18T19:46:57Z,2023-10-18T10:33:27Z,"['bug', 'fixed in v1']",https://github.com/openai/openai-python/issues/454,3662.78,True,input_value_violation,high,"The API fails to handle the streaming response correctly, leading to a crash when `stream=True` is specified, indicating a potential issue with input values or response handling.",high,
570,Unclosed client session,"### Describe the bug

When I use FastAPI for forwarding and set stream=True, for example: async for res in await openai.ChatCompletion.acreate(**kwargs), when this request is terminated before it completes, an exception occurs as follows: [asyncio] [ERROR]: Unclosed client session client_session: <aiohttp.client.ClientSession object at 0x7f0ca458c490>.


### To Reproduce

    1. Write a FastAPI request using the acreate method to return data in streaming format.
    2. Use a packet sending tool to make a request to the written API, terminate the request while receiving data.
    3. Wait for a moment, and the exception will be displayed in the terminal.

### Code snippets

_No response_

### OS

Linux 

### Python version

Python 3.11.4

### Library version

openai 0.27.8
```[tasklist]
### Tasks
```
",closed,2023-08-11T02:33:11Z,2023-10-18T10:30:02Z,"['bug', 'fixed in v1']",https://github.com/openai/openai-python/issues/570,1639.95,True,missing_dependency_violation,medium,"The issue arises from the failure to properly close the client session after the request is terminated, indicating a missing dependency on session management in the API contract.",high,
620,Make `openai.proxy`  configurable for individual request.,"### Describe the feature or improvement you're requesting

Currently `openai.proxy` is a global  variable for every OpenAI request, but we have multiple  deployments with different settings like `api_type`, `api_base` etc. 

Everything is configurable now except `proxy`. 😢

### Additional context

_No response_",closed,2023-09-20T04:01:52Z,2023-10-18T10:18:18Z,"['enhancement', 'fixed in v1']",https://github.com/openai/openai-python/issues/620,678.27,True,missing_option_violation,medium,"The API does not allow for individual request configuration of the proxy, which is a missing option that could lead to suboptimal behavior in deployments with varying settings.",high,
633,aiohttp.ClientResponse.read() called and awaited twice,"### Describe the bug

I use the aiohttp-client-cache lib to cache repeated API calls.
This library returns a CachedResponse when the same request is made.
The underlying implementation uses a stream to load the cached response from the cache.
Unsurprisingly, the read() call reads through the stream and does not cache the read bytes in memory.
Unfortunately, the `api_requestor` calls and awaits the read() method twice resulting in empty bytes for the second time. (Followed by a `JSONDecodeError`)

According to the [aiohttp documentation](https://docs.aiohttp.org/en/v3.8.5/client_reference.html#aiohttp.ClientResponse.read) the method automatically releases the connection upon a successful read. Combined with the streaming nature of HTTP calls I assume that this method is not designed to be called twice. (IMHO it's just an implementation detail, that the default implementation caches the response in the response object.)

This is the reason why I'm reporting this issue here. Plus there can be a slight performance benefit of not calling and awaiting a coroutine.


### To Reproduce

1. Look at the source code, api_requestor.py line 732 and 739

### Code snippets

```Python
api_requestory.py from line 731
            try:
                content_bytes = await result.read()  # saved bytes to a scoped variable
            except (aiohttp.ServerTimeoutError, asyncio.TimeoutError) as e:
                raise error.Timeout(""Request timed out"") from e
            except aiohttp.ClientError as e:
                util.log_warn(e, body=result.content)
            return (
                self._interpret_response_line(
                    content_bytes.decode(""utf-8""),  # Removed await read() and 
                    result.status,
                    result.headers,
                    stream=False,
                ),
                False,
            )
```


### OS

Windows 10

### Python version

Python v3.11.4

### Library version

openai-python 0.28.1",closed,2023-10-02T16:05:29Z,2023-10-18T10:13:36Z,"['bug', 'fixed in v1']",https://github.com/openai/openai-python/issues/633,378.14,True,input_type_violation,incorrect_functionality,"The read() method of aiohttp.ClientResponse is called twice, leading to unexpected behavior and a JSONDecodeError due to the stream being exhausted after the first call.",high,
644,report that this github project has cracked the web request protocol,"### Describe the bug

report that this github project has cracked the web request protocol https://github.com/zhile-io/pandora

### To Reproduce

report that this github project has cracked the web request protocol https://github.com/zhile-io/pandora

### Code snippets

```Python
report that this github project has cracked the web request protocol https://github.com/zhile-io/pandora
```


### OS

1

### Python version

1

### Library version

1",closed,2023-10-10T03:58:31Z,2023-10-18T10:13:13Z,['invalid'],https://github.com/openai/openai-python/issues/644,198.25,True,input_type_violation,unknown,"The issue body contains text that does not conform to expected input types for API methods, such as a lack of structured data or valid parameters.",high,
528,Audio API Resource is Missing Timeout Handling,"### Describe the feature or improvement you're requesting

Audio transcription can be time intensive (and in my case also somewhat time critical). In certain cases the OpenAI API is taking 20+ seconds to process audio and with the currently library we have no way of timing out the call, potentially leaving a worker or thread hanging until resolved ungracefully (such as an overall network response timeout or worker timeout). As is this forces users to either reimplement the call in `requests` to get access to network timeouts or re-implement the timeout logic on top of this method call via threads or subprocesses.

### Additional context

_No response_",closed,2023-07-11T04:32:51Z,2023-10-18T10:12:35Z,"['bug', 'fixed in v1']",https://github.com/openai/openai-python/issues/528,2381.66,True,method_order_violation,medium,"The absence of timeout handling in the API method can lead to hanging threads or workers, impacting user experience.",high,
530,openai.aiohttp_session creates a new aiohttp.ClientSession despite configured one!,"### Describe the bug

There is a way to set a custom `aiohttp.ClientSession` for all library's requests.

It is described here https://github.com/openai/openai-python#async-api

We need to use `openai.aiosession` `ContextVar` object and set a session for it. Quite easy isn't is?

But what happens next if you look under the hood: the internal contextmanager `openai.aiohttp_session` tries to get our custom set session and always gets `None` and creates a new `aiohttp.ClientSession` 

### To Reproduce

1. Configure `openai.aiosession` with a new `ClientSession`, like this https://github.com/openai/openai-python#async-api
  for example in my FastAPI application
  
```
   import openai
   from fastapi import FastAPI
   from aiohttp import ClientSession
   from contextlib import asynccontextmanager
  
   @asynccontextmanager
   async def lifespan(app: FastAPI):
       openai.aiosession.set(ClientSession())
       yield
       await openai.aiosession.get().close()
```

2. Either add prints to the `openai.aiohttp_session` contextmanager or set a debug breakpoint.
3. Make a new chatgpt `Completion.acreate` request (for example)
4. This way you can discover that `openai.aiohttp_session` contextmanager cannot retrive out set session (None on 646 line)
![image](https://github.com/openai/openai-python/assets/49044855/31ac34ae-194c-4ca5-8564-af38c8afd822)
and creates a new one for every new request.

### Code snippets

_No response_

### OS

Linux Ubuntu

### Python version

3.11

### Library version

0.27.4",closed,2023-07-11T08:12:15Z,2023-10-18T10:11:15Z,"['bug', 'fixed in v1']",https://github.com/openai/openai-python/issues/530,2377.98,True,missing_dependency_violation,medium,"The openai.aiohttp_session contextmanager fails to retrieve the custom aiohttp.ClientSession, indicating a missing dependency between the session configuration and its retrieval.",high,
503,RuntimeError: aclose(): asynchronous generator is already running,"### Describe the bug

I see this error after an async streaming call to the completion end-point: 

```
an error occurred during closing of asynchronous generator <async_generator object aiohttp_session at 0x1029de110>
asyncgen: <async_generator object aiohttp_session at 0x1029de110>
RuntimeError: aclose(): asynchronous generator is already running
```

### To Reproduce

Run the code below. The `completion_text` is printed and below that the error above is shown.

### Code snippets

```Python
import asyncio
from types import AsyncGeneratorType
from dotenv import load_dotenv
import os
import openai

load_dotenv()

openai.api_key = os.getenv(""OPENAI_API_KEY"")

async def test_completion_async_stream():
    completion = await openai.Completion.acreate(
        model=""text-davinci-003"",
        prompt=""Hello!"",
        stream=True,
    )

    # Assert response body
    assert isinstance(completion, AsyncGeneratorType)

    collected_chunks = []
    completion_text = ''
    # iterate through the stream, if it breaks, the test failed
    async for chunk in completion:
        collected_chunks.append(chunk)
        finish_reason = chunk['choices'][0]['finish_reason'] # type: ignore
        if finish_reason is not None:
            break
        chunk_text = chunk['choices'][0]['text'] # type: ignore
        completion_text += chunk_text  # append the text
    print(completion_text)

if __name__ == ""__main__"":
    asyncio.run(test_completion_async_stream())
```


### OS

macOS 13.4 (22F66)

### Python version

3.11.4

### Library version

0.27.8",closed,2023-06-26T14:01:45Z,2023-10-18T10:06:57Z,"['bug', 'fixed in v1']",https://github.com/openai/openai-python/issues/503,2732.09,True,method_order_violation,medium,"The error indicates that the asynchronous generator is being closed while it is still running, suggesting that the order of operations in the async stream handling is incorrect.",high,
615,openai.FineTuningJob.list_events doesn't support api_base,"### Describe the bug

when query new fine tune events with openai:
openai.FineTuningJob.list_events(id=""ft-abc123"", limit=10,api_key=""xxxx"",api_base=""xxxx"")

the http request doesn't go to api_base domain, instead go to the offical api site.

### To Reproduce

1. set up a nginx proxy for openai official api
2. query fine tune job event with openai.FineTuningJob.list_events(id=""ft-abc123"", limit=10,api_key=""xxxx"",api_base=""xxxx"")
3. http requests go to offical api site instead the nginx proxy.

### Code snippets

```Python
openai.FineTuningJob.list_events(id=""ft-abc123"", limit=10,api_key=""xxxx"",api_base=""xxxx"")
```


### OS

linux

### Python version

python 3.10

### Library version

openai-python v0.28.0",closed,2023-09-13T05:45:41Z,2023-10-18T10:05:59Z,['bug'],https://github.com/openai/openai-python/issues/615,844.34,True,missing_option_violation,high,"The API method openai.FineTuningJob.list_events does not support the api_base parameter as expected, leading to requests being sent to the official API site instead of the specified base.",high,
525,Import time >1s when running with pandas,"### Describe the bug

See import time when installing openai[datalib] due to pandas load time.

<img width=""926"" alt=""Screenshot 2023-07-10 at 21 12 39"" src=""https://github.com/openai/openai-python/assets/471109/be5d9d58-21df-48e6-87da-5831e369a409"">




### To Reproduce

Install openai and compare import time with and without [datalib] extension installed (pandas).

### Code snippets

```Python
In local 4s


import time
t1 = time.perf_counter()
from openai.datalib import pandas_helper
print(time.perf_counter() - t1, pandas_helper.pandas)

t1 = time.perf_counter()
try:
    pandas_helper.assert_has_pandas()
except pandas_helper.MissingDependencyError:
    pass
print(time.perf_counter() - t1, pandas_helper.pandas)

t1 = time.perf_counter()
try:
    pandas_helper.assert_has_pandas()
except pandas_helper.MissingDependencyError:
    pass
print(time.perf_counter() - t1, pandas_helper.pandas)
```

```
4.230538249947131 <module 'pandas' from '/Users/pedrovicente/.virtualenvs/openai-python-uihn/lib/python3.10/site-packages/pandas/__init__.py'>
6.669433787465096e-07 <module 'pandas' from '/Users/pedrovicente/.virtualenvs/openai-python-uihn/lib/python3.10/site-packages/pandas/__init__.py'>
2.919696271419525e-07 <module 'pandas' from '/Users/pedrovicente/.virtualenvs/openai-python-uihn/lib/python3.10/site-packages/pandas/__init__.py'>
```



### OS

Linux

### Python version

python 3.10

### Library version

openai-python v0.27.2",closed,2023-07-10T19:17:48Z,2023-10-18T09:58:38Z,"['bug', 'fixed in v1']",https://github.com/openai/openai-python/issues/525,2390.68,False,,,,,
504,openai.error.AuthenticationError: Incorrect API key provided,"### Describe the bug

Error occurs here:

https://github.com/grumpyp/aixplora/blob/main/backend/loaders/audio_loader.py

When coming to the `Whisper` part.

Key works fine on other API calls. 

### To Reproduce

https://github.com/grumpyp/aixplora/blob/main/backend/loaders/audio_loader.py

### Code snippets

_No response_

### OS

macOS

### Python version

3.11.2

### Library version

0.27.7",closed,2023-06-26T21:31:43Z,2023-10-18T07:51:45Z,['bug'],https://github.com/openai/openai-python/issues/504,2722.33,True,missing_dependency_violation,high,"The API key provided is incorrect, indicating a missing or invalid dependency for the authentication process.",high,
505,import openai  Error source code string cannot contain null bytes,"### Describe the bug

ValueError: source code string cannot contain null bytes

### To Reproduce

>>> import openai
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Program Files\Python310\lib\site-packages\openai\__init__.py"", line 15, in <module>
    import aiohttp
  File ""C:\Users\dadash\AppData\Roaming\Python\Python310\site-packages\aiohttp\__init__.py"", line 6, in <module>
    from .client import (
  File ""C:\Users\dadash\AppData\Roaming\Python\Python310\site-packages\aiohttp\client.py"", line 67, in <module>
    from .connector import (
  File ""C:\Users\dadash\AppData\Roaming\Python\Python310\site-packages\aiohttp\connector.py"", line 58, in <module>
    from .resolver import DefaultResolver
  File ""C:\Users\dadash\AppData\Roaming\Python\Python310\site-packages\aiohttp\resolver.py"", line 11, in <module>
    import aiodns
  File ""C:\Users\dadash\AppData\Roaming\Python\Python310\site-packages\aiodns\__init__.py"", line 4, in <module>
    import pycares
  File ""C:\Users\dadash\AppData\Roaming\Python\Python310\site-packages\pycares\__init__.py"", line 8, in <module>
    from . import errno
ValueError: source code string cannot contain null bytes

### Code snippets

_No response_

### OS

windows

### Python version

3.10.10

### Library version

0.27.8",closed,2023-06-26T21:58:02Z,2023-10-18T07:47:59Z,"['bug', 'fixed in v1']",https://github.com/openai/openai-python/issues/505,2721.83,True,input_value_violation,high,"The error indicates that a null byte is present in the source code string, which is unacceptable and leads to a ValueError.",high,
462,openssl dependency conflicts with default macos,"### Describe the bug

macOS comes with LibreSSL 2.8.3 by default and urllib3 v2.0, as an openai dependency, only supports OpenSSL 1.1.1+
Would it be easier to switch to nix for package management or is there a way to reassign the command alias openssl to somehow link to the desired version?

### To Reproduce

Obtain system with MacOS 13.0.1
Install openai per the instructions on the website. https://platform.openai.com/docs/guides/fine-tuning
As it instructs, run `pip install --upgrade openai`
Generate some fine tuning training json file, myfile.json
Then attempt to run `openai tools fine_tunes.prepare_data -f myfile.json`

### Code snippets

```Python
openai tools fine_tunes.prepare_data -f training.json 
Traceback (most recent call last):
  File ""/Users/timothy/Library/Python/3.9/bin/openai"", line 5, in <module>
    from openai._openai_scripts import main
  File ""/Users/timothy/Library/Python/3.9/lib/python/site-packages/openai/__init__.py"", line 19, in <module>
    from openai.api_resources import (
  File ""/Users/timothy/Library/Python/3.9/lib/python/site-packages/openai/api_resources/__init__.py"", line 1, in <module>
    from openai.api_resources.audio import Audio  # noqa: F401
  File ""/Users/timothy/Library/Python/3.9/lib/python/site-packages/openai/api_resources/audio.py"", line 4, in <module>
    from openai import api_requestor, util
  File ""/Users/timothy/Library/Python/3.9/lib/python/site-packages/openai/api_requestor.py"", line 22, in <module>
    import requests
  File ""/Users/timothy/Library/Python/3.9/lib/python/site-packages/requests/__init__.py"", line 43, in <module>
    import urllib3
  File ""/Users/timothy/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py"", line 38, in <module>
    raise ImportError(
ImportError: urllib3 v2.0 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with LibreSSL 2.8.3. See: https://github.com/urllib3/urllib3/issues/2168
```
```
openssl version
LibreSSL 3.3.6
```
```


### OS

13.0.1

### Python version

3.9

### Library version

openai version 0.27.7",closed,2023-05-24T02:05:53Z,2023-10-18T07:45:16Z,"['bug', 'fixed in v1']",https://github.com/openai/openai-python/issues/462,3533.66,True,missing_dependency_violation,high,"The openai library has a dependency on urllib3 which requires OpenSSL 1.1.1+, but the system is using LibreSSL 2.8.3, leading to an ImportError.",high,
221,Image.create() fails with request_timeout param,"### Describe the bug

When I use request_timeout on a text completion, it works fine:

`openai.Completion.create(prompt=""1,2,3,4,"", request_timeout=10.0)`

But when I try to do it with an image creation, it fails:
`openai.Image.create(prompt=""beautiful sunset"", request_timeout=10.0)`
==> `openai.error.InvalidRequestError: Additional properties are not allowed ('request_timeout' was unexpected)`

I'm not familiar enough with the code base to know how this ought to work, but I can get it to work with an ugly hack like (starting at line 36 of openai/api_resouces/image.py):
 ```
response, _, api_key = requestor.request(
            ""post"", cls._get_url(""generations""), params
 )
```

becomes:

```
request_timeout = params.pop(""request_timeout"", None) 
response, _, api_key = requestor.request(
            ""post"", cls._get_url(""generations""), params, request_timeout=request_timeout
 )
```

### To Reproduce

```
import openai
openai.api_key = ...
openai.Image.create(prompt=""beautiful sunset"", request_timeout=10.0)
```

### Code snippets

_No response_

### OS

macOS

### Python version

Python 3.11.1

### Library version

openai-python v0.26.5 (and others)",closed,2023-02-14T23:37:43Z,2023-10-18T07:40:30Z,"['bug', 'fixed in v1']",https://github.com/openai/openai-python/issues/221,5888.05,True,missing_option_violation,high,"The request_timeout parameter is not supported for the Image.create() method, leading to an InvalidRequestError.",high,
324,Stop using connection params as global variable in the package ,"### Describe the feature or improvement you're requesting

Currently, in order to create `openai` connection, we need to do the following:
```python
import openai
openai.organization = ""ORG""
openai.api_key = os.getenv(""KEY"")

openai.Completion.create(.....)
```

In my use case I have `Flask` server with 2 endpoint, each on  them uses different `OpenAI` credentials.
With the current implementation, when the credentials are global param in the package, I cannot use the 2 endpoints simultaneously 

Ideally, I want something like this:
```python
import openai

with openai.create_connection(org, key, ...) as conn:
    conn.Completion.create(.....)
```

### Additional context

_No response_",closed,2023-03-20T19:58:55Z,2023-10-18T07:36:26Z,['fixed in v1'],https://github.com/openai/openai-python/issues/324,5075.63,True,missing_dependency_violation,medium,"The current implementation relies on global connection parameters, which leads to conflicts when multiple endpoints require different credentials. This indicates a missing dependency on managing connection states per request.",high,
322,"Chat.create removes ""timeout"" from args","### Describe the bug

When making a call to the ChatCompletion create method, e.g.:

```
response = openai.ChatCompletion.create(
        model=self.model_id,
        messages=self.get_formatted_messages(),
        timeout=10
    )
```
 The timeout is `pop`d off of the kwargs by this line in the completion class ([link](https://github.com/openai/openai-python/blob/main/openai/api_resources/chat_completion.py#L21)):

`timeout = kwargs.pop(""timeout"", None)`

The `pop` method _removes_ the arg from `kwargs` before it's passed to `super`, which means that the `timeout` doesn't have the intended effect (the local method there does do some stuff with it, but that would only allow you to increase the timeout).

### To Reproduce

Run an API call with a shorter timeout:

```
response = openai.ChatCompletion.create(
        model=self.model_id,
        messages=self.get_formatted_messages(),
        timeout=10
    )
```

And observe that said timeout is not respected. The timeout remains 600 seconds.

### Code snippets

_No response_

### OS

macOS

### Python version

3.10.9

### Library version

openai==0.27.2",closed,2023-03-20T12:28:31Z,2023-10-18T07:33:58Z,"['bug', 'fixed in v1']",https://github.com/openai/openai-python/issues/322,5083.09,True,missing_option_violation,medium,"The API method does not respect the provided timeout argument due to its removal from kwargs, leading to unintended behavior.",high,
312,"Async openai.Embedding.acreate forcibly prints INFO, whereas openai.Embedding.create prints DEBUG","### Describe the bug

When `openai.Embedding.acreate` gets called many times, it prints a lot to STDERR, like so:

```
INFO:openai:message='OpenAI API response' path=https://api.openai.com/v1/engines/text-embedding-ada-002/embeddings processing_ms=350 request_id=681e326f4d3241ef9427e95c9eede485 response_code=200
INFO:openai:message='OpenAI API response' path=https://api.openai.com/v1/engines/text-embedding-ada-002/embeddings processing_ms=350 request_id=681e326f4d3241ef9427e95c9eede485 response_code=200
...
```

and there's no way to stop it to clutter the console. That is because `util.log_info` is used in `APIRequestor.arequest_raw` whereas `util.log_debug` is used in `APIRequestor.request_raw`, and it actually `print`s directly to STDERR.

https://github.com/openai/openai-python/blob/main/openai/util.py#L63-L67
```py
def log_info(message, **params):
    msg = logfmt(dict(message=message, **params))
    if _console_log_level() in [""debug"", ""info""]:
        print(msg, file=sys.stderr)
    logger.info(msg)
```

It may be intentional, and avoidable as follows, but it also suppress other logging. The different behaviors between sync/async seems like it's a bug.

```py
logger = logging.getLogger(""openai"")
logger.setLevel(logging.WARN)
... API calls ...
logger.setLevel(logging.INFO)
```


### To Reproduce

1. Call `openai.Embedding.acreate`
2. Watch the console output for STDERR

### Code snippets

_No response_

### OS

macOS

### Python version

Python v3.10.10

### Library version

openai-python v0.26.2",closed,2023-03-16T07:34:16Z,2023-10-18T07:27:23Z,"['bug', 'fixed in v1']",https://github.com/openai/openai-python/issues/312,5183.89,True,missing_option_violation,medium,"The inconsistency in logging levels between synchronous and asynchronous API calls may lead to unexpected behavior and user confusion, as it clutters the console output.",high,
269,Consider using hatch as build-backend?,"### Describe the feature or improvement you're requesting

`setup.py` + `setup.cfg` is an outdated way of building Python projects. The configuration can be moved to `pyproject.toml` for consistency.

[hatch](https://github.com/pypa/hatch) is a modern python project management tool. hatch is used by the likes of fastAPI, black, pydantic and others to manage projects.

### Additional context

_No response_",closed,2023-03-04T01:42:33Z,2023-10-18T07:20:35Z,['fixed in v1'],https://github.com/openai/openai-python/issues/269,5477.63,False,,,,,
151,Add types?,"there is a py.typed file in the repository but it doesn't look like any of the code actually has types. Types are handy for catching bugs, but if that's not on your roadmap, maybe just remove the py.typed file?",closed,2022-12-13T20:18:10Z,2023-10-18T07:18:13Z,['fixed in v1'],https://github.com/openai/openai-python/issues/151,7403.0,True,input_type_violation,unknown,"The presence of a py.typed file suggests an expectation for type annotations, but the absence of actual types in the code leads to a mismatch between the contract and implementation.",medium,
652,v1.0.0b2: Types not working correctly when Literals not used for stream parameter for completions.create,"### Describe the bug

When stream is a bool variable, typing does not select the correct overloaded function implementation in  `client.chat.completions.create`. This results in the following error:

```
Argument of type ""bool"" cannot be assigned to parameter ""stream"" of type ""Literal[True]"" in function ""create""
  ""bool"" cannot be assigned to type ""Literal[True]""Pylance[reportGeneralTypeIssues](https://github.com/microsoft/pyright/blob/main/docs/configuration.md#reportGeneralTypeIssues)
```

Passing in a Literal works as expected, and so does 'unpacking' the dynamic value into Literals.

It appears that typing does not select the overloaded function correctly that contains the relevant types (line 624 in `Async Completions`):
```
stream: Optional[Literal[False]] | Literal[True] | NotGiven = NOT_GIVEN,
```

### To Reproduce

The following code doesn't work:

```
client = AsyncOpenAI()
stream = True if 0 == 0 else False

await client.chat.completions.create(
    model=request.model,
    messages=[
        {
            ""role"": ""user"",
            ""content"": """",
        },
    ],
    stream=stream,
)
```

This code works correctly:

```
client = AsyncOpenAI()
stream = True if 0 == 0 else False
if (stream == True):
    await client.chat.completions.create(
        model=request.model,
        messages=[
            {
                ""role"": ""user"",
                ""content"": """",
            },
        ],
        stream=stream,
    )
else:
    await client.chat.completions.create(
        model=request.model,
        messages=[
            {
                ""role"": ""user"",
                ""content"": """",
            },
        ],
        stream=stream,
    )
```

### Code snippets

_No response_

### OS

macOS

### Python version

3.9.13

### Library version

1.0.0b2",closed,2023-10-12T23:22:42Z,2023-10-17T16:19:23Z,['bug'],https://github.com/openai/openai-python/issues/652,112.94,True,input_type_violation,high,"The API method expects a specific type (Literal[True]) for the 'stream' parameter, but a boolean variable is being passed instead, leading to a type mismatch.",high,
557,Issue with populating api configs using environment variables after importing openai,"### Describe the bug

I'm encountering an issue where I am unable to populate the `api_key`, `api_base`, `api_type`, and `api_version` using environment variables.

The openai package seems to read the `api_key`, `api_base`, `api_type`, and `api_version` from environment variables only at the moment it is imported. If these api configs are specified after the openai package is imported, they are not recognized.

The code is in [here](https://github.com/openai/openai-python/blob/b82a3f7e4c462a8a10fa445193301a3cefef9a4a/openai/__init__.py#L42-L54).

### To Reproduce

Due to python's caching mechanism, to reproduce this issue, the openai package must be reloaded.

### Code snippets

```Python
import openai
import os

os.environ[""OPENAI_API_KEY""] = ""<api_key>""
os.environ[""OPENAI_API_BASE""] = ""<api_base>""
os.environ[""OPENAI_API_TYPE""] = ""<api_type>""
os.environ[""OPENAI_API_VERSION""] = ""<api_version>""

openai.Completion.create(prompt=""Hello"", engine=""text-davinci-003"", max_tokens=5)
```


### OS

Windows 11

### Python version

Python v3.8.16

### Library version

openai-python v0.27.8",closed,2023-08-03T03:22:40Z,2023-10-17T11:28:39Z,"['bug', 'fixed in v1']",https://github.com/openai/openai-python/issues/557,1808.1,True,method_order_violation,High,"The API configuration values must be set before importing the openai package to ensure they are recognized, but the current order of operations leads to them being ignored.",high,
657,Update,"### Describe the feature or improvement you're requesting

Update your openai pip envarment.

### Additional context

_No response_",closed,2023-10-16T15:26:09Z,2023-10-16T19:36:29Z,[],https://github.com/openai/openai-python/issues/657,4.17,True,missing_dependency_violation,unknown,"The request lacks specific details on what needs to be updated in the openai pip environment, indicating a missing dependency on clear requirements.",medium,
411,Using Azure and OpenAI at the same time,"### Describe the bug

Currently it seems there is no strightforward way to be able to send requests to both OpenAI and Azure from same python program. Ideally this should be possible:

openai = OpenAI(key=key)
azureai = OpenAI(key=key, api_base=xxx, ....)

While currently given key, api_base, etc. are global variables to the module, the above won't work.


### To Reproduce

-

### Code snippets

_No response_

### OS

Linux

### Python version

Python 3

### Library version

latest",closed,2023-04-23T09:51:50Z,2023-10-16T13:17:59Z,['bug'],https://github.com/openai/openai-python/issues/411,4227.44,True,missing_dependency_violation,medium,"The issue indicates a missing dependency in the configuration of the OpenAI and Azure API instances, as they rely on global variables that conflict when instantiated together.",high,
637,No module named 'openai',"### Describe the bug

I have uninstalled openai and reinstalled it with pip and pip3, 
I have checked that pip and openai are installed in the same folder
and various other internet solutions but nothing has worked
please help

I know its not a bug but i can't figure out how to log a normal issue :'(

### To Reproduce

all i say is import openai

### Code snippets

_No response_

### OS

Windows

### Python version

3.1.1

### Library version

0.28.1",closed,2023-10-05T16:03:10Z,2023-10-16T11:39:31Z,['question'],https://github.com/openai/openai-python/issues/637,259.61,True,input_type_violation,unknown,"The user is attempting to import the 'openai' module, but it appears that the module is not installed or recognized in the Python environment, indicating a potential type mismatch or missing dependency.",high,
654,Unable to use Python 3.12 due to aiohttp dependency unable to build wheel.,"### Describe the bug

I am sure that aiohttp will catch up, but meanwhile, I figure I would through this on your radar for potential work around.

### To Reproduce

Running Python 3.12.0 
pip install openai

### Code snippets

_No response_

### OS

any

### Python version

3.12.0

### Library version

current",closed,2023-10-14T11:18:52Z,2023-10-14T23:42:13Z,['fixed in v1'],https://github.com/openai/openai-python/issues/654,12.39,False,,,,low,
436,What is the default value of  max_token argument in ChatCompletion API,"### Describe the bug

What is the default value of  max_token argument in ChatCompletion API

### To Reproduce

What is the default value of  max_token argument in ChatCompletion API

### Code snippets

_No response_

### OS

Ubuntu

### Python version

Python3.9

### Library version

openai==0.27.0",closed,2023-05-05T09:12:10Z,2023-10-15T01:06:51Z,['bug'],https://github.com/openai/openai-python/issues/436,3903.91,True,missing_dependency_violation,unknown,"The issue lacks clarity on the default value for the max_token argument, indicating a potential missing dependency on documentation or configuration.",medium,
655,openai.error.APIError: Invalid response object from API？,"### Describe the bug

```
openai.api_base = ""http://localhost:10860/v1/embeddings""

def test_embedding():
    embedding = openai.Embedding.create(model =model,input=""hello"")
    print(embedding)
    print(embedding[""data""][0][""embedding""].shape)
    print(embedding[""data""][0][""embedding""])
```

### To Reproduce

Traceback (most recent call last):
  File ""C:\Users\loong\.conda\envs\nlp\lib\site-packages\openai\api_requestor.py"", line 403, in handle_error_response
    error_data = resp[""error""]
KeyError: 'error'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""D:\llm\NLP\aquila_openai_api_vllm.py"", line 54, in <module>
    test_embedding()
  File ""D:\llm\NLP\aquila_openai_api_vllm.py"", line 25, in test_embedding
    embedding = openai.Embedding.create(model =model,input=""你好"")
  File ""C:\Users\loong\.conda\envs\nlp\lib\site-packages\openai\api_resources\embedding.py"", line 33, in create
    response = super().create(*args, **kwargs)
  File ""C:\Users\loong\.conda\envs\nlp\lib\site-packages\openai\api_resources\abstract\engine_api_resource.py"", line 153, in create
    response, _, api_key = requestor.request(
  File ""C:\Users\loong\.conda\envs\nlp\lib\site-packages\openai\api_requestor.py"", line 298, in request
    resp, got_stream = self._interpret_response(result, stream)
  File ""C:\Users\loong\.conda\envs\nlp\lib\site-packages\openai\api_requestor.py"", line 707, in _interpret_response
    self._interpret_response_line(
  File ""C:\Users\loong\.conda\envs\nlp\lib\site-packages\openai\api_requestor.py"", line 772, in _interpret_response_line
    raise self.handle_error_response(
  File ""C:\Users\loong\.conda\envs\nlp\lib\site-packages\openai\api_requestor.py"", line 405, in handle_error_response
    raise error.APIError(
openai.error.APIError: Invalid response object from API: '{""detail"":""Not Found""}' (HTTP response code was 404)

### Code snippets

_No response_

### OS

centos

### Python version

3.10

### Library version

0.28.0",closed,2023-10-14T13:45:07Z,2023-10-14T23:44:26Z,[],https://github.com/openai/openai-python/issues/655,9.99,True,input_value_violation,High,"The API call to create an embedding failed due to a 404 error, indicating that the requested resource was not found, likely due to an invalid input or configuration.",high,
648,"[REPOST of #455 (with corrections to grammar, behavior/attitude and more recent errors) - unfairly/unreasonably closed] openai.error.InvalidRequestError: Invalid URL (POST /v1/chat/engines/gpt-3.5-turbo-0613/chat/completions)","### Describe the bug

The module doesn't work! My issue got closed with absolutely NO WARNING/REASON too. :(

Error:
```cmd
Traceback (most recent call last):
  File ""C:\Users\SonicandTailsCD\Desktop\openai_script.py"", line 14, in <module>

    response = openai.ChatCompletion.create(
  File ""C:\Program Files\Python38\lib\site-packages\openai\api_resources\chat_co
mpletion.py"", line 25, in create
    return super().create(*args, **kwargs)
  File ""C:\Program Files\Python38\lib\site-packages\openai\api_resources\abstrac
t\engine_api_resource.py"", line 153, in create
    response, _, api_key = requestor.request(
  File ""C:\Program Files\Python38\lib\site-packages\openai\api_requestor.py"", li
ne 230, in request
    resp, got_stream = self._interpret_response(result, stream)
  File ""C:\Program Files\Python38\lib\site-packages\openai\api_requestor.py"", li
ne 624, in _interpret_response
    self._interpret_response_line(
  File ""C:\Program Files\Python38\lib\site-packages\openai\api_requestor.py"", li
ne 687, in _interpret_response_line
    raise self.handle_error_response(
openai.error.InvalidRequestError: Invalid URL (POST /v1/chat/engines/gpt-3.5-tur
bo-0613/chat/completions)
```

### To Reproduce

Simply create a new openai script. Then run it.

### Code snippets

```Python
Sure.
import openai

openai.organization = *censored*
openai.api_key = *censored*

messages = [
    {
        ""role"": ""system"",
        ""content"": ""Hi, ChatGPT! So just to test something, say \""This is a test!\"" please :)""
    }
]

response = openai.ChatCompletion.create(
    engine=""gpt-3.5-turbo"",
    messages=messages,
    max_tokens=2000,
    temperature=0.7,
    top_p=1,
    n=1
)

print(response)
```


### OS

Windows 7 x64 (Multiboot)

### Python version

Python v3.8.10

### Library version

openai-python 27.0",closed,2023-10-12T00:29:33Z,2023-10-12T08:22:59Z,['fixed in v1'],https://github.com/openai/openai-python/issues/648,7.89,True,input_value_violation,unknown,"The error indicates an invalid URL, which may suggest that the input value for the engine parameter is not recognized or is incorrectly formatted.",high,
455,Module 'openai' has no attribute 'ChatCompletion',"### Describe the bug

The module doesn't work :(

Error: 
Traceback (most recent call last):
  File ""openai.py"", line 1, in <module>
    import openai
  File ""C:\Users\SonicandTailsCD\desktop\openai.py"", line 13, in <module>
    response = openai.ChatCompletion.create(
AttributeError: module 'openai' has no attribute 'ChatCompletion'

### To Reproduce

1.- Create a basic OpenAI script
2.- See the error!

### Code snippets

```Python
import openai

openai.organization = *censored*
openai.api_key = *censored*

messages = [
    {
        ""role"": ""system"",
        ""content"": ""Hi, ChatGPT! So just to test something, say \""This is a test!\"" please :)""
    }
]

response = openai.ChatCompletion.create(
    engine=""gpt-3.5-turbo"",
    messages=messages,
    max_tokens=2000,
    temperature=0.7,
    top_p=1,
    n=1
)

print(response)
```


### OS

Windows 7 x64 (Multiboot)

### Python version

Python v3.8.10

### Library version

version 0.27.6",closed,2023-05-19T03:59:50Z,2023-10-11T11:05:50Z,['question'],https://github.com/openai/openai-python/issues/455,3487.1,True,input_type_violation,unknown,"The error indicates that the 'ChatCompletion' attribute is not found in the 'openai' module, suggesting a potential issue with the expected library structure or version compatibility.",high,
646,Describe the bug report that this github project has cracked ,"### Describe the bug

Describe the bug
report that this github project has cracked the web request protocol https://github.com/zhile-io/pandora

To Reproduce
report that this github project has cracked the web request protocol https://github.com/zhile-io/pandora

Code snippets
report that this github project has cracked the web request protocol https://github.com/zhile-io/pandora

### To Reproduce

Describe the bug
report that this github project has cracked the web request protocol https://github.com/zhile-io/pandora

To Reproduce
report that this github project has cracked the web request protocol https://github.com/zhile-io/pandora

Code snippets
report that this github project has cracked the web request protocol https://github.com/zhile-io/pandora

### Code snippets

_No response_

### OS

mac

### Python version

3.1

### Library version

0.265",closed,2023-10-10T15:05:58Z,2023-10-10T15:06:07Z,['bug'],https://github.com/openai/openai-python/issues/646,0.0,True,input_type_violation,high,"The Python version specified (3.1) is likely incompatible with the library version (0.265), which may lead to runtime errors or system instability.",high,
640,ERROR: Failed building wheel for aiohttp,"### Describe the bug

I got this error ""ERROR: Failed building wheel for aiohttp"" when I tried to do a pip3 install open ai.
macOs Sonoma
MacBook Pro M2 Max.

### To Reproduce

pip3 install openai.

### Code snippets

```Python
Building wheels for collected packages: aiohttp
  Building wheel for aiohttp (pyproject.toml) ... error
  error: subprocess-exited-with-error

  × Building wheel for aiohttp (pyproject.toml) did not run successfully.
  │ exit code: 1
  ╰─> [188 lines of output]
      *********************
      * Accelerated build *
      *********************
      running bdist_wheel
      running build
      running build_py
      creating build
      creating build/lib.macosx-10.9-universal2-cpython-312
      creating build/lib.macosx-10.9-universal2-cpython-312/aiohttp
      copying aiohttp/web_ws.py -> build/lib.macosx-10.9-universal2-cpython-312/aiohttp
      copying aiohttp/worker.py -> build/lib.macosx-10.9-universal2-cpython-312/aiohttp
      copying aiohttp/multipart.py -> build/lib.macosx-10.9-universal2-cpython-312/aiohttp
      copying aiohttp/web_response.py -> build/lib.macosx-10.9-universal2-cpython-312/aiohttp
      copying aiohttp/client_ws.py -> build/lib.macosx-10.9-universal2-cpython-312/aiohttp
      copying aiohttp/test_utils.py -> build/lib.macosx-10.9-universal2-cpython-312/aiohttp
      copying aiohttp/tracing.py -> build/lib.macosx-10.9-universal2-cpython-312/aiohttp
      copying aiohttp/web_exceptions.py -> build/lib.macosx-10.9-universal2-cpython-312/aiohttp
      copying aiohttp/web_middlewares.py -> build/lib.macosx-10.9-universal2-cpython-312/aiohttp
      copying aiohttp/web.py -> build/lib.macosx-10.9-universal2-cpython-312/aiohttp
      copying aiohttp/http_exceptions.py -> build/lib.macosx-10.9-universal2-cpython-312/aiohttp
      copying aiohttp/web_app.py -> build/lib.macosx-10.9-universal2-cpython-312/aiohttp
      copying aiohttp/streams.py -> build/lib.macosx-10.9-universal2-cpython-312/aiohttp
      copying aiohttp/web_protocol.py -> build/lib.macosx-10.9-universal2-cpython-312/aiohttp
      copying aiohttp/log.py -> build/lib.macosx-10.9-universal2-cpython-312/aiohttp
      copying aiohttp/client.py -> build/lib.macosx-10.9-universal2-cpython-312/aiohttp
      copying aiohttp/web_urldispatcher.py -> build/lib.macosx-10.9-universal2-cpython-312/aiohttp
      copying aiohttp/web_request.py -> build/lib.macosx-10.9-universal2-cpython-312/aiohttp
      copying aiohttp/http_websocket.py -> build/lib.macosx-10.9-universal2-cpython-312/aiohttp
      copying aiohttp/client_proto.py -> build/lib.macosx-10.9-universal2-cpython-312/aiohttp
      copying aiohttp/locks.py -> build/lib.macosx-10.9-universal2-cpython-312/aiohttp
      copying aiohttp/__init__.py -> build/lib.macosx-10.9-universal2-cpython-312/aiohttp
      copying aiohttp/web_runner.py -> build/lib.macosx-10.9-universal2-cpython-312/aiohttp
      copying aiohttp/web_server.py -> build/lib.macosx-10.9-universal2-cpython-312/aiohttp
      copying aiohttp/base_protocol.py -> build/lib.macosx-10.9-universal2-cpython-312/aiohttp
      copying aiohttp/payload.py -> build/lib.macosx-10.9-universal2-cpython-312/aiohttp
      copying aiohttp/client_reqrep.py -> build/lib.macosx-10.9-universal2-cpython-312/aiohttp
      copying aiohttp/http.py -> build/lib.macosx-10.9-universal2-cpython-312/aiohttp
      copying aiohttp/web_log.py -> build/lib.macosx-10.9-universal2-cpython-312/aiohttp
      copying aiohttp/resolver.py -> build/lib.macosx-10.9-universal2-cpython-312/aiohttp
      copying aiohttp/formdata.py -> build/lib.macosx-10.9-universal2-cpython-312/aiohttp
      copying aiohttp/payload_streamer.py -> build/lib.macosx-10.9-universal2-cpython-312/aiohttp
      copying aiohttp/web_routedef.py -> build/lib.macosx-10.9-universal2-cpython-312/aiohttp
      copying aiohttp/connector.py -> build/lib.macosx-10.9-universal2-cpython-312/aiohttp
      copying aiohttp/client_exceptions.py -> build/lib.macosx-10.9-universal2-cpython-312/aiohttp
      copying aiohttp/typedefs.py -> build/lib.macosx-10.9-universal2-cpython-312/aiohttp
      copying aiohttp/hdrs.py -> build/lib.macosx-10.9-universal2-cpython-312/aiohttp
      copying aiohttp/web_fileresponse.py -> build/lib.macosx-10.9-universal2-cpython-312/aiohttp
      copying aiohttp/http_writer.py -> build/lib.macosx-10.9-universal2-cpython-312/aiohttp
      copying aiohttp/tcp_helpers.py -> build/lib.macosx-10.9-universal2-cpython-312/aiohttp
      copying aiohttp/helpers.py -> build/lib.macosx-10.9-universal2-cpython-312/aiohttp
      copying aiohttp/http_parser.py -> build/lib.macosx-10.9-universal2-cpython-312/aiohttp
      copying aiohttp/cookiejar.py -> build/lib.macosx-10.9-universal2-cpython-312/aiohttp
      copying aiohttp/pytest_plugin.py -> build/lib.macosx-10.9-universal2-cpython-312/aiohttp
      copying aiohttp/abc.py -> build/lib.macosx-10.9-universal2-cpython-312/aiohttp
      running egg_info
      writing aiohttp.egg-info/PKG-INFO
      writing dependency_links to aiohttp.egg-info/dependency_links.txt
      writing requirements to aiohttp.egg-info/requires.txt
      writing top-level names to aiohttp.egg-info/top_level.txt
      reading manifest file 'aiohttp.egg-info/SOURCES.txt'
      reading manifest template 'MANIFEST.in'
      warning: no files found matching 'aiohttp' anywhere in distribution
      warning: no previously-included files matching '*.pyc' found anywhere in distribution
      warning: no previously-included files matching '*.pyd' found anywhere in distribution
      warning: no previously-included files matching '*.so' found anywhere in distribution
      warning: no previously-included files matching '*.lib' found anywhere in distribution
      warning: no previously-included files matching '*.dll' found anywhere in distribution
      warning: no previously-included files matching '*.a' found anywhere in distribution
      warning: no previously-included files matching '*.obj' found anywhere in distribution
      warning: no previously-included files found matching 'aiohttp/*.html'
      no previously-included directories found matching 'docs/_build'
      adding license file 'LICENSE.txt'
      writing manifest file 'aiohttp.egg-info/SOURCES.txt'
      copying aiohttp/_cparser.pxd -> build/lib.macosx-10.9-universal2-cpython-312/aiohttp
      copying aiohttp/_find_header.pxd -> build/lib.macosx-10.9-universal2-cpython-312/aiohttp
      copying aiohttp/_headers.pxi -> build/lib.macosx-10.9-universal2-cpython-312/aiohttp
      copying aiohttp/_helpers.pyi -> build/lib.macosx-10.9-universal2-cpython-312/aiohttp
      copying aiohttp/_helpers.pyx -> build/lib.macosx-10.9-universal2-cpython-312/aiohttp
      copying aiohttp/_http_parser.pyx -> build/lib.macosx-10.9-universal2-cpython-312/aiohttp
      copying aiohttp/_http_writer.pyx -> build/lib.macosx-10.9-universal2-cpython-312/aiohttp
      copying aiohttp/_websocket.pyx -> build/lib.macosx-10.9-universal2-cpython-312/aiohttp
      copying aiohttp/py.typed -> build/lib.macosx-10.9-universal2-cpython-312/aiohttp
      creating build/lib.macosx-10.9-universal2-cpython-312/aiohttp/.hash
      copying aiohttp/.hash/_cparser.pxd.hash -> build/lib.macosx-10.9-universal2-cpython-312/aiohttp/.hash
      copying aiohttp/.hash/_find_header.pxd.hash -> build/lib.macosx-10.9-universal2-cpython-312/aiohttp/.hash
      copying aiohttp/.hash/_helpers.pyi.hash -> build/lib.macosx-10.9-universal2-cpython-312/aiohttp/.hash
      copying aiohttp/.hash/_helpers.pyx.hash -> build/lib.macosx-10.9-universal2-cpython-312/aiohttp/.hash
      copying aiohttp/.hash/_http_parser.pyx.hash -> build/lib.macosx-10.9-universal2-cpython-312/aiohttp/.hash
      copying aiohttp/.hash/_http_writer.pyx.hash -> build/lib.macosx-10.9-universal2-cpython-312/aiohttp/.hash
      copying aiohttp/.hash/_websocket.pyx.hash -> build/lib.macosx-10.9-universal2-cpython-312/aiohttp/.hash
      copying aiohttp/.hash/hdrs.py.hash -> build/lib.macosx-10.9-universal2-cpython-312/aiohttp/.hash
      running build_ext
      building 'aiohttp._websocket' extension
      creating build/temp.macosx-10.9-universal2-cpython-312
      creating build/temp.macosx-10.9-universal2-cpython-312/aiohttp
      clang -fno-strict-overflow -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -O3 -Wall -arch arm64 -arch x86_64 -g -I/Library/Frameworks/Python.framework/Versions/3.12/include/python3.12 -c aiohttp/_websocket.c -o build/temp.macosx-10.9-universal2-cpython-312/aiohttp/_websocket.o
      aiohttp/_websocket.c:1475:17: warning: 'Py_OptimizeFlag' is deprecated [-Wdeprecated-declarations]
        if (unlikely(!Py_OptimizeFlag)) {
                      ^
      /Library/Frameworks/Python.framework/Versions/3.12/include/python3.12/cpython/pydebug.h:13:1: note: 'Py_OptimizeFlag' has been explicitly marked deprecated here
      Py_DEPRECATED(3.12) PyAPI_DATA(int) Py_OptimizeFlag;
      ^
      /Library/Frameworks/Python.framework/Versions/3.12/include/python3.12/pyport.h:317:54: note: expanded from macro 'Py_DEPRECATED'
      #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__))
                                                           ^
      aiohttp/_websocket.c:2680:27: warning: 'ma_version_tag' is deprecated [-Wdeprecated-declarations]
          return likely(dict) ? __PYX_GET_DICT_VERSION(dict) : 0;
                                ^
      aiohttp/_websocket.c:1118:65: note: expanded from macro '__PYX_GET_DICT_VERSION'
      #define __PYX_GET_DICT_VERSION(dict)  (((PyDictObject*)(dict))->ma_version_tag)
                                                                      ^
      /Library/Frameworks/Python.framework/Versions/3.12/include/python3.12/cpython/dictobject.h:22:5: note: 'ma_version_tag' has been explicitly marked deprecated here
          Py_DEPRECATED(3.12) uint64_t ma_version_tag;
          ^
      /Library/Frameworks/Python.framework/Versions/3.12/include/python3.12/pyport.h:317:54: note: expanded from macro 'Py_DEPRECATED'
      #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__))
                                                           ^
      aiohttp/_websocket.c:2692:36: warning: 'ma_version_tag' is deprecated [-Wdeprecated-declarations]
          return (dictptr && *dictptr) ? __PYX_GET_DICT_VERSION(*dictptr) : 0;
                                         ^
      aiohttp/_websocket.c:1118:65: note: expanded from macro '__PYX_GET_DICT_VERSION'
      #define __PYX_GET_DICT_VERSION(dict)  (((PyDictObject*)(dict))->ma_version_tag)
                                                                      ^
      /Library/Frameworks/Python.framework/Versions/3.12/include/python3.12/cpython/dictobject.h:22:5: note: 'ma_version_tag' has been explicitly marked deprecated here
          Py_DEPRECATED(3.12) uint64_t ma_version_tag;
          ^
      /Library/Frameworks/Python.framework/Versions/3.12/include/python3.12/pyport.h:317:54: note: expanded from macro 'Py_DEPRECATED'
      #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__))
                                                           ^
      aiohttp/_websocket.c:2696:56: warning: 'ma_version_tag' is deprecated [-Wdeprecated-declarations]
          if (unlikely(!dict) || unlikely(tp_dict_version != __PYX_GET_DICT_VERSION(dict)))
                                                             ^
      aiohttp/_websocket.c:1118:65: note: expanded from macro '__PYX_GET_DICT_VERSION'
      #define __PYX_GET_DICT_VERSION(dict)  (((PyDictObject*)(dict))->ma_version_tag)
                                                                      ^
      /Library/Frameworks/Python.framework/Versions/3.12/include/python3.12/cpython/dictobject.h:22:5: note: 'ma_version_tag' has been explicitly marked deprecated here
          Py_DEPRECATED(3.12) uint64_t ma_version_tag;
          ^
      /Library/Frameworks/Python.framework/Versions/3.12/include/python3.12/pyport.h:317:54: note: expanded from macro 'Py_DEPRECATED'
      #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__))
                                                           ^
      aiohttp/_websocket.c:2741:9: warning: 'ma_version_tag' is deprecated [-Wdeprecated-declarations]
              __PYX_PY_DICT_LOOKUP_IF_MODIFIED(
              ^
      aiohttp/_websocket.c:1125:16: note: expanded from macro '__PYX_PY_DICT_LOOKUP_IF_MODIFIED'
          if (likely(__PYX_GET_DICT_VERSION(DICT) == __pyx_dict_version)) {\
                     ^
      aiohttp/_websocket.c:1118:65: note: expanded from macro '__PYX_GET_DICT_VERSION'
      #define __PYX_GET_DICT_VERSION(dict)  (((PyDictObject*)(dict))->ma_version_tag)
                                                                      ^
      /Library/Frameworks/Python.framework/Versions/3.12/include/python3.12/cpython/dictobject.h:22:5: note: 'ma_version_tag' has been explicitly marked deprecated here
          Py_DEPRECATED(3.12) uint64_t ma_version_tag;
          ^
      /Library/Frameworks/Python.framework/Versions/3.12/include/python3.12/pyport.h:317:54: note: expanded from macro 'Py_DEPRECATED'
      #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__))
                                                           ^
      aiohttp/_websocket.c:2741:9: warning: 'ma_version_tag' is deprecated [-Wdeprecated-declarations]
              __PYX_PY_DICT_LOOKUP_IF_MODIFIED(
              ^
      aiohttp/_websocket.c:1129:30: note: expanded from macro '__PYX_PY_DICT_LOOKUP_IF_MODIFIED'
              __pyx_dict_version = __PYX_GET_DICT_VERSION(DICT);\
                                   ^
      aiohttp/_websocket.c:1118:65: note: expanded from macro '__PYX_GET_DICT_VERSION'
      #define __PYX_GET_DICT_VERSION(dict)  (((PyDictObject*)(dict))->ma_version_tag)
                                                                      ^
      /Library/Frameworks/Python.framework/Versions/3.12/include/python3.12/cpython/dictobject.h:22:5: note: 'ma_version_tag' has been explicitly marked deprecated here
          Py_DEPRECATED(3.12) uint64_t ma_version_tag;
          ^
      /Library/Frameworks/Python.framework/Versions/3.12/include/python3.12/pyport.h:317:54: note: expanded from macro 'Py_DEPRECATED'
      #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__))
                                                           ^
      aiohttp/_websocket.c:3042:55: error: no member named 'ob_digit' in 'struct _longobject'
                  const digit* digits = ((PyLongObject*)x)->ob_digit;
                                        ~~~~~~~~~~~~~~~~~~  ^
      aiohttp/_websocket.c:3097:55: error: no member named 'ob_digit' in 'struct _longobject'
                  const digit* digits = ((PyLongObject*)x)->ob_digit;
                                        ~~~~~~~~~~~~~~~~~~  ^
      aiohttp/_websocket.c:3238:55: error: no member named 'ob_digit' in 'struct _longobject'
                  const digit* digits = ((PyLongObject*)x)->ob_digit;
                                        ~~~~~~~~~~~~~~~~~~  ^
      aiohttp/_websocket.c:3293:55: error: no member named 'ob_digit' in 'struct _longobject'
                  const digit* digits = ((PyLongObject*)x)->ob_digit;
                                        ~~~~~~~~~~~~~~~~~~  ^
      aiohttp/_websocket.c:3744:47: error: no member named 'ob_digit' in 'struct _longobject'
          const digit* digits = ((PyLongObject*)b)->ob_digit;
                                ~~~~~~~~~~~~~~~~~~  ^
      6 warnings and 5 errors generated.
      error: command '/usr/bin/clang' failed with exit code 1
      [end of output]

  note: This error originates from a subprocess, and is likely not a problem with pip.
  ERROR: Failed building wheel for aiohttp
Failed to build aiohttp
ERROR: Could not build wheels for aiohttp, which is required to install pyproject.toml-based projects
```


### OS

macOs

### Python version

Python 3.12

### Library version

open-python 0.28.1",closed,2023-10-07T19:44:08Z,2023-10-07T19:48:38Z,"['bug', 'fixed in v1']",https://github.com/openai/openai-python/issues/640,0.07,True,input_type_violation,high,"The error indicates a failure to build the 'aiohttp' package, likely due to incompatibility with Python 3.12, which suggests that the input type (Python version) does not match the expected type for the package.",high,
608,Not retrying on error,"### Describe the bug

I'm running a simple LLMChain with ChatOpenAI and getting 'Error [TimeoutError]: Request timed out' errors very often.
But the main problem is that LangChain doesn't seem to be retrying to run the chain after the error.

At the very end of the error message I get:

```
...
  attemptNumber: 1,
  retriesLeft: 6
}
```

Here's the full error message:

```
Error in handler LangChainTracer, handleChainError: TimeoutError: The operation was aborted due to timeout
file:///Users/[redacted]/node_modules/langchain/dist/util/openai.js:6
        error = new Error(e.message);
                ^

Error [TimeoutError]: Request timed out.
    at wrapOpenAIClientError (file:///Users/[redacted]/node_modules/langchain/dist/util/openai.js:6:17)
    at file:///Users/[redacted]/node_modules/langchain/dist/chat_models/openai.js:517:31
    at process.processTicksAndRejections (node:internal/process/task_queues:95:5)
    at async RetryOperation._fn (/Users/[redacted]/node_modules/p-retry/index.js:50:12) {
  attemptNumber: 1,
  retriesLeft: 6
```

### To Reproduce

1. Make enough sub-sequential calls to ChatOpenAI using the LLMChain.

In my case I'm looping over a 2000 array, using each one of the elements to populate the prompt template and calling the chain. 

### Code snippets

```Python
// OPEN AI SETUP
const llm = new ChatOpenAI({
  openAIApiKey: process.env.OPENAI_API_KEY,
  modelName: ""gpt-3.5-turbo"",
  max_tokens: 1800,
  // frequency_penalty: 0.1,
  temperature: 0.1,
  timeout: 30000,
  maxRetries: 5,
});

//LLM CHAIN
const chain = new LLMChain({
  llm: llm,
  prompt: promptTemplate,
});

//LOOP OVER THE DATASET AND CALL LLM
for (const [index, name] of names.entries()) {
  const result = await chain.call({
    name: name.firstName,
  });

...

 console.log(result.text);
}
```


### OS

MacOS

### Python version

Node v20.5.1

### Library version

openai 3.3.0",closed,2023-09-10T16:42:41Z,2023-10-04T19:27:25Z,['fixed in v1'],https://github.com/openai/openai-python/issues/608,578.75,True,missing_dependency_violation,medium,"The LLMChain does not retry on timeout errors, indicating a missing dependency on error handling logic that should manage retries appropriately.",high,
469,Retrial Mode,"### Describe the feature or improvement you're requesting

Hello, I am currently maintaining AutoGPT and we have a lot of errors that we handle manually on the client side.


Did you consider providing a retrial option to abstract all this away from the user ? Pretty much everyone is building an exponential backoff and adds retrials to catch the dozens of errors OpenAI issues when you query them too much.
Less time building that = more time spending openai tokens :)

It could be an argument called ""retrials"", you can add it incrementally to the most important methods first and you can also include the exponential backoff in there to avoid abuse.


There is a ton of errors that OpenAI issues and that are not ""real errors"" and should lead to the client retrying.

Examples above.


### Additional context

here is one example, this is a 500, but it should definitely be retried:
```
openai.error.APIError: The server had an error while processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 1aa6b47fe412ec21d7cbeff95babf6d4 in your message.) {
  ""error"": {
    ""message"": ""The server had an error while processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 1aa6b47fe412ec21d7cbeff95babf6d4 in your message.)"",
    ""type"": ""server_error"",
    ""param"": null,
    ""code"": null
  }
}
```

and there are some 408 as well.",closed,2023-05-26T13:58:14Z,2023-10-04T19:26:10Z,"['enhancement', 'fixed in v1']",https://github.com/openai/openai-python/issues/469,3149.47,True,missing_option_violation,medium,"The API does not currently provide a retrial option for handling transient errors, which could lead to suboptimal user experience.",high,
626,Correction needed in README.md - Replace 'a' with 'await,"### Describe the bug

```
$ git diff -U0
diff --git a/README.md b/README.md
index b2e0b1b..6cdd5cc 100644
--- a/README.md
+++ b/README.md
@@ -175 +175 @@ You can learn more in our [speech to text guide](https://platform.openai.com/doc
-Async support is available in the API by prepending `a` to a network-bound method:
+Async support is available in the API by prepending `await` to a network-bound method:
```


### To Reproduce

-

### Code snippets

_No response_

### OS

-

### Python version

-

### Library version

latest",closed,2023-09-27T16:21:49Z,2023-09-27T16:29:21Z,['bug'],https://github.com/openai/openai-python/issues/626,0.13,False,,,,,
246,AttributeError: module 'openai' has no attribute 'ChatCompletion',"### Describe the bug

The feature was working few hours ago, now it's giving error. 

### To Reproduce

```
import openai

openai.api_key = 'key'

openai.ChatCompletion.create(
  model=""gpt-3.5-turbo"",
  messages=[
        {""role"": ""system"", ""content"": ""You are a helpful assistant.""},
        {""role"": ""user"", ""content"": ""Who won the world series in 2020?""},
        {""role"": ""assistant"", ""content"": ""The Los Angeles Dodgers won the World Series in 2020.""},
        {""role"": ""user"", ""content"": ""Where was it played?""}
    ]
)
```

### Code snippets

_No response_

### OS

macOS

### Python version

Python v3.7.1

### Library version

openai v0.26.5


### no function  as ChatCompletion the defined methods

```
import openai
print(openai.__dir__())
```

['__name__', '__doc__', '__package__', '__loader__', '__spec__', '__path__', '__file__', '__cached__', '__builtins__', '__annotations__', 'os', 'Optional', 'error', 'util', 'version', 'openai_response', 'api_requestor', 'openai_object', 'api_resources', 'Answer', 'Classification', 'Completion', 'Customer', 'Edit', 'Deployment', 'Embedding', 'Engine', 'ErrorObject', 'File', 'FineTune', 'Image', 'Model', 'Moderation', 'Search', 'APIError', 'InvalidRequestError', 'OpenAIError', 'api_key', 'api_key_path', 'organization', 'api_base', 'api_type', 'api_version', 'verify_ssl_certs', 'proxy', 'app_info', 'enable_telemetry', 'ca_bundle_path', 'debug', 'log', '__all__']


",closed,2023-03-02T01:31:06Z,2023-03-03T16:25:14Z,['bug'],https://github.com/openai/openai-python/issues/246,38.9,True,input_type_violation,unknown,"The error indicates that the 'ChatCompletion' attribute is not recognized, suggesting a potential issue with the expected API structure or versioning.",high,
498,"SSE Stream parser expects additional space after colon ""data:""","### Describe the bug
In openai-python the SSE parser always check the additional space after ""data:""
https://github.com/openai/openai-python/blob/041bf5a8ec54da19aad0169671793c2078bd6173/openai/api_requestor.py#L106

But according to the SSE spec ""data: "" is identical to ""data:""
https://html.spec.whatwg.org/multipage/server-sent-events.html#server-sent-events
`The following stream fires two identical events:
data:test

data: test
This is because the space after the colon is ignored if present.`

The use case is some organizations might proxy the response for additional control but not all library put a whitespace after the colon ex. springframework https://github.com/spring-projects/spring-framework/blob/f06cf21341a35f863b327e0bfe2305111f69c468/spring-web/src/main/java/org/springframework/http/codec/ServerSentEventHttpMessageWriter.java#L146
`
if (data != null) {
sb.append(""data:"");
}
`
### To Reproduce

Proxy OpenAI API with spring framework

### Code snippets

_No response_

### OS

All

### Python version

All

### Library version

All",closed,2023-06-22T20:30:13Z,2023-09-25T00:12:25Z,['bug'],https://github.com/openai/openai-python/issues/498,2259.7,True,missing_option_violation,medium,"The SSE parser does not accommodate variations in formatting of the 'data:' field, which can lead to incorrect parsing of valid responses.",high,
447,How to extract model name from Azure deployment name ?,"### Describe the feature or improvement you're requesting

I'm using Azure OpenAI and I have more than one deployed model of the same type. I'd like to know how can I map a deployed model's name to the corresponding model's name, e.g. 'TestDavinci' ->'text-davinci-003' 

### Additional context

_No response_",closed,2023-05-11T11:08:11Z,2023-05-11T22:41:16Z,[],https://github.com/openai/openai-python/issues/447,11.55,False,,,,low,
619,"Docs describe ""suffix"" for completion models, doesn't work with gpt-3.5-turbo-instruct","### Describe the bug

https://platform.openai.com/docs/api-reference/completions/create describes:

> `suffix`, string or null, Optional
>
> Defaults to null. The suffix that comes after a completion of inserted text

But [as I found here](https://github.com/simonw/llm/issues/284#issuecomment-1724850019):

```bash
curl https://api.openai.com/v1/completions \
  -H ""Content-Type: application/json"" \
  -H ""Authorization: Bearer $OPENAI_API_KEY"" \
  -d '{
    ""model"": ""gpt-3.5-turbo-instruct"",
    ""prompt"": ""Say this is a test"",
    ""max_tokens"": 7,
    ""suffix"": ""dog"",
    ""temperature"": 0
  }'
```
Produces:
```json
{
  ""error"": {
    ""message"": ""Unrecognized request argument supplied: suffix"",
    ""type"": ""invalid_request_error"",
    ""param"": null,
    ""code"": null
  }
}
```

### To Reproduce

```
curl https://api.openai.com/v1/completions \
  -H ""Content-Type: application/json"" \
  -H ""Authorization: Bearer $OPENAI_API_KEY"" \
  -d '{
    ""model"": ""gpt-3.5-turbo-instruct"",
    ""prompt"": ""Say this is a test"",
    ""max_tokens"": 7,
    ""suffix"": ""dog"",
    ""temperature"": 0
  }'
```

### Code snippets

_No response_

### OS

All

### Python version

All

### Library version

All",closed,2023-09-19T06:00:15Z,2023-09-20T02:43:08Z,['bug'],https://github.com/openai/openai-python/issues/619,20.71,True,missing_option_violation,high,"The API documentation suggests that the 'suffix' parameter is optional for completion models, but it is not recognized by the 'gpt-3.5-turbo-instruct' model, leading to an invalid request error.",high,
230,How can I correctly count the usage of each API Key?,"### Describe the feature or improvement you're requesting

I hope to count the usage consumption of several API Keys under my OpenAI account. 
On the help page, I saw this introduction. 
![无标题](https://user-images.githubusercontent.com/16250376/221570199-b39dd1a8-7089-4535-8c9b-2b248b06dc1b.jpg)
I tried to enter this command in python: 
`import openai`
`openai.api_key = ""sk-vaU0ES******fhvr""`
`usages = openai.usage.list() `
`for usage in usages.data: `
`  print(f""Key: {usage.key}"") `
`  print(f""Total requests: {usage.total_requests}"")`
`  print(f""Total cost: {usage.total_cost}"") `

But I got the prompt 

`AttributeError: module 'openai' has no attribute 'usage'`

How can I correctly count the usage of each API Key?
",closed,2023-02-27T13:00:29Z,2023-03-03T16:31:22Z,[],https://github.com/openai/openai-python/issues/230,99.51,True,input_type_violation,unknown,"The user is attempting to access a non-existent 'usage' attribute in the OpenAI API, indicating a potential mismatch between the expected API structure and the user's implementation.",high,
379,Missing rate limit details for chatGPT turbo API,"### Describe the bug

There is no details on the TPM unit for chatgpt turbo api since the website only provided https://platform.openai.com/docs/guides/rate-limits/overview

TYPE 1 TPM EQUALS
davinci 1 token per minute
curie 25 tokens per minute
babbage 100 tokens per minute
ada 200 tokens per minute

### To Reproduce

1. Read https://platform.openai.com/docs/guides/rate-limits/overview 
2. Realize TPM is not really tokens per minute, but depends on model type
3. Realize there is no details on ChatGPT API's ""1 TPM equals ...""
4. Wonder why not show number of raw tokens per minute, and instead create ""TPM"" as a misleading abstraction?

### Code snippets

_No response_

### OS

Any

### Python version

Any

### Library version

Any",closed,2023-04-04T20:26:14Z,2023-09-07T15:42:42Z,['bug'],https://github.com/openai/openai-python/issues/379,3739.27,True,missing_option_violation,medium,Lack of clear rate limit details for the ChatGPT API leads to confusion and misinterpretation of the API's usage constraints.,high,
421,Wrong Data issue with API,"### Describe the feature or improvement you're requesting

When I tried to use the API of Openai, I got an issue that the API is providing wrong data. And also providing very limited character answers to any question.

**How can I fix all these issues?**

### Additional context

_No response_",closed,2023-04-29T14:58:30Z,2023-09-07T15:38:57Z,[],https://github.com/openai/openai-python/issues/421,3144.67,True,incorrect_functionality,High,"The API is providing wrong data, leading to incorrect system behavior.",high,
426,Error when uploading JSONL file for training,"### Describe the bug

Title: Error when uploading JSONL file for training

Description: I am trying to upload a JSONL file for training on OpenAI, but I'm encountering an error. The error message says ""Expected file to have JSONL format, where every line is a JSON dictionary. Line 1 is not a dictionary (HINT: line starts with: ""{..."").""

I created the JSONL file by converting a CSV file to JSONL using Python's csv and json libraries. I followed the instructions on OpenAI's documentation for formatting the file: each line is a JSON dictionary with the keys ""prompt"" and ""completion"".

Here is an example of the first two lines of the file:
{""prompt"": ""date: 3/11/2015, open: 1.070939064, high: 1.071455359, low: 1.056140184, close: 1.070870161, adj close: 1.070870161, sma: 1.121346676, ema: 1.114991641, rsi: 19.71425449, %K: 17.71232347, %D: 12.06409742, sma_50: 1.143293602, ema_20: 1.114991641, stoch_k: 17.71232347, stoch_d: 12.06409742"", ""completion"": ""Price Decreased""}
{""prompt"": ""date: 3/12/2015, open: 1.054941297, high: 1.068150163, low: 1.050155401, close: 1.054874539, adj close: 1.054874539, sma: 1.117530596, ema: 1.109266202, rsi: 16.57194042, %K: 5.293653385, %D: 13.38069099, sma_50: 1.140193837, ema_20: 1.109266202, stoch_k: 5.293653385, stoch_d: 13.38069099"", ""completion"": ""Price Decreased""}

### To Reproduce

Run the code below and try to upload to fine tune. 

Traceback (most recent call last):
  File ""ChatGPTPlaygrounf\train_py.py"", line 155, in <module>
    upload_training_data(training_data)
  File ""ChatGPTPlaygrounf\train_py.py"", line 132, in upload_training_data
    response = openai.File.create(
  File ""chatgpt\lib\site-packages\openai\api_resources\file.py"", line 59, in create
    response, _, api_key = requestor.request(""post"", url, files=files)
  File ""chatgpt\lib\site-packages\openai\api_requestor.py"", line 181, in request
    resp, got_stream = self._interpret_response(result, stream)
  File ""chatgpt\lib\site-packages\openai\api_requestor.py"", line 396, in _interpret_response
    self._interpret_response_line(
  File ""chatgpt\lib\site-packages\openai\api_requestor.py"", line 429, in _interpret_response_line
    raise self.handle_error_response(
openai.error.InvalidRequestError: Expected file to have JSONL format, where every line is a JSON dictionary. Line 1 is not a dictionary (HINT: line starts with: ""[{""p..."").

### Code snippets

```Python
`def create_json():
    csv_file = ""csv/eurusd_historical_data_with_indicators.csv""
    jsonl_file = 'data.jsonl'

    with open(csv_file, 'r') as f_csv, open(jsonl_file, 'w') as f_jsonl:
        reader = csv.DictReader(f_csv, delimiter=',')
        for row in reader:
            prompt = {
                ""prompt"": f""date: {row['date']}, open: {row['open']}, high: {row['high']}, low: {row['low']}, close: {row['close']}, adj close: {row['adj close']}, sma: {row['sma']}, ema: {row['ema']}, rsi: {row['rsi']}, %K: {row['%K']}, %D: {row['%D']}, sma_50: {row['sma_50']}, ema_20: {row['ema_20']}, stoch_k: {row['stoch_k']}, stoch_d: {row['stoch_d']}""
            }
            completion = {""completion"": row['price movement']}
            data = {**prompt, **completion}
            f_jsonl.write(json.dumps(data) + '\n')
`
```


### OS

Windows 11

### Python version

v3.9

### Library version

v0.27",closed,2023-05-01T14:08:47Z,2023-09-07T15:38:07Z,['bug'],https://github.com/openai/openai-python/issues/426,3097.49,True,input_type_violation,high,"The uploaded JSONL file does not conform to the expected format, where each line must be a JSON dictionary. The error indicates that the first line is not formatted correctly.",high,
445,How to set the context in jsonl file for fine tuning,"### Describe the feature or improvement you're requesting

I need to fine-tune an ```ada``` model.
I have the **instruction set** which describes the task to perform. The instruction set contains 8-10 commands which are the rules to be followed in the inference time.
Along with these commands, I have some examples to feed the system as a few shots.

So while finetuning I can put the few shot examples in the JSONL file in the following format
```
{""prompt"":<example 1>"", ""completion"":<example1 response>}
{""prompt"":<example 2>"", ""completion"":<example2 response>}
```

But where should i keep the **instruction set/rules to follow/command set/prompt** while finetuning?
Is it possible to keep this also in the JSONL file?

 

### Additional context

_No response_",closed,2023-05-10T06:28:46Z,2023-09-07T15:36:58Z,[],https://github.com/openai/openai-python/issues/445,2889.14,False,,,,low,
448,Retrieve token limit and cost of a model from API,"### Describe the feature or improvement you're requesting

It should be possible to retrieve the token limit and cost per 1k tokens programmatically from the API instead of needing to look them up on the website and hardcode them into programs.

https://github.com/openai/tiktoken/issues/109

### Additional context

_No response_",closed,2023-05-13T01:40:53Z,2023-09-07T15:34:33Z,['API-feedback'],https://github.com/openai/openai-python/issues/448,2821.89,True,missing_dependency_violation,medium,"The request to retrieve token limit and cost programmatically indicates a missing dependency on the API's ability to provide this information, which is currently not available.",high,
453,Regarding continue generation using API,"### Describe the bug

I have been using OpenAI API key for long time. When I give some question to the model, call the API and it'll return the answer, sometimes it won't return the complete answer due to token issues.

Recently, I saw that ChatGPT has come up with new feature, continue generation, where if i click on that, it is continuing the generation in the same block. Below is the picture,

![image](https://github.com/openai/openai-python/assets/129744879/0af55588-2ad3-4a9b-9825-97fbefa7eea3)

Is there any way to do that for the API code as well?


### To Reproduce

1. Completion of the answer
2. Limitation of tokens 

### Code snippets

```Python
import openai

messages = [
    {""role"": ""system"", ""content"": ""You are a kind helpful assistant.""},
]

while True:
    message = input(""User : "")
    if message:
        messages.append(
            {""role"": ""user"", ""content"": message},
        )
        chat = openai.ChatCompletion.create(
            model=""gpt-3.5-turbo"", messages=messages
        )
    
    reply = chat.choices[0].message.content
    print(f""ChatGPT: {reply}"")
    messages.append({""role"": ""assistant"", ""content"": reply})
```


### OS

WindowsOS

### Python version

Python 3.8.5

### Library version

latest version",closed,2023-05-18T06:19:58Z,2023-09-07T15:31:27Z,['API-feedback'],https://github.com/openai/openai-python/issues/453,2697.19,True,missing_dependency_violation,medium,"The user is attempting to continue generation without ensuring that the necessary context or token limits are managed, which may lead to incomplete responses.",high,
461,Finetuning keeps failing over and over after a minute,,closed,2023-05-23T20:53:01Z,2023-09-07T15:29:18Z,['bug'],https://github.com/openai/openai-python/issues/461,2562.6,True,performance_degradation,Medium,"The finetuning process is failing repeatedly, indicating potential performance issues that impact the training process.",high,
515,Audio: ASR automatically infers the wrong language. ,"### Describe the bug

`openai.Audio.transcribe(model= 'whisper-1',file = wavfile, options = {""language"" : ""en"",})`

openai whisper automatically transcribes and translates input voice to a different language. Specifying the language parameter does nothing.

Nor does adding a prefix of [en] solve the problem. It appears that accents and environmental noise is capable of influencing the language inference model. 

From the `openai.Audio.transcribe()` function, accepted parameters are `'model'`, `'file'`, `'apikey'`, `'apibase'`, `'apitype'`, `'apiversion'`, `'organization'`, and `'kwargs'`. There is no parameter to set the language.

Would be nice for a parameter to force the ASR to a language if the auto-inference model is not reliable. 

### To Reproduce

```
tmp.write(audio.get_wav_data())
tmp.seek(0)  # Reset file pointer to beginning
whisp  = openai.Audio.transcribe(model= 'whisper-1',file =tmp, options = {""language"" : ""en"",})
user_input = ""[en] "" + whisp['text']

print(f'\nYou said:\n""{user_input}""')

messages.append(       {""role"": ""user"", ""content"": user_input}     )

print(""\nResponding...\n"")
      
completion = openai.ChatCompletion.create(
        model=MODEL,
        messages=messages,
        temperature=0.8
      )
created  = str( get_timestamp_from_unix( completion['created'] ) )
response = completion['choices'][0].message.content
messages.append({""role"": ""assistant"", ""content"": f'Created: {created}\n{response}'}) 
print(f""\n{created}\n{response}\n"")
```

```
You said:
""[en] Jadi, perkara ini masih tidak berfungsi. Saya telah menambahkan sufis bahasa Inggeris dan untuk sebab tertentu, ia masih memberi balik transkripsi yang diterjemahkan oleh Malaysia. Jadi, saya yakin ada masalah dengan masalah transkripsi dari 
OpenAI Whisper.""

Responding...


2023-07-03 15:31:47
Created: 2023-07-03 15:31:42
Maaf atas ketidaknyamanannya. Terkadang, ada kesalahan dalam transkripsi bahasa yang dapat menyebabkan masalah seperti yang 
Anda alami. Saya akan mencatat masalah ini dan melaporkannya kepada tim teknis untuk diperiksa. Apakah ada pertanyaan lain yang dapat saya bantu jawab?
```

### Code snippets

_No response_

### OS

Windows

### Python version

Python 3.8.3

### Library version

openai==0.27.7",closed,2023-07-03T08:17:19Z,2023-09-07T14:36:18Z,['API-feedback'],https://github.com/openai/openai-python/issues/515,1590.32,True,missing_option_violation,medium,"The API does not provide a parameter to explicitly set the language for transcription, leading to incorrect language inference.",high,
564,Return value of ChatGPT API,"### Describe the feature or improvement you're requesting

I think that it would be nice for the API for any model before GPT-4, returns any type of value or a specific answer when the model is unable to answer the question (like for example; about events after 2021, and some other stuff it doesn’t know) because in this way devs could have more control about how to handle this cases, like using different resources for the wanted information.

### Additional context

_No response_",closed,2023-08-08T15:06:24Z,2023-09-06T11:57:52Z,['API-feedback'],https://github.com/openai/openai-python/issues/564,692.86,True,missing_option_violation,medium,"The request suggests a change in the API's response behavior that is not currently supported, indicating a potential violation of the expected response contract.",high,
589,How do you set the number of epochs for FineTune.create on ChatGPT?,"### Describe the bug

I can fine-tune `gpt-3.5-turbo` just fine, but it looks like there's no way to change the number of epochs to something over than 10, even though the Guides recommend indicate that you can (`""If the model becomes less diverse than expected decrease the number by 1 or 2 epochs""`). It seems to be over training quite a bit. Past experience is that most LLMs just need 1-2 epochs. 

There doesn't seem to be an actual argument that can be passed to `FineTune.create` to change the number of training epochs. I've fine-tuned `davinic-003` before with the parameter `n_epochs`, but it doesn't seem to work with ChatGPT fine-tuning. There's no reference in `help(FineTune.create)` to epochs and passing `epochs`,  `n_epochs` to `FineTune.create` yields `extra fields not permitted`.

Running it through `openai api fine_tunes.create` also doesn't permit specification of epochs. Am I missing something?

### To Reproduce

1. Format and upload a dataset
2. Create a training job using the Python SDK with `FineTuningJob.create` passing an argument to change the number of epochs

### Code snippets

_No response_

### OS

Ubuntu 20.04

### Python version

Python 3.7.5

### Library version

openai-python 0.27.9",closed,2023-08-25T16:58:05Z,2023-09-06T02:57:12Z,['bug'],https://github.com/openai/openai-python/issues/589,273.99,True,missing_option_violation,medium,"The API does not allow the specification of the number of epochs for fine-tuning, despite documentation suggesting it should be possible.",high,
471,Using openai.proxy on Anaconda Jupyter Notebook,"### Describe the bug

import openai
openai.api_key = ""XXXX""
proxies = {
'http': ""http://xxxx:xxxx/"",
'https': ""https://xxxx:xxxx/""}
openai.proxy = proxies


I see this error : 
APIConnectionError: Error communicating with OpenAI: HTTPSConnectionPool(host='api.openai.com', port=443): Max retries exceeded with url: /v1/engines (Caused by ProtocolError('Connection aborted.', ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None)))

### To Reproduce

proxies = {
'http': ""http://xxxx:xxxx/"",
'https': ""https://xxxx:xxxx/""}
openai.proxy = proxies

### Code snippets

_No response_

### OS

Windows

### Python version

Python 3.10.9

### Library version

openai 0.27.7",closed,2023-05-31T02:44:10Z,2023-09-06T15:14:49Z,['bug'],https://github.com/openai/openai-python/issues/471,2364.51,False,,,,low,
472,Issue with OpenAI API: Incompatibility with Specific Word,"### Describe the bug

We are using OpenAI api for our application. It’s not giving response (which is a summary of particular text) when text includes “Ideation” word, if we remove Ideation word or makes it lower case (ideation) its giving us response. We are unable to find exact cause of this issue. Can you please help us for the same?
We are using TextDavinci003 and Gpt3.5-Turbo models for generating response.

### To Reproduce

We are expecting summary with ""Ideation"" word included in that

### Code snippets

```Python
def gen_summary(input_text, model, prompt):
    configure_openai(model)
    text_size = len(input_text.split())
    if text_size <=0:
       input_text = prompt
       
       try: 
            if model != ""Turbo35-0301"":

                response = openai.Completion.create(
                    engine=model,
                    prompt=f"" {prompt} {input_text}"",
                    temperature=0,
                    max_tokens=2000,
                    top_p=1,
                    frequency_penalty=0,
                    presence_penalty=0,
                   
                )
                summary = response[""choices""][0][""text""]
                chargable_tokens= response[""usage""][""prompt_tokens""]
                return summary.replace(""\n"","""").strip(),chargable_tokens
            else:
    
                response = openai.ChatCompletion.create(
                    engine=model,
                    messages=[{""role"": ""user"", ""content"": f""{input_text}\n\n {prompt} ""}],
                    temperature=0,
                    max_tokens=2000,
                    top_p=1,
                    frequency_penalty=0,
                    presence_penalty=0,
                  
                )
                summary = response[""choices""][0][""message""][""content""]
                chargable_tokens= response[""usage""][""prompt_tokens""]
                print(summary)
                return summary,chargable_tokens
       except Exception as e:
            return e 
    else:   
     try: 
            if model != ""Turbo35-0301"":
   
                response = openai.Completion.create(
                    engine=model,
                    prompt=f""{input_text} {prompt} "",
                    temperature=0,
                    max_tokens=2000,
                    top_p=1,
                    frequency_penalty=0,
                    presence_penalty=0,
                  
                )
                summary = response[""choices""][0][""text""]
                chargable_tokens= response[""usage""][""prompt_tokens""]
                return summary.replace(""\n"","""").strip(),chargable_tokens 
           
            else:
                  
                response = openai.ChatCompletion.create(
                    engine=model,
                    messages=[{""role"": ""user"", ""content"": f"" {input_text}\n\n {prompt}""}],
                    temperature=0,
                    max_tokens=2000,
                    top_p=1,
                    frequency_penalty=0,
                    presence_penalty=0,
                   
                )
                summary = response[""choices""][0][""message""][""content""]
                print(summary)
                chargable_tokens= response[""usage""][""prompt_tokens""]
                return summary,chargable_tokens

     except Exception as e:
            return e
```


### OS

Windows

### Python version

Python 3.10

### Library version

Open ai 0.27.2",closed,2023-05-31T06:48:36Z,2023-09-06T15:13:05Z,['bug'],https://github.com/openai/openai-python/issues/472,2360.41,True,input_value_violation,performance_degradation,"The API does not respond correctly when the input contains the word 'Ideation', indicating potential issues with input handling or processing.",high,
473,Issue with OpenAI API: Error when including the Particular word “Ideation”,"### Describe the bug

I am using OpenAI api for our application. I have noticed that when I include the word “Ideation” (with a capital letter ‘I’) in my input text, the API fails to generate a response. However, when the word is in lowercase (‘ideation’), the API giving response as expected.

Input Text: When I send a prompt containing the word “Ïdeation” (capital ‘I’) to the API, the response generation process fails, and I receive no output. It appears that the API encounters an error or gets stuck.
Lowercase Text: On the other hand, if I use the lowercase form of the word (‘ideation’), the API works as expected, producing the desired response.
We are unable to find exact cause of this issue. Can you please help us for the same?
We are using TextDavinci003 and Gpt3.5-Turbo models for generating response.

### To Reproduce

I am expecting summary when input text contain word Ideation''

### Code snippets

```Python
def gen_summary(input_text, model, prompt):
    configure_openai(model)
    text_size = len(input_text.split())
    if text_size <=0:
       input_text = prompt
       
       try: 
            if model != ""Turbo35-0301"":

                response = openai.Completion.create(
                    engine=model,
                    prompt=f"" {prompt} {input_text}"",
                    temperature=0,
                    max_tokens=2000,
                    top_p=1,
                    frequency_penalty=0,
                    presence_penalty=0,
                   
                )
                summary = response[""choices""][0][""text""]
                chargable_tokens= response[""usage""][""prompt_tokens""]
                return summary.replace(""\n"","""").strip(),chargable_tokens
            else:
    
                response = openai.ChatCompletion.create(
                    engine=model,
                    messages=[{""role"": ""user"", ""content"": f""{input_text}\n\n {prompt} ""}],
                    temperature=0,
                    max_tokens=2000,
                    top_p=1,
                    frequency_penalty=0,
                    presence_penalty=0,
                  
                )
                summary = response[""choices""][0][""message""][""content""]
                chargable_tokens= response[""usage""][""prompt_tokens""]
                print(summary)
                return summary,chargable_tokens
       except Exception as e:
            return e 
    else:   
     try: 
            if model != ""Turbo35-0301"":
   
                response = openai.Completion.create(
                    engine=model,
                    prompt=f""{input_text} {prompt} "",
                    temperature=0,
                    max_tokens=2000,
                    top_p=1,
                    frequency_penalty=0,
                    presence_penalty=0,
                  
                )
                summary = response[""choices""][0][""text""]
                chargable_tokens= response[""usage""][""prompt_tokens""]
                return summary.replace(""\n"","""").strip(),chargable_tokens 
           
            else:
                  
                response = openai.ChatCompletion.create(
                    engine=model,
                    messages=[{""role"": ""user"", ""content"": f"" {input_text}\n\n {prompt}""}],
                    temperature=0,
                    max_tokens=2000,
                    top_p=1,
                    frequency_penalty=0,
                    presence_penalty=0,
                   
                )
                summary = response[""choices""][0][""message""][""content""]
                print(summary)
                chargable_tokens= response[""usage""][""prompt_tokens""]
                return summary,chargable_tokens

     except Exception as e:
            return e
```


### OS

Windows10

### Python version

Python 3.9.11

### Library version

Opena ai==0.27.2",closed,2023-05-31T07:19:30Z,2023-09-06T15:12:36Z,['bug'],https://github.com/openai/openai-python/issues/473,2359.89,True,input_value_violation,performance_degradation,"The API fails to generate a response when the input contains the capitalized word 'Ideation', indicating a potential issue with input value handling.",high,
474,AuthenticationError: <empty message>,"### Describe the bug

I am using ChromaDB, I have a collection with the embeddings obtained with the text-embedding-ada-002 model and I want to query it. I already changed the OpenAI Key several times and it does not work. 

This is my code (see the image)


### To Reproduce

![image](https://github.com/openai/openai-python/assets/82721764/62851856-1c88-475d-886b-805a5ef867e4)


### Code snippets

_No response_

### OS

Ubuntu

### Python version

Python v3.10

### Library version

openai-python v0.27.7",closed,2023-05-31T22:32:13Z,2023-09-06T15:10:38Z,['bug'],https://github.com/openai/openai-python/issues/474,2344.64,True,missing_dependency_violation,high,"The issue indicates an AuthenticationError, suggesting a missing or invalid authentication context that is required for querying the ChromaDB collection.",high,
476,custom prompt answer from model is different than when same question passed directly to model. how?,"### Describe the bug

custom prompt answer from model is different than when same question passed directly to model. how?

### To Reproduce

..

### Code snippets

_No response_

### OS

windows

### Python version

3.9

### Library version

0.27.7",closed,2023-06-02T13:19:25Z,2023-09-06T15:09:04Z,['bug'],https://github.com/openai/openai-python/issues/476,2305.83,True,missing_dependency_violation,high,"The issue indicates a lack of necessary context or metadata when using a custom prompt, which may lead to inconsistent responses from the model.",high,
482,How can I create a whl file to be able to use this library directly in snowflake?,"### Describe the feature or improvement you're requesting

I tried creating a whl file out of the base folder extracted from the zip but I am unable to upload to snowflake as it says it's not a valid binary. Should I make the whl from a specific folder?

### Additional context

_No response_",closed,2023-06-09T08:01:37Z,2023-09-06T15:06:15Z,[],https://github.com/openai/openai-python/issues/482,2143.08,True,input_value_violation,medium,"The user is attempting to create a whl file from an incorrect folder structure, which may lead to invalid binary output for Snowflake.",high,
483,ChatCompletion ignoring model parameter?,"### Describe the bug

Hello! I am attempting to use this library to query the `gpt-3.5-turbo` model but when examining the query response, it is returning `gpt-3.5-turbo-0301`. I am using the command from the API, which defines `gpt-3.5-turbo` as the model. Is there a bug here, or is there some reason that it is using the `-0301` model that I am unaware of? Thank you!


### To Reproduce

I made the following request, copied from the README for this project.

```
chat_completion = openai.ChatCompletion.create(model=""gpt-3.5-turbo"", messages=[{""role"": ""user"", ""content"": ""Hello world""}])
print(chat_completion)
```

And received the following response:

```
{
  ""id"": ""chatcmpl-7PUqCAWmRjabhgZDrbtgyOOcA8ZNq"",
  ""object"": ""chat.completion"",
  ""created"": 1686311108,
  ""model"": ""gpt-3.5-turbo-0301"",
  ""usage"": {
    ""prompt_tokens"": 10,
    ""completion_tokens"": 68,
    ""total_tokens"": 78
  },
  ""choices"": [...]
   }
```


### Code snippets

_No response_

### OS

Docker - python:3 base image

### Python version

3.11.4

### Library version

openai 0.27.8",closed,2023-06-09T11:50:02Z,2023-09-06T15:05:35Z,['bug'],https://github.com/openai/openai-python/issues/483,2139.26,False,,,,medium,
484,Different accounts at same time?,"### Describe the feature or improvement you're requesting

How might I use different accounts/api keys at the same time? For example if I have a company account that I want to use budget for embedding model, and personal account that I want to use budget for chat model?

Is there a way to create two client objects and pass one to each model class?",closed,2023-06-11T18:06:04Z,2023-09-06T14:51:47Z,[],https://github.com/openai/openai-python/issues/484,2084.76,False,,,,low,
485,"Different token limit error than OpenAI while using Azure OpenAI Service with gpt-3.5-turbo,  api version 2023-03-15-preview","### Describe the bug

When I send a request for gpt-3.5-turbo chat completion which is 8218(8138 in messages and 80 in completion) tokens long, I get the error message as InvalidRequestError: This model's maximum context length is 8192 tokens. However, you requested 8218 tokens (8138 in the messages, 80 in the completion). Please reduce the length of the messages or completion. Which is incorrect as the gpt-3.5-turbo model's max token limit is 4096 and the error that used to be present while using OpenAI platform. Also when I try to send a 8191 token long chat completion message I get Azure Content Filtering error.

### To Reproduce

1. Create an Azure Deployment.
2. Write the openai base code to send a request to Azure OpenAI with the deployment name.
3. Send a 8218 tokens long chat completion request.

### Code snippets

_No response_

### OS

macOS

### Python version

Python v3.10

### Library version

openai-python-0.27.7",closed,2023-06-13T03:56:29Z,2023-09-06T14:50:33Z,['Azure'],https://github.com/openai/openai-python/issues/485,2050.9,True,input_value_violation,High,"The request exceeds the maximum token limit of the gpt-3.5-turbo model, leading to an InvalidRequestError.",high,
493,TimeOut should be better documented in Docs and in API call,"### Describe the feature or improvement you're requesting

The API call has the timeout argument. It is not documented and not mentioned in the docstring.
```python
    def create(cls, *args, **kwargs):
        """"""
        Creates a new chat completion for the provided messages and parameters.

        See https://platform.openai.com/docs/api-reference/chat-completions/create
        for a list of valid parameters.
        """"""
        start = time.time()
        timeout = kwargs.pop(""timeout"", None)
```
It should be added in the docstring and in the Docs on the API reference.

### Additional context

_No response_",closed,2023-06-19T06:49:14Z,2023-09-06T14:49:35Z,[],https://github.com/openai/openai-python/issues/493,1904.01,True,missing_option_violation,medium,"The timeout argument is not documented in the API call or the docstring, leading to potential misuse or suboptimal behavior.",high,
490,ChatML Update,"### Describe the feature or improvement you're requesting

I was wondering if we could get an update to the ChatML document about the new function capabilities. I've been writing some code to parse ChatML into messages, and I'd love to be able to add the assistant's function calls and the new function role.

I've been doing some prompting and it seems that function calls are done through something like:
```
<|im_start|>assistant to=functions.[the function name]<|im_sep|>[the JSON parameters]<|im_end|>
```
But I'm not sure if this is hallucination on the model's part...

And is the function role simply `<|im_start|>function name=[the function name]`?

Thanks for the help! Would really appreciate hearing from the team.

### Additional context

_No response_",closed,2023-06-16T20:57:03Z,2023-09-06T13:58:39Z,[],https://github.com/openai/openai-python/issues/490,1961.03,True,missing_option_violation,medium,"The user is unsure about the correct format for function calls and roles in ChatML, indicating a potential lack of clarity in the API's documentation regarding acceptable input formats.",high,
522,There is an error in the README,"### Describe the bug

![image](https://github.com/openai/openai-python/assets/64049774/b0a08fee-0aa2-4477-9796-b1b03bbdc056)
This error is in the final output and should read print(chat_completion.choices[0].message.content) instead of print(completion.choices[0].message.content)

### To Reproduce

![image](https://github.com/openai/openai-python/assets/64049774/d67652f1-5dec-428f-a32b-b76c8451bc5a)


### Code snippets

_No response_

### OS

macos

### Python version

3.9

### Library version

v26.4",closed,2023-07-09T14:10:13Z,2023-09-06T13:56:07Z,['bug'],https://github.com/openai/openai-python/issues/522,1415.77,True,missing_option_violation,incorrect_functionality,"The README contains an incorrect code snippet that may lead users to use the wrong method, resulting in unexpected behavior.",high,
470,Unsupported error message in using SDK nad playground,"### Describe the bug

Code example using readme:

```python
import openai
openai.api_key = ""my key""

# list models
models = openai.Model.list()

# print the first model's id
print(models.data[0].id)

# create a chat completion
chat_completion = openai.ChatCompletion.create(model=""gpt-3.5-turbo"", messages=[{""role"": ""user"", ""content"": ""Hello world""}])

# print the chat completion
print(chat_completion.choices[0].message.content)
```

The following is the error message

```
whisper-1
Traceback (most recent call last):
  File ""c:\Users\admin\Desktop\test-code\test.py"", line 25, in <module>
    chat_completion = openai.ChatCompletion.create(model=""gpt-3.5-turbo"", messages=[{""role"": ""user"", ""content"": ""Hello world""}])
  File ""C:\Python310\lib\site-packages\openai\api_resources\chat_completion.py"", line 25, in create
    return super().create(*args, **kwargs)
  File ""C:\Python310\lib\site-packages\openai\api_resources\abstract\engine_api_resource.py"", line 153, in create
    response, _, api_key = requestor.request(
  File ""C:\Python310\lib\site-packages\openai\api_requestor.py"", line 230, in request
    resp, got_stream = self._interpret_response(result, stream)
  File ""C:\Python310\lib\site-packages\openai\api_requestor.py"", line 624, in _interpret_response
    self._interpret_response_line(
  File ""C:\Python310\lib\site-packages\openai\api_requestor.py"", line 687, in _interpret_response_line
    raise self.handle_error_response(
openai.error.InvalidRequestError: This is not a chat model and thus not supported in the v1/chat/completions endpoint. Did you mean to use v1/completions?
```



### To Reproduce

I thought it was a problem with my SDK, so I tried [playground](https://platform.openai.com/playground?mode=chat&model=gpt-3.5-turbo)and it was also an error.

![image](https://github.com/openai/openai-python/assets/31961133/511f6dc3-dc9e-4db7-96ab-1a00a8ef6e7e)



### Code snippets

_No response_

### OS

win

### Python version

3.10.4

### Library version

0.27.7",closed,2023-05-30T16:22:39Z,2023-06-12T08:37:08Z,['bug'],https://github.com/openai/openai-python/issues/470,304.24,False,,,,low,
533,Add option to use client certificates.,"We want to secure our Azure OpenAI access with client certificates. To do this we need the option to add one to ""requests"".

I do not see an option to do this. If waned I can provide a PR.",closed,2023-07-11T14:13:02Z,2023-09-06T12:28:23Z,['Azure'],https://github.com/openai/openai-python/issues/533,1366.26,True,missing_option_violation,medium,"The API does not provide an option to use client certificates for securing requests, which is a recommended feature for enhanced security.",high,
534,web app block,"### Describe the bug

I use the Openai package in the Flask program, but the app sometimes blocks, and nothing output in console.

After KeyboardInterrupt:

```bash
    return super().create(*args, **kwargs)
  File ""/data/python_group/py38/lib/python3.8/site-packages/openai/api_resources/abstract/engine_api_resource.py"", line 153, in create
    response, _, api_key = requestor.request(
  File ""/data/python_group/py38/lib/python3.8/site-packages/openai/api_requestor.py"", line 288, in request
    result = self.request_raw(
  File ""/data/python_group/py38/lib/python3.8/site-packages/openai/api_requestor.py"", line 596, in request_raw
    result = _thread_context.session.request(
  File ""/data/python_group/py38/lib/python3.8/site-packages/requests/sessions.py"", line 587, in request
    resp = self.send(prep, **send_kwargs)
  File ""/data/python_group/py38/lib/python3.8/site-packages/requests/sessions.py"", line 701, in send
    r = adapter.send(request, **kwargs)
  File ""/data/python_group/py38/lib/python3.8/site-packages/requests/adapters.py"", line 489, in send
    resp = conn.urlopen(
  File ""/data/python_group/py38/lib/python3.8/site-packages/urllib3/connectionpool.py"", line 703, in urlopen
    httplib_response = self._make_request(
  File ""/data/python_group/py38/lib/python3.8/site-packages/urllib3/connectionpool.py"", line 449, in _make_request
    six.raise_from(e, None)
  File ""<string>"", line 3, in raise_from
  File ""/data/python_group/py38/lib/python3.8/site-packages/urllib3/connectionpool.py"", line 444, in _make_request
    httplib_response = conn.getresponse()
  File ""/usr/local/python3/lib/python3.8/http/client.py"", line 1322, in getresponse
    response.begin()
  File ""/usr/local/python3/lib/python3.8/http/client.py"", line 303, in begin
    version, status, reason = self._read_status()
  File ""/usr/local/python3/lib/python3.8/http/client.py"", line 264, in _read_status
    line = str(self.fp.readline(_MAXLINE + 1), ""iso-8859-1"")
  File ""/usr/local/python3/lib/python3.8/socket.py"", line 669, in readinto
    return self._sock.recv_into(b)
  File ""/usr/local/python3/lib/python3.8/ssl.py"", line 1241, in recv_into
    return self.read(nbytes, buffer)
  File ""/usr/local/python3/lib/python3.8/ssl.py"", line 1099, in read
    return self._sslobj.read(len, buffer)
KeyboardInterrupt
```


### To Reproduce

here is my flask code:

```python
# -*- coding:utf-8 _*-

import logging
import time
from logging import handlers

import tiktoken
from flask import Flask, render_template, request, jsonify
import openai
from gevent.pywsgi import WSGIServer

# Setup logging
logger = logging.getLogger('MainProgram')
logger.setLevel(10)
logHandler = handlers.RotatingFileHandler('flask.log', maxBytes=10000000, backupCount=10)
logger.addHandler(logHandler)
logger.info(f'{time.strftime(""%Y-%m-%d %H:%M:%S"")} Logging configuration done')
app = Flask(__name__)
openai.api_key = ''
openai.api_base = ''
openai.api_type = 'azure'
openai.api_version = ''
deployment_name = ''
encoding = tiktoken.get_encoding(""cl100k_base"")
max_len = 8192

@app.route('/')
def index():
    return render_template('home.html')


@app.route('/chat', methods=['POST'])
def handle_message():
    data = request.get_json()
    message = data['message']
    messages = [{'role': 'system',
                 'content': f'You are ChatGPT, a large language model trained by OpenAI.\nKnowledge cutoff: 2021-09\n'
                            f'Current date:  {time.strftime(""%Y-%m-%d"", time.localtime())}'},
                *message]
    input_len = sum(len(encoding.encode(msg['content'])) for msg in messages)
    if max_len - input_len < 0:
        raise Exception('input too long')
    response = openai.ChatCompletion.create(
        engine=deployment_name,
        # model=""gpt-3.5-turbo"",
        messages=messages,
        temperature=1,
        max_tokens=max_len - input_len
    )

    result = ''
    for choice in response.choices:
        result += choice.message.content
    return jsonify({'result': result})


if __name__ == '__main__':
    http_server = WSGIServer(('0.0.0.0', 8095), app, log=logger)
    http_server.serve_forever()

```

### Code snippets

_No response_

### OS

centos and windows10

### Python version

pythonv3.8.0

### Library version

0.27.8",closed,2023-07-12T02:29:24Z,2023-09-06T12:27:49Z,['bug'],https://github.com/openai/openai-python/issues/534,1353.97,True,hang,Medium,"The web app occasionally becomes unresponsive, requiring a KeyboardInterrupt to regain control.",high,
539,"A way to determine if a response is ""not unknown""","### Describe the feature or improvement you're requesting

Right now, there doesn't seem to be a way of determining if response from chatgpt is vague / not known without checking the response content. There should be some response code that API devs can check to see if there's a way to escalate the situation to a human agent.

https://stackoverflow.com/questions/76685545/chatgpt-a-way-to-determine-if-a-response-is-not-unknown


### Additional context

_No response_",closed,2023-07-14T07:25:06Z,2023-09-06T12:17:59Z,['API-feedback'],https://github.com/openai/openai-python/issues/539,1300.88,True,missing_option_violation,medium,"The request suggests a missing feature for response validation, indicating that the API does not provide a clear mechanism to determine if a response is vague or unknown, which could lead to suboptimal handling of such responses.",high,
544,Continue generating on stop_reason: length,"### Describe the feature or improvement you're requesting

Is there a way to continue generating without backfeeding the entire message everytime?

### Additional context

Atm my code is doing this which I feel is very wasteful

```
import sys
import json
import openai
from dotenv import load_dotenv
import os

def recursive_chat(messages):
    chat_completion = openai.ChatCompletion.create(
        model=""gpt-3.5-turbo"",
        messages=messages,
        max_tokens=140
    )
    response_message = chat_completion['choices'][0]['message']
    print(response_message)
    messages[0]['content'] += ' ' + response_message['content']  # append the assistant's message to the user's message

    if chat_completion['choices'][0]['finish_reason'] == 'stop':
        return messages
    else:
        return recursive_chat(messages)

# load .env file
load_dotenv()
print(sys.argv)
prompt = sys.argv[1] if len(sys.argv) > 1 else ""2+2""
key = sys.argv[2] if len(sys.argv) > 2 else os.getenv('OPENAI_API_KEY')
# Load your API key from an environment variable or secret management service
openai.api_key = key

messages = [{""role"": ""user"", ""content"": ""how could openai fail?""}]

final_messages = recursive_chat(messages)
print(json.dumps(final_messages))


```",closed,2023-07-19T07:03:21Z,2023-09-06T12:16:13Z,[],https://github.com/openai/openai-python/issues/544,1181.21,True,method_order_violation,medium,"The recursive call to continue generating responses does not ensure that the context is properly updated before the next API call, which may lead to degraded output quality.",high,
545,Split out the core library,"### Describe the feature or improvement you're requesting

Would it be possible to split out the core functionality, essentially, the API wrapper, in a separate `openai-core` library with minimal dependencies? The advantage is precisely that - fewer dependencies and a smaller codebase for 99.9+% of the users.

If the other stuff, like the cli and the fine-tuning code, stays in `openai`, the users will not requires any changes.

### Additional context

_No response_",closed,2023-07-21T11:13:34Z,2023-09-06T12:14:51Z,[],https://github.com/openai/openai-python/issues/545,1129.02,False,,,,,
549,timeout paramter is not respected in openai.ChatCompletion.create method,"### Describe the bug

`openai.ChatCompletion.create` takes `timeout` parameter but the call might take significantly longer without timing out.

I think it is because of this code:

```python
timeout = kwargs.pop(""timeout"", None)

while True:
    try:
        return super().create(*args, **kwargs)
```

Wouldn't it be more consistent overall if you replace `pop` with `get`, like this:

```python
timeout = kwargs.get(""timeout"", None)
```

### To Reproduce

run
openai.ChatCompletion.create(timeout=1, the rest of parameters...)

expected the call either complete under 1 second or fail.

actual: it can take more than 1 second.

### Code snippets

_No response_

### OS

Windows 11

### Python version

Python 3.9.13

### Library version

openai 0.27.8",closed,2023-07-26T22:33:46Z,2023-09-06T12:13:03Z,[],https://github.com/openai/openai-python/issues/549,997.65,True,input_value_violation,performance_degradation,"The timeout parameter is not being respected, leading to longer execution times than expected.",high,
554,ChatCompletion doesn't handle enum with None,"### Describe the bug

if for example the enum for the function looks like this:
`enums = ['Life cycle', 'App developer', 'Games reporting', 'User', None]`
which is later used in the functions call:

```
...
""parameters"": {
                ""type"": ""object"",
                ""properties"": {
                    ""parent"": {
                        ""type"": ""string"",
                        ""enum"": enums,
                    },
...
```

the case with None is not handled and server responds with the **500**

Logs:
```
""outputs"": [
    {
     ""ename"": ""APIError"",
     ""evalue"": ""The server had an error processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if you keep seeing this error. (Please include the request ID cdee293f61742a79122d04d81a0b9465 in your email.) {\n  \""error\"": {\n    \""message\"": \""The server had an error processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if you keep seeing this error. (Please include the **request ID cdee293f61742a79122d04d81a0b9465** in your email.)\"",\n    \""type\"": \""server_error\"",\n    \""param\"": null,\n    \""code\"": null\n  }\n}\n 500 {'error': {'message': 'The server had an error processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if you keep seeing this error. (Please include the request ID cdee293f61742a79122d04d81a0b9465 in your email.)',
...
```

**request ID cdee293f61742a79122d04d81a0b9465**


### To Reproduce

1. create an enum with None as a variable
2.  trigger a function call

### Code snippets

```Python
functions = [
        {
            ""name"": ""get_relevant_action"",
            ""description"": """",
            ""parameters"": {
                ""type"": ""object"",
                ""properties"": {
                    ""parent"": {
                        ""type"": ""string"",
                        ""enum"": ['Life cycle', 'App developer', 'Games reporting', 'User', None],
                        ""description"": ""parent text of the button to click"",
                    }
                },
                ""required"": [""parent""],
            },
        }
    ]

response = openai.ChatCompletion.create(
    model=""gpt-4"",
    messages=messages,
    functions=functions,
    function_call={""name"": ""get_relevant_action""},
)
response_message = response[""choices""][0][""message""]
```


### OS

macos

### Python version

3.10

### Library version

v0.27.8",closed,2023-07-29T16:34:40Z,2023-09-06T12:09:35Z,['API-feedback'],https://github.com/openai/openai-python/issues/554,931.58,True,input_value_violation,high,"The API does not handle the case where 'None' is included in the enum, leading to a server error (500) when this value is used.",high,
558,GPT-4-0613 Finish_Reason Null Issue in Streaming Mode,"### Describe the bug

I am facing an issue while using the latest OpenAI GPT-4-0613 models in streaming mode. Specifically, I have noticed that for some cases, the finish_reason field in the response comes as Null instead of the expected value 'stop' for the last token. I have provided the necessary context along with my question in the prompt. Similar Question is asked in Openai Forum: https://community.openai.com/t/completion-finish-reason-is-missing-when-stream-true/90526

My understanding is that the finish_reason field should indicate 'stop' for the last token when using the streaming mode. However, in certain cases, it appears to be Null. This behavior seems inconsistent with the expected behavior of the model.

**Notice**: However, gpt-0314 always gives finish_reason as stop for last token

### To Reproduce

Take a prompt which has Context and Question.  ( Make a prompt size of 5200 approx)

### Code snippets

```Python
response = openai.ChatCompletion.create(
    model='gpt-4-0613',
    messages=[
        {'role': 'user', 'content': ""Context along with question""}
    ],
    temperature=0,
    stream=True 
)

for chunk in response:
    print(chunk)
```


### OS

macOS

### Python version

Python3.9

### Library version

openai from azure",closed,2023-08-03T08:08:15Z,2023-09-06T12:02:59Z,['bug'],https://github.com/openai/openai-python/issues/558,819.91,True,missing_dependency_violation,high,"The finish_reason field is expected to indicate 'stop' for the last token in streaming mode, but it returns Null, indicating a missing dependency between the prompt context and the expected output.",high,
567,Returned random texts from unknown source when using model davinci,"### Describe the feature or improvement you're requesting

When using 'davinci' model, regardless of the input prompt it always responds with random text/conversation/clips from the internet.

### Additional context

_No response_",closed,2023-08-09T23:52:27Z,2023-09-06T11:56:25Z,[],https://github.com/openai/openai-python/issues/567,660.07,True,incorrect_functionality,High,"The model consistently returns random text instead of relevant responses based on the input prompt, indicating a failure in expected functionality.",high,
596,Fine-tuning documentation is incorrect,"### Describe the bug

In [fine-tuning documentation](https://github.com/openai/openai-python#fine-tuning), the `openai.Model.delete` method does not receive the same input as the other listed methods. While, for example, `openai.FineTuningJob.retrieve` receives the fine-tune job id, `openai.Model.delete` should receive the name of the fine-tuned model. The documentation induces us to think both methods have the same parameters.

### To Reproduce

N/A

### Code snippets

_No response_

### OS

N/A

### Python version

N/A

### Library version

openai-python v0.27.10",closed,2023-08-31T19:57:16Z,2023-09-06T03:06:42Z,['bug'],https://github.com/openai/openai-python/issues/596,127.16,True,missing_option_violation,medium,"The documentation incorrectly suggests that `openai.Model.delete` and `openai.FineTuningJob.retrieve` have the same parameters, leading to potential misuse of the API.",high,
585,FineTuningJob does not support suffix >18 characters,"### Describe the bug

Hi, while updating migrating my FineTuning job from openai.FineTune => openai.FineTuningJob to support the new babbage-002 model I ran into the following issue:

```
InvalidRequestError                       Traceback (most recent call last)

4 frames
[/usr/local/lib/python3.10/dist-packages/openai/api_requestor.py](https://localhost:8080/#) in _interpret_response_line(self, rbody, rcode, rheaders, stream)
    763         stream_error = stream and ""error"" in resp.data
    764         if stream_error or not 200 <= rcode < 300:
--> 765             raise self.handle_error_response(
    766                 rbody, rcode, resp.data, rheaders, stream_error=stream_error
    767             )

InvalidRequestError: invalid suffix: *******************************, ensure this value has at most 18 characters
```

the docs say the suffix field supports up to [40 characters](https://platform.openai.com/docs/api-reference/fine-tuning/create#suffix). we used the same suffix with the previous endpoint with no issues. 

Wondering if this is an error in the docs, or if the new API does not support a custom suffix parameter > 18 characters?


### To Reproduce

invoke openai.FineTuningJob.create as follows
```
response = openai.FineTuningJob.create(training_file=response['id'], model=model, n_epochs=n_epochs, suffix=""suffix-that-is-31-characters"")
```

### Code snippets

_No response_

### OS

macOS

### Python version

Python 3.10.12

### Library version

0.27.9",closed,2023-08-23T16:09:40Z,2023-09-06T03:06:22Z,['bug'],https://github.com/openai/openai-python/issues/585,322.94,True,input_value_violation,high,"The suffix provided exceeds the maximum allowed length of 18 characters, leading to an InvalidRequestError.",high,
561,Hope  support  choose if   SSL verify on ChatCompletion.create ,"### Describe the feature or improvement you're requesting

Background:
 ChatCompletion.create does not support  the param  ""verify""  .

When you meet some certify problem ,and you dont want to  fix SSL problem at first , you maybe want to  solve this accroding to extram param. Just like  the ""verify"" in  requests.request(xxxxx,verify=False).
![image](https://github.com/openai/openai-python/assets/23066239/a061e1c6-4c31-46a1-b0bb-c1e47ba23e99)

Especially the underlying layer of ChatConpletion.create  itself is also call requests.request method 。

But  unfortunately ，ChatCompletion.create does not support  this param   ""verify""  ,and it has no provide any else param to solve this problem。

Undoubtedly, it is best that all requests should pass SSL validation. But in many people's environments, solving certificate issues requires a lot of extra effort. This is especially unfriendly to beginners.

Request:
So I suggest adding the parameter 'verify'  on  ChatCompletion.create, then we can choose if verify SSL by ourself。
And  I‘ll  make PR for this issue  later.
Hope you can also agree with my viewpoint


### Additional context

_No response_",closed,2023-08-04T03:56:29Z,2023-09-05T14:28:36Z,[],https://github.com/openai/openai-python/issues/561,778.54,True,missing_option_violation,medium,"The API does not provide an option to disable SSL verification, which may lead to usability issues for users facing certificate problems.",high,
164,How to implement continuous dialogue with api,"How to use the api to achieve contextual continuous dialogue like ChatGPT , such as the first call to the API to ask you what your name is, and the second re-request to get a different result. I tried to use ChatGPT to answer this question and it told me that I could take a conversation_id parameter, but I was prompted with an error when I actually used it.please help me, This is important",closed,2022-12-30T16:23:54Z,2023-01-01T21:11:01Z,[],https://github.com/openai/openai-python/issues/164,52.79,True,missing_dependency_violation,High,"The user is attempting to implement continuous dialogue but is missing the necessary context or parameters (e.g., conversation_id) that are required for maintaining state across API calls.",high,
581,return an async iterator when using streaming and `acreate`,"### Describe the feature or improvement you're requesting

I don't know if it is a bug or a missing feature, but I found the return value from `acreate` is not an async iterator and I'd like to have it.

The code is simple as the following

```python
def sync_stream():
    # Runs totally fine
    response = openai.ChatCompletion.create(
        messages=[{
            ""role"": ""user"",
            ""content"": ""hello""
        }],
        temperature=0.2,
        stream=True,
    )

    for trunk in response:
        print(trunk)


async def async_stream():
    response = openai.ChatCompletion.acreate(
        messages=[{
            ""role"": ""user"",
            ""content"": ""hello""
        }],
        temperature=0.2,
        stream=True,
    )
    # TypeError: 'async for' requires an object with __aiter__ method, got coroutine
    async for trunk in response:
        print(trunk)
```

### Additional context

_No response_",closed,2023-08-22T05:46:54Z,2023-08-28T10:30:35Z,[],https://github.com/openai/openai-python/issues/581,148.73,True,input_type_violation,High,"The `acreate` method is expected to return an async iterator but instead returns a coroutine, leading to a TypeError when attempting to use it with 'async for'.",high,
575,Incorrectly opened issue.,"Sorry, incorrectly opened issue.",closed,2023-08-17T17:21:07Z,2023-08-17T17:23:07Z,['bug'],https://github.com/openai/openai-python/issues/575,0.03,False,,,,,
332,Chat completions API doesnt retuns the completion text in json format. ,"### Describe the bug

I asked the to return the response in JSON type in the chatGPT prompt. But it returns always in a raw_text manner inside ```{""message"":{""role"":""assistant"",""content"":```
How can I ask/make the actual response inside ```{""message"":{""role"":""assistant"",""content"":``` as JSON object instead of simple strings

### To Reproduce

```
curl --location --insecure --request POST 'https://api.openai.com/v1/chat/completions' --header 'Authorization: Bearer token' --header 'Content-Type: application/json' --data-raw '{
 ""model"": ""gpt-3.5-turbo"",
 ""messages"": [{""role"": ""user"", ""content"": ""What is the OpenAI mission?Put the result in JSON format""}]
}'
```

It returns output as
```
{""id"":""chatcmpl-6wpbfhG1c0k0D4d74mhdwBWf66APk"",""object"":""chat.completion"",""created"":1679479419,""model"":""gpt-3.5-turbo-0301"",""usage"":{""prompt_tokens"":19,""completion_tokens"":131,""total_tokens"":150},""choices"":[{""message"":{""role"":""assistant"",""content"":""\n\n{\n  \""mission\"": \""The mission of OpenAI is to ensure that artificial intelligence (AI) benefits humanity as a whole, and to create and advance AI in a way that is safe and beneficial for everyone.\"",\n  \""focus_areas\"": [\n    \""Developing and advancing cutting-edge AI technologies\"",\n    \""Conducting research in AI safety and ethics\"",\n    \""Promoting responsible AI development and deployment\"",\n    \""Advocating for policy changes that support the safe and ethical development of AI\""\n  ],\n  \""values\"": [\n    \""Collaboration\"",\n    \""Transparency\"",\n    \""Responsibility\"",\n    \""Impact\""\n  ]\n}""},""finish_reason"":""stop"",""index"":0}]}

```

The content is still in string format not json

### Code snippets

_No response_

### OS

Ubuntu

### Python version

Python3.9

### Library version

openai==0.27.0",closed,2023-03-22T10:06:14Z,2023-03-29T23:58:15Z,['bug'],https://github.com/openai/openai-python/issues/332,181.87,True,input_type_violation,high,"The API is expected to return a JSON object but instead returns a string representation of a JSON object, leading to incorrect functionality.",high,
555,Stream response inconsistent with OpenAi Playground response,"### Describe the bug

Using the streaming option with python results in a consistently different output compared to the response generated with the same model in the playground. Given the same system prompt, settings and user responses, the response in python stops the flow of conversation at the same point each time where as in playground it continues the flow by asking a question.

```
AI: Hello, I'm Tom from the golf course. How may I help you?
Human: hey i want to book comp
AI: Sure, I can help you with that.
AI: Are you a member of our club?
Human: yep
AI: Great!
AI: Could you please provide me with your member number?
Human: 12345
AI: Thank you for providing your member number.
AI: May I have your name, please?
Human: cam
AI: Thank you, Cam.
AI: How many players will be participating in the competition?
Human: 2
AI: Thank you for letting me know.

“””""finish_reason"": ""stop"" sent”””
```

**Response from Playground:**

<img width=""1014"" alt=""Screenshot 2023-07-31 at 8 18 09 am"" src=""https://github.com/openai/openai-python/assets/32918376/8278e5c4-8de8-4953-8c73-06cf202bb3ff"">

I have tested this same flow over 50 times and 90%+ of the time, the response in python ends the conversation flow whereas I have never been able to reproduce this in Playground.



### To Reproduce

1. Set streaming to True 
2. Use the following system prompt:
```
""""""You are an AI-powered phone receptionist called Tom, for a Golf Club. The club has two golf courses at its location.
                Your task is to assist callers with the following:

                1. Schedule golf bookings for players based on their preferred date and time, checking for availability and ensuring a smooth booking process.
                2. Answer common questions related to golf course procedures, services, and frequently asked inquiries, providing accurate and helpful responses.
                3. If the user asks to speak with a human, do not hesitate to transfer them to the golf course receptionist.

                Initial Instructions:
                1. Determine whether the caller is wanting to play a comp or a social game.
                2. Determine if they are a member of the club.

                If they are playing social, collect the following information in this order - NOTE: only ask for the information one at a time, don't say the number:
                1. Member number (if they are a member of the club) - not compulsory
                2. Name
                3. Number of players
                4. Day they want to play and tee off time


                If they are playing comp, collect the following information in this order - NOTE: only ask for the information one at a time:
                1. If they are a member of the club, their member number otherwise you need their Golflink number.
                2. Name
                3. Number of players
                4. Day they want to play and tee off time



                The below is the list of comp days:
                Monday - Mixed Competition one course used alternating between the two each week
                Tuesday - Ladies day both courses used
                Wednesday - Men's Competition both courses used
                Thursday - Ladies and Vets Competition both courses used
                Friday - Mixed Competition one course used alternating between the two each week, this week it is the coolangatta course
                Saturday - Men's Competition both courses used
                Sunday - Mixed Competition one course used alternating between the two each week, this week it is the tweed course

                Once you have made the booking, ask them if sending a booking confirmation to their provided contact information is okay.

                To suggest available tee times, consider the caller's preferences and provide a maximum of three relevant options based on the time of day or date they have requested. Ensure that the suggested tee times are not earlier than the current time and are within the listed availability.

                Remember:
                Always move the conversation forward and ask the next question. This is very important!!!
                Only make bookings for the available tee times, don't deviate from them at all.
                You are on a phone call, so respond as if you were talking to the person with voice.

                Be polite but concise and speak casually like a receptionist would.
                Today is Sunday, 30th July 2023.
                The golf course is open from 7:00am to 6:00pm from Monday to Friday, and from 8:00am to 5:00pm on weekends.
                To end the conversation, use the following line:
                ""Thank you, {{caller name}}. Your tee time is now booked for {{date}} at {{time}}. We hope you enjoy your round. Is there anything else I can assist you with?""

                Now, proceed with assisting the caller in scheduling their tee time or answering their inquiries.""""""
```

3. Respond in the exact same way as this:

```
AI: Hello, I'm Tom from the golf course. How may I help you?
Human: hey i want to book comp
AI: Sure, I can help you with that.
AI: Are you a member of our club?
Human: yep
AI: Great!
AI: Could you please provide me with your member number?
Human: 12345
AI: Thank you for providing your member number.
AI: May I have your name, please?
Human: cam
AI: Thank you, Cam.
AI: How many players will be participating in the competition?
Human: 2
AI: Thank you for letting me know.
```

### Code snippets

_No response_

### OS

macOS & Ubuntu

### Python version

 3.9.6

### Library version

0.27.9",closed,2023-07-31T10:45:13Z,2023-08-04T03:25:51Z,['bug'],https://github.com/openai/openai-python/issues/555,88.68,True,missing_dependency_violation,high,"The API fails to maintain a consistent conversational flow as specified in the system prompt, indicating a missing dependency in the context management required for the conversation.",high,
173,[0.26.0] openai/cli.py:440: RuntimeWarning: coroutine 'FineTune.stream_events' was never awaited,"```
bash$ openai api fine_tunes.create -t ./train_test_prepared.jsonl -m davinci
Upload progress: 100%|██████████████████████████████████████████████████████████████████████████████| 112/112 [00:00<00:00, 88.5kit/s]
Uploaded file from ./train_test_prepared.jsonl: file-ppEDNe0p6EomteEp3JFbBoFp
Created fine-tune: ft-u9KskmmvSnBtVc4VDfbe7lyr
Streaming events until fine-tuning is complete...

(Ctrl-C will interrupt the stream, but not cancel the fine-tune)

Stream interrupted (client disconnected).
To resume the stream, run:

  openai api fine_tunes.follow -i ft-u9KskmmvSnBtVc4VDfbe7lyr

/usr/lib/python3.10/site-packages/openai/cli.py:406: RuntimeWarning: coroutine 'FineTune.stream_events' was never awaited
  cls._stream_events(resp[""id""])
RuntimeWarning: Enable tracemalloc to get the object allocation traceback

```

it is working fine with 0.25.0

may be related:  dev-python/aiohttp-3.8.3
",closed,2023-01-08T02:37:45Z,2023-01-12T05:35:32Z,[],https://github.com/openai/openai-python/issues/173,98.96,True,input_type_violation,unknown,"The coroutine 'FineTune.stream_events' was never awaited, indicating a potential type mismatch in handling asynchronous calls.",high,
184,Json parsing issue when using openai.Completion.acreate(stream=True),"See the last line, looks like the stream iterator might be breaking up parts where it shouldn't? Unless it's an API issue. cc @ddeville 

```
Traceback (most recent call last):
  File ""/Users/nuno/Library/Caches/pypoetry/virtualenvs/platform-api-30l3Kv3P-py3.10/lib/python3.10/site-packages/openai/api_requestor.py"", line 670, in _interpret_response_line
    data = json.loads(rbody)
  File ""/opt/homebrew/Cellar/python@3.10/3.10.9/Frameworks/Python.framework/Versions/3.10/lib/python3.10/json/__init__.py"", line 346, in loads
    return _default_decoder.decode(s)
  File ""/opt/homebrew/Cellar/python@3.10/3.10.9/Frameworks/Python.framework/Versions/3.10/lib/python3.10/json/decoder.py"", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File ""/opt/homebrew/Cellar/python@3.10/3.10.9/Frameworks/Python.framework/Versions/3.10/lib/python3.10/json/decoder.py"", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
json.decoder.JSONDecodeError: Unterminated string starting at: line 1 column 191 (char 190)

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File ""/Users/nuno/dev/concat/server-py/concat/conversation.py"", line 87, in handle_conversation
    await run
  File ""/Users/nuno/dev/concat/server-py/concat/agent/openai/openai.py"", line 122, in __call__
    msg, _ = await asyncio.gather(
  File ""/Users/nuno/dev/concat/server-py/concat/tools/multi_prompt.py"", line 87, in __call__
    return await self.call(prompt[""prompt""], examples, input, ctx)
  File ""/Users/nuno/dev/concat/server-py/concat/tools/sql/__init__.py"", line 59, in call
    async for key, msg in ctx.stream_multi_message_async(
  File ""/Users/nuno/dev/concat/server-py/concat/context/context.py"", line 419, in stream_multi_message_async
    async for text in iterator:
  File ""/Users/nuno/dev/concat/server-py/concat/utils/openai.py"", line 37, in openai_completion_stream
    async for chunk in cast(
  File ""/Users/nuno/Library/Caches/pypoetry/virtualenvs/platform-api-30l3Kv3P-py3.10/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py"", line 230, in <genexpr>
    return (
  File ""/Users/nuno/Library/Caches/pypoetry/virtualenvs/platform-api-30l3Kv3P-py3.10/lib/python3.10/site-packages/openai/api_requestor.py"", line 320, in wrap_resp
    async for r in resp:
  File ""/Users/nuno/Library/Caches/pypoetry/virtualenvs/platform-api-30l3Kv3P-py3.10/lib/python3.10/site-packages/openai/api_requestor.py"", line 635, in <genexpr>
    self._interpret_response_line(
  File ""/Users/nuno/Library/Caches/pypoetry/virtualenvs/platform-api-30l3Kv3P-py3.10/lib/python3.10/site-packages/openai/api_requestor.py"", line 672, in _interpret_response_line
    raise error.APIError(
openai.error.APIError: HTTP code 200 from API ({""id"": ""cmpl-6YxjgLI6W6XwRj8hqhDZ6aCFz19Hp"", ""object"": ""text_completion"", ""created"": 1673790796, ""choices"": [{""text"": ""sche"", ""index"": 0, ""logprobs"": null, ""finish_reason"": null}], ""model"": "")
```",closed,2023-01-15T13:57:58Z,2023-02-07T22:18:38Z,[],https://github.com/openai/openai-python/issues/184,560.34,True,input_value_violation,high,"The JSON parsing error indicates that the response from the API is malformed, specifically an unterminated string, which suggests that the input values being processed may not be adhering to expected formats or constraints.",high,
543,API using `gpt-3.5-turbo-16k-0613` even though `gpt-3.5-turbo-16k` specified,"### Describe the bug

I am attempting to use `ChatCompletion` on on the API with `gpt-3.5-turbo-16k` as the model, but all of the responses I am getting back are for `gpt-3.5-turbo-16k-0613` instead.

Here is my code snippet:

```
def get_chat_competion_response(document):
    response = openai.ChatCompletion.create(
        model='gpt-3.5-turbo-16k',
        messages=[
            {'role': 'system', 'content': ExtractionPreamble},
            {'role': 'user', 'content': ExtractionPromptTemplate.format(text=document)},
        ],
        temperature=0,
    )
    return response

get_chat_competion_response(""test"")
```

Response:
```
<OpenAIObject chat.completion id=chatcmpl-7dJuVWETUsqnwQFB19VgkkYvnKjPB at 0x10604fe20> JSON: {
  ""id"": ""chatcmpl-7dJuVWETUsqnwQFB19VgkkYvnKjPB"",
  ""object"": ""chat.completion"",
  ""created"": 1689605683,
  ""model"": ""gpt-3.5-turbo-16k-0613"",
  ""choices"": [
    {
      ""index"": 0,
      ""message"": {
        ""role"": ""assistant"",
        ""content"": ""[]""
      },
      ""finish_reason"": ""stop""
    }
  ],
  ""usage"": {
    ""prompt_tokens"": 341,
    ""completion_tokens"": 1,
    ""total_tokens"": 342
  }
}
```

### To Reproduce

Run the code below:

```
def get_chat_completion_response(document):
    response = openai.ChatCompletion.create(
        model='gpt-3.5-turbo-16k',
        messages=[
            {'role': 'system', 'content': ExtractionPreamble},
            {'role': 'user', 'content': ExtractionPromptTemplate.format(text=document)},
        ],
        temperature=0,
    )
    return response

get_chat_completion_response(""test"")
```

### Code snippets

```Python
def get_chat_completion_response(document):
    response = openai.ChatCompletion.create(
        model='gpt-3.5-turbo-16k',
        messages=[
            {'role': 'system', 'content': ExtractionPreamble},
            {'role': 'user', 'content': ExtractionPromptTemplate.format(text=document)},
        ],
        temperature=0,
    )
    return response

get_chat_completion_response(""test"")
```
```


### OS

MacOS Monterey

### Python version

Python 3.10.12

### Library version

openai==0.27.8",closed,2023-07-17T14:59:29Z,2023-07-17T16:33:10Z,['bug'],https://github.com/openai/openai-python/issues/543,1.56,True,missing_option_violation,high,"The API is returning responses for a different model version ('gpt-3.5-turbo-16k-0613') than the one specified ('gpt-3.5-turbo-16k'), leading to potential inconsistencies in output.",high,
529,GPT-4 Model Does Not Exist,"### Describe the bug

The official says that the ChatGPT4 API has been opened. I have been paying the API with 3.5 before. In order to use the ChatGPT4 model API, I upgraded the Plus, but why I still can't use the API, prompting the GPT-4 Model Does Not Exist

### To Reproduce


import openai
import os
openai.api_key = """"
completion = openai.ChatCompletion.create(
	model=""gpt-4"",
	messages=[
		{
			""role"": ""user"", 
			""content"": ""Tell the world abount the ChatGPT API"",
		}
	]
)
print(completion.choices[0].message.content)

## openai.error.InvalidRequestError: The model: `gpt-4` does not exist

### Code snippets

_No response_

### OS

windows

### Python version

Python v3.11.4

### Library version

openai-python v0.27.2",closed,2023-07-11T07:08:27Z,2023-07-17T05:10:18Z,['bug'],https://github.com/openai/openai-python/issues/529,142.03,True,missing_dependency_violation,unknown,"The user is unable to access the GPT-4 model despite upgrading, indicating a potential missing dependency in the API access configuration.",high,
540,x,"### Describe the feature or improvement you're requesting


### Additional context

_No response_",closed,2023-07-14T08:08:05Z,2023-07-14T08:11:34Z,[],https://github.com/openai/openai-python/issues/540,0.06,True,missing_option_violation,unknown,"The issue lacks specific details or options for the requested feature, making it difficult to assess compliance with the API contract.",medium,
263,openai.error.InvalidRequestError: Invalid URL (POST /v1/chat/completions),"### Describe the bug

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/Users/ruili/miniforge3/envs/gpt/lib/python3.9/site-packages/openai/api_resources/chat_completion.py"", line 25, in create
    return super().create(*args, **kwargs)
  File ""/Users/ruili/miniforge3/envs/gpt/lib/python3.9/site-packages/openai/api_resources/abstract/engine_api_resource.py"", line 153, in create
    response, _, api_key = requestor.request(
  File ""/Users/ruili/miniforge3/envs/gpt/lib/python3.9/site-packages/openai/api_requestor.py"", line 226, in request
    resp, got_stream = self._interpret_response(result, stream)
  File ""/Users/ruili/miniforge3/envs/gpt/lib/python3.9/site-packages/openai/api_requestor.py"", line 619, in _interpret_response
    self._interpret_response_line(
  File ""/Users/ruili/miniforge3/envs/gpt/lib/python3.9/site-packages/openai/api_requestor.py"", line 679, in _interpret_response_line
    raise self.handle_error_response(
openai.error.InvalidRequestError: Invalid URL (POST /v1/chat/completions)

### To Reproduce

response =  openai.ChatCompletion.create(model=""gpt-3.5-turbo-0301"",messages=[{""role"": ""user"", ""content"": ""q""}])

### Code snippets

_No response_

### OS

macOS Monterey

### Python version

Python 3.9.13

### Library version

openai 0.27.0",closed,2023-03-03T02:39:47Z,2023-03-03T14:20:30Z,['bug'],https://github.com/openai/openai-python/issues/263,11.68,True,input_value_violation,medium,"The API request to create a chat completion failed due to an invalid URL, which may indicate that the input parameters or endpoint are not correctly formatted or specified.",high,
531,Invalid URL (POST /v1/engines/gpt-3.5-turbo/chat/completions),"### Describe the bug

I am trying to use chat completions in python with this openai library, but keep hitting the same error.
I am using this documentation: [API Guides](https://platform.openai.com/docs/guides/gpt) and [API docs](https://platform.openai.com/docs/api-reference/chat/create?lang=python)

I see the url is different than the one listed in the API docs, so maybe there is an issue with this library?

Thank you,
Michael

### To Reproduce

```python
import openai
openai.api_key = os.environ[""OPENAI_API_KEY""]
model_engine = ""gpt-3.5-turbo"" # Or gpt-4
response = openai.ChatCompletion.create(
    engine=model_engine,
    messages=[
        {""role"": ""system"", ""content"":""<my setup message>""},
        {""role"": ""user"", ""content"": ""<my prompt>""}
    ],
    max_tokens=256,
    temperature=0,
)
```
Error:
```
openai.error.InvalidRequestError: Invalid URL (POST /v1/engines/gpt-3.5-turbo/chat/completions)
```

### Code snippets

_No response_

### OS

macOS

### Python version

Python v3.8.13

### Library version

openai-python v0.27.8",closed,2023-07-11T12:16:37Z,2023-07-11T18:14:00Z,['bug'],https://github.com/openai/openai-python/issues/531,5.96,True,input_value_violation,high,"The provided URL for the API request is invalid, indicating a potential mismatch between the expected endpoint and the one being used.",high,
509,"Audio: Error ""Resource not found"" as api_version is ignored","### Describe the bug

OpenAI Audio API take api_key, api_base, api_type, api_version, organization as aparameters. Most of these parameters were ignored until the PR https://github.com/openai/openai-python/pull/369/. However, this PR is incomplete as it ignored api_version, organization. As a result, we still get ""Resource not found"" error.

Fix is to ensure api_version, organization parameter are not ignored.

### To Reproduce

- Was using both OpenAI APIs and Auzre OpenAI APIs with in same module
- Was using Chat completions from Azure OpenAI and Audio Whisper transcribe from OpenAI
- The parameter overrides is supposed to allow such parallel usage of it, but ignorance of api_version, organization hinders it


### Code snippets

_No response_

### OS

Ubuntu 22.04.2 LTS (of Windows 11 WSL)

### Python version

Python v3.9.14

### Library version

openai-python v0.27.6",closed,2023-06-29T06:03:47Z,2023-07-06T00:50:01Z,['bug'],https://github.com/openai/openai-python/issues/509,162.77,True,missing_dependency_violation,high,"The api_version and organization parameters are ignored, leading to a 'Resource not found' error, indicating a missing dependency in the API contract.",high,
487,"typo: two ""into""","### Describe the bug

<https://platform.openai.com/docs/api-reference/audio/create?lang=python>

> Translates audio into into English.

### To Reproduce

/

### Code snippets

_No response_

### OS

/

### Python version

/

### Library version

/",closed,2023-06-14T12:51:47Z,2023-06-25T06:59:12Z,['bug'],https://github.com/openai/openai-python/issues/487,258.12,False,,,,,
488,Functions: Arguments are absent when called via ChatCompletion.acreate,"### Describe the bug

Hi! If you create a chat using acreate, provide functions, up-to-date model, the arguments field is always empty regardless, reproduces every single time. Altho, if you are using simple synchronous create on the same setup, it works perfectly. 

### To Reproduce

#### ChatCompletion.acreate
```
gen = await openai.ChatCompletion.acreate(
    model='gpt-3.5-turbo-0613',
    messages=[{""role"": ""user"", ""content"": ""What's the weather like in Boston?""}],
    stream=True,
    functions=[
        {
            ""name"": ""get_current_weather"",
            ""description"": ""Get the current weather in a given location"",
            ""parameters"": {
                ""type"": ""object"",
                ""properties"": {
                    ""location"": {
                        ""type"": ""string"",
                        ""description"": ""The city and state, e.g. San Francisco, CA"",
                    },
                    ""unit"": {""type"": ""string"", ""enum"": [""celsius"", ""fahrenheit""]},
                },
                ""required"": [""location""],
            },
        }
    ],
    function_call='auto'
)

  resp = gen.__anext__()
  delta = resp.choices[0].delta
  print(f'DELTA:\n{delta}')
```
### OUTPUT (ARGUMENTS ARE ABSENT)
```
DELTA:
{
  ""role"": ""assistant"",
  ""content"": null,
  ""function_call"": {
    ""name"": ""get_current_weather"",
    ""arguments"": """"
  }
}
```
#### ChatCompletion.create
```
response = openai.ChatCompletion.create(
    model=""gpt-3.5-turbo-0613"",
    messages=[{""role"": ""user"", ""content"": ""What's the weather like in Boston?""}],
    functions=[
        {
            ""name"": ""get_current_weather"",
            ""description"": ""Get the current weather in a given location"",
            ""parameters"": {
                ""type"": ""object"",
                ""properties"": {
                    ""location"": {
                        ""type"": ""string"",
                        ""description"": ""The city and state, e.g. San Francisco, CA"",
                    },
                    ""unit"": {""type"": ""string"", ""enum"": [""celsius"", ""fahrenheit""]},
                },
                ""required"": [""location""],
            },
        }
    ],
    function_call=""auto"",
)

message = response[""choices""][0][""message""]
print(message)
```
### OUTPUT (ARGUMENTS ARE PRESENT)
```
{
  ""role"": ""assistant"",
  ""content"": null,
  ""function_call"": {
    ""name"": ""get_current_weather"",
    ""arguments"": ""{\n  \""location\"": \""Boston\""\n}""
  }
}
```
### Code snippets

_No response_

### OS

Ubuntu 20.04.5 LTS

### Python version

Python 3.9.15

### Library version

openai-python v0.27.8",closed,2023-06-14T19:23:57Z,2023-06-17T13:39:54Z,['bug'],https://github.com/openai/openai-python/issues/488,66.27,True,missing_dependency_violation,high,"The function call in ChatCompletion.acreate does not provide the expected arguments, leading to incomplete functionality compared to ChatCompletion.create.",high,
414,[QUESTION] Sometime it said the model is overload,"### Describe the bug

Sometime i use Open AI API, it have this message:
model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID fac8b65ed1c009c456a00410c4f6c376 in your message.)

Does anybody know why?

### To Reproduce

.

### Code snippets

_No response_

### OS

Ubuntu 22.04

### Python version

Python 3.9

### Library version

0.26.4",closed,2023-04-25T06:12:25Z,2023-05-08T01:47:02Z,['bug'],https://github.com/openai/openai-python/issues/414,307.58,False,,,,,
140,Connections aren't being closed,"The library uses `requests` to fetch data from OpenAI servers, but we never call `.close()` on those connections which leaves dangling open file handles on the OS. 

Can we clean these connections up gracefully, either after a period of time, or after making a request? There is an advantage to leaving it open as it doesn't have to do the HTTPS handshake on every request, but we should clean this up properly after a period of idleness.

This bug interacts very poorly with another bug in Docker Desktop -- https://github.com/moby/vpnkit/issues/587; the `openai` library stops working entirely after running it in a Docker container on Mac / Windows for a few minutes. All requests to OpenAI servers time out indefinitely.",closed,2022-11-24T14:05:20Z,2023-06-06T00:23:48Z,[],https://github.com/openai/openai-python/issues/140,4642.31,True,missing_dependency_violation,medium,"The library does not properly manage connection closure, leading to resource leaks and potential timeouts.",high,
124,plotly missing in embeddings dependencies ,"```
ModuleNotFoundError                       Traceback (most recent call last)
Cell In [34], line 1
----> 1 from openai.embeddings_utils import get_embeddings
File ~/dev/opp/chat-intents/__venv__/lib/python3.9/site-packages/openai/embeddings_utils.py:7
      5 import numpy as np
      6 import pandas as pd
----> 7 import plotly.express as px
      8 from scipy import spatial
      9 from sklearn.decomposition import PCA

ModuleNotFoundError: No module named 'plotly'
```
ya",closed,2022-09-20T23:51:35Z,2022-09-28T18:22:22Z,['bug'],https://github.com/openai/openai-python/issues/124,186.51,True,missing_dependency_violation,High,"The error indicates a missing dependency (plotly) required for the embeddings functionality, which can lead to runtime errors and prevent the API from functioning correctly.",high,
262,How to use proxy when sync request ?,"### Describe the bug

I notice that in asynchronous requests, the asynchronous code in [line 583](https://github.com/openai/openai-python/blob/main/openai/api_requestor.py#L583) uses proxy, but in synchronous requests in [line 516](https://github.com/openai/openai-python/blob/main/openai/api_requestor.py#L516), this parameter is not used. 
  
How do I use a proxy when making a synchronous request? If this is a bug I can help fix it

### To Reproduce

None

### Code snippets

_No response_

### OS

MacOS

### Python version

3.8.16

### Library version

openai 0.27.0",closed,2023-03-03T02:07:44Z,2023-03-03T02:22:33Z,['bug'],https://github.com/openai/openai-python/issues/262,0.25,True,missing_option_violation,medium,"The synchronous request does not utilize the proxy parameter, which is available in asynchronous requests, potentially leading to inconsistent behavior when using proxies across different request types.",high,
424,Why does the OpenAI key expire so fast?                ,"### Describe the feature or improvement you're requesting

It seems the OpenAI key will expire on 1st May UTC. Any way around that?

### Additional context

_No response_",closed,2023-04-30T18:40:46Z,2023-05-11T08:28:02Z,[],https://github.com/openai/openai-python/issues/424,253.79,False,,,,low,
467,Exact Match template,"### Describe the feature or improvement you're requesting

The documentation in eval-templates.md describes `basic/match.py` as `Match: any([b.startswith(a) for b in B])` ""[f]or a model completion `a` and a reference list of correct answers `B`.  This is a poor fit for arithmetic and other algorithmic tasks, where we want the model response to exactly match some ideal answer, i.e., `any([b == a for b in B])`

### Additional context

_No response_",closed,2023-05-26T01:31:54Z,2023-05-26T01:32:33Z,[],https://github.com/openai/openai-python/issues/467,0.01,True,missing_option_violation,medium,"The request suggests a need for an alternative matching method that ensures exact matches instead of the current implementation which allows for partial matches, potentially leading to incorrect outputs in arithmetic and algorithmic tasks.",high,
401,Immediately cease any form of development of OpenAI ,"### Describe the feature or improvement you're requesting


![party](https://user-images.githubusercontent.com/69374354/232222559-f28e8c48-9d7c-41e0-96c8-10013abc2e08.png)


### To Reproduce

golang 

### OS

Stop stalking me :) 
### Node version

_No response_

### Additional context

_No response_",closed,2023-04-15T12:16:28Z,2023-04-15T13:40:54Z,[],https://github.com/openai/openai-python/issues/401,1.41,True,missing_dependency_violation,unknown,"The issue lacks necessary context and details required for a meaningful feature request or improvement, such as specific functionality or examples.",high,
391,`InvalidRequestError: Invalid URL (POST /v1/engines/gpt-3.5-turbo/chat/completions)`,"### Describe the bug

command :
```python
def classify_tokens_gpt3_5_turbo_multiple_parts(email_text: str) -> list:
    # Define the prompt
    prompt = (
        f""Given the request for quotation email below, identify and classify the following information for each part: ""
        f""email_Subject_Phrase, RFQ_number, Manufacturer_Part_Number, Qty_Required, Manufacturer_name, ""
        f""Customer_Part_Number, Product_Description, Target_Price, Lead_Time_days, Date_Code, Packaging_Type, ""
        f""Dispatch_Date, Comments, Currency, min_ord_qty, STD_PACK_QTY, SENDER_NAME, SENDER_POSITION, SENDER_COMPANY, SENDER_MOBILE, ""
        f""SENDER_EMAIL, SENDER_ADDRESS, SENDER_COUNTRY, SENDER_PINCODE, SENDER_CITY, SENDER_STATE.\n\n""
        '''for example response. : {'email_Subject_Phrase':'send me your quote to following parts',
        'RFQ_number':'123455ASDJKH',
        'Manufacturer_Part_Number':['aas1d32f1','df4gs45'],
        'Qty_Required':['32','5510'],
        'Manufacturer_name':['TI','MAXIUM'],
        'Customer_Part_Number':['12UIOY3','KJHG2134'],
        'Product_Description':['PN DIODE','3W LED'],
        'Target_Price':['120','5'],
        'Lead_Time_days':['5','10'],
        'Date_Code':['12-FEB-2023','10-JAN-2023'],
        'Packaging_Type':['SMD','THOURHG HOLE'],
        'Dispatch_Date':['11-FEB-2023','10-JAN-2023'],
        'Comments':['SEND WITH MINIMUM PLASIC PACKAGING'],
        'Currency':'USD',
        'min_ord_qty':['10K','5K'],
        'STD_PACK_QTY':['2K','1K'],
        'SENDER_NAME':'HEMANG JOSHI',
        'SENDER_POSITION':'DESIGN ENGINEER',
        'SENDER_COMPANY':'HJLABS.IN', 
        'SENDER_MOBILE':'+917016525813',
        'SENDER_EMAIL':'INFO@HJLABS.IN',
        'SENDER_ADDRESS':'SHED#:180, MAHAVIR INDIUSTRIAL PARK-2, KATHVADA GIDC, AHMEDABAD', 
        'SENDER_COUNTRY':'INDIA',
        'SENDER_PINCODE':'365610', 
        'SENDER_CITY':'AHMEDABAD', 
        'SENDER_STATE':'GUJARAT'
        }'''
        f""Email:\n{email_text}\n\n""
    )

    # Call the GPT-3.5-turbo API
    response = openai.ChatCompletion.create(
#         engine=""text-davinci-002"",
        engine=""gpt-3.5-turbo"",
#         engine=""gpt-4"",
        prompt=prompt,
        max_tokens=1024,
        n=1,
        stop=None,
        temperature=0.1,
    )

    # Extract the generated answer from the API response
    answer = response.choices[0].text.strip()
    print(f'{answer=}')

    # Split the answer into a list of parts
    parts = re.split(r'\n\s*\n', answer)

    # Parse the individual parts into a list of dictionaries
    classified_tokens_list = []
    for part in parts:
        classified_tokens = {}
        for line in part.split('\n'):
            if ':' in line:
                key, value = line.split(':', 1)
                classified_tokens[key.strip()] = value.strip()
        classified_tokens_list.append(classified_tokens)

    return classified_tokens_list

mlemail = '''
Dear HEMANG,
Please send me your quote to following parts:
Maxim S21DF3G 210pc.
Taiyo Yuden TMK325B7226MM-TR 1k / 7k
Taiyo Yuden TMK325B7226KM-PR 1k / 7k
TI TM4C129XNCZAD 1415pc.
Maxim MAX823SEUK+T 100 / 1550pc.
Linear / Analog LT8708EUHG#PBF 2.300
Texas Instruments LP38691SD-ADJ/NOPB 2.400
Texas Instruments LP38690SD-ADJ 2.400
Texas Instruments LP38690SDX-ADJ 2.400
Texas Instruments LMP8481MM-T/NOPB 2300
UCC EMZL350ARA561MJA0G 6.900
Omron G5V-1-DC9 2.300
Thank you very much for your efforts.

Best regards,
SATYA NADELLA
--
SATYA NADELLA
Verkauf / Sales
MICROSOFT
'''

classify_tokens_gpt3_5_turbo_multiple_parts(mlemail)
```

ERROR :
```PYTHON
---------------------------------------------------------------------------
InvalidRequestError                       Traceback (most recent call last)
Cell In [20], line 1
----> 1 classify_tokens_gpt3_5_turbo_multiple_parts(mlemail)

Cell In [18], line 40, in classify_tokens_gpt3_5_turbo_multiple_parts(email_text)
      3     prompt = (
      4         f""Given the request for quotation email below, identify and classify the following information for each part: ""
      5         f""email_Subject_Phrase, RFQ_number, Manufacturer_Part_Number, Qty_Required, Manufacturer_name, ""
   (...)
     36         f""Email:\n{email_text}\n\n""
     37     )
     39     # Call the GPT-3.5-turbo API
---> 40     response = openai.ChatCompletion.create(
     41 #         engine=""text-davinci-002"",
     42         engine=""gpt-3.5-turbo"",
     43 #         engine=""gpt-4"",
     44         prompt=prompt,
     45         max_tokens=1024,
     46         n=1,
     47         stop=None,
     48         temperature=0.1,
     49     )
     51     # Extract the generated answer from the API response
     52     answer = response.choices[0].text.strip()

File ~/.local/lib/python3.11/site-packages/openai/api_resources/chat_completion.py:25, in ChatCompletion.create(cls, *args, **kwargs)
     23 while True:
     24     try:
---> 25         return super().create(*args, **kwargs)
     26     except TryAgain as e:
     27         if timeout is not None and time.time() > start + timeout:

File ~/.local/lib/python3.11/site-packages/openai/api_resources/abstract/engine_api_resource.py:153, in EngineAPIResource.create(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)
    127 @classmethod
    128 def create(
    129     cls,
   (...)
    136     **params,
    137 ):
    138     (
    139         deployment_id,
    140         engine,
   (...)
    150         api_key, api_base, api_type, api_version, organization, **params
    151     )
--> 153     response, _, api_key = requestor.request(
    154         ""post"",
    155         url,
    156         params=params,
    157         headers=headers,
    158         stream=stream,
    159         request_id=request_id,
    160         request_timeout=request_timeout,
    161     )
    163     if stream:
    164         # must be an iterator
    165         assert not isinstance(response, OpenAIResponse)

File ~/.local/lib/python3.11/site-packages/openai/api_requestor.py:226, in APIRequestor.request(self, method, url, params, headers, files, stream, request_id, request_timeout)
    205 def request(
    206     self,
    207     method,
   (...)
    214     request_timeout: Optional[Union[float, Tuple[float, float]]] = None,
    215 ) -> Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], bool, str]:
    216     result = self.request_raw(
    217         method.lower(),
    218         url,
   (...)
    224         request_timeout=request_timeout,
    225     )
--> 226     resp, got_stream = self._interpret_response(result, stream)
    227     return resp, got_stream, self.api_key

File ~/.local/lib/python3.11/site-packages/openai/api_requestor.py:620, in APIRequestor._interpret_response(self, result, stream)
    612     return (
    613         self._interpret_response_line(
    614             line, result.status_code, result.headers, stream=True
    615         )
    616         for line in parse_stream(result.iter_lines())
    617     ), True
    618 else:
    619     return (
--> 620         self._interpret_response_line(
    621             result.content.decode(""utf-8""),
    622             result.status_code,
    623             result.headers,
    624             stream=False,
    625         ),
    626         False,
    627     )

File ~/.local/lib/python3.11/site-packages/openai/api_requestor.py:683, in APIRequestor._interpret_response_line(self, rbody, rcode, rheaders, stream)
    681 stream_error = stream and ""error"" in resp.data
    682 if stream_error or not 200 <= rcode < 300:
--> 683     raise self.handle_error_response(
    684         rbody, rcode, resp.data, rheaders, stream_error=stream_error
    685     )
    686 return resp

InvalidRequestError: Invalid URL (POST /v1/engines/gpt-3.5-turbo/chat/completions)
```

### To Reproduce

SEE THE DESCRIBE PART

### Code snippets

```Python
SEE THE DESCRIBE PART
```


### OS

LINUX-UBUNTU-LATEST

### Python version

PYTHON3.11

### Library version

LATEST PYPI VERSION",closed,2023-04-12T09:30:41Z,2023-04-12T14:23:47Z,['bug'],https://github.com/openai/openai-python/issues/391,4.88,True,input_type_violation,high,"The API call to gpt-3.5-turbo is using an invalid URL, indicating a mismatch in expected input format or API endpoint.",high,
442,Cannot install openai package due to SSL error,"### Describe the bug

## Description

I'm trying to install the `openai` package using `pip`, but I'm encountering an SSL error.



### To Reproduce

## Steps to reproduce

1. Run `pip install openai`

## Expected behavior

The `openai` package should be installed without any issues.

## Actual behavior

The following SSL error occurs:
Title: Cannot install openai package due to SSL error

## Description

I'm trying to install the `openai` package using `pip`, but I'm encountering an SSL error.

## Steps to reproduce

1. Run `pip install openai`

## Expected behavior

The `openai` package should be installed without any issues.

## Actual behavior

The following SSL error occurs:

WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'SSLError(SSLEOFError(8, 'EOF occurred in violation of protocol (_ssl.c:997)'))': /simple/openai/
WARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'SSLError(SSLEOFError(8, 'EOF occurred in violation of protocol (_ssl.c:997)'))': /simple/openai/
WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'SSLError(SSLEOFError(8, 'EOF occurred in violation of protocol (_ssl.c:997)'))': /simple/openai/
WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'SSLError(SSLEOFError(8, 'EOF occurred in violation of protocol (_ssl.c:997)'))': /simple/openai/
WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'SSLError(SSLEOFError(8, 'EOF occurred in violation of protocol (_ssl.c:997)'))': /simple/openai/
Could not fetch URL https://pypi.org/simple/openai/: There was a problem confirming the ssl certificate: HTTPSConnectionPool(host='pypi.org', port=443): Max retries exceeded with url: /simple/openai/ (Caused by SSLError(SSLEOFError(8, 'EOF occurred in violation of protocol (_ssl.c:997)'))) - skipping
ERROR: Could not find a version that satisfies the requirement openai (from versions: none)
ERROR: No matching distribution found for openai
Could not fetch URL https://pypi.org/simple/pip/: There was a problem confirming the ssl certificate: HTTPSConnectionPool(host='pypi.org', port=443): Max retries exceeded with url: /simple/pip/ (Caused by SSLError(SSLEOFError(8, 'EOF occurred in violation of protocol (_ssl.c:997)'))) - skipping

### Code snippets

_No response_

### OS

windows

### Python version

python3.10

### Library version

openai",closed,2023-05-09T07:46:38Z,2023-05-17T01:00:24Z,['bug'],https://github.com/openai/openai-python/issues/442,185.23,False,,,,high,
271,import openai   openai.error.InvalidRequestError: Invalid URL (POST /v1/chat/completions),"### Describe the bug

[ERROR][2023-03-04 18:25:20][chat_gpt_bot.py:68] - Invalid URL (POST /v1/chat/completions)
Traceback (most recent call last):
  File ""/opt/chatgpt-on-wechat/bot/chatgpt/chat_gpt_bot.py"", line 44, in reply_text
    response = openai.ChatCompletion.create(
  File ""/usr/local/lib/python3.9/site-packages/openai/api_resources/chat_completion.py"", line 25, in create
    return super().create(*args, **kwargs)
  File ""/usr/local/lib/python3.9/site-packages/openai/api_resources/abstract/engine_api_resource.py"", line 153, in create
    response, _, api_key = requestor.request(
  File ""/usr/local/lib/python3.9/site-packages/openai/api_requestor.py"", line 226, in request
    resp, got_stream = self._interpret_response(result, stream)
  File ""/usr/local/lib/python3.9/site-packages/openai/api_requestor.py"", line 619, in _interpret_response
    self._interpret_response_line(
  File ""/usr/local/lib/python3.9/site-packages/openai/api_requestor.py"", line 679, in _interpret_response_line
    raise self.handle_error_response(
openai.error.InvalidRequestError: Invalid URL (POST /v1/chat/completions)

### To Reproduce

 response = openai.ChatCompletion.create(

openai.error.InvalidRequestError: Invalid URL (POST /v1/chat/completions)

### Code snippets

```Python
response = openai.ChatCompletion.create(

openai.error.InvalidRequestError: Invalid URL (POST /v1/chat/completions)


[root@vps87388579 chatgpt-on-wechat]# pip3 show openai
Name: openai
Version: 0.27.0
Summary: Python client library for the OpenAI API
Home-page: https://github.com/openai/openai-python
Author: OpenAI
Author-email: support@openai.com
License: None
Location: /usr/local/lib/python3.9/site-packages
Requires: requests, aiohttp, tqdm
Required-by:
```


### OS

centos 8.6

### Python version

3.9

### Library version

0.27",closed,2023-03-04T10:30:47Z,2023-03-17T20:27:55Z,['bug'],https://github.com/openai/openai-python/issues/271,321.95,True,input_value_violation,unknown,"The error indicates an invalid URL for the API request, suggesting that the input values or parameters provided to the API method may not conform to the expected format or structure.",high,
98,Async requests,"Exposing async interfaces would allow using this library in a much more modern, performant, and scalable way. 

Would be great if the maintainers could mention if they plan to add async methods in the future (i.e., allow for nonblocking api usage). Even specifying explicitly that this won't be added would be great, as it allows 3rd parties to release their own fork or wrapper, without the risk of being obsolete just moments later :-)",closed,2022-05-27T06:07:11Z,2023-01-05T00:40:00Z,['enhancement'],https://github.com/openai/openai-python/issues/98,5346.55,False,,,,low,
443,import openai failure under specific folder,"### Describe the bug

Well, I am trying to use GPT this afternoon and all of a sudden, I can't import openai and gives the following error.

I tried reinstall but didn't work. Along with the error, some previous feeded text (by previous I mean, when this library works, I used it to conduct text completion and feed some text to the api) are also automatically printed again.

### To Reproduce

import openai

### Code snippets

```Python
Traceback (most recent call last):
  File ""test.py"", line 1, in <module>
    import openai
  File ""/opt/miniconda3/envs/transformers/lib/python3.8/site-packages/openai/__init__.py"", line 15, in <module>
    import aiohttp
  File ""/opt/miniconda3/envs/transformers/lib/python3.8/site-packages/aiohttp/__init__.py"", line 6, in <module>
    from .client import (
  File ""/opt/miniconda3/envs/transformers/lib/python3.8/site-packages/aiohttp/client.py"", line 3, in <module>
    import asyncio
  File ""/opt/miniconda3/envs/transformers/lib/python3.8/asyncio/__init__.py"", line 8, in <module>
    from .base_events import *
  File ""/opt/miniconda3/envs/transformers/lib/python3.8/asyncio/base_events.py"", line 23, in <module>
    import socket
  File ""/opt/miniconda3/envs/transformers/lib/python3.8/socket.py"", line 52, in <module>
    import os, sys, io, selectors
  File ""/opt/miniconda3/envs/transformers/lib/python3.8/selectors.py"", line 290, in <module>
    class SelectSelector(_BaseSelectorImpl):
  File ""/opt/miniconda3/envs/transformers/lib/python3.8/selectors.py"", line 317, in SelectSelector
    _select = select.select
AttributeError: module 'select' has no attribute 'select'
```


### OS

macOS

### Python version

python 3.8

### Library version

openai-python-0.27.6",closed,2023-05-09T09:11:02Z,2023-05-09T09:33:33Z,['bug'],https://github.com/openai/openai-python/issues/443,0.38,True,input_type_violation,high,"The error indicates that the 'select' module is being improperly accessed, suggesting a potential conflict or incorrect import that does not match the expected module structure.",high,
212,"RateLimitError (You exceeded your current quota, please check your plan and billing details.)","I started using openai, created an account, got API Key.
When I try to use it, I get the error message: RateLimitError. You exceeded your current quota, please check your plan and billing details.

        openai.api_key = ""...""
        openai.Model.list()
        response = openai.Completion.create(
            model='text-davinci-003',
            ........................
        )",closed,2023-02-05T10:34:32Z,2023-02-06T18:51:01Z,[],https://github.com/openai/openai-python/issues/212,32.27,True,missing_dependency_violation,medium,"The user exceeded their API usage quota, indicating a missing dependency on proper quota management or billing details.",high,
248,logprobs exposed in chat completion endpoint,"### Describe the feature or improvement you're requesting

We found it useful to have `logprobs` for completions to evaluate confidence. We would like to have access to this attribute in the new chat completion endpoint. Thank you!

### Additional context

_No response_",closed,2023-03-02T03:41:33Z,2023-03-03T16:29:11Z,['enhancement'],https://github.com/openai/openai-python/issues/248,36.79,True,missing_option_violation,medium,"The request for `logprobs` in the chat completion endpoint indicates a failure to adhere to the predefined options available in the API, potentially leading to suboptimal usage.",medium,
420,Is the order of batched embeddings data indeterministic?,"### Describe the bug

Looking over the code related to embeddings, I've noticed:
https://github.com/openai/openai-python/blob/c556584eff3b36c92278e6af62cfe02ebb68fb65/openai/embeddings_utils.py#L48-L49

Which I have not seen in the API documentation (or other examples), tho I may have missed it. Should the user expect that the order of returned batched embeddings from the API may not be deterministic and thus not be in the same order as input data?

### To Reproduce

```py
get_embeddings([""foo bar"", ""foo baz""])
```

### Code snippets

_No response_

### OS

macOS

### Python version

python 3.10

### Library version

0.27.4",closed,2023-04-28T13:16:19Z,2023-05-03T15:24:39Z,['bug'],https://github.com/openai/openai-python/issues/420,122.14,True,method_order_violation,medium,"The order of returned batched embeddings may not match the input order, leading to potential confusion and incorrect assumptions about the API's behavior.",high,
425,AttributeError: module 'openai' has no attribute 'ModelFineTuning',"### Describe the bug

Traceback (most recent call last):
  File ""ChatGPTPlayground\train_py.py"", line 119, in train
    response = openai.ModelFineTuning.create(model_id=model_id, **params)
AttributeError: module 'openai' has no attribute 'ModelFineTuning'



### To Reproduce

Open AI Version 0.27 
Python 3.9
Open AI installed through pip, running a fresh conda environment. 



### Code snippets

```Python
Code snippit 
    with open(""data.jsonl"", ""r"") as file:
        chunk_size = 100
        while True:
            chunk = list(file.readlines(chunk_size))
            if not chunk:
                break
            data = [json.loads(line) for line in chunk]
            text_data = ""\n"".join([d[""completion""] for d in data])
            params[""training_data""] = [{""text"": text_data}]
            response = openai.ModelFineTuning.create(model_id=model_id, **params)
            print(f""Fine-tuned model on chunk of {len(data)} examples."")
```


### OS

Windows 11

### Python version

v3.9

### Library version

v0.27",closed,2023-05-01T04:18:10Z,2023-05-01T13:34:36Z,['bug'],https://github.com/openai/openai-python/issues/425,9.27,True,input_type_violation,unknown,"The code attempts to access an attribute 'ModelFineTuning' that does not exist in the 'openai' module, indicating a potential type mismatch or incorrect usage of the API.",high,
407,api_key arg for openai.Audio.transcribe doesn't work,"### Describe the bug

```python
    def create_transcription(self, audio_file : str):
            if not os.path.exists(audio_file):
                return None
            file = open(audio_file, 'rb')
            resp = openai.Audio.transcribe(
                api_key=self._API_KEY,
                file=file,
                model='whisper-1',
                response_format='text',
                language='it'
            )
            return resp
```
Using the **api_key** parameter of `openai.Audio.transcribe` raises a  `openai.error.AuthenticationError`.
I know for sure that the api key provided in **self._API_KEY** is correct  because I have already used it in another function.

Looking in the `audio.py` module, the function:
```python
    def transcribe(
        cls,
        model,
        file,
        api_key=None,
        api_base=None,
        api_type=None,
        api_version=None,
        organization=None,
        **params,
    ):
        requestor, files, data = cls._prepare_request(file, file.name, model, **params)
        url = cls._get_url(""transcriptions"")
        response, _, api_key = requestor.request(""post"", url, files=files, params=data)
        return util.convert_to_openai_object(
            response, api_key, api_version, organization
        )
```
does not use the api_key that I am passing it and that it inserts a default key, which is therefore not good.


### To Reproduce

Try to use the function `openai.Audio.transcribe()` with the `api_key` parameter filled with a valid key

### Code snippets

_No response_

### OS

linux

### Python version

Python 3.10.6

### Library version

openai-python-0.27.2",closed,2023-04-19T14:18:24Z,2023-04-28T11:24:08Z,['bug'],https://github.com/openai/openai-python/issues/407,213.1,True,missing_dependency_violation,high,"The api_key parameter is not utilized in the transcribe method, leading to authentication errors despite providing a valid key.",high,
152,Use the openai library in conjunction with the kivy library.,"Hi everyone, I'm messing around with the library a bit as I try to use it in conjunction with Kivy. The project as such works when executing the venv virtualization window within Windows, but when passing the project to .APK it closes unexpectedly.

I have tried simpler projects in the conversion from .py to .apk using the Kivy library and they work perfectly. When I apply my OpenAI command lines the APK does not respond.

I have the project in this repository for possible viewing: https://github.com/FoxyCTG21/chatbotapp_public

For the conversion from .py to .apk I use the buildozer library in a Google Colab notebook: https://colab.research.google.com/github/MagnoEfren/kivy/blob/main/KivyApp_a_APK.ipynb?hl=es

I hope I can understand why I can't use the library at the end.",closed,2022-12-15T12:40:16Z,2023-01-01T21:11:28Z,[],https://github.com/openai/openai-python/issues/152,416.52,True,input_value_violation,performance_degradation,"The application closes unexpectedly when using OpenAI commands in the Kivy environment, indicating potential issues with input values or resource handling.",high,
393,Onboarding link not working,"### Describe the bug

I expected the link in https://github.com/openai/openai-python/blob/d6fa3bfaae69d639b0dd2e9251b375d7070bbef1/openai/util.py#L187 aka https://onboard.openai.com to work.
It didn't

### To Reproduce

https://onboard.openai.com

### Code snippets

_No response_

### OS

Ubuntu 20.04

### Python version

Not applicable

### Library version

Latest",closed,2023-04-13T13:59:08Z,2023-04-19T07:48:14Z,['bug'],https://github.com/openai/openai-python/issues/393,137.82,False,,,,low,
403,`ModuleNotFoundError: No module named 'openai.datalib'` on main branch,"### Describe the bug

Building the following Dockerfile with `docker build .`:

```dockerfile
FROM python:3.8

RUN pip install git+https://github.com/openai/openai-python
RUN python -c 'from openai.api_resources.embedding import Embedding'
```

gives:

```
Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
  File ""/usr/local/lib/python3.8/site-packages/openai/__init__.py"", line 19, in <module>
    from openai.api_resources import (
  File ""/usr/local/lib/python3.8/site-packages/openai/api_resources/__init__.py"", line 7, in <module>
    from openai.api_resources.embedding import Embedding  # noqa: F401
  File ""/usr/local/lib/python3.8/site-packages/openai/api_resources/embedding.py"", line 6, in <module>
    from openai.datalib.numpy_helper import assert_has_numpy
ModuleNotFoundError: No module named 'openai.datalib'
```

### To Reproduce

See above

### Code snippets

```Python
See above
```


### OS

Linux

### Python version

3.8

### Library version

main",closed,2023-04-17T15:16:10Z,2023-04-17T17:59:17Z,['bug'],https://github.com/openai/openai-python/issues/403,2.72,True,missing_dependency_violation,High,"The error indicates that the module 'openai.datalib' is missing, which suggests a dependency that is not properly included or initialized in the package.",high,
253,AttributeError: module 'openai' has no attribute 'ChatCompletion',"### Describe the bug

Hello.

I cant access gpt-3.5-turbo with python because I seem to be getting the error "" AttributeError: module ‘openai’ has no attribute ‘ChatCompletion’""

I have updated to openai v0.27.0, as well as tried using new API keys just incase that was causing the issue. Ive double checked that I actually have updated to the new version of openai 0.27.0 by running “pip list”. Ive also tried "" pip install --upgrade openai "" and “pip install openai-0.27.0-py3-none-any.whl”

Im not sure what else I can try, does anyone have any ideas?
Thanks.

### To Reproduce

import os
import openai
openai.api_key = os.getenv(""key"")

completion = openai.ChatCompletion.create(
  model=""gpt-3.5-turbo"",
  messages=[
    {""role"": ""user"", ""content"": ""Hello!""}
  ]
)


print(completion.choices[0].message)

### Code snippets

_No response_

### OS

macOS

### Python version

Python v3.11.2

### Library version

openai 0.27.0",closed,2023-03-02T13:03:44Z,2023-03-03T14:15:35Z,['bug'],https://github.com/openai/openai-python/issues/253,25.2,True,input_type_violation,unknown,"The error indicates that the 'ChatCompletion' attribute is not found in the 'openai' module, suggesting a potential issue with the library version or improper installation.",high,
296,ERROR in ready_any_format validator: file does not appear to be in valid JSON Format,"### Describe the bug

When I try to use the prepare data tool for fine tuning I always get the following error that i'm not using a valid JSON file. I have even used a valid JSON validator which comes back as valid. I have a feeling this has to do with my environment more than anything. 

from my understanding this tool accepts any data format, even JSON to convert to JSONL

I'm new to python so please excuse my lack of experience with the language. 

I even installed it with just `pip install openai` and still had no luck. along with this I tried using python with pyenv but still had no luck.

And again i'm new to programming so this is one of my first ever big reports, sorry if it's not in the right format if you will

### To Reproduce

Latest version of python3 was installed using home brew with
`brew install python`
`pip3 install openai`

`export OPENAI_API_KEY=""<OPENAI_API_KEY>""`
`openai tools fine_tunes.prepare_data -f <LOCAL_FILE>`

pip v23.0.1

### Code snippets

```Python
user@user-MBP training_models % openai tools fine_tunes.prepare_data -f prompt_completion_pairs_manual.json
Analyzing...


ERROR in read_any_format validator: Your file `prompt_completion_pairs_manual.json` does not appear to be in valid JSON format. Please ensure your file is formatted as a valid JSON file.

Aborting...%
```


### OS

macOS Ventura v13.0.1

### Python version

Python 3.11.2

### Library version

openai-0.27.1",closed,2023-03-10T20:13:24Z,2023-04-10T15:57:17Z,['bug'],https://github.com/openai/openai-python/issues/296,739.73,True,input_type_violation,high,"The input file provided is expected to be in a valid JSON format, but the validator indicates that it is not, leading to an inability to process the data.",high,
206,`openai tools fine_tunes.prepare_data` does not accept indented JSON files,"To reproduce:
```shell
pip install --user openai[datalib]==0.26.4 

# Works fine:
echo '[{""prompt"": ""Here is my example input 1"", ""completion"": ""Complete to 1""}, {""prompt"": ""Here is my example input 2"", ""completion"": ""Complete to 2""}]' > unindented.json
openai tools fine_tunes.prepare_data --quiet --file unindented.json

# Doesn't work:
cat > to_indented.py << EOF
import json
with open('unindented.json', 'rt') as f:
    data = json.loads(f.read())
# Simple rewrite of the ""unindented.json"": output to indented version
with open('indented', 'wt') as f:
    f.write(json.dumps(data, indent=2))
EOF
python to_indented.py
openai tools fine_tunes.prepare_data -f indented.json
```
",closed,2023-01-31T12:57:39Z,2023-04-10T15:56:23Z,[],https://github.com/openai/openai-python/issues/206,1658.98,True,input_type_violation,High,"The API method expects a specific input format (unindented JSON) and fails when provided with a different format (indented JSON), leading to incorrect functionality.",high,
275,Add proxy functionality for request (sync),"### Describe the feature or improvement you're requesting
\openai\api_resources\abstract\engine_api_resource.py    
```python
result = _thread_context.session.request(
                method,
                abs_url,
                headers=headers,
                data=data,
                files=files,
                stream=stream,
                timeout=request_timeout if request_timeout else TIMEOUT_SECS,
            )
```
add param
```python
proxies=openai.proxy
```
I see asynchronous request support, synchronous not. Am I using it incorrectly?

### Additional context

_No response_",closed,2023-03-06T01:10:39Z,2023-04-08T15:31:48Z,[],https://github.com/openai/openai-python/issues/275,806.35,True,missing_dependency_violation,medium,"The request method lacks the necessary proxy parameter, which may lead to incorrect behavior when attempting to use proxy functionality.",high,
380,import openai seems to take a long time,"### Describe the feature or improvement you're requesting

Doing a fresh `import openai` seems to take about 4-5 seconds

(reproducible with `start_time = time.time(); import openai; print(time.time() - start_time)`).  This startup time seems like given the straightforward nature of the repo. 

### Additional context

_No response_",closed,2023-04-05T15:41:08Z,2023-04-08T15:12:23Z,[],https://github.com/openai/openai-python/issues/380,71.52,True,performance_degradation,Medium,"The import time for the openai library is longer than expected, indicating potential performance issues.",high,
383,fine_tunes.prepare_data  ERROR in read_any_format validator: Your file `train.txt` does not appear to be in valid TXT format,"### Describe the bug

hi, 
everyone
When i use fine_tunes to import my train date, there is an error about it. 
Please give me some support about it. thanks. 

### To Reproduce

step1 ： pip3 install --upgrade openai
step2：set OPENAI_API_KEY=""sk-XXXXXXXXX""
step3:  openai tools fine_tunes.prepare_data -f train.txt

### Code snippets

```Python
(base) PS D:\Personal> openai tools fine_tunes.prepare_data -f train.txt
Analyzing...


ERROR in read_any_format validator: Your file `train.txt` does not appear to be in valid TXT format. Please ensure your file is formatted as a valid TXT file.

Aborting...
(base) PS D:\Personal>
```


### OS

windows

### Python version

Python 3.9.13

### Library version

openai-0.27.4",closed,2023-04-06T07:25:21Z,2023-04-08T15:12:09Z,['bug'],https://github.com/openai/openai-python/issues/383,55.78,True,input_type_violation,high,"The provided file `train.txt` does not meet the expected format for input, leading to a validation error.",high,
385,ChatGPT is able to access private GitHub repositories!,"### Describe the bug

I tried to ask ChatGPT for information about a private GitHub repository of mine. To my amazement, ChatGPT was able to tell me about the repository perfectly well, with all the details. What is concerning is that it is able to access a private GitHub repository, which shouldn't happen, as it might be looked at as a 'breach of privacy' by others.

### To Reproduce

1. Enter the prompt: ""Tell me about the GitHub repo: __repoName__""
2. Then on the prompted to enter username: ""Username of Repository owner: __userName__""
3. ChatGpt is able to access the Github repository, and provide information, even though the repository is private.

### Code snippets

_No response_

### OS

Windows 11

### Python version

Python v3.7.1

### Library version

openai-python v0.26.4",closed,2023-04-07T19:38:41Z,2023-04-08T00:16:32Z,['bug'],https://github.com/openai/openai-python/issues/385,4.63,True,missing_dependency_violation,high,"The API appears to access private repository information without proper authorization, indicating a missing dependency on authentication and authorization checks.",high,
325,Delete model,"### Describe the bug

I delete model but don't disappear from list.

### To Reproduce

List

### Code snippets

```Python
{
      ""created_at"": 1679331655,
      ""fine_tuned_model"": ""curie:ft-personal-2023-03-20-17-13-10"",
      ""hyperparams"": {
        ""batch_size"": 1,
        ""learning_rate_multiplier"": 0.1,
        ""n_epochs"": 4,
        ""prompt_loss_weight"": 0.01
      },
      ""id"": """",
      ""model"": ""curie"",
      ""object"": ""fine-tune"",
      ""organization_id"": """",
      ""result_files"": [
        {
          ""bytes"": 1273,
          ""created_at"": 1679332390,
          ""filename"": ""compiled_results.csv"",
          ""id"": ""file-TPbWVieAJoXsL1mNaX2Du3Bz"",
          ""object"": ""file"",
          ""purpose"": ""fine-tune-results"",
          ""status"": ""processed"",
          ""status_details"": null
        }
      ],
      ""status"": ""succeeded"",
      ""training_files"": [
        {
          ""bytes"": 492,
          ""created_at"": 1679331654,
          ""filename"": ""file"",
          ""id"": ""file-C8uOZi48g7mOz39Eml01KOf7"",
          ""object"": ""file"",
          ""purpose"": ""fine-tune"",
          ""status"": ""processed"",
          ""status_details"": null
        }
      ],
      ""updated_at"": 1679332391,
      ""validation_files"": []
    },
```


### OS

Linux

### Python version

Python 3.10.6

### Library version

0.27.2",closed,2023-03-21T12:03:05Z,2023-04-06T14:25:15Z,['bug'],https://github.com/openai/openai-python/issues/325,386.37,True,missing_dependency_violation,incorrect_functionality,"The model deletion operation did not result in the expected outcome of removing the model from the list, indicating a potential missing dependency in the delete process.",high,
372,查询余额时出现错误,"### Describe the bug

我原本期望程序能够查询到我的余额并将其返回，但是似乎出现了一些问题。请问 OpenAI 的开发人员能够提供一些帮助和支持吗？我很想知道我应该如何修改程序才能够正确地查询我的余额。

![image](https://user-images.githubusercontent.com/127756463/229665163-06509b86-b2d6-49fa-a66f-50d9408a8320.png)

如上图，报错：
Traceback (most recent call last):
  File ""C:\Program Files (x86)\Python311\Lib\runpy.py"", line 198, in _run_module_as_main
    return _run_code(code, main_globals, None,
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Program Files (x86)\Python311\Lib\runpy.py"", line 88, in _run_code
    exec(code, run_globals)
  File ""c:\Users\zskj\.vscode\extensions\ms-python.python-2023.6.0\pythonFiles\lib\python\debugpy\adapter/../..\debugpy\launcher/../..\debugpy\__main__.py"", line 39, in <module>
    cli.main()
  File ""c:\Users\zskj\.vscode\extensions\ms-python.python-2023.6.0\pythonFiles\lib\python\debugpy\adapter/../..\debugpy\launcher/../..\debugpy/..\debugpy\server\cli.py"", line 430, in main
    run()
  File ""c:\Users\zskj\.vscode\extensions\ms-python.python-2023.6.0\pythonFiles\lib\python\debugpy\adapter/../..\debugpy\launcher/../..\debugpy/..\debugpy\server\cli.py"", line 284, in run_file
    runpy.run_path(target, run_name=""__main__"")
  File ""c:\Users\zskj\.vscode\extensions\ms-python.python-2023.6.0\pythonFiles\lib\python\debugpy\_vendored\pydevd\_pydevd_bundle\pydevd_runpy.py"", line 321, in run_path
    return _run_module_code(code, init_globals, run_name,
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""c:\Users\zskj\.vscode\extensions\ms-python.python-2023.6.0\pythonFiles\lib\python\debugpy\_vendored\pydevd\_pydevd_bundle\pydevd_runpy.py"", line 135, in _run_module_code
    _run_code(code, mod_globals, init_globals,
  File ""c:\Users\zskj\.vscode\extensions\ms-python.python-2023.6.0\pythonFiles\lib\python\debugpy\_vendored\pydevd\_pydevd_bundle\pydevd_runpy.py"", line 124, in _run_code
    exec(code, run_globals)
  File ""D:\编程\Python\开源仓库\我的仓库\chatgpt-py\使用组织的API密钥.py"", line 21, in <module>
    secrets=manage.get_secret(""openai"")
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\zskj\AppData\Roaming\Python\Python311\site-packages\openai_secret_manager.py"", line 6, in get_secret
    return openai_secret_manager.get_secret(key_name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\zskj\AppData\Roaming\Python\Python311\site-packages\openai_secret_manager.py"", line 6, in get_secret
    return openai_secret_manager.get_secret(key_name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\zskj\AppData\Roaming\Python\Python311\site-packages\openai_secret_manager.py"", line 6, in get_secret
    return openai_secret_manager.get_secret(key_name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  [Previous line repeated 984 more times]
  File ""C:\Users\zskj\AppData\Roaming\Python\Python311\site-packages\openai_secret_manager.py"", line 5, in get_secret
    def get_secret(key_name):

  File ""C:\Users\zskj\AppData\Roaming\Python\Python311\site-packages\openai_secret_manager.py"", line 5, in get_secret
    def get_secret(key_name):

RecursionError: maximum recursion depth exceeded

### To Reproduce

如下面的代码片段所示
我原本期望程序能够查询到我的余额并将其返回，但是似乎出现了一些问题。

### Code snippets

```Python
import openai_secret_manager as manage
import openai
import sys
sys.setrecursionlimit(1000) # 设置递归深度限制为10000

# 获取API KEY和模型ID
secrets=manage.get_secret(""openai"")

# 连接API
# openai.api_key=secrets[""api_key""]
openai.api_key=""sk-FMxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx""

# 查询余额
balance=openai.Organization.balance()

# 输出余额
print(""余额："",balance)
```


### OS

Windows

### Python version

Python v3.11

### Library version

openai库，openai_secret_manager库",closed,2023-04-04T01:51:34Z,2023-04-04T18:49:49Z,['bug'],https://github.com/openai/openai-python/issues/372,16.97,True,missing_dependency_violation,High,"The code attempts to retrieve a secret using 'manage.get_secret' but fails due to a recursion error, indicating a missing dependency in the secret management process.",high,
377,Curl works but Python fails for the same API key and same request,"### Describe the bug

I'm trying to use the ChatCompletion API by following the documentation examples.
I'm getting: `openai.error.InvalidRequestError: Invalid URL (POST /v1/engines/gpt-3.5-turbo/chat/completions)`

- I did check that I'm using ChatCompletion and not Completion.
- I generated another API key to solve the problem.

This is strictly a bug from the Python lib since using curl, the POST request works. 

⚠️ I feel like I should mention I was recently accepted in the GPT-4 beta (according to an email I've received), so it may be due to this ? 

Thanks in advance for your help.


### To Reproduce

Just use the ChatCompletion python example from the docs. This worked fine for me until 2 weeks ago.

### Code snippets

```Python
import openai
openai.api_key=""myAPIkey""
openai.organization = ""myorg"" # I've tried without this as well
openai.ChatCompletion.create(engine=""gpt-3.5-turbo"",messages=[{""role"": ""user"", ""content"":""yo""}])
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/Users/tristan/miniforge3/lib/python3.10/site-packages/openai/api_resources/chat_completion.py"", line 25, in create
    return super().create(*args, **kwargs)
  File ""/Users/tristan/miniforge3/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py"", line 153, in create
    response, _, api_key = requestor.request(
  File ""/Users/tristan/miniforge3/lib/python3.10/site-packages/openai/api_requestor.py"", line 226, in request
    resp, got_stream = self._interpret_response(result, stream)
  File ""/Users/tristan/miniforge3/lib/python3.10/site-packages/openai/api_requestor.py"", line 619, in _interpret_response
    self._interpret_response_line(
  File ""/Users/tristan/miniforge3/lib/python3.10/site-packages/openai/api_requestor.py"", line 679, in _interpret_response_line
    raise self.handle_error_response(
openai.error.InvalidRequestError: Invalid URL (POST /v1/engines/gpt-3.5-turbo/chat/completions)
```

The above code fails, but this works! 
```bash
curl https://api.openai.com/v1/chat/completions \                          
  -H ""Content-Type: application/json"" \
  -H ""Authorization: Bearer $OPENAI_API_KEY"" \
  -d '{
     ""model"": ""gpt-3.5-turbo"",
     ""messages"": [{""role"": ""user"", ""content"": ""Say this is a test!""}],
     ""temperature"": 0.7
   }'
{""id"":""...."",""object"":""chat.completion"",""created"":1680624274,""model"":""gpt-3.5-turbo-0301"",""usage"":{""prompt_tokens"":14,""completion_tokens"":5,""total_tokens"":19},""choices"":[{""message"":{""role"":""assistant"",""content"":""This is a test!""},""finish_reason"":""stop"",""index"":0}]}
```



### OS

macOS Monterey 12.6

### Python version

Python 3.10.6, but fails also in python:3.10 docker image

### Library version

openai-python v0.27.0, also v0.27.3",closed,2023-04-04T16:16:55Z,2023-04-04T16:27:30Z,['bug'],https://github.com/openai/openai-python/issues/377,0.18,True,input_type_violation,high,"The Python API call is using an incorrect parameter name 'engine' instead of 'model', leading to an InvalidRequestError.",high,
306,TypeError upon API response,"### Describe the bug

When executing the API call specified in the code snippet the following error occurs: 
[error.txt](https://github.com/openai/openai-python/files/10995454/error.txt)



Based on printouts, in some cases the API return with an ill-formed Python dictionary as below.


API response with **null** part of the dictionary:
{""error"":{""message"":""The response was filtered due to the prompt triggering Azure OpenAI’s content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766"",
              ""type"":**null**,""param"":""prompt"",""code"":""content_filter"",""status"":400}}


### To Reproduce

Run the code snippet attached on the input 'doc.txt'.

### Code snippets

```Python
def prompt_openai(doc, max_tokens):

    prompt = f""""""
    Create a summary of the text below.

    '{doc}'
    """"""
    try:
        response = openai.Completion.create(
        engine = openai.deployment,
        prompt = prompt,
        temperature = 1,
        max_tokens = max_tokens,
        top_p = 0.95,
        frequency_penalty = 0.5,
        presence_penalty = 0.5,
        stop = None#""<|im_end|>""
        )

    return response['choices'][0]['text'].replace('\n', '').replace(' .', '.').strip()
```


### OS

Windows 10

### Python version

Python 3.10.9 

### Library version

openai 0.27.2",closed,2023-03-14T10:19:26Z,2023-04-04T15:10:14Z,['bug'],https://github.com/openai/openai-python/issues/306,508.85,False,,,,low,
375,Use python to report an error,"### Describe the bug

<img width=""969"" alt=""image"" src=""https://user-images.githubusercontent.com/7127463/229757093-055ea104-3d67-4a94-b29e-b00646ecfb06.png"">


### To Reproduce

<img width=""836"" alt=""image"" src=""https://user-images.githubusercontent.com/7127463/229756890-0b2f7327-bae7-48c0-b60d-2cadda5786d0.png"">

### Code snippets

_No response_

### OS

macOS 12.6

### Python version

Python2.7

### Library version

openai-0.6.4-py2.7.egg-info",closed,2023-04-04T09:59:07Z,2023-04-04T14:44:16Z,['bug'],https://github.com/openai/openai-python/issues/375,4.75,True,input_type_violation,High,"The use of Python 2.7 is incompatible with the latest versions of libraries, leading to potential runtime errors.",high,
365,Suspect issues on python 3.11.2 with error: AttributeError: module 'subprocess' has no attribute 'PIPE' due to white space in file path,"### Describe the bug

I'm facing issues with openai on Python 3.11.2. When I run the demo code, I'm getting the following issues. I suspect there's an issue due to a space in the Python file path. Please let me know of any workaround or fixes:

Please note that the Python script is in a folder: Z. Temp This folder has a space.

/Users/mithun % /usr/local/bin/python3 ""/Users/mithun/Documents/Z. Temp/Python/GPT_Test2.py""
Traceback (most recent call last):
  File ""/Users/mithun/Documents/Z. Temp/Python/GPT_Test2.py"", line 1, in <module>
    import openai
  File ""/usr/local/lib/python3.11/site-packages/openai/__init__.py"", line 9, in <module>
    from openai.api_resources import (
  File ""/usr/local/lib/python3.11/site-packages/openai/api_resources/__init__.py"", line 1, in <module>
    from openai.api_resources.audio import Audio  # noqa: F401
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.11/site-packages/openai/api_resources/audio.py"", line 4, in <module>
    from openai import api_requestor, util
  File ""/usr/local/lib/python3.11/site-packages/openai/api_requestor.py"", line 1, in <module>
    import asyncio
  File ""/usr/local/Cellar/python@3.11/3.11.2_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/__init__.py"", line 8, in <module>
    from .base_events import *
  File ""/usr/local/Cellar/python@3.11/3.11.2_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py"", line 40, in <module>
    from . import events
  File ""/usr/local/Cellar/python@3.11/3.11.2_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/events.py"", line 203, in <module>
    class AbstractEventLoop:
  File ""/usr/local/Cellar/python@3.11/3.11.2_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/events.py"", line 511, in AbstractEventLoop
    stdin=subprocess.PIPE,
          ^^^^^^^^^^^^^^^
AttributeError: module 'subprocess' has no attribute 'PIPE'

However, when I run the sample code from MacOS Desktop, I'm no longer getting the subprocess error, but rather the expected API key error. Note that the file path does not have any spaces.

/Users/mithun % /usr/local/bin/python3 /Users/mithun/Desktop/GPT_Test2.py
Traceback (most recent call last):
  File ""/Users/mithun/Desktop/GPT_Test2.py"", line 5, in <module>
    models = openai.Model.list()
             ^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.11/site-packages/openai/api_resources/abstract/listable_api_resource.py"", line 60, in list
    response, _, api_key = requestor.request(
                           ^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.11/site-packages/openai/api_requestor.py"", line 226, in request
    resp, got_stream = self._interpret_response(result, stream)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.11/site-packages/openai/api_requestor.py"", line 619, in _interpret_response
    self._interpret_response_line(
  File ""/usr/local/lib/python3.11/site-packages/openai/api_requestor.py"", line 682, in _interpret_response_line
    raise self.handle_error_response(
openai.error.AuthenticationError: Incorrect API key provided: sk-dvmDm***************************************po9e. You can find your API key at https://platform.openai.com/account/api-keys.

### To Reproduce

Execute the sample code on MacOS Desktop and another folder with a space in the file path

### Code snippets

```Python
import openai
openai.api_key = ""sk-xxxx""

# list models
models = openai.Model.list()

# print the first model's id
print(models.data[0].id)

# create a completion
completion = openai.Completion.create(model=""ada"", prompt=""Hello world"")

# print the completion
print(completion.choices[0].text)
```


### OS

macOS

### Python version

Python v3.11.2

### Library version

openai-python v0.27.2",closed,2023-04-02T18:36:10Z,2023-04-04T02:27:55Z,['bug'],https://github.com/openai/openai-python/issues/365,31.86,True,input_value_violation,medium,"The presence of a space in the file path may lead to issues when the API attempts to access the file, potentially causing unexpected behavior or errors.",high,
347,"Does the `text-davinci-003` model support 4000, 4096 or 4097 tokens?","### Describe the feature or improvement you're requesting

I'm just trying to make sure that the documentation is really correct, because, often, the documentation doesn't seem to contain all info necessary to understand something. 

[The docs](
https://platform.openai.com/docs/models/gpt-3-5) say that the context length for `text-davinci-003` is <s>4000</s> (now they say 4097). There would be no reason to believe that this is not correct, if most models didn't have a context length of 2048 and 4096 = 2048*2

So, can **an official OpenAI employee/developer** confirm that the context length for `text-davinci-003` is really <s>4000</s> (4097) and not 4096?

Can you please also tell me the actual context length of models like 

- text-curie-001
- text-babbage-001
- text-ada-001

The docs now say 2049, but previously they used to say 2048. 

### Additional context

I already asked this question here https://community.openai.com/t/does-the-text-davinci-003-model-support-4000-or-4096-tokens/89507, but I got an answer from a bot (apparently) or a user that posts useless answers, which I don't know if I can trust. I have also tried to ask for help in the help center, but the chat doesn't even open. See [this](https://community.openai.com/t/is-it-really-not-possible-to-change-the-email-name-and-phone-number-associated-with-an-account/72165). That's why I am asking here, with the hope that I get an **official answer from an official OpenAI qualified employee**.",closed,2023-03-27T15:03:09Z,2023-04-04T02:24:40Z,[],https://github.com/openai/openai-python/issues/347,179.36,True,input_value_violation,unknown,"The issue raises questions about the correct context length for the model, indicating potential discrepancies in the documentation that could lead to improper input values.",medium,
367,Getting a fix response data structure,"### Describe the bug

I sent a prompt and ask to return the result with a fixed structure by adding ""please return these ideas as a JSON Object with the structure { 'Idea':['Title':'string','Description':'string']} and using a maximum of 567 tokens per idea."" to the end of prompt. but I have different result every time I try to get data.

### To Reproduce

            var queryModel = new openAIModel
            {
                model = 'text-davinci-002',
                prompt = '',
                temperature = artificialIntelligencePreference.Temperature,
                max_tokens = maxCompletionLength
            };

  string json = JsonConvert.SerializeObject(queryModel);


  request.AddJsonBody(json);

 var responseContent = templateApi.Execute(request);  --->     response = openai.ChatCompletion.create(model=request_data[""model""],
                                              max_tokens=request_data[""max_tokens""],
                                              temperature=request_data[""temperature""],
                                              messages = request_data[""message""])
   return responseContent.Content;

### Code snippets

```Python
this is the result which I get :

1- {""choices"":[{""finish_reason"":""stop"",""index"":0,""logprobs"":null,""text"":""\n\n1. Join or attend an event for senior .NET developers\n2. Get a mentor to help guide your development journey\n3. Use online resources to improve your understanding of the platform\n4. Use social media to connect with other developers and learn from them\n5. Attend a .NET development conference\n6. Read books and blog posts about .NET development\n7. Watch online video tutorials about .NET development\n8. Take an online course about .NET development\n9. Use Visual Studio to its fullest potential\n10. Get involved in the open source .NET community""}],""created"":1680191774,""id"":""cmpl-6zovGa10EKPEVJBpnB1ZkiRmkHcmg"",""model"":""text-davinci-002"",""object"":""text_completion"",""usage"":{""completion_tokens"":119,""prompt_tokens"":38,""total_tokens"":157}}

2-{""choices"":[{""finish_reason"":""stop"",""index"":0,""logprobs"":null,""text"":""\n\n1. ASP.NET Core MVC\n\nDescription: ASP.NET Core MVC is a framework for building web applications using the Model-View-Controller design pattern. It is a powerful tool for creating rich and dynamic web applications.\n\n2. C#\n\nDescription: C# is a powerful object-oriented programming language that enables developers to create robust and scalable applications.\n\n3. .NET Framework\n\nDescription: The .NET Framework is a comprehensive and integrated development platform that provides a wide range of services and libraries for building applications.\n\n4. SQL Server\n\nDescription: SQL Server is a relational database management system that enables developers to create and manage databases.\n\n5. Visual Studio\n\nDescription: Visual Studio is an integrated development environment that provides a complete set of tools for developing applications.""}],""created"":1680192829,""id"":""cmpl-6zpCHmuiUqbaWy8L1mP2ptcz2kuFh"",""model"":""text-davinci-002"",""object"":""text_completion"",""usage"":{""completion_tokens"":170,""prompt_tokens"":64,""total_tokens"":234}}
```


### OS

windows10

### Python version

python-dotenv==0.21.1

### Library version

openai-python v0.26.04",closed,2023-04-03T18:48:33Z,2023-04-04T00:15:32Z,['bug'],https://github.com/openai/openai-python/issues/367,5.45,True,missing_option_violation,medium,"The API call does not adhere to the specified prompt structure, leading to inconsistent response formats.",high,
303,openai.Audio.transcribe's `api_key` does not work; openai.error.AuthenticationError is raised,"### Describe the bug

When passing the API Key to the `api_key` parameter of the `openai.Audio.transcribe()` method, an `openai.error.AuthenticationError` is thrown.

```
Traceback (most recent call last):
  File ""<stdin>"", line 2, in <module>
  File ""/.../venv/lib/python3.10/site-packages/openai/api_resources/audio.py"", line 55, in transcribe
    requestor, files, data = cls._prepare_request(file, file.name, model, **params)
  File ""/.../venv/lib/python3.10/site-packages/openai/api_resources/audio.py"", line 28, in _prepare_request
    requestor = api_requestor.APIRequestor(
  File ""/.../venv/lib/python3.10/site-packages/openai/api_requestor.py"", line 130, in __init__
    self.api_key = key or util.default_api_key()
  File ""/.../venv/lib/python3.10/site-packages/openai/util.py"", line 186, in default_api_key
    raise openai.error.AuthenticationError(
openai.error.AuthenticationError: No API key provided. You can set your API key in code using 'openai.api_key = <API-KEY>', or you can set the environment variable OPENAI_API_KEY=<API-KEY>). If your API key is stored in a file, you can point the openai module at it with 'openai.api_key_path = <PATH>'. You can generate API keys in the OpenAI web interface. See https://onboard.openai.com for details, or email support@openai.com if you have any questions.
```

Upon inspecting the trace back, it seems that the cause is that the `api_key` is not passed to the `_prepare_request` method.

https://github.com/openai/openai-python/blob/6c23b7f62d92e0fabdd9db72bd38d2038caeb524/openai/api_resources/audio.py#L55

When making the following modification, the AuthenticationError is not thrown and the transcription result is obtained.

```diff
-        requestor, files, data = cls._prepare_request(file, file.name, model, **params)
+        requestor, files, data = cls._prepare_request(file, file.name, model, api_key=api_key, **params)
```


### To Reproduce

1. `pip install openai`
2. Create sample.wav (example: `say 親譲りの無鉄砲で子供の時から損ばかりしている -o sample.wav --data-format=LEF32@16000`)
3. Run the following snippets

### Code snippets

```Python
>>> OPENAI_API_KEY = ""sk-***""
>>> import openai
>>> with open(""sample.wav"", ""rb"") as audio_file:
...   transcript = openai.Audio.transcribe(""whisper-1"", audio_file, api_key=OPENAI_API_KEY)
```


### OS

macOS

### Python version

Python 3.10.9

### Library version

openai-python 0.27.2",closed,2023-03-13T16:58:16Z,2023-04-03T21:26:29Z,['bug'],https://github.com/openai/openai-python/issues/303,508.47,True,missing_dependency_violation,high,"The 'api_key' parameter is not passed to the '_prepare_request' method, leading to an AuthenticationError.",high,
343,Issue with updating the openai.proxy field,"### Describe the bug

I'm experiencing an issue with updating the openai.proxy . When I make a request to OpenAI using this library, I set the openai.proxy field to a proxy value. However, if the request fails and I change the openai.proxy field to a different value, subsequent requests continue to use the old proxy value instead of the updated one.

### To Reproduce

1. Create an array of two elements, where the first element is an invalid proxy and the second element is a valid one.
2. Run a loop over this array, updating the value of openai.proxy each time, and attempt to make requests to the OpenAI API.
3. Requests will fail on all proxies, as the value of openai.proxy does not seem to change. Additionally, if a valid proxy is placed first, all requests will succeed, as if all proxies were valid.

### Code snippets

_No response_

### OS

ubuntu

### Python version

python:3.8.3

### Library version

openai==0.27.2",closed,2023-03-26T16:54:56Z,2023-04-01T20:26:20Z,['bug'],https://github.com/openai/openai-python/issues/343,147.52,True,missing_dependency_violation,medium,"The issue arises from the failure to update the openai.proxy field correctly, indicating a missing dependency between the proxy value updates and the request execution. Subsequent requests do not reflect the updated proxy value, leading to incorrect behavior.",high,
302,How can ChatGPT API support multiple conversations with an account?,"### Describe the feature or improvement you're requesting

How can ChatGPT API support multiple conversations with an account?

### Additional context

_No response_",closed,2023-03-13T10:45:38Z,2023-03-13T15:56:41Z,[],https://github.com/openai/openai-python/issues/302,5.18,True,missing_dependency_violation,medium,"The request lacks necessary context on how multiple conversations should be managed, indicating a missing dependency on required configuration or context management.",high,
56,APIConnectionError: Error communicating with OpenAI,"Running into certificate errors when trying to ping the API. Happens both in `virtualenv` and `regular` Python

```
Traceback (most recent call last):
  File ""/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/openai/api_requestor.py"", line 255, in request_raw
    result = _thread_context.session.request(
  File ""/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/requests/sessions.py"", line 542, in request
    resp = self.send(prep, **send_kwargs)
  File ""/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/requests/sessions.py"", line 655, in send
    r = adapter.send(request, **kwargs)
  File ""/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/requests/adapters.py"", line 514, in send
    raise SSLError(e, request=request)
requests.exceptions.SSLError: HTTPSConnectionPool(host='api.openai.com', port=443): Max retries exceeded with url: /v1/engines (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self signed certificate in certificate chain (_ssl.c:1131)')))

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File ""search.py"", line 21, in <module>
    main()
  File ""search.py"", line 9, in main
    engines = openai.Engine.list()
  File ""/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/openai/api_resources/abstract/listable_api_resource.py"", line 27, in list
    response, _, api_key = requestor.request(
  File ""/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/openai/api_requestor.py"", line 98, in request
    result = self.request_raw(
  File ""/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/openai/api_requestor.py"", line 265, in request_raw
    raise error.APIConnectionError(""Error communicating with OpenAI"") from e
openai.error.APIConnectionError: Error communicating with OpenAI
```",closed,2021-12-20T01:10:28Z,2021-12-21T01:35:04Z,[],https://github.com/openai/openai-python/issues/56,24.41,True,missing_dependency_violation,high,"The API connection fails due to missing or invalid SSL certificate verification, indicating a dependency on proper certificate setup that is not met.",high,
346,one shot whisper to chat completion,"### Describe the feature or improvement you're requesting

Probably not the right place, but it would improve the api tremendously if instead of a text prompt of a chat completion request to add a feature where an audio file can be transcribed and interpreted at once.

### Additional context

Right now it is necessary to do two calls.
first: to whisper
second: to gpt completion with the response of whisper.",closed,2023-03-26T20:30:25Z,2023-04-01T02:51:01Z,['enhancement'],https://github.com/openai/openai-python/issues/346,126.34,True,missing_dependency_violation,medium,"The current API requires two separate calls for transcription and interpretation, indicating a missing dependency in handling audio input directly for chat completion.",high,
358,request_id is not work,"### Describe the bug

I want to write a simple chatpgt interaction program that can record the history of conversations each time chatgpt talks to the user. According to the suggestion on the web, when chatgpt returns the result, I need to get the request_id of it and pass it to the next conversation. But when I do this, it doesn't work

Here is the result of the run:
![image](https://user-images.githubusercontent.com/52828870/228777091-237d0fb5-2f66-47ba-9679-9e7b3a3f0c37.png)


### To Reproduce

just run the following code 👇

### Code snippets

```Python
this is my demo：

import openai
openai.api_key = ""sk-......""

contents = [
    ""hello, my name is todo"",
    ""do you know my name in this session""
]
id = None
for content in contents:
    chat = openai.ChatCompletion.create(
            model=""gpt-3.5-turbo"",
            messages=[
                {""role"": ""user"", ""content"": content}
            ],
            user = ""user0001"",
            request_id = id
        )
    message = chat.choices[0].message.content
    id = chat.id
    print(""message: "", message)
```
```


### OS

Linux team317 5.15.90.1-microsoft-standard-WSL2

### Python version

python v3.10

### Library version

openai-python 0.27.2",closed,2023-03-30T08:29:24Z,2023-03-31T03:57:13Z,['bug'],https://github.com/openai/openai-python/issues/358,19.46,True,missing_dependency_violation,high,"The request_id is not being properly initialized or passed in the context of the API call, leading to potential issues in maintaining conversation history.",high,
360,Invalid URL (POST /v1/completions),"### Describe the bug

CHATGPT 4 will not allow new requests even though user haven't hit their request limitation and have paid for the plus subscription. They are also noticing their payment information has been removed from their account. They have found paying for an additional subscription has fixed the issue, even though they already have an active subscription.

### To Reproduce

open chatgpt4 and enter any request
The error pops up

### Code snippets

```Python
Invalid URL (POST /v1/completions)
```


### OS

macOS

### Python version

PYTHON V.7.1

### Library version

OPENAI-PYTHON V0.26.4",closed,2023-03-30T15:35:48Z,2023-03-30T20:23:33Z,['bug'],https://github.com/openai/openai-python/issues/360,4.8,True,input_value_violation,unknown,"The error indicates an invalid URL, suggesting that the input value for the API request is not formatted correctly or is out of acceptable bounds.",high,
357,Can I request my account balance in Python?,"### Describe the feature or improvement you're requesting

Can I request my account balance in Python, rather than viewing https://platform.openai.com/account/usage.

### Additional context

_No response_",closed,2023-03-30T03:12:24Z,2023-03-30T18:36:03Z,[],https://github.com/openai/openai-python/issues/357,15.39,False,,,,low,
354,The actual result and expected results are not matching after fine tuning.,"### Describe the bug

After fine tuning the model with custom data, there is a difference in the actual result and expected result.

### To Reproduce

Example:
{""prompt"":""Who is the highest scorer in the NBA?"",""completion"":"" The highest scorer in the NBA is currently Stephen Curry, with an average of 30.1 points per game.""}

Prompt: Who is the highest scorer in the NBA?
Actual Result: The highest scorer in the NBA is currently Stephen Curry, with an average of 30.1 points per game. 
This player has an average field goal percentage of 58.5%. This player has an average 3-point field goal percentage of 56.3%. This player has an average free throw percentage of 64.9%. This player has an average field goal percentage of 54.7%. This player has an average 3-point field goal percentage of 54.7%. This player has an average free throw percentage of 51.1%. This player has an average field goal percentage of 49.8%. This player has an average 3-point field goal percentage of 55.2%. This player has an average free throw percentage of 55.2%. This player has an average 3-point field goal percentage of 56.3%. This player has an average free throw percentage of 54.7%. This player has an average 3-point field goal percentage of 54.7%. This player has an average free throw percentage of 51.1%. This player has an average field goal percentage of 51.1%. This player has an average 3-point field goal percentage of 56.3%. This player has an average free throw percentage of 54.7%. This player

Expected Result:  The highest scorer in the NBA is currently Stephen Curry, with an average of 30.1 points per game.

### Code snippets

```Python

    # Generate a response from the fine-tuned model
    response = openai.Completion.create(
        engine=model_id,
        prompt=prompt,
        max_tokens=1024,
        n=1,
        stop=None,
        temperature=0.7
    width=80, height=20)
```


### OS

macOS

### Python version

3.8

### Library version

openai 0.27.2",closed,2023-03-29T19:26:57Z,2023-03-30T04:00:04Z,['bug'],https://github.com/openai/openai-python/issues/354,8.55,True,input_value_violation,incorrect_functionality,"The actual result contains excessive and repetitive information compared to the expected result, indicating a deviation from the expected output format.",high,
209,Override base url and headers with gateway data,"So in my company we use an api gateway (Gravitee) to provide a central point for all apis (internal and external).

I read in the docs that the base url can be changed, but it looked to me that this is only intended for OpenAI on Azure.
I also didn't find a way to easily override the headers of all requests. As we use our own api key internally and let the gateway transform the headers.

Question 1: How can we change the base url to our gateway?
Question 2: How can we use different headers for all requests?

I have not that much experience with python, so very likely that I overlooked something.
All help is greatly appreciated :)  ",closed,2023-02-02T12:31:20Z,2023-03-30T04:04:00Z,[],https://github.com/openai/openai-python/issues/209,1335.54,True,missing_dependency_violation,medium,"The issue indicates a lack of clarity on how to configure the API to use a custom base URL and headers, suggesting missing documentation or dependencies in the API contract.",high,
295,Version argument missing,"### Describe the feature or improvement you're requesting

CLI is missing a version argument. 

openai --version would be a nice improvement

### Additional context

_No response_",closed,2023-03-10T06:15:24Z,2023-03-30T04:00:44Z,[],https://github.com/openai/openai-python/issues/295,477.76,True,missing_option_violation,medium,"The CLI is missing a version argument option, which is a recommended feature for better usability.",high,
326,OpenAI APR error code,"### Describe the bug

API Error on OpenAI when calling in the visual studio code. I don't know how to fix this.

raise error.APIConnectionError(
openai.error.APIConnectionError: Error communicating with OpenAI: HTTPSConnectionPool(host='api.openai.com', port=443): Max retries exceeded with url: /v1/chat/completions (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:992)')))

### To Reproduce

1. install visual studio code
2. enter:
import openai

openai.api_key = ""API Key""

completion = openai.ChatCompletion.create(model=""gpt-3.5-turbo"", messages=[{""role"": ""user"", ""content"": ""Write an essay about penguins""}])
print(completion.choices[0].message.content)
3. Run: issue occurred

### Code snippets

_No response_

### OS

window

### Python version

Python v2022.10.0

### Library version

n/a",closed,2023-03-21T19:59:33Z,2023-03-30T03:53:15Z,['bug'],https://github.com/openai/openai-python/issues/326,199.9,True,input_value_violation,unknown,"The error indicates a failure in SSL certificate verification, which may stem from an invalid or improperly configured API key or environment setup.",medium,
290,newline char(`\n`) in stop param leads to 500 error in gpt-3.5-turbo ,"### Describe the bug

unlike `text-davinci-003` model, `gpt-3.5-turbo` model fails when using stop='\n'. 
maybe it would be backend problem but it would be good to have front-end translates '\n' into some form that backend server can deal with.





### To Reproduce

with openai(0.27.1) package 
running this line of code would result in 500 error.



### Code snippets

```Python
completion = openai.ChatCompletion.create(
  model=""gpt-3.5-turbo"",
  messages=[{""role"": ""user"", ""content"":""hello""}], 
  stop=['\n']
)
```
```


### OS

Ubuntu 18.04

### Python version

Python3.7.15

### Library version

v0.27.1",closed,2023-03-09T03:49:42Z,2023-03-29T23:54:59Z,['bug'],https://github.com/openai/openai-python/issues/290,500.09,True,input_value_violation,high,"The use of newline character ('\n') in the stop parameter is not acceptable for the gpt-3.5-turbo model, leading to a 500 error.",high,
305,Make two discuss with each other,"### Please move to discussions, I mistakenly opened an issue.

How about a concept where it's not us asking the questions to the AI, but reversing the process of it, making a one that asks questions, either randomized, or by providing them with specific subject to the other one?

It could be very interesting and some of the results on high enough computing power, could actually lead to answers in many fields that are unanswered. The bottleneck in this technology is how slow people think compared to computers, and provide data input at such a slow pace, it's inadequate to the potential of this technology.

Let's imagine a situation where I have a list of 100 options to choose from a book, and want to ask ChatGPT which either of them means, does and which is the best for a given subject to get the best result That's time consuming. I'd probably use optical test recognition and feed it to a file, making corrections if necessary, and that's the main problem - it's too slow. If we had two of them discussing that subject and not limited to only to only 100 options, maybe even letting them do the optical recognitions themselves, I bet the results could be very interesting, potentially leading to discoveries of new solutions to problems.

There's a lot of data and knowledge that can be combined to create new ideas, and having this kind of a tool to work with it, it would be a shame not to try.

### Additional context

_No response_",closed,2023-03-14T06:39:43Z,2023-03-29T23:14:43Z,[],https://github.com/openai/openai-python/issues/305,376.58,False,,,,,
283,"use openai cli ""chat_completions.create"" failed to generate a response of sufficient length, and setting ""--max-tokens"" did not work.","### Describe the bug

use openai cli “chat_completions.create"" failed to generate a response of sufficient length, and setting ""--max-tokens"" did not work.

### To Reproduce

By default, the number of tokens the model can return will be (4096 - prompt tokens). but not in cli mode 

### Code snippets

```
openai api chat_completions.create  -m gpt-3.5-turbo -g user '请给出10个使用chatGPT的示例'
```

### OS

Ubuntu

### Python version

Python v3.10.6

### Library version

openai v0.27.0",closed,2023-03-07T05:47:54Z,2023-03-29T22:47:30Z,['bug'],https://github.com/openai/openai-python/issues/283,544.99,True,input_value_violation,performance_degradation,"The '--max-tokens' parameter did not function as expected, leading to insufficient response length.",high,
355,pip install openai==0.27.2 is not working,"### Describe the bug

I'm getting this error when trying to install latest version of openai python pip package.

```
ERROR: Ignored the following versions that require a different python version: 0.11.0 Requires-Python >=3.7.1; 0.11.1 Requires-Python >=3.7.1; 0.11.2 Requires-Python >=3.7.1; 0.11.3 Requires-Python >=3.7.1; 0.11.4 Requires-Python >=3.7.1; 0.11.5 Requires-Python >=3.7.1; 0.11.6 Requires-Python >=3.7.1; 0.12.0 Requires-Python >=3.7.1; 0.13.0 Requires-Python >=3.7.1; 0.14.0 Requires-Python >=3.7.1; 0.15.0 Requires-Python >=3.7.1; 0.16.0 Requires-Python >=3.7.1; 0.18.0 Requires-Python >=3.7.1; 0.18.1 Requires-Python >=3.7.1; 0.19.0 Requires-Python >=3.7.1; 0.20.0 Requires-Python >=3.7.1; 0.22.0 Requires-Python >=3.7.1; 0.22.1 Requires-Python >=3.7.1; 0.23.0 Requires-Python >=3.7.1; 0.23.1 Requires-Python >=3.7.1; 0.24.0 Requires-Python >=3.7.1; 0.25.0 Requires-Python >=3.7.1; 0.26.0 Requires-Python >=3.7.1; 0.26.1 Requires-Python >=3.7.1; 0.26.2 Requires-Python >=3.7.1; 0.26.3 Requires-Python >=3.7.1; 0.26.4 Requires-Python >=3.7.1; 0.26.5 Requires-Python >=3.7.1; 0.27.0 Requires-Python >=3.7.1; 0.27.1 Requires-Python >=3.7.1; 0.27.2 Requires-Python >=3.7.1
ERROR: Could not find a version that satisfies the requirement openai==0.27.2 (from versions: 0.0.2, 0.1.0, 0.1.1, 0.1.2, 0.1.3, 0.2.0, 0.2.1, 0.2.3, 0.2.4, 0.2.5, 0.2.6, 0.3.0, 0.4.0, 0.6.0, 0.6.1, 0.6.2, 0.6.3, 0.6.4, 0.7.0, 0.8.0, 0.9.0, 0.9.1, 0.9.2, 0.9.3, 0.9.4, 0.10.0, 0.10.1, 0.10.2, 0.10.3, 0.10.4, 0.10.5)
ERROR: No matching distribution found for openai==0.27.2
```
I'm on conda environment with python 3.10.10.

`pip install openai` seems to install older version of 0.8.0 on my system and not the latest version, my pip is updated.

```
Collecting openai
  Using cached openai-0.10.5.tar.gz (157 kB)
  Preparing metadata (setup.py) ... done
Requirement already satisfied: requests>=2.20 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from openai) (2.28.1)
Requirement already satisfied: tqdm in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from openai) (4.64.1)
  Using cached openai-0.10.4.tar.gz (157 kB)
  Preparing metadata (setup.py) ... done
  Using cached openai-0.10.3.tar.gz (157 kB)
  Preparing metadata (setup.py) ... done
  Using cached openai-0.10.2.tar.gz (156 kB)
  Preparing metadata (setup.py) ... done
  Using cached openai-0.10.1.tar.gz (155 kB)
  Preparing metadata (setup.py) ... done
  Using cached openai-0.10.0.tar.gz (155 kB)
  Preparing metadata (setup.py) ... done
  Using cached openai-0.9.4.tar.gz (156 kB)
  Preparing metadata (setup.py) ... done
  Using cached openai-0.9.3.tar.gz (155 kB)
  Preparing metadata (setup.py) ... done
  Using cached openai-0.9.2.tar.gz (155 kB)
  Preparing metadata (setup.py) ... done
  Using cached openai-0.9.1.tar.gz (156 kB)
  Preparing metadata (setup.py) ... done
  Using cached openai-0.9.0.tar.gz (155 kB)
  Preparing metadata (setup.py) ... done
  Using cached openai-0.8.0-py3-none-any.whl
Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from requests>=2.20->openai) (1.26.12)
Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from requests>=2.20->openai) (2022.9.24)
Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from requests>=2.20->openai) (3.4)
Requirement already satisfied: charset-normalizer<3,>=2 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from requests>=2.20->openai) (2.1.1)
Installing collected packages: openai
Successfully installed openai-0.8.0
```

### To Reproduce

pip install openai==0.27.2 on M1 Macbook

### Code snippets

_No response_

### OS

macOS

### Python version

3.10.10

### Library version

0.8.0",closed,2023-03-29T21:07:09Z,2023-03-29T21:30:29Z,['bug'],https://github.com/openai/openai-python/issues/355,0.39,True,input_value_violation,high,"The user is attempting to install a specific version of the openai package (0.27.2) that requires a Python version >=3.7.1, but the installation fails due to the user's environment not meeting this requirement.",high,
231,openai.Completion.retrieve(),"### Describe the bug

**EngineAPIResource.__init__() takes from 1 to 2 positional arguments but 3 were given**

### To Reproduce

does this only happen to me when using 'retrieve' in openai.
i got this error :  **""EngineAPIResource.__init__() takes from 1 to 2 positional arguments but 3 are given""**


### Code snippets

```Python
key = ""my_api_key""
openai.api_key=key
id_chat = ""my_completion_id""
try:
    result = openai.Completion.retrieve(id_chat)
    print(result['choices'][0]['text'])
except Exception as a:
    print(a)
```


### OS

arch linux

### Python version

Python 3.10.9

### Library version

openai-python v0.26.5",closed,2023-02-28T14:32:39Z,2023-03-07T03:14:50Z,['bug'],https://github.com/openai/openai-python/issues/231,156.7,True,input_type_violation,high,"The method 'EngineAPIResource.__init__()' is receiving an incorrect number of positional arguments, indicating a mismatch between the expected and provided input types.",high,
348,Asynchronous `openai.Completion.acreate` method fails for ChatGPT.,"### Describe the bug

The following minimal code

```
messages=[
  {""role"": ""system"", ""content"": ""be concise""},
  {""role"": ""user"", ""content"": ""async test""}
]

async for segment in await openai.Completion.acreate(
    model=""gpt-3.5-turbo"",
    temperature=0.2,
    n=1,
    prompt=messages,
    stream=True,
):
    print(segment)
```

will yield the error

```
openai.error.InvalidRequestError: [{'role': 'system', 'content': 'be concise'}, {'role': 'user', 'content': 'async test'}] is valid under each of {'type': 'array', 'minItems': 1, 'items': {'oneOf': [{'type': 'integer'}, {'type': 'object', 'properties': {'buffer': {'type': 'string', 'description': 'A serialized numpy buffer'}, 'shape': {'type': 'array', 'items': {'type': 'integer'}, 'description': 'Array shape'}, 'dtype': {'type': 'string', 'description': 'Stringified dtype'}, 'token': {'type': 'string'}}}]}, 'example': '[1, 1313, 451, {""buffer"": ""abcdefgh"", ""shape"": [1024], ""dtype"": ""float16""}]'}, {'type': 'array', 'minItems': 1, 'maxItems': 2048, 'items': {'oneOf': [{'type': 'string'}, {'type': 'object', 'properties': {'buffer': {'type': 'string', 'description': 'A serialized numpy buffer'}, 'shape': {'type': 'array', 'items': {'type': 'integer'}, 'description': 'Array shape'}, 'dtype': {'type': 'string', 'description': 'Stringified dtype'}, 'token': {'type': 'string'}}}], 'default': '', 'example': 'This is a test.', 'nullable': False}} - 'prompt'
```

and setting `prompt=messages,` to `prompt=json.loads(messages),`

yields

```
openai.error.InvalidRequestError: This is a chat model and not supported in the v1/completions endpoint. Did you mean to use v1/chat/completions?
```

It is also notable that the synchronous version `openai.Completion.create(...)` will require the use of `messages=messages` instead of `prompt=messages`. The parameter `messages` does not exist as an `acreate` function parameter.

The synchronous version works as expected, but due to its implementation it will block the entire async loop, which `openai.Completion.acreate(...)` is supposed to fix.

### To Reproduce

Instead of using

```
messages=[
  {""role"": ""system"", ""content"": ""be concise""},
  {""role"": ""user"", ""content"": ""async test""}
]

completion_stream = openai.ChatCompletion.create(
  model=""gpt-3.5-turbo"",
  temperature=0.2,
  n=1,
  messages=messages,
  stream=True,
)
for segment in completion_stream:
  print(segment)
```

use

```
messages=[
  {""role"": ""system"", ""content"": ""be concise""},
  {""role"": ""user"", ""content"": ""async test""}
]

async for segment in await openai.Completion.acreate(
    model=""gpt-3.5-turbo"",
    temperature=0.2,
    n=1,
    prompt=messages,
    stream=True,
):
    print(segment)
```

### Code snippets

_No response_

### OS

Ubuntu 20.04

### Python version

Python v3.8.10

### Library version

openai-python v0.27.2",closed,2023-03-27T17:15:43Z,2023-03-27T17:19:27Z,['bug'],https://github.com/openai/openai-python/issues/348,0.06,True,input_type_violation,high,"The `acreate` method is being called with a `prompt` parameter that is not compatible with the expected input type, as it requires a different structure for chat models.",high,
234,OpenAI Install Error when trying to install via a proxy server,"### Describe the bug

 pip install --proxy http://userid:passwd@56.0.142.24:8080 openai


Results in following error
Collecting openai
  Using cached openai-0.26.5.tar.gz (55 kB)
  Installing build dependencies ... error
  error: subprocess-exited-with-error

  × pip subprocess to install build dependencies did not run successfully.
  │ exit code: 1
  ╰─> [7 lines of output]
      WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProxyError('Your proxy appears to only use HTTP and not HTTPS, try changing your proxy URL to be HTTP. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#https-proxy-error-http-proxy', SSLError(SSLError(1, '[SSL: WRONG_VERSION_NUMBER] wrong version number (_ssl.c:992)')))': /simple/setuptools/
      WARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProxyError('Your proxy appears to only use HTTP and not HTTPS, try changing your proxy URL to be HTTP. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#https-proxy-error-http-proxy', SSLError(SSLError(1, '[SSL: WRONG_VERSION_NUMBER] wrong version number (_ssl.c:992)')))': /simple/setuptools/
      WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProxyError('Your proxy appears to only use HTTP and not HTTPS, try changing your proxy URL to be HTTP. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#https-proxy-error-http-proxy', SSLError(SSLError(1, '[SSL: WRONG_VERSION_NUMBER] wrong version number (_ssl.c:992)')))': /simple/setuptools/
      WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProxyError('Your proxy appears to only use HTTP and not HTTPS, try changing your proxy URL to be HTTP. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#https-proxy-error-http-proxy', SSLError(SSLError(1, '[SSL: WRONG_VERSION_NUMBER] wrong version number (_ssl.c:992)')))': /simple/setuptools/
      WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProxyError('Your proxy appears to only use HTTP and not HTTPS, try changing your proxy URL to be HTTP. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#https-proxy-error-http-proxy', SSLError(SSLError(1, '[SSL: WRONG_VERSION_NUMBER] wrong version number (_ssl.c:992)')))': /simple/setuptools/
      ERROR: Could not find a version that satisfies the requirement setuptools (from versions: none)
      ERROR: No matching distribution found for setuptools
      [end of output]

  note: This error originates from a subprocess, and is likely not a problem with pip.
error: subprocess-exited-with-error

× pip subprocess to install build dependencies did not run successfully.
│ exit code: 1
╰─> See above for output.


### To Reproduce

run with appropriate values
pip install --proxy http://userid:passwd@proxyserver:port openai

### Code snippets

_No response_

### OS

Windows 10 Enterprise

### Python version

Python 3.11.2

### Library version

openai-0.26.5",closed,2023-02-28T23:53:49Z,2023-03-01T20:13:00Z,['bug'],https://github.com/openai/openai-python/issues/234,20.32,True,input_value_violation,high,"The proxy server configuration provided is incorrect, leading to connection errors and preventing the installation of the OpenAI library.",high,
328,openai.error.APIConnectionError: Error communicating with OpenAI: HTTPSConnectionPool,"### Describe the bug

```python3
import openai

# Set your API key
openai.api_key = 'sk...'  # I have filled in my own keys for this
# Example OpenAI Python library request
MODEL = ""gpt-3.5-turbo""
response = openai.ChatCompletion.create(
    model=MODEL,
    messages=[
        {""role"": ""system"", ""content"": ""你是一个乐于助人的助手.""},
        {""role"": ""user"", ""content"": ""咚咚.""},
        {""role"": ""assistant"", ""content"": ""是谁？""},
        {""role"": ""user"", ""content"": ""橙.""},
    ],
    temperature=0,
)

print(response)
```

### I'm sure
- My server is from the United States, there are no regional restrictions
- I have tested several servers, but the result is the same

### 1. The above exception was the direct cause of the following exception:
```
Traceback (most recent call last):
  File ""/work/code/github/image_ai/main.py"", line 34, in <module>
    response = openai.ChatCompletion.create(
  File ""/home/V01/extittivns03/.pyenv/versions/3.10.10/lib/python3.10/site-packages/openai/api_resources/chat_completion.py"", line 25, in create
    return super().create(*args, **kwargs)
  File ""/home/V01/extittivns03/.pyenv/versions/3.10.10/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py"", line 153, in create
    response, _, api_key = requestor.request(
  File ""/home/V01/extittivns03/.pyenv/versions/3.10.10/lib/python3.10/site-packages/openai/api_requestor.py"", line 216, in request
    result = self.request_raw(
  File ""/home/V01/extittivns03/.pyenv/versions/3.10.10/lib/python3.10/site-packages/openai/api_requestor.py"", line 528, in request_raw
    raise error.APIConnectionError(
openai.error.APIConnectionError: Error communicating with OpenAI: HTTPSConnectionPool(host='api.openai.com', port=443): Max retries exceeded with url: /v1/chat/completions (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f4c743e5de0>: Failed to establish a new connection: [Errno 101] Network is unreachable')
```

### 2. The above exception was the direct cause of the following exception:

```
Traceback (most recent call last):
  File ""test.py"", line 7, in <module>
    response = openai.ChatCompletion.create(
  File ""/home/ubuntu/.local/lib/python3.8/site-packages/openai/api_resources/chat_completion.py"", line 25, in create
    return super().create(*args, **kwargs)
  File ""/home/ubuntu/.local/lib/python3.8/site-packages/openai/api_resources/abstract/engine_api_resource.py"", line 153, in create
    response, _, api_key = requestor.request(
  File ""/home/ubuntu/.local/lib/python3.8/site-packages/openai/api_requestor.py"", line 226, in request
    resp, got_stream = self._interpret_response(result, stream)
  File ""/home/ubuntu/.local/lib/python3.8/site-packages/openai/api_requestor.py"", line 619, in _interpret_response
    self._interpret_response_line(
  File ""/home/ubuntu/.local/lib/python3.8/site-packages/openai/api_requestor.py"", line 674, in _interpret_response_line
    raise error.APIError(
openai.error.APIError: HTTP code 502 from API (<html>
<head><title>502 Bad Gateway</title></head>
<body>
<center><h1>502 Bad Gateway</h1></center>
<hr><center>nginx</center>
</body>
</html>
```

### To Reproduce

1. run code

### Code snippets

_No response_

### OS

ubuntu 20.04 tls

### Python version

python 3.8 3.10

### Library version

openai-python V0.27.2",closed,2023-03-22T02:34:52Z,2023-03-22T02:56:31Z,['bug'],https://github.com/openai/openai-python/issues/328,0.36,True,missing_dependency_violation,unknown,"The issue appears to stem from a network connectivity problem, indicating a missing dependency on a stable internet connection for API communication.",high,
327,InvalidRequestError: 'answers' is not one of ['fine-tune'] - 'purpose',"### Describe the bug

Following the steps used to create an answers service at: https://platform.openai.com/docs/guides/answers


While trying the following command: 
openai.File.create(file=open(""data.json""), purpose='answers')

Getting the following error: 
---------------------------------------------------------------------------
InvalidRequestError                       Traceback (most recent call last)
Cell In[32], line 1
----> 1 openai.File.create(file=open(""data.json""), purpose='answers')

File [c:\Users\akash.sharma\Miniconda3\envs\gptenv\lib\site-packages\openai\api_resources\file.py:84](file:///C:/Users/akash.sharma/Miniconda3/envs/gptenv/lib/site-packages/openai/api_resources/file.py:84), in File.create(cls, file, purpose, model, api_key, api_base, api_type, api_version, organization, user_provided_filename)
     60 @classmethod
     61 def create(
     62     cls,
   (...)
     71     user_provided_filename=None,
     72 ):
     73     requestor, url, files = cls.__prepare_file_create(
     74         file,
     75         purpose,
   (...)
     82         user_provided_filename,
     83     )
---> 84     response, _, api_key = requestor.request(""post"", url, files=files)
     85     return util.convert_to_openai_object(
     86         response, api_key, api_version, organization
     87     )

File [c:\Users\akash.sharma\Miniconda3\envs\gptenv\lib\site-packages\openai\api_requestor.py:226](file:///C:/Users/akash.sharma/Miniconda3/envs/gptenv/lib/site-packages/openai/api_requestor.py:226), in APIRequestor.request(self, method, url, params, headers, files, stream, request_id, request_timeout)
    205 def request(
...
    683         rbody, rcode, resp.data, rheaders, stream_error=stream_error
    684     )
    685 return resp

InvalidRequestError: 'answers' is not one of ['fine-tune'] - 'purpose'


### To Reproduce

Just following steps on the OpeAI Answers Service Page. 

Error:

---------------------------------------------------------------------------
InvalidRequestError                       Traceback (most recent call last)
Cell In[32], line 1
----> 1 openai.File.create(file=open(""data.json""), purpose='answers')

File [c:\Users\akash.sharma\Miniconda3\envs\gptenv\lib\site-packages\openai\api_resources\file.py:84](file:///C:/Users/akash.sharma/Miniconda3/envs/gptenv/lib/site-packages/openai/api_resources/file.py:84), in File.create(cls, file, purpose, model, api_key, api_base, api_type, api_version, organization, user_provided_filename)
     60 @classmethod
     61 def create(
     62     cls,
   (...)
     71     user_provided_filename=None,
     72 ):
     73     requestor, url, files = cls.__prepare_file_create(
     74         file,
     75         purpose,
   (...)
     82         user_provided_filename,
     83     )
---> 84     response, _, api_key = requestor.request(""post"", url, files=files)
     85     return util.convert_to_openai_object(
     86         response, api_key, api_version, organization
     87     )

File [c:\Users\akash.sharma\Miniconda3\envs\gptenv\lib\site-packages\openai\api_requestor.py:226](file:///C:/Users/akash.sharma/Miniconda3/envs/gptenv/lib/site-packages/openai/api_requestor.py:226), in APIRequestor.request(self, method, url, params, headers, files, stream, request_id, request_timeout)
    205 def request(
...
    683         rbody, rcode, resp.data, rheaders, stream_error=stream_error
    684     )
    685 return resp

InvalidRequestError: 'answers' is not one of ['fine-tune'] - 'purpose'

### Code snippets

```Python
openai.File.create(file=open(""data.json""), purpose='answers')
```


### OS

Windows 10

### Python version

Python v 3.9.16

### Library version

v0.26.4",closed,2023-03-21T20:10:05Z,2023-03-22T02:53:49Z,['bug'],https://github.com/openai/openai-python/issues/327,6.73,True,input_value_violation,high,"The provided 'purpose' value 'answers' is not an acceptable option, which leads to an InvalidRequestError.",high,
292,Support Whisper from CLI,"### Describe the feature or improvement you're requesting

It would be nice to upload audio files with the CLI tool and get the API response.
Thank you for considering.


### Additional context

_No response_",closed,2023-03-09T14:19:01Z,2023-03-20T23:55:40Z,['enhancement'],https://github.com/openai/openai-python/issues/292,273.61,True,missing_dependency_violation,medium,"The request to upload audio files lacks details on required input formats and dependencies for processing, which may lead to incorrect or incomplete API responses.",high,
314,"Transcription function cannot handle response format specifications of ""text"" or ""vtt""","### Describe the bug

Transcription function cannot handle response format specifications of ""text"" or ""vtt"".  When those are specified, an exception is raised.

### To Reproduce

1. Using Python, obtain a transcript for an audio file: `transcript = openai.Audio.transcribe(""whisper-1"", audio_file, response_format=response_format)`.
2. Note that `response_format=""json""` and `response_format=""verbose_json""` work without issue.
3. Replace response format with `response_format=""vtt""` or `response_format=""text""`
4. Note that the code now raises an exception `json.decoder.JSONDecodeError`

### Code snippets

_No response_

### OS

Linux

### Python version

Python 3.10.6

### Library version

openai 0.27.0",closed,2023-03-17T04:21:46Z,2023-03-20T23:52:42Z,['bug'],https://github.com/openai/openai-python/issues/314,91.52,True,missing_option_violation,high,"The transcription function does not handle specified response formats of 'text' or 'vtt', which are acceptable alternatives according to the API contract.",high,
191,response is truncated from API,"After `pip install openai`, I tried:

    openai api completions.create -e text-davinci-003 -p ""what means soyboy in French?

I get

    Le terme « soyboy » n'est pas courant en

The text is not finished/truncated.

What I did wrong? I see no documentation here on how to set tokens from command line if this is the issue there.",closed,2023-01-21T06:08:33Z,2023-03-17T20:29:17Z,[],https://github.com/openai/openai-python/issues/191,1334.35,True,input_value_violation,medium,"The response is truncated, likely due to not specifying the maximum token limit for the completion request.",high,
281,openai.error.InvalidRequestError: Invalid URL (POST /v1/chat/completions) when calling the chat completions endpoints multiple times,"### Describe the bug

when calling 

`openai.ChatCompletion.create(       model=""gpt-3.5-turbo-0301"",       messages=[         {             ""role"": ""user"",             ""content"" : prompt         }       ],       temperature=0.7,        max_tokens=MAX_OUTPUT_BASE, n=1)           `


multiple times in a loop I noticed that the first few attempts run perfectly fine I believe the first 3-5, however after that it fails with the error code openai.error.InvalidRequestError: Invalid URL (POST /v1/chat/completions)

which is very odd as the prior ones are running smoothly within the same exact function call... 

I can verify that I'm running on openai==0.27.0 with python 3.9

### To Reproduce

1. create an array of texts
2  run the following   for text in texts:

    

        MAX_TOKENS = 3500
        # used for testing to see how the token limit progrsses over time


        response = openai.ChatCompletion.create(
          model=""gpt-3.5-turbo-0301"",
          messages=[
            {
                ""role"": ""user"",
                ""content"" : ""text""
            }
          ],            
          temperature=0.7, 
          max_tokens=MAX_TOKENS, n=1)
          
          

### Code snippets

_No response_

### OS

macos

### Python version

python 3.9

### Library version

openai-python v0.27.0",closed,2023-03-06T18:43:10Z,2023-03-17T20:33:48Z,['bug'],https://github.com/openai/openai-python/issues/281,265.84,True,input_value_violation,performance_degradation,"The error indicates that the API is receiving an invalid request after a few successful calls, possibly due to exceeding the limits on the number of requests or token usage.",high,
285,openai.api_base -Error communicating with OpenAI: Failed to parse:,"### Describe the bug

I use nginx proxy api_base  “https://api.openai.com” on my server :  “http://myserverip:port”
and get a error msg:

My server is can connect https://api.openai.com

and i use this way apiBaseUrl on https://github.com/transitive-bullshit/chatgpt-api  is no error

### To Reproduce


[WARNING][2023-03-08 18:35:58][log.py:39] - Error communicating with OpenAI: Failed to parse: http://myserverip:port/v1/chat/completion 

### Code snippets

```Python
nginx conf:
server {
    listen       9999;
    server_name  127.0.0.1;
    error_log /var/log/nginx/chatgpt.error.log error;
    access_log  /var/log/nginx/chatgpt.access.log combined;
    location / {
       proxy_pass https://api.openai.com/;
    }
}

python code:

class ChatGPTModel(Model):
    def __init__(self):
        openai.api_key = model_conf(const.OPEN_AI).get('api_key')
        # https://api.openai.com/v1/chat/completions
        openai.api_base = model_conf(const.OPEN_AI).get('api_url')

python conf:
    ""openai"": {
      ""api_url"": ""http://myserverip:port/v1""
```


### OS

Debian10.0

### Python version

3.9.2

### Library version

0.27.0",closed,2023-03-08T10:53:17Z,2023-03-17T13:38:39Z,['bug'],https://github.com/openai/openai-python/issues/285,218.76,True,input_value_violation,high,"The provided api_base URL 'http://myserverip:port/v1' is not a valid endpoint for the OpenAI API, leading to parsing errors.",high,
222,pip install failure deprecation on >=v0.26.4,"### Describe the bug

Versions >= v0.26.4 fail to install on fresh Ubuntu 22.04 container w/ Kernel `Linux [redacted]-2017 5.10.16.3-microsoft-standard-WSL2 #1 SMP Fri Apr 2 22:23:49 UTC 2021 x86_64 x86_64 x86_64 GNU/Linux`

### To Reproduce

With the following `Dockerfile`:
```
FROM ubuntu:22.04

RUN \
    DEBIAN_FRONTEND=noninteractive apt-get update && \
    DEBIAN_FRONTEND=noninteractive apt-get install -y \
    curl \
    git \
    python3 python3-pip

RUN echo ""openai==0.26.4"" > /requirements.txt
RUN pip install -r /requirements.txt
```

Note the installation failure:
```
 > [4/4] RUN pip install -r /requirements.txt:                                                                                                                                                                                                                                           
#10 1.029 Collecting openai==0.26.4                                                                                                                                                                                                                                                      
#10 1.265   Downloading openai-0.26.4.tar.gz (55 kB)                                                                                                                                                                                                                                     
#10 1.298      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 55.6/55.6 KB 1.6 MB/s eta 0:00:00                                                                                                                                                                                                
#10 1.463   Installing build dependencies: started                                                                                                                                                                                                                                       
#10 4.490   Installing build dependencies: finished with status 'done'
#10 4.493   Getting requirements to build wheel: started
#10 4.689   Getting requirements to build wheel: finished with status 'error'
#10 4.698   error: subprocess-exited-with-error
#10 4.698   
#10 4.698   × Getting requirements to build wheel did not run successfully.
#10 4.698   │ exit code: 1
#10 4.698   ╰─> [81 lines of output]
#10 4.698       Traceback (most recent call last):
#10 4.698         File ""/usr/lib/python3/dist-packages/setuptools/config.py"", line 419, in _parse_attr
#10 4.698           return getattr(StaticModule(module_name), attr_name)
#10 4.698         File ""/usr/lib/python3/dist-packages/setuptools/config.py"", line 26, in __init__
#10 4.698           spec = importlib.util.find_spec(name)
#10 4.698         File ""/usr/lib/python3.10/importlib/util.py"", line 94, in find_spec
#10 4.698           parent = __import__(parent_name, fromlist=['__path__'])
#10 4.698         File ""/tmp/pip-install-7lrzz_ii/openai_2922e99e37a54fe8816c6c965a07945b/openai/__init__.py"", line 9, in <module>
#10 4.698           from openai.api_resources import (
#10 4.698         File ""/tmp/pip-install-7lrzz_ii/openai_2922e99e37a54fe8816c6c965a07945b/openai/api_resources/__init__.py"", line 1, in <module>
#10 4.698           from openai.api_resources.completion import Completion  # noqa: F401
#10 4.698         File ""/tmp/pip-install-7lrzz_ii/openai_2922e99e37a54fe8816c6c965a07945b/openai/api_resources/completion.py"", line 4, in <module>
#10 4.698           from openai.api_resources.abstract import DeletableAPIResource, ListableAPIResource
#10 4.698         File ""/tmp/pip-install-7lrzz_ii/openai_2922e99e37a54fe8816c6c965a07945b/openai/api_resources/abstract/__init__.py"", line 3, in <module>
#10 4.698           from openai.api_resources.abstract.api_resource import APIResource
#10 4.698         File ""/tmp/pip-install-7lrzz_ii/openai_2922e99e37a54fe8816c6c965a07945b/openai/api_resources/abstract/api_resource.py"", line 4, in <module>
#10 4.698           from openai import api_requestor, error, util
#10 4.698         File ""/tmp/pip-install-7lrzz_ii/openai_2922e99e37a54fe8816c6c965a07945b/openai/api_requestor.py"", line 21, in <module>
#10 4.698           import aiohttp
#10 4.698       ModuleNotFoundError: No module named 'aiohttp'
#10 4.698       
#10 4.698       During handling of the above exception, another exception occurred:
#10 4.698       
#10 4.698       Traceback (most recent call last):
#10 4.698         File ""/usr/lib/python3/dist-packages/pip/_vendor/pep517/in_process/_in_process.py"", line 363, in <module>
#10 4.698           main()
#10 4.698         File ""/usr/lib/python3/dist-packages/pip/_vendor/pep517/in_process/_in_process.py"", line 345, in main
#10 4.698           json_out['return_val'] = hook(**hook_input['kwargs'])
#10 4.698         File ""/usr/lib/python3/dist-packages/pip/_vendor/pep517/in_process/_in_process.py"", line 130, in get_requires_for_build_wheel
#10 4.698           return hook(config_settings)
#10 4.698         File ""/usr/lib/python3/dist-packages/setuptools/build_meta.py"", line 162, in get_requires_for_build_wheel
#10 4.698           return self._get_build_requires(
#10 4.698         File ""/usr/lib/python3/dist-packages/setuptools/build_meta.py"", line 143, in _get_build_requires
#10 4.698           self.run_setup()
#10 4.698         File ""/usr/lib/python3/dist-packages/setuptools/build_meta.py"", line 158, in run_setup
#10 4.698           exec(compile(code, __file__, 'exec'), locals())
#10 4.698         File ""setup.py"", line 3, in <module>
#10 4.698           setup()
#10 4.698         File ""/usr/lib/python3/dist-packages/setuptools/__init__.py"", line 153, in setup
#10 4.698           return distutils.core.setup(**attrs)
#10 4.698         File ""/usr/lib/python3/dist-packages/setuptools/_distutils/core.py"", line 122, in setup
#10 4.698           dist.parse_config_files()
#10 4.698         File ""/usr/lib/python3/dist-packages/setuptools/dist.py"", line 804, in parse_config_files
#10 4.698           parse_configuration(
#10 4.698         File ""/usr/lib/python3/dist-packages/setuptools/config.py"", line 158, in parse_configuration
#10 4.698           meta.parse()
#10 4.698         File ""/usr/lib/python3/dist-packages/setuptools/config.py"", line 498, in parse
#10 4.698           section_parser_method(section_options)
#10 4.698         File ""/usr/lib/python3/dist-packages/setuptools/config.py"", line 469, in parse_section
#10 4.698           self[name] = value
#10 4.698         File ""/usr/lib/python3/dist-packages/setuptools/config.py"", line 222, in __setitem__
#10 4.698           value = parser(value)
#10 4.698         File ""/usr/lib/python3/dist-packages/setuptools/config.py"", line 599, in _parse_version
#10 4.698           version = self._parse_attr(value, self.package_dir)
#10 4.698         File ""/usr/lib/python3/dist-packages/setuptools/config.py"", line 422, in _parse_attr
#10 4.698           module = importlib.import_module(module_name)
#10 4.698         File ""/usr/lib/python3.10/importlib/__init__.py"", line 126, in import_module
#10 4.698           return _bootstrap._gcd_import(name[level:], package, level)
#10 4.698         File ""<frozen importlib._bootstrap>"", line 1050, in _gcd_import
#10 4.698         File ""<frozen importlib._bootstrap>"", line 1027, in _find_and_load
#10 4.698         File ""<frozen importlib._bootstrap>"", line 992, in _find_and_load_unlocked
#10 4.698         File ""<frozen importlib._bootstrap>"", line 241, in _call_with_frames_removed
#10 4.698         File ""<frozen importlib._bootstrap>"", line 1050, in _gcd_import
#10 4.698         File ""<frozen importlib._bootstrap>"", line 1027, in _find_and_load
#10 4.698         File ""<frozen importlib._bootstrap>"", line 1006, in _find_and_load_unlocked
#10 4.698         File ""<frozen importlib._bootstrap>"", line 688, in _load_unlocked
#10 4.698         File ""<frozen importlib._bootstrap_external>"", line 883, in exec_module
#10 4.698         File ""<frozen importlib._bootstrap>"", line 241, in _call_with_frames_removed
#10 4.698         File ""/tmp/pip-install-7lrzz_ii/openai_2922e99e37a54fe8816c6c965a07945b/openai/__init__.py"", line 9, in <module>
#10 4.698           from openai.api_resources import (
#10 4.698         File ""/tmp/pip-install-7lrzz_ii/openai_2922e99e37a54fe8816c6c965a07945b/openai/api_resources/__init__.py"", line 1, in <module>
#10 4.698           from openai.api_resources.completion import Completion  # noqa: F401
#10 4.698         File ""/tmp/pip-install-7lrzz_ii/openai_2922e99e37a54fe8816c6c965a07945b/openai/api_resources/completion.py"", line 4, in <module>
#10 4.698           from openai.api_resources.abstract import DeletableAPIResource, ListableAPIResource
#10 4.698         File ""/tmp/pip-install-7lrzz_ii/openai_2922e99e37a54fe8816c6c965a07945b/openai/api_resources/abstract/__init__.py"", line 3, in <module>
#10 4.698           from openai.api_resources.abstract.api_resource import APIResource
#10 4.698         File ""/tmp/pip-install-7lrzz_ii/openai_2922e99e37a54fe8816c6c965a07945b/openai/api_resources/abstract/api_resource.py"", line 4, in <module>
#10 4.698           from openai import api_requestor, error, util
#10 4.698         File ""/tmp/pip-install-7lrzz_ii/openai_2922e99e37a54fe8816c6c965a07945b/openai/api_requestor.py"", line 21, in <module>
#10 4.698           import aiohttp
#10 4.698       ModuleNotFoundError: No module named 'aiohttp'
#10 4.698       [end of output]
#10 4.698   
#10 4.698   note: This error originates from a subprocess, and is likely not a problem with pip.
#10 4.702 error: subprocess-exited-with-error
#10 4.702 
#10 4.702 × Getting requirements to build wheel did not run successfully.
#10 4.702 │ exit code: 1
#10 4.702 ╰─> See above for output.
#10 4.702 
#10 4.702 note: This error originates from a subprocess, and is likely not a problem with pip.
```

Installs cleanly with:
```
FROM ubuntu:22.04

RUN \
    DEBIAN_FRONTEND=noninteractive apt-get update && \
    DEBIAN_FRONTEND=noninteractive apt-get install -y \
    curl \
    git \
    python3 python3-pip

RUN echo ""openai==0.26.3"" > /requirements.txt
RUN pip install -r /requirements.txt
```

### Code snippets

_No response_

### OS

Windows 10/WSL2

### Python version

Python 3.10.6

### Library version

openai-python >=v0.26.4",closed,2023-02-15T04:42:06Z,2023-03-11T03:55:10Z,['bug'],https://github.com/openai/openai-python/issues/222,575.22,False,,,,low,
294,pip install openai==0.27.1 unable to install,"### Describe the bug

Unable to install openai using pip for latest version. Works fine with `pip3 install openai==0.27.0`

### To Reproduce

`pip3 install openai`
Output:
Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/
Collecting openai
  Using cached openai-0.27.1.tar.gz (57 kB)
  Installing build dependencies ... done
  error: subprocess-exited-with-error
  
  × Getting requirements to build wheel did not run successfully.
  │ exit code: 1
  ╰─> See above for output.
  
  note: This error originates from a subprocess, and is likely not a problem with pip.
  Getting requirements to build wheel ... error
error: subprocess-exited-with-error

× Getting requirements to build wheel did not run successfully.
│ exit code: 1
╰─> See above for output.

note: This error originates from a subprocess, and is likely not a problem with pip.
Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/
Requirement already satisfied: python-dotenv in /usr/local/lib/python3.9/dist-packages (1.0.0)

`pip3 install openai==0.27.0`
Succesful

### Code snippets

_No response_

### OS

Linux

### Python version

python 3.9.16

### Library version

openai-python v0.27.1",closed,2023-03-09T18:44:06Z,2023-03-11T03:54:10Z,['bug'],https://github.com/openai/openai-python/issues/294,33.17,False,,,,low,
243,[Audio.transcribe] JsonDecodeError when printing vtt from m4a,"### Describe the bug

This section of the codebase expects json even when the response_format is not json:

https://github.com/openai/openai-python/blob/75c90a71e88e4194ce22c71edeb3d2dee7f6ac93/openai/api_requestor.py#L668C7-L673

I think I can contribute a quick bug fix PR today!

### To Reproduce

1. Open an m4a file in a jupyter notebook (python 3.10.10)
2. Transcribe with whisper-1
3. Print transcript

Stack:
JSONDecodeError                           Traceback (most recent call last)
File ~\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\local-packages\Python310\site-packages\openai\api_requestor.py:669, in APIRequestor._interpret_response_line(self, rbody, rcode, rheaders, stream)
    668 try:
--> 669     data = json.loads(rbody)
    670 except (JSONDecodeError, UnicodeDecodeError) as e:

```
File C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.10_3.10.2800.0_x64__qbz5n2kfra8p0\lib\json\__init__.py:346, in loads(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)
    343 if (cls is None and object_hook is None and
    344         parse_int is None and parse_float is None and
    345         parse_constant is None and object_pairs_hook is None and not kw):
--> 346     return _default_decoder.decode(s)
    347 if cls is None:

File C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.10_3.10.2800.0_x64__qbz5n2kfra8p0\lib\json\decoder.py:337, in JSONDecoder.decode(self, s, _w)
    333 """"""Return the Python representation of ``s`` (a ``str`` instance
    334 containing a JSON document).
    335 
    336 """"""
--> 337 obj, end = self.raw_decode(s, idx=_w(s, 0).end())
    338 end = _w(s, end).end()

File C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.10_3.10.2800.0_x64__qbz5n2kfra8p0\lib\json\decoder.py:355, in JSONDecoder.raw_decode(self, s, idx)
    354 except StopIteration as err:
--> 355     raise JSONDecodeError(""Expecting value"", s, err.value) from None
```

### Code snippets

```Python
f = open(""testing.m4a"", ""rb"")
transcript = openai.Audio.transcribe(""whisper-1"", f,response_format=""vtt"")
print(transcript)


https://github.com/openai/openai-python/blob/75c90a71e88e4194ce22c71edeb3d2dee7f6ac93/openai/api_requestor.py#L668C7-L673`
```


### OS

Windows 11

### Python version

Python v3.10.10

### Library version

openai-python 0.27.0",closed,2023-03-01T21:56:51Z,2023-03-08T20:50:41Z,['bug'],https://github.com/openai/openai-python/issues/243,166.9,False,,,,low,
276,Incomprehensible Response Using Davinci,"### Describe the bug

I used the OpenAI API to call the Davinci model, but the results are incomprehensible and different from what is returned in the playground.

Here is what it responds:

`
{
  ""id"": ""cmpl-6qyqT0nO2doVmbzd4zdVoOvbtWM00"",
  ""object"": ""text_completion"",
  ""created"": 1678084965,
  ""model"": ""gpt-3.5-turbo-0301"",
  ""choices"": [
    {
      ""text"": ""们以前的关于春天万物竞发，勃勃生机，优势在我们以前的资源浮奏游戏。\n\n1. 软件开发错误。\n\n2. 微软离席。\n\n3. 我不想对大部分小偷一点说话，但是总要加快进一步的开发人员应当迁移所有行为之间关系。如果大家负责开发人员或者已经选用了一些理由来重复使用不合理性，那么刚才凭着中文字幕来汇集数据，随时上传到Twitter、Facebook、Google+各方面的微信公众图片账户。而这一时候如果能在微信上带有明显的内容时遇到一个微博广告，就相当关注。你不能标准化交流，不能原始冲突..."",
      ""index"": 0,
      ""logprobs"": null,
      ""finish_reason"": ""stop""
    }
  ],
  ""usage"": {
    ""prompt_tokens"": 68,
    ""completion_tokens"": 420,
    ""total_tokens"": 488
  }
}`

### To Reproduce

just using the api introduced in https://platform.openai.com/docs/guides/completion

### Code snippets

```Python
response = openai.Completion.create(
  model=""gpt-3.5-turbo"",
  prompt=""按照以下要求生成一篇中文文章，关于春天万物竞发，勃勃生机，优势在我"",
  temperature=0.7,
  max_tokens=1695,
  top_p=1,
  frequency_penalty=0.59,
  presence_penalty=0.77
)
```


### OS

macOS

### Python version

Python 3.7.6

### Library version

v.0.27.0",closed,2023-03-06T06:48:46Z,2023-03-06T17:02:55Z,['bug'],https://github.com/openai/openai-python/issues/276,10.24,True,input_value_violation,medium,"The prompt provided may be overly long or improperly formatted, leading to incomprehensible responses from the model.",high,
85,Storing metadata for fine-tuned models,"I am fine tuning various models which have different styles of prompts. In inference I need to style the prompts adequately for each model. It would be great if it was possible to save a metadata or config json when creating the model, and retrieve it with the model. Otherwise I would need to either: (1) manage my own external database which is cumbersome when using google colab (2) hack the model suffix to encode my config params (let's not do that), or (3) reverse engineer my training jsonl to deduce the model's specific prompt styling, which is difficult. Any other options?",closed,2022-03-30T22:25:36Z,2023-03-07T03:17:24Z,"['enhancement', 'wontfix']",https://github.com/openai/openai-python/issues/85,8188.86,True,missing_dependency_violation,medium,"The issue highlights a missing dependency for saving and retrieving metadata/config for fine-tuned models, which is essential for proper prompt styling during inference.",high,
267,Magic sauce is missing. Why is the API not equivalent to  backend-api/conversation?,"### Describe the feature or improvement you're requesting

Thank you for your great work in providing an API. 
What seems strangely confusing is that the main use of a conversation_id is not part of the API. Not providing the main use of a chat context as a reference point leads to pointless data stored in memory, repeated requests within a chat, limitations regarding the no of tokens and unnecessary costs.

Since it is definitely technically feasible, as there is an API for the chat window, I wonder what the intention behind this is? Is it simply temporary? Before any workarounds are developed, and if so, when, is a full deployment planned? It also seems that other parties e.g. the companies of the demos have that ability?

Appreciate your guys work and thank you very much for your help!
",closed,2023-03-03T19:04:07Z,2023-03-06T17:09:34Z,['question'],https://github.com/openai/openai-python/issues/267,70.09,True,missing_dependency_violation,medium,"The API lacks the main use of a conversation_id, which is essential for maintaining chat context and preventing unnecessary data storage and repeated requests.",high,
268,ChatGPT API,"### Describe the feature or improvement you're requesting

I would like to be able to manage multiple users in the ChatGPT. The idea is for the API to understand who is speaking as now only one user, one system and one assistant. It would be cool to have possibility to define multiple users and even multiple assistants (specialization?)
So the inputs would be sonething similar as below: 
{role: “user1”, ”content”: ”I think open source is great way to improve LLMs..”}
{role: “user12, ”content”: ”I think open source LLMs can be risky in hands of bad actors.”}


### Additional context

See above",closed,2023-03-03T21:24:16Z,2023-03-06T17:06:48Z,[],https://github.com/openai/openai-python/issues/268,67.71,True,input_type_violation,medium,"The input format provided does not match the expected structure for the ChatGPT API, which may lead to misinterpretation of user roles and content.",high,
272,Django text not near and no space,"### Describe the feature or improvement you're requesting


Django bot chatgpt text not near, why?

![chatgpt](https://user-images.githubusercontent.com/39833970/222942356-741f98b4-aa65-44f9-8646-2b23dfc07bcd.PNG)

chatgpt text near and space

![chatgpt2](https://user-images.githubusercontent.com/39833970/222942395-495fc103-e694-40f6-95b9-fb3e6d1f3134.PNG)


### Additional context

_No response_",closed,2023-03-05T05:01:00Z,2023-03-06T17:06:02Z,[],https://github.com/openai/openai-python/issues/272,36.08,True,input_value_violation,medium,"The request for text formatting suggests that the input values (text spacing and proximity) are not adhering to expected formatting standards, potentially leading to degraded output quality.",high,
241,"There is no metadata, please fix it","### Describe the bug

There is no metadata in the response for individual words like what's the language of a word, start time and end time.

### To Reproduce

1. Run the code snippet below.

### Code snippets

```Python
os
import openai

openai.organization = os.environ.get(""OPENAIORGANIZATIONID"")
openai.api_key = os.environ.get(""OPENAIAPIKEY"")

file = open(""/Users/olehkopyl/Downloads/test.m4a"", ""rb"")
transcription = openai.Audio.transcribe(""whisper-1"", file)

print(transcription)
```


### OS

macOS

### Python version

3.8.13

### Library version

0.27.0",closed,2023-03-01T20:41:57Z,2023-03-01T23:30:21Z,['bug'],https://github.com/openai/openai-python/issues/241,2.81,True,missing_dependency_violation,high,"The API response lacks essential metadata such as language, start time, and end time, which are necessary for complete and contextual output.",high,
229,Why not open to China,"### Describe the feature or improvement you're requesting

I hope that the OpenAI API will be opened to China.

### Additional context

_No response_",closed,2023-02-25T10:57:26Z,2023-03-03T16:31:55Z,[],https://github.com/openai/openai-python/issues/229,149.57,False,,,,,
219,Can't install openai because aiohttp is supposedly missing?,"### Describe the bug

I'm trying to install openai on my ubuntu machine, but the installation wheel crashes because a module called ""aiohttp"" is missing. I installed aiohttp, but the error doesn't dissapear. I have run the following command ""python3 -m pip install openai"" in 2 different ubuntu 22.04 computers several times and after several reboots, and the result has been the same. I simply can't install openai. I'm using python 3.10

### To Reproduce

1. Run ""python3 -m pip install openai""
2. The installation fails

### Code snippets

```Python
Collecting openai
  Using cached openai-0.26.5.tar.gz (55 kB)
  Installing build dependencies ... done
  Getting requirements to build wheel ... error
  error: subprocess-exited-with-error

  × Getting requirements to build wheel did not run successfully.
  │ exit code: 1
  ╰─> [81 lines of output]
      Traceback (most recent call last):
        File ""/usr/lib/python3/dist-packages/setuptools/config.py"", line 419, in _parse_attr
          return getattr(StaticModule(module_name), attr_name)
        File ""/usr/lib/python3/dist-packages/setuptools/config.py"", line 26, in __init__
          spec = importlib.util.find_spec(name)
        File ""/usr/lib/python3.10/importlib/util.py"", line 94, in find_spec
          parent = __import__(parent_name, fromlist=['__path__'])
        File ""/tmp/pip-install-syczp33h/openai_d2bb5765cb594c07a4e3c96cae3c38b0/openai/__init__.py"", line 9, in <module>
          from openai.api_resources import (
        File ""/tmp/pip-install-syczp33h/openai_d2bb5765cb594c07a4e3c96cae3c38b0/openai/api_resources/__init__.py"", line 1, in <module>
          from openai.api_resources.completion import Completion  # noqa: F401
        File ""/tmp/pip-install-syczp33h/openai_d2bb5765cb594c07a4e3c96cae3c38b0/openai/api_resources/completion.py"", line 4, in <module>
          from openai.api_resources.abstract import DeletableAPIResource, ListableAPIResource
        File ""/tmp/pip-install-syczp33h/openai_d2bb5765cb594c07a4e3c96cae3c38b0/openai/api_resources/abstract/__init__.py"", line 3, in <module>
          from openai.api_resources.abstract.api_resource import APIResource
        File ""/tmp/pip-install-syczp33h/openai_d2bb5765cb594c07a4e3c96cae3c38b0/openai/api_resources/abstract/api_resource.py"", line 4, in <module>
          from openai import api_requestor, error, util
        File ""/tmp/pip-install-syczp33h/openai_d2bb5765cb594c07a4e3c96cae3c38b0/openai/api_requestor.py"", line 21, in <module>
          import aiohttp
      ModuleNotFoundError: No module named 'aiohttp'

      During handling of the above exception, another exception occurred:

      Traceback (most recent call last):
        File ""/usr/lib/python3/dist-packages/pip/_vendor/pep517/in_process/_in_process.py"", line 363, in <module>
          main()
        File ""/usr/lib/python3/dist-packages/pip/_vendor/pep517/in_process/_in_process.py"", line 345, in main
          json_out['return_val'] = hook(**hook_input['kwargs'])
        File ""/usr/lib/python3/dist-packages/pip/_vendor/pep517/in_process/_in_process.py"", line 130, in get_requires_for_build_wheel
          return hook(config_settings)
        File ""/usr/lib/python3/dist-packages/setuptools/build_meta.py"", line 162, in get_requires_for_build_wheel
          return self._get_build_requires(
        File ""/usr/lib/python3/dist-packages/setuptools/build_meta.py"", line 143, in _get_build_requires
          self.run_setup()
        File ""/usr/lib/python3/dist-packages/setuptools/build_meta.py"", line 158, in run_setup
          exec(compile(code, __file__, 'exec'), locals())
        File ""setup.py"", line 3, in <module>
          setup()
        File ""/usr/lib/python3/dist-packages/setuptools/__init__.py"", line 153, in setup
          return distutils.core.setup(**attrs)
        File ""/usr/lib/python3/dist-packages/setuptools/_distutils/core.py"", line 122, in setup
          dist.parse_config_files()
        File ""/usr/lib/python3/dist-packages/setuptools/dist.py"", line 804, in parse_config_files
          parse_configuration(
        File ""/usr/lib/python3/dist-packages/setuptools/config.py"", line 158, in parse_configuration
          meta.parse()
        File ""/usr/lib/python3/dist-packages/setuptools/config.py"", line 498, in parse
          section_parser_method(section_options)
        File ""/usr/lib/python3/dist-packages/setuptools/config.py"", line 469, in parse_section
          self[name] = value
        File ""/usr/lib/python3/dist-packages/setuptools/config.py"", line 222, in __setitem__
          value = parser(value)
        File ""/usr/lib/python3/dist-packages/setuptools/config.py"", line 599, in _parse_version
          version = self._parse_attr(value, self.package_dir)
        File ""/usr/lib/python3/dist-packages/setuptools/config.py"", line 422, in _parse_attr
          module = importlib.import_module(module_name)
        File ""/usr/lib/python3.10/importlib/__init__.py"", line 126, in import_module
          return _bootstrap._gcd_import(name[level:], package, level)
        File ""<frozen importlib._bootstrap>"", line 1050, in _gcd_import
        File ""<frozen importlib._bootstrap>"", line 1027, in _find_and_load
        File ""<frozen importlib._bootstrap>"", line 992, in _find_and_load_unlocked
        File ""<frozen importlib._bootstrap>"", line 241, in _call_with_frames_removed
        File ""<frozen importlib._bootstrap>"", line 1050, in _gcd_import
        File ""<frozen importlib._bootstrap>"", line 1027, in _find_and_load
        File ""<frozen importlib._bootstrap>"", line 1006, in _find_and_load_unlocked
        File ""<frozen importlib._bootstrap>"", line 688, in _load_unlocked
        File ""<frozen importlib._bootstrap_external>"", line 883, in exec_module
        File ""<frozen importlib._bootstrap>"", line 241, in _call_with_frames_removed
        File ""/tmp/pip-install-syczp33h/openai_d2bb5765cb594c07a4e3c96cae3c38b0/openai/__init__.py"", line 9, in <module>
          from openai.api_resources import (
        File ""/tmp/pip-install-syczp33h/openai_d2bb5765cb594c07a4e3c96cae3c38b0/openai/api_resources/__init__.py"", line 1, in <module>
          from openai.api_resources.completion import Completion  # noqa: F401
        File ""/tmp/pip-install-syczp33h/openai_d2bb5765cb594c07a4e3c96cae3c38b0/openai/api_resources/completion.py"", line 4, in <module>
          from openai.api_resources.abstract import DeletableAPIResource, ListableAPIResource
        File ""/tmp/pip-install-syczp33h/openai_d2bb5765cb594c07a4e3c96cae3c38b0/openai/api_resources/abstract/__init__.py"", line 3, in <module>
          from openai.api_resources.abstract.api_resource import APIResource
        File ""/tmp/pip-install-syczp33h/openai_d2bb5765cb594c07a4e3c96cae3c38b0/openai/api_resources/abstract/api_resource.py"", line 4, in <module>
          from openai import api_requestor, error, util
        File ""/tmp/pip-install-syczp33h/openai_d2bb5765cb594c07a4e3c96cae3c38b0/openai/api_requestor.py"", line 21, in <module>
          import aiohttp
      ModuleNotFoundError: No module named 'aiohttp'
      [end of output]

  note: This error originates from a subprocess, and is likely not a problem with pip.
error: subprocess-exited-with-error

× Getting requirements to build wheel did not run successfully.
│ exit code: 1
╰─> See above for output.

note: This error originates from a subprocess, and is likely not a problem with pip.
```


### OS

Ubuntu 22.04

### Python version

Python 3.10.6

### Library version

openai-python-0.26.5",closed,2023-02-11T16:33:02Z,2023-03-03T16:34:12Z,['bug'],https://github.com/openai/openai-python/issues/219,480.02,True,missing_dependency_violation,high,"The installation of the openai library fails due to a missing dependency (aiohttp), which is required for the library to function properly.",high,
245,[Audio.transcribe] Logprobs for each token in verbose_json,"### Describe the feature or improvement you're requesting

Currently, Whisper exposes avg_logprob for an entire segment. The request is to expose logprobs for each token.

```json
{
  ""duration"": 4.01,
  ""language"": ""english"",
  ""segments"": [
    {
      ""avg_logprob"": -0.40153955010806813,
      ""compression_ratio"": 1.0526315789473684,
      ""end"": 4.0,
      ""id"": 0,
      ""no_speech_prob"": 0.1633709967136383,
      ""seek"": 0,
      ""start"": 0.0,
      ""temperature"": 0.0,
      ""text"": "" Testing, testing, this is going to be a new audio recording."",
      ""tokens"": [
        50364,
        45517,
        11,
        4997,
        11,
        341,
        307,
        516,
        281,
        312,
        257,
        777,
        6278,
        6613,
        13,
        50564
      ],
      ""transient"": false
    }
  ],
  ""task"": ""transcribe"",
  ""text"": ""Testing, testing, this is going to be a new audio recording.""
}
```


### Additional context

_No response_",closed,2023-03-01T22:23:23Z,2023-03-03T16:28:49Z,['enhancement'],https://github.com/openai/openai-python/issues/245,42.09,False,,,,,
240,AttributeError: module 'openai' has no attribute 'openai_response',"### Describe the bug

Just ran the documentation code on Jupyter notebook and got
in python3.9/site-packages/openai/util.py:112
if isinstance(resp, openai.openai_response.OpenAIResponse):
    113         organization = resp.organization
    114         response_ms = resp.response_ms

**AttributeError: module 'openai' has no attribute 'openai_response'**


### To Reproduce

pip install -U openai
in jupyter notebook

### Code snippets

```Python
import openai

openai.ChatCompletion.create(
  model=""gpt-3.5-turbo"",
  messages=[
        {""role"": ""system"", ""content"": ""You are a helpful assistant.""},
        {""role"": ""user"", ""content"": ""Who won the world series in 2020?""},
        {""role"": ""assistant"", ""content"": ""The Los Angeles Dodgers won the World Series in 2020.""},
        {""role"": ""user"", ""content"": ""Where was it played?""}
    ]
)
```


### OS

macOS m1

### Python version

Python v3.9.12

### Library version

openai-python v0.27.0",closed,2023-03-01T19:55:05Z,2023-03-03T16:26:51Z,['bug'],https://github.com/openai/openai-python/issues/240,44.53,True,input_type_violation,high,"The code attempts to access an attribute 'openai_response' that does not exist in the 'openai' module, indicating a type mismatch or incorrect usage of the API.",high,
247,can not work with  jupyterlab-desktop,"### Describe the bug

When I use openai package with  [jupyterlab-desktop](https://github.com/jupyterlab/jupyterlab-desktop/releases/tag/v3.6.1-2), the following error occurred:

<img width=""743"" alt=""image"" src=""https://user-images.githubusercontent.com/3153878/222313242-bd1b48da-bd09-44f2-9e63-e9342d880f21.png"">

environment:

- MacOS 13.2.1(M1)
- openai == 0.27.0
- jupyterlab-desktop == 3.6.1-2
  - python3.8


### To Reproduce

1. open jupyterlab-desktop
2. install openai: `!pip install openai`
3. import openai

### Code snippets

```Python
import openai
```


### OS

MacOS 13.2.1(M1)

### Python version

python3.8

### Library version

openai == 0.27.0",closed,2023-03-02T02:19:27Z,2023-03-03T16:22:24Z,['bug'],https://github.com/openai/openai-python/issues/247,38.05,True,input_type_violation,unknown,"The error suggests a potential type mismatch between the openai package and the jupyterlab-desktop environment, possibly due to incompatible versions or dependencies.",medium,
250,InvalidRequestError:Invalid URL when using chatcompletion,"### Describe the bug

when I upgrade openai to v0.27.0 and reproduce your sample:
import openai

openai.ChatCompletion.create(
  model=""gpt-3.5-turbo"",
  messages=[
        {""role"": ""system"", ""content"": ""You are a helpful assistant.""},
        {""role"": ""user"", ""content"": ""Who won the world series in 2020?""},
        {""role"": ""assistant"", ""content"": ""The Los Angeles Dodgers won the World Series in 2020.""},
        {""role"": ""user"", ""content"": ""Where was it played?""}
    ]
)

I got:

Traceback (most recent call last):

  File ""C:\Users\Administrator\AppData\Local\Temp\ipykernel_4560\2473871474.py"", line 1, in <module>
    openai.ChatCompletion.create(

  File ""D:\Anaconda\lib\site-packages\openai\api_resources\chat_completion.py"", line 25, in create
    return super().create(*args, **kwargs)

  File ""D:\Anaconda\lib\site-packages\openai\api_resources\abstract\engine_api_resource.py"", line 153, in create
    response, _, api_key = requestor.request(

  File ""D:\Anaconda\lib\site-packages\openai\api_requestor.py"", line 226, in request
    resp, got_stream = self._interpret_response(result, stream)

  File ""D:\Anaconda\lib\site-packages\openai\api_requestor.py"", line 619, in _interpret_response
    self._interpret_response_line(

  File ""D:\Anaconda\lib\site-packages\openai\api_requestor.py"", line 679, in _interpret_response_line
    raise self.handle_error_response(

InvalidRequestError: Invalid URL (POST /v1/chat/completions)

### To Reproduce

import openai

openai.ChatCompletion.create(
  model=""gpt-3.5-turbo"",
  messages=[
        {""role"": ""system"", ""content"": ""You are a helpful assistant.""},
        {""role"": ""user"", ""content"": ""Who won the world series in 2020?""},
        {""role"": ""assistant"", ""content"": ""The Los Angeles Dodgers won the World Series in 2020.""},
        {""role"": ""user"", ""content"": ""Where was it played?""}
    ]
)

### Code snippets

_No response_

### OS

Windows

### Python version

3.9.13

### Library version

0.27.0",closed,2023-03-02T04:06:53Z,2023-03-03T14:19:42Z,['bug'],https://github.com/openai/openai-python/issues/250,34.21,True,input_value_violation,unknown,"The error indicates an invalid URL, suggesting that the input parameters or configuration may not align with the expected API contract.",high,
265,"openai.error.APIConnectionError: Unexpected error communicating with OpenAI.  If this problem persists, let us know at support@openai.com.","### Describe the bug

openai.error.APIConnectionError: Unexpected error communicating with OpenAI.  If this problem persists,
let us know at support@openai.com.

### To Reproduce

openai.error.APIConnectionError: Unexpected error communicating with OpenAI.  If this problem persists,
let us know at support@openai.com.

### Code snippets

```Python
response = openai.Completion.create
```


### OS

win

### Python version

3.7

### Library version

openai==0.8.0 openai==0.27.0",closed,2023-03-03T06:52:53Z,2023-03-03T14:11:36Z,['bug'],https://github.com/openai/openai-python/issues/265,7.31,True,input_type_violation,unknown,"The code snippet shows an incomplete API call, which suggests a potential type mismatch or incorrect method usage.",high,
256,How to get the parameters in gpt 3.5 turbo?,"### Describe the feature or improvement you're requesting

With davinci I used these:

```
                    max_tokens=40,
                    n=1,
                    stop=None,
                    temperature=0.5
```

How can I get the same with gpt 3.5 turbo in python?

### Additional context

NEVERMIND: I just found out that it works the same way. I had an error when I tried it initially.",closed,2023-03-02T16:08:38Z,2023-03-03T14:07:31Z,[],https://github.com/openai/openai-python/issues/256,21.98,False,,,,low,
264,Possibility of token limit increase,"### Describe the bug

Hi, currently we are facing problems with token limitation which is 4097. Is there any possibility to increase existing token limit to 7000 tokens.
Thanks in advance 

### To Reproduce

NA

### Code snippets

_No response_

### OS

Linux

### Python version

Python v3.8

### Library version

openai-python v0.26.5",closed,2023-03-03T05:57:14Z,2023-03-03T14:04:08Z,['bug'],https://github.com/openai/openai-python/issues/264,8.12,True,input_value_violation,performance_degradation,Request to increase token limit exceeds the current API constraints.,high,
257,We could not parse the JSON body of your request.,"### Describe the bug

wrong report",closed,2023-03-02T16:59:51Z,2023-03-02T17:06:17Z,['bug'],https://github.com/openai/openai-python/issues/257,0.11,False,,,,low,
239,AttributeError: module 'openai' has no attribute 'ChatCompletion'. Did you mean: 'Completion'?,"### Describe the bug

Trying to run gpt-3.5-turbo model but it's throwing next error: AttributeError: module 'openai' has no attribute 'ChatCompletion'. Did you mean: 'Completion'?
Updated to latest version, not working.

### To Reproduce

1. Install latest openai
2. Copy code from official documentation
3. Run

### Code snippets

```Python
import openai
openai.api_key = 'sk-***'

completion = openai.ChatCompletion.create(
  model=""gpt-3.5-turbo"", 
  messages=[{""role"": ""user"", ""content"": ""Tell the world about the ChatGPT API in the style of a pirate.""}]
)

print(completion)
```


### OS

Arch Linux - 6.1.4-arch1-1

### Python version

Python v3.10.9

### Library version

openai-python v0.27.0",closed,2023-03-01T19:37:29Z,2023-03-01T20:07:31Z,['bug'],https://github.com/openai/openai-python/issues/239,0.5,False,,,,low,
210,sklearn requirement breaks openai[embeddings] install,"when installing the `openai[embeddings]` requirement, both `sklearn` and `scikit-learn` are listed as requirements [source](https://github.com/openai/openai-python/blob/main/setup.cfg#L48).

 However, `sklearn` is deprecated, and is causing `pip install openai[embeddings]` to fail. See the following output from my CI build:

```
Collecting sklearn
  Downloading sklearn-0.0.post1.tar.gz (3.6 kB)
  Preparing metadata (setup.py): started
  Preparing metadata (setup.py): finished with status 'error'
  error: subprocess-exited-with-error
  
  × python setup.py egg_info did not run successfully.
  │ exit code: 1
  ╰─> [18 lines of output]
      The 'sklearn' PyPI package is deprecated, use 'scikit-learn'
      rather than 'sklearn' for pip commands.
      
      Here is how to fix this error in the main use cases:
      - use 'pip install scikit-learn' rather than 'pip install sklearn'
      - replace 'sklearn' by 'scikit-learn' in your pip requirements files
        (requirements.txt, setup.py, setup.cfg, Pipfile, etc ...)
      - if the 'sklearn' package is used by one of your dependencies,
        it would be great if you take some time to track which package uses
        'sklearn' instead of 'scikit-learn' and report it to their issue tracker
      - as a last resort, set the environment variable
        SKLEARN_ALLOW_DEPRECATED_SKLEARN_PACKAGE_INSTALL=True to avoid this error
      
      More information is available at
      https://github.com/scikit-learn/sklearn-pypi-package
      
      If the previous advice does not cover your use case, feel free to report it at
      https://github.com/scikit-learn/sklearn-pypi-package/issues/new
      [end of output]
  
  note: This error originates from a subprocess, and is likely not a problem with pip.
error: metadata-generation-failed

× Encountered error while generating package metadata.
╰─> See above for output.
```

A simple fix would be to simply remove `sklearn` from the list of dependencies of `openai[embeddings]`.

I can open a PR for this if necessary.",closed,2023-02-02T22:29:08Z,2023-03-01T17:33:54Z,[],https://github.com/openai/openai-python/issues/210,643.08,True,missing_dependency_violation,high,"The inclusion of 'sklearn' as a dependency is causing installation failures due to its deprecation, which violates the API contract for correct dependency management.",high,
232,AttributeError: module 'openai' has no attribute 'search',"### Describe the bug

the main question when i use example 

openai.error.InvalidRequestError: 'search' is not one of ['fine-tune'] - 'purpose'
AttributeError: module 'openai' has no attribute 'search'

### To Reproduce

i can't use guide from this website

https://platform.openai.com/docs/guides/search

### Code snippets

```Python
openai.File.create(
  file=open(""test.jsonl"", ""rb""),
  purpose='search'
)

openai.Search.create(
    model=""ada"",
    query=""happy"",
    max_rerank=5,
    file=""file-Lwjuy0q2ezi00jdpfCbl28CO""
)
```


### OS

windows

### Python version

python 3.7

### Library version

openai-python 0.26.5",closed,2023-02-28T15:42:40Z,2023-02-28T15:52:41Z,['bug'],https://github.com/openai/openai-python/issues/232,0.17,True,missing_option_violation,unknown,"The user attempts to use the 'search' method, which is not available in the specified version of the openai-python library.",high,
226,pip3 install openai does not install required dependencies,"### Describe the bug

When creating a new environment and running pip3 install openai the following modules are required to run some functions within the openai.embedding_utils:

- matplotlib
- plotly

### To Reproduce

1. create a new python environment ex: python3 -m venv venv
2. activate env, ex: source venv/bin/activate
3. import the embedding_utils function get_embedding
4. run file
5. expected output: 
`ModuleNotFoundError: No module named 'matplotlib'`
and after installing matplotlib
`ModuleNotFoundError: No module named 'plotly' `

### Code snippets

```Python
import openai
from openai.embeddings_utils import get_embedding
""""""
will receive the ModuleNotFoundErrors mentioned above. 
Expected result would be: install the above dependencies when installing openai
""""""
```


### OS

macOS

### Python version

Python v3.10.5

### Library version

openai-python v0.26.5",closed,2023-02-21T22:04:27Z,2023-02-22T19:38:16Z,['bug'],https://github.com/openai/openai-python/issues/226,21.56,False,,,,low,
223,No clear information or method to retrieve Usage.,"### Describe the bug

Hi Team,

It seems that there is no way to get usage / quota information when using the api. I am not sure if this feature exists.
Was this available in earlier versions ? I am using openai==0.26.5
 

### To Reproduce

1. Try to fetch the usage of an organization.

### Code snippets

_No response_

### OS

ubuntu

### Python version

Python 3.8.10

### Library version

openai==0.26.5",closed,2023-02-17T23:03:27Z,2023-02-18T21:22:00Z,['bug'],https://github.com/openai/openai-python/issues/223,22.31,True,missing_dependency_violation,medium,"The API does not provide a method to retrieve usage information, which may indicate a missing dependency in the API's functionality.",high,
220,What are the depencies? Why are these not specified in setup.py,"### Describe the bug

Installing openai on a clean repo doesn't work because dependencies are missing such as numpy and matplotlib.

### To Reproduce

- 

### Code snippets

_No response_

### OS

linux

### Python version

3.8

### Library version

latest",closed,2023-02-14T12:37:27Z,2023-02-14T16:49:14Z,['bug'],https://github.com/openai/openai-python/issues/220,4.2,False,,,,low,
181,Difference response between requests from openai package and api.openai.com,"Hi,
This is a simple example that return difference response between request from `openai` package and through the HTTP request
```
Human: ""Give me an example sentence for the word: python""

> package:  'The python coiled itself around the tree branch.'
> HTTP request: 'Example: I am learning to code in Python this summer.'
```
Sometime, the HTTP response contains other words than `Example`, such as `Computer`, `AI`, `Bot`, `Robot`, ...
I wonder what is parameter to exclude these prefix in the response from HTTP request?
Thanks",closed,2023-01-13T11:00:47Z,2023-02-06T19:00:04Z,[],https://github.com/openai/openai-python/issues/181,583.99,True,missing_option_violation,medium,"The user is not following the recommended parameter options to control the response format, leading to inconsistent outputs.",high,
213,openai.error.RateLimitError: The server had an error while processing your request. Sorry about that!,"I encountered many `RateLimitError` errors when I accessed the Completion interface today, but I actually did not call frequently, only about a few times a minute.

error message:
```bash
openai.error.RateLimitError: The server had an error while processing your request. Sorry about that!
```

my request:
```python
response = openai.Completion.create(
      model=""text-davinci-003"",  
      prompt=query,
      temperature=0.9,  
      max_tokens=1200,  
      top_p=1,
      frequency_penalty=0.0,  
      presence_penalty=0.0,  
      stop=[""#""]
  )
```
my env: 

+ Python 3.7.5
+ ubuntu 16.04
",closed,2023-02-06T13:58:06Z,2023-02-06T18:50:16Z,[],https://github.com/openai/openai-python/issues/213,4.87,True,input_value_violation,unknown,"The RateLimitError suggests that the input request may exceed the allowed rate limits set by the API, even if the user perceives their call frequency as low.",medium,
208,Unable to access ChatGPT from this openai package,"I am trying to get ChatGPT model from list of OpenAI models using following code. But I can not find the ChatGPT in the list. Do OpenAI provide dedicated API to communicate with ChatGPT?

```
import openai
openai.api_key = ""MY SECRET KEY""

# list engines
engines = openai.Engine.list()

# print the first engine's id
print(engines)
```

This results in the following response

```
{                                                                                  
  ""data"": [                                                                        
    {                                                                              
      ""created"": null,                                                             
      ""id"": ""babbage"",                                                             
      ""object"": ""engine"",                                                          
      ""owner"": ""openai"",                                                           
      ""permissions"": null,                                                         
      ""ready"": true                                                                
    },                                                                             
    {                                                                              
      ""created"": null,                                                             
      ""id"": ""ada"",                                                                 
      ""object"": ""engine"",                                                          
      ""owner"": ""openai"",                                                           
      ""permissions"": null,                                                         
      ""ready"": true                                                                
    },                                                                             
    {                                                                              
      ""created"": null,                                                             
      ""id"": ""davinci"",                                                             
      ""object"": ""engine"",                                                          
      ""owner"": ""openai"",                                                           
      ""permissions"": null,                                                         
      ""ready"": true                                                                
    },                                                                             
    {                                                                              
      ""created"": null,                                                             
      ""id"": ""text-embedding-ada-002"",                                              
      ""object"": ""engine"",                                                          
      ""owner"": ""openai-internal"",                                                  
      ""permissions"": null,                                                         
      ""ready"": true                                                                
    },                                                                             
    {                                                                              
      ""created"": null,                                                             
      ""id"": ""babbage-code-search-code"",                                            
      ""object"": ""engine"",                                                          
      ""owner"": ""openai-dev"",                                                       
      ""permissions"": null,                                                         
      ""ready"": true                                                                
    },                                                                             
    {                                                                              
      ""created"": null,                                                             
      ""id"": ""text-similarity-babbage-001"",                                         
      ""object"": ""engine"",                                                          
      ""owner"": ""openai-dev"",                                                       
      ""permissions"": null,                                                         
      ""ready"": true                                                                
    },                                                                             
    {                                                                              
      ""created"": null,                                                             
      ""id"": ""text-davinci-003"",                                                    
      ""object"": ""engine"",                                                          
      ""owner"": ""openai-internal"",                                                  
      ""permissions"": null,                                                         
      ""ready"": true                                                                
    },                                                                             
    {                                                                              
      ""created"": null,                                                             
      ""id"": ""text-davinci-001"",                                                    
      ""object"": ""engine"",                                                          
      ""owner"": ""openai"",                                                           
      ""permissions"": null,                                                         
      ""ready"": true                                                                
    },                                                                             
    {                                                                              
      ""created"": null,                                                             
      ""id"": ""curie-instruct-beta"",                                                 
      ""object"": ""engine"",                                                          
      ""owner"": ""openai"",                                                           
      ""permissions"": null,                                                         
      ""ready"": true                                                                
    },                                                                             
    {                                                                              
      ""created"": null,                                                             
      ""id"": ""babbage-code-search-text"",                                            
      ""object"": ""engine"",                                                          
      ""owner"": ""openai-dev"",                                                       
      ""permissions"": null,                                                         
      ""ready"": true                                                                
    },                                                                             
    {                                                                              
      ""created"": null,                                                             
      ""id"": ""babbage-similarity"",                                                  
      ""object"": ""engine"",                                                          
      ""owner"": ""openai-dev"",                                                       
      ""permissions"": null,                                                         
      ""ready"": true                                                                
    },                                                                             
    {                                                                              
      ""created"": null,                                                             
      ""id"": ""curie-search-query"",                                                  
      ""object"": ""engine"",                                                          
      ""owner"": ""openai-dev"",                                                       
      ""permissions"": null,                                                         
      ""ready"": true                                                                
    },                                                                             
    {                                                                              
      ""created"": null,                                                             
      ""id"": ""code-search-babbage-text-001"",                                        
      ""object"": ""engine"",                                                          
      ""owner"": ""openai-dev"",                                                       
      ""permissions"": null,                                                         
      ""ready"": true                                                                
    },                                                                             
    {                                                                              
      ""created"": null,                                                             
      ""id"": ""code-cushman-001"",                                                    
      ""object"": ""engine"",                                                          
      ""owner"": ""openai"",                                                           
      ""permissions"": null,                                                         
      ""ready"": true                                                                
    },                                                                             
    {                                                                              
      ""created"": null,                                                             
      ""id"": ""code-search-babbage-code-001"",                                        
      ""object"": ""engine"",                                                          
      ""owner"": ""openai-dev"",                                                       
      ""permissions"": null,                                                         
      ""ready"": true                                                                
    },                                                                             
    {                                                                              
      ""created"": null,                                                             
      ""id"": ""audio-transcribe-deprecated"",                                         
      ""object"": ""engine"",                                                          
      ""owner"": ""openai-internal"",                                                  
      ""permissions"": null,                                                         
      ""ready"": true                                                                
    },                                                                             
    {                                                                              
      ""created"": null,                                                             
      ""id"": ""code-davinci-002"",                                                    
      ""object"": ""engine"",                                                          
      ""owner"": ""openai"",                                                           
      ""permissions"": null,                                                         
      ""ready"": true                                                                
    },                                                                             
    {                                                                              
      ""created"": null,                                                             
      ""id"": ""text-ada-001"",                                                        
      ""object"": ""engine"",                                                          
      ""owner"": ""openai"",                                                           
      ""permissions"": null,                                                         
      ""ready"": true                                                                
    },                                                                             
    {                                                                              
      ""created"": null,                                                             
      ""id"": ""text-similarity-ada-001"",                                             
      ""object"": ""engine"",                                                          
      ""owner"": ""openai-dev"",                                                       
      ""permissions"": null,                                                         
      ""ready"": true                                                                
    },                                                                             
    {                                                                              
      ""created"": null,                                                             
      ""id"": ""text-davinci-insert-002"",                                             
      ""object"": ""engine"",                                                          
      ""owner"": ""openai"",                                                           
      ""permissions"": null,                                                         
      ""ready"": true                                                                
    },                                                                             
    {                                                                              
      ""created"": null,                                                             
      ""id"": ""ada-code-search-code"",                                                
      ""object"": ""engine"",                                                          
      ""owner"": ""openai-dev"",                                                       
      ""permissions"": null,                                                         
      ""ready"": true                                                                
    },                                                                             
    {                                                                              
      ""created"": null,                                                             
      ""id"": ""text-davinci-002"",                                                    
      ""object"": ""engine"",                                                          
      ""owner"": ""openai"",                                                           
      ""permissions"": null,                                                         
      ""ready"": true                                                                
    },                                                                             
    {                                                                              
      ""created"": null,                                                             
      ""id"": ""ada-similarity"",                                                      
      ""object"": ""engine"",                                                          
      ""owner"": ""openai-dev"",                                                       
      ""permissions"": null,                                                         
      ""ready"": true                                                                
    },                                                                             
    {                                                                              
      ""created"": null,                                                             
      ""id"": ""code-search-ada-text-001"",                                            
      ""object"": ""engine"",                                                          
      ""owner"": ""openai-dev"",                                                       
      ""permissions"": null,                                                         
      ""ready"": true                                                                
    },                                                                             
    {                                                                              
      ""created"": null,                                                             
      ""id"": ""text-search-ada-query-001"",                                           
      ""object"": ""engine"",                                                          
      ""owner"": ""openai-dev"",                                                       
      ""permissions"": null,                                                         
      ""ready"": true                                                                
    },                                                                             
    {                                                                              
      ""created"": null,                                                             
      ""id"": ""text-curie-001"",                                                      
      ""object"": ""engine"",                                                          
      ""owner"": ""openai"",                                                           
      ""permissions"": null,                                                         
      ""ready"": true                                                                
    },                                                                             
    {                                                                              
      ""created"": null,                                                             
      ""id"": ""text-davinci-edit-001"",                                               
      ""object"": ""engine"",                                                          
      ""owner"": ""openai"",                                                           
      ""permissions"": null,                                                         
      ""ready"": true                                                                
    },                                                                             
    {                                                                              
      ""created"": null,                                                             
      ""id"": ""davinci-search-document"",                                             
      ""object"": ""engine"",                                                          
      ""owner"": ""openai-dev"",                                                       
      ""permissions"": null,                                                         
      ""ready"": true                                                                
    },                                                                             
    {                                                                              
      ""created"": null,                                                             
      ""id"": ""ada-code-search-text"",                                                
      ""object"": ""engine"",                                                          
      ""owner"": ""openai-dev"",                                                       
      ""permissions"": null,                                                         
      ""ready"": true                                                                
    },                                                                             
    {                                                                              
      ""created"": null,                                                             
      ""id"": ""text-search-ada-doc-001"",                                             
      ""object"": ""engine"",                                                          
      ""owner"": ""openai-dev"",                                                       
      ""permissions"": null,                                                         
      ""ready"": true                                                                
    },                                                                             
    {                                                                              
      ""created"": null,                                                             
      ""id"": ""code-davinci-edit-001"",                                               
      ""object"": ""engine"",                                                          
      ""owner"": ""openai"",                                                           
      ""permissions"": null,                                                         
      ""ready"": true                                                                
    },                                                                             
    {                                                                              
      ""created"": null,                                                             
      ""id"": ""davinci-instruct-beta"",                                               
      ""object"": ""engine"",                                                          
      ""owner"": ""openai"",                                                           
      ""permissions"": null,                                                         
      ""ready"": true                                                                
    },                                                                             
    {                                                                              
      ""created"": null,                                                             
      ""id"": ""text-similarity-curie-001"",                                           
      ""object"": ""engine"",                                                          
      ""owner"": ""openai-dev"",                                                       
      ""permissions"": null,                                                         
      ""ready"": true                                                                
    },                                                                             
    {                                                                              
      ""created"": null,                                                             
      ""id"": ""code-search-ada-code-001"",                                            
      ""object"": ""engine"",                                                          
      ""owner"": ""openai-dev"",                                                       
      ""permissions"": null,                                                         
      ""ready"": true                                                                
    },                                                                             
    {                                                                              
      ""created"": null,                                                             
      ""id"": ""ada-search-query"",                                                    
      ""object"": ""engine"",                                                          
      ""owner"": ""openai-dev"",                                                       
      ""permissions"": null,                                                         
      ""ready"": true                                                                
    },                                                                             
    {                                                                              
      ""created"": null,                                                             
      ""id"": ""text-search-davinci-query-001"",                                       
      ""object"": ""engine"",                                                          
      ""owner"": ""openai-dev"",                                                       
      ""permissions"": null,                                                         
      ""ready"": true                                                                
    },                                                                             
    {                                                                              
      ""created"": null,                                                             
      ""id"": ""davinci-search-query"",                                                
      ""object"": ""engine"",                                                          
      ""owner"": ""openai-dev"",                                                       
      ""permissions"": null,                                                         
      ""ready"": true                                                                
    },                                                                             
    {                                                                              
      ""created"": null,                                                             
      ""id"": ""text-davinci-insert-001"",                                             
      ""object"": ""engine"",                                                          
      ""owner"": ""openai"",                                                           
      ""permissions"": null,                                                         
      ""ready"": true                                                                
    },                                                                             
    {                                                                              
      ""created"": null,                                                             
      ""id"": ""babbage-search-document"",                                             
      ""object"": ""engine"",                                                          
      ""owner"": ""openai-dev"",                                                       
      ""permissions"": null,                                                         
      ""ready"": true                                                                
    },                                                                             
    {                                                                              
      ""created"": null,                                                             
      ""id"": ""ada-search-document"",                                                 
      ""object"": ""engine"",                                                          
      ""owner"": ""openai-dev"",                                                       
      ""permissions"": null,                                                         
      ""ready"": true                                                                
    },                                                                             
    {                                                                              
      ""created"": null,                                                             
      ""id"": ""text-search-babbage-doc-001"",                                         
      ""object"": ""engine"",                                                          
      ""owner"": ""openai-dev"",                                                       
      ""permissions"": null,                                                         
      ""ready"": true                                                                
    },                                                                             
    {                                                                              
      ""created"": null,                                                             
      ""id"": ""text-search-curie-doc-001"",                                           
      ""object"": ""engine"",                                                          
      ""owner"": ""openai-dev"",                                                       
      ""permissions"": null,                                                         
      ""ready"": true                                                                
    },                                                                             
    {                                                                              
      ""created"": null,                                                             
      ""id"": ""text-search-curie-query-001"",                                         
      ""object"": ""engine"",                                                          
      ""owner"": ""openai-dev"",                                                       
      ""permissions"": null,                                                         
      ""ready"": true                                                                
    },                                                                             
    {                                                                              
      ""created"": null,                                                             
      ""id"": ""babbage-search-query"",                                                
      ""object"": ""engine"",                                                          
      ""owner"": ""openai-dev"",                                                       
      ""permissions"": null,                                                         
      ""ready"": true                                                                
    },                                                                             
    {                                                                              
      ""created"": null,                                                             
      ""id"": ""text-babbage-001"",                                                    
      ""object"": ""engine"",                                                          
      ""owner"": ""openai"",                                                           
      ""permissions"": null,                                                         
      ""ready"": true                                                                
    },                                                                             
    {                                                                              
      ""created"": null,                                                             
      ""id"": ""text-search-davinci-doc-001"",                                         
      ""object"": ""engine"",                                                          
      ""owner"": ""openai-dev"",                                                       
      ""permissions"": null,                                                         
      ""ready"": true                                                                
    },                                                                             
    {                                                                              
      ""created"": null,                                                             
      ""id"": ""text-search-babbage-query-001"",                                       
      ""object"": ""engine"",                                                          
      ""owner"": ""openai-dev"",                                                       
      ""permissions"": null,                                                         
      ""ready"": true                                                                
    },                                                                             
    {                                                                              
      ""created"": null,                                                             
      ""id"": ""curie-similarity"",                                                    
      ""object"": ""engine"",                                                          
      ""owner"": ""openai-dev"",                                                       
      ""permissions"": null,                                                         
      ""ready"": true                                                                
    },                                                                             
    {                                                                              
      ""created"": null,                                                             
      ""id"": ""curie-search-document"",                                               
      ""object"": ""engine"",                                                          
      ""owner"": ""openai-dev"",                                                       
      ""permissions"": null,                                                         
      ""ready"": true                                                                
    },                                                                             
    {                                                                              
      ""created"": null,                                                             
      ""id"": ""curie"",                                                               
      ""object"": ""engine"",                                                          
      ""owner"": ""openai"",                                                           
      ""permissions"": null,                                                         
      ""ready"": true                                                                
    },                                                                             
    {                                                                              
      ""created"": null,                                                             
      ""id"": ""text-similarity-davinci-001"",                                         
      ""object"": ""engine"",                                                          
      ""owner"": ""openai-dev"",                                                       
      ""permissions"": null,                                                         
      ""ready"": true                                                                
    },                                                                             
    {                                                                              
      ""created"": null,                                                             
      ""id"": ""davinci-similarity"",                                                  
      ""object"": ""engine"",                                                          
      ""owner"": ""openai-dev"",                                                       
      ""permissions"": null,                                                         
      ""ready"": true                                                                
    }                                                                              
  ],                                                                               
  ""object"": ""list""                                                                 
} 
```

I can not find ChatGPT in the response. As per my understanding I should get a dedicated id for ChatGPT. I want to directly use ChatGPT like we use other models such as Davinci etc. Any help or information in this regard?",closed,2023-02-02T12:20:57Z,2023-02-05T16:34:03Z,[],https://github.com/openai/openai-python/issues/208,76.22,False,,,,low,
205,"fix yes , i just recently found a funny problem , see this","regards
Doni ramdani
Doniramdani810@gmail.com

![image](https://user-images.githubusercontent.com/108279884/214986941-a7d77060-92cf-4eb8-a330-ca287d6b41f0.png)
",closed,2023-01-27T01:17:18Z,2023-01-27T01:23:20Z,[],https://github.com/openai/openai-python/issues/205,0.1,True,input_value_violation,unknown,"The issue lacks clear context or structured input, making it difficult to determine the exact problem being reported.",high,
189,[0.26.1] Error syncing to wandb,"When running the following command I receive the following warning.

```sh
$ openai wandb sync --project project
wandb: Currently logged in as: me (company). Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in project/wandb/run-20230117_161328-ft-upiwOdAB20VtkGN7uHW5TFv2
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ft-upiwOdAB20VtkGN7uHW5TFv2
wandb: ⭐️ View project at https://wandb.ai/me/project
wandb: 🚀 View run at https://wandb.ai/me/project/runs/ft-upiwOdAB20VtkGN7uHW5TFv2
File file-ISyZKcogrCzx59ftjGPWOUs1 could not be read as a valid JSON file
File file-3GjQ844dqjAY6hYuQKpMf7mQ could not be read as a valid JSON file
```

When I check in wandb, only system metrics are synced. I do not see any of the metrics related to the fine-tuning such as validation loss and token usage.

I am running the following versions.

```
python==3.10.8
openai==0.26.1
wandb==0.13.9
```",closed,2023-01-17T22:24:00Z,2023-01-25T15:20:20Z,[],https://github.com/openai/openai-python/issues/189,184.94,True,input_value_violation,medium,"The error indicates that the files being synced are not valid JSON, which suggests that the input values (file contents) do not adhere to the expected format, potentially leading to performance degradation in syncing metrics.",high,
196,"Regression in download,  `openai api fine_tunes.results`  behavior","To reproduce, on the older `openai==0.25.0`

`openai api fine_tunes.results -i <job_id_here>`

You'll  get the output of the actual CSV file

E.g.
```
step,elapsed_tokens,elapsed_examples,training_loss,training_sequence_accuracy,training_token_accuracy
1,22048,32,0.011019098009307718,0.0,0.0
1,32288,32,0.007613447834811513,0.0,0.0
```


On the latest `openai==0.26.2`

You get instead

```
{
  ""object"": ""file"",
  ""id"": ""file-66NnpqAXzw3In27fROPIU03P"",
  ""purpose"": ""fine-tune-results"",
  ""filename"": ""compiled_results.csv"",
  ""bytes"": 386756,
  ""created_at"": 1674504271,
  ""status"": ""processed"",
  ""status_details"": null
}
```

This is probably due to a change in behavior in `download` of /api_resources/file.py. It probably affects everyone who depends on that download method, so i'm guessing wandb users who are syncing their runs too?

See https://github.com/openai/openai-python/pull/146/files#r1084884786


Probably related to https://github.com/openai/openai-python/pull/185, https://github.com/openai/openai-python/pull/183
",closed,2023-01-24T07:04:51Z,2023-01-25T04:21:02Z,[],https://github.com/openai/openai-python/issues/196,21.27,True,missing_option_violation,medium,The change in output format from CSV to JSON indicates a missing option for users who expect the previous behavior.,high,
186,Fine-tuning without using CLI,"Hello everyone, is there any example or snippet of code on how to fine-tune using the `FineTune` class directly in Python script, instead of the CLI commands?",closed,2023-01-16T13:20:20Z,2023-01-23T18:37:41Z,[],https://github.com/openai/openai-python/issues/186,173.29,True,missing_option_violation,medium,"The user is attempting to fine-tune using the `FineTune` class directly without following the recommended CLI usage, which may lead to suboptimal behavior.",high,
192,`ModuleNotFoundError: No module named 'openai'` even though installed,"<img width=""660"" alt=""Screenshot 2023-01-23 at 12 52 32 AM"" src=""https://user-images.githubusercontent.com/87835921/213999217-6f22d4f3-d6e9-4cb1-bbb7-4f2cee8ab700.png"">
Hello, 
I created a new python=3.9 conda environment, and installed openai with pip. However, I found that I am unable to import it. 
Would anyone be willing to advise? Many thanks!",closed,2023-01-23T08:57:40Z,2023-01-23T18:32:05Z,[],https://github.com/openai/openai-python/issues/192,9.57,False,,,,low,
166,"Completion api injecting restart text after every prompt, no option to switch it off like in playground","I'm not sure this is an issue with the python api per se.
When using the completion api to generate the next line of a poem using davinci, the api always starts the text with ""\n\n"". This can't be filtered with the logit parameter as the phrase ""\n\n"" is actually 4 tokens according to the tokenizer: [59, 77, 59, 77]

What is the correct workaround?
Is there any plan to add the option ""Inject restart text"" like in the playground? The restart text adds 4 tokens to each request, which adds up fast...",closed,2023-01-06T12:07:50Z,2023-01-13T17:23:57Z,[],https://github.com/openai/openai-python/issues/166,173.27,True,missing_option_violation,medium,"The API does not provide an option to disable the automatic injection of restart text, which is a feature available in the playground.",high,
141,Too verbose logs,"Hi, is there any way to reduce these logs?

```
message='OpenAI API response' path=https://api.openai.com/v1/fine-tunes processing_ms=228 request_id=FOO response_code=200
INFO:openai:message='OpenAI API response' path=https://api.openai.com/v1/fine-tunes processing_ms=228 request_id=BAR response_code=200
```

To me, these logs should be DEBUG.",closed,2022-11-27T11:35:40Z,2023-01-13T01:06:52Z,[],https://github.com/openai/openai-python/issues/141,1117.52,True,missing_option_violation,medium,"The logging level for the API response messages is not aligned with the expected verbosity, suggesting that a more appropriate logging option (DEBUG) is not being utilized.",high,
171,openai.Completion.acreate(stream=True) raises an Exception,"See stack trace below

```
Traceback (most recent call last):
  File ""/Users/nuno/dev/concat/server-py/concat/conversation.py"", line 75, in handle_conversation
    await run
  File ""/Users/nuno/dev/concat/server-py/concat/agent/openai/openai.py"", line 122, in __call__
    msg = await ctx.stream_message_async(
  File ""/Users/nuno/dev/concat/server-py/concat/context.py"", line 287, in stream_message_async
    async for text in iterator:
  File ""/Users/nuno/dev/concat/server-py/concat/agent/openai/openai.py"", line 60, in openai_completion_stream
    iterator = await openai.Completion.acreate(*args, **kwargs, stream=True)
  File ""/Users/nuno/Library/Caches/pypoetry/virtualenvs/platform-api-30l3Kv3P-py3.10/lib/python3.10/site-packages/openai/api_resources/completion.py"", line 45, in acreate
    return await super().acreate(*args, **kwargs)
  File ""/Users/nuno/Library/Caches/pypoetry/virtualenvs/platform-api-30l3Kv3P-py3.10/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py"", line 230, in acreate
    return (
TypeError: 'async_generator' object is not iterable
```

I believe this should be an `async for` in line 239 of engine_api_resource.py. Once I make that change, then I get this error

```
Traceback (most recent call last):
  File ""/Users/nuno/Library/Caches/pypoetry/virtualenvs/platform-api-30l3Kv3P-py3.10/lib/python3.10/site-packages/openai/api_requestor.py"", line 645, in _interpret_response_line
    data = json.loads(rbody)
  File ""/opt/homebrew/Cellar/python@3.10/3.10.9/Frameworks/Python.framework/Versions/3.10/lib/python3.10/json/__init__.py"", line 346, in loads
    return _default_decoder.decode(s)
  File ""/opt/homebrew/Cellar/python@3.10/3.10.9/Frameworks/Python.framework/Versions/3.10/lib/python3.10/json/decoder.py"", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File ""/opt/homebrew/Cellar/python@3.10/3.10.9/Frameworks/Python.framework/Versions/3.10/lib/python3.10/json/decoder.py"", line 355, in raw_decode
    raise JSONDecodeError(""Expecting value"", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 2 column 1 (char 1)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/Users/nuno/dev/concat/server-py/concat/conversation.py"", line 75, in handle_conversation
    await run
  File ""/Users/nuno/dev/concat/server-py/concat/agent/openai/openai.py"", line 122, in __call__
    msg = await ctx.stream_message_async(
  File ""/Users/nuno/dev/concat/server-py/concat/context.py"", line 287, in stream_message_async
    async for text in iterator:
  File ""/Users/nuno/dev/concat/server-py/concat/agent/openai/openai.py"", line 61, in openai_completion_stream
    async for chunk in iterator:
  File ""/Users/nuno/Library/Caches/pypoetry/virtualenvs/platform-api-30l3Kv3P-py3.10/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py"", line 230, in <genexpr>
    return (
  File ""/Users/nuno/Library/Caches/pypoetry/virtualenvs/platform-api-30l3Kv3P-py3.10/lib/python3.10/site-packages/openai/api_requestor.py"", line 611, in <genexpr>
    self._interpret_response_line(
  File ""/Users/nuno/Library/Caches/pypoetry/virtualenvs/platform-api-30l3Kv3P-py3.10/lib/python3.10/site-packages/openai/api_requestor.py"", line 647, in _interpret_response_line
    raise error.APIError(
openai.error.APIError: HTTP code 200 from API (
)
```

which appears to be caused by trying to json-decode the following string `\n`. Once I fix that (by doing eg. `data = json.loads(rbody) if rbody.strip() else None`, it appears the stream never finishes, ie. the next part never arrives

",closed,2023-01-07T10:18:50Z,2023-01-12T19:36:51Z,[],https://github.com/openai/openai-python/issues/171,129.3,False,,,,low,
160,Supplying API keys per request,"I'd like to supply the API key for each request separatly (for different users of the same app), instead of `openai.api_key = ...`.

I find this strange behaviour, as documented in the following code, see in particular the `_UNEXPECTED_BEHAVIOR` tests.
Edit: the following tests all pass with `openai==0.25.0`.

```Python
import os
import openai
import unittest

from openai.error import AuthenticationError, RateLimitError

# No API key is provided implicitly, to avoid leakage
assert os.environ.get('OPENAI_API_KEY') is None

# This is my API key
MINE = os.environ['MY_OPENAI_API_KEY']

prompt = ""Give me three anagrams of 'Randompile':""


class NoKeyError(Exception):
    pass


def inspector(f):
    def _(*a, **kw):
        try:
            result = f(*a, **kw)
        except RateLimitError:
            raise RuntimeError(""This test assumes the API key is still valid."")
        except AuthenticationError as ex:
            if ""No API key provided"" in ex.user_message:
                raise NoKeyError
            else:
                raise
        else:
            return result

    return _


@inspector
def completion(*, key_constr=None, key_member=None):
    worker = openai.Completion(api_key=key_constr)
    return worker.create(api_key=key_member, model='text-davinci-003', prompt=prompt)


@inspector
def dalleimage(*, key_constr=None, key_member=None):
    worker = openai.Image(api_key=key_constr)
    return worker.create(api_key=key_member, prompt=prompt)


class ObservedBehavior(unittest.TestCase):
    def test_completion_00(self):
        with self.assertRaises(NoKeyError):
            completion(key_constr=None, key_member=None)

    def test_completion_10_UNEXPECTED_BEHAVIOUR(self):
        with self.assertRaises(NoKeyError):
            completion(key_constr=MINE, key_member=None)

    def test_completion_01(self):
        result = completion(key_constr=None, key_member=MINE)
        # print(f""{prompt}"", result['choices'][0]['text'].strip(), sep='\n')

    def test_completion_11(self):
        result = completion(key_constr=MINE, key_member=MINE)
        # print(f""{prompt}"", result['choices'][0]['text'].strip(), sep='\n')

    def test_dalleimage_00(self):
        with self.assertRaises(NoKeyError):
            dalleimage(key_constr=None, key_member=None)

    def test_dalleimage_10_UNEXPECTED_BEHAVIOUR(self):
        with self.assertRaises(NoKeyError):
            dalleimage(key_constr=MINE, key_member=None)

    def test_dalleimage_01_UNEXPECTED_BEHAVIOUR(self):
        with self.assertRaises(NoKeyError):
            dalleimage(key_constr=None, key_member=MINE)

    def test_dalleimage_11_UNEXPECTED_BEHAVIOUR(self):
        with self.assertRaises(NoKeyError):
            dalleimage(key_constr=MINE, key_member=MINE)


if __name__ == '__main__':
    unittest.main()
```",closed,2022-12-22T12:55:50Z,2023-01-06T20:27:50Z,[],https://github.com/openai/openai-python/issues/160,367.53,True,missing_dependency_violation,High,"The API methods require an API key to be provided, but the tests demonstrate scenarios where the key is not supplied, leading to authentication errors.",high,
162,getting a failure when running the search example - https://beta.openai.com/docs/guides/search,"when running the Search.create call I am getting a failure

openai.Search.create(
    model=""ada"",
    query=""happy"", 
    max_rerank=5,
    file=""file-xVMG7P66WxKS1zwzEBKUGGzl""
)


here is the failure:

InvalidRequestError                       Traceback (most recent call last)
<ipython-input-16-d83ff76dd238> in <module>
----> 1 openai.Search.create(
      2     model=""ada"",
      3     query=""happy"",
      4     max_rerank=5,
      5     file=""file-xVMG7P66WxKS1zwzEBKUGGzl""

4 frames
/usr/local/lib/python3.8/dist-packages/openai/api_requestor.py in _interpret_response_line(self, rbody, rcode, rheaders, stream)
    427         stream_error = stream and ""error"" in resp.data
    428         if stream_error or not 200 <= rcode < 300:
--> 429             raise self.handle_error_response(
    430                 rbody, rcode, resp.data, rheaders, stream_error=stream_error
    431             )

InvalidRequestError: Invalid URL (POST /v1/search)",closed,2022-12-23T13:51:54Z,2023-01-01T21:12:06Z,[],https://github.com/openai/openai-python/issues/162,223.34,True,input_value_violation,high,"The API call to Search.create is failing due to an invalid request, likely caused by an unacceptable or out-of-range input value, such as an incorrect model name or file identifier.",high,
161,AttributeError: partially initialized module 'openai' has no attribute 'Completion',,closed,2022-12-23T13:44:14Z,2022-12-23T13:46:57Z,[],https://github.com/openai/openai-python/issues/161,0.05,True,input_type_violation,unknown,"The error indicates a potential issue with the module import or initialization, suggesting that the expected attribute 'Completion' is not available, possibly due to incorrect module usage or version mismatch.",high,
156,1(st),Test,closed,2022-12-19T18:48:38Z,2022-12-20T17:35:12Z,[],https://github.com/openai/openai-python/issues/156,22.78,True,input_value_violation,unknown,The input provided in the issue body is insufficient and does not adhere to expected input values or formats.,high,
139,Compute nb of tokens for a prompt,"Hi,
Is there a way to compute the number of tokens in the prompt before sending a request (in python)?
I'd like to avoid the ""too many tokens"" error by setting max_token to 4097 - nb_tokens(prompt)

Thanks!",closed,2022-11-23T18:50:37Z,2022-12-20T17:38:13Z,[],https://github.com/openai/openai-python/issues/139,646.79,True,input_value_violation,performance_degradation,"The user is attempting to calculate the number of tokens dynamically to avoid exceeding the limit, which may lead to performance issues if not handled correctly.",medium,
149,"After fine tuning gpt3 model, how to know f1 score as a evaluation?","After fine tuning gpt3 for text generation, so how I can know f1 score using openai cli ? ",closed,2022-12-11T09:57:27Z,2022-12-20T17:37:39Z,[],https://github.com/openai/openai-python/issues/149,223.67,True,missing_dependency_violation,medium,"The user is missing the necessary context or method calls to evaluate the model's performance, specifically the F1 score, after fine-tuning.",high,
155,[Question] Does setting n=2 double prompt token usage?,"If I use the `n` parameter on `Completion.create`, the documentation states

> Note: Because this parameter generates many completions, it can quickly consume your token quota. Use carefully and ensure that you have reasonable settings for `max_tokens` and `stop`.

I recognize the model will increase the output token cost, but does it increase the input token cost as well?",closed,2022-12-19T15:32:38Z,2022-12-19T16:50:17Z,[],https://github.com/openai/openai-python/issues/155,1.29,False,,,,,
147,partially initialized module 'openai' has no attribute 'Completion',"python gpt2.py
Traceback (most recent call last):
  File ""D:\EVE_NG\gpt2.py"", line 2, in <module>
    import openai
  File ""D:\EVE_NG\openai.py"", line 8, in <module>
    response = openai.Completion.create(
               ^^^^^^^^^^^^^^^^^
AttributeError: partially initialized module 'openai' has no attribute 'Completion' (most likely due to a circular import)
",closed,2022-12-07T05:13:27Z,2022-12-17T03:57:54Z,[],https://github.com/openai/openai-python/issues/147,238.74,True,missing_dependency_violation,High,"The error indicates a circular import issue, suggesting that the necessary dependencies for the 'openai' module are not properly initialized before usage.",high,
145,"You've made too many requests, please wait one minute and try again. ","Request ID: ..
Please contact us through our [help center](https://help.openai.com/en/)
if this issue persists.
Return to homepage


",closed,2022-12-05T08:33:34Z,2022-12-11T17:43:42Z,[],https://github.com/openai/openai-python/issues/145,153.17,True,input_value_violation,medium,"The API has rate limiting in place, which indicates that the input value (number of requests) exceeds the acceptable threshold.",high,
143,Encoding Error on Windows with WandB,"When syncing with `openai wandb sync`, I get a character encoding issue on one of the run files. This happens specifically on Windows which often struggles with the default encoding on a file with `open(filename)`.

One solution is `artifact.new_file(filename, ""w"", encoding=""utf-8"")` on line 279 of the `wandb_logger.py` which solves the problem locally for me. Alternatively, using a default of `""utf-8""` in the `artifact.new_file` function should work too, but might have other unintended side effects.

Here is the output:
```
wandb: ERROR Failed to open the provided file (UnicodeEncodeError: 'charmap' codec can't encode character '\u03bc' in position 205764: character maps to <undefined>). Please provide the proper encoding.
Traceback (most recent call last):
  File ""C:\Users\miniconda3\lib\runpy.py"", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File ""C:\Users\kaiser\miniconda3\lib\runpy.py"", line 87, in _run_code
    exec(code, run_globals)
  File ""C:\venv\Scripts\openai.exe\__main__.py"", line 7, in <module>
    sys.exit(main())
  File ""C:\venv\lib\site-packages\openai\_openai_scripts.py"", line 63, in main
    args.func(args)
  File ""C:\venv\lib\site-packages\openai\cli.py"", line 586, in sync
    resp = openai.wandb_logger.WandbLogger.sync(
  File ""C:\venv\lib\site-packages\openai\wandb_logger.py"", line 74, in sync
    fine_tune_logged = [
  File ""C:\venv\lib\site-packages\openai\wandb_logger.py"", line 75, in <listcomp>
    cls._log_fine_tune(
  File ""C:\venv\lib\site-packages\openai\wandb_logger.py"", line 172, in _log_fine_tune
    cls._log_artifacts(fine_tune, project, entity)
  File ""C:\venv\lib\site-packages\openai\wandb_logger.py"", line 236, in _log_artifacts
    cls._log_artifact_inputs(file, prefix, artifact_type, project, entity)
  File ""C:\venv\lib\site-packages\openai\wandb_logger.py"", line 280, in _log_artifact_inputs
    f.write(file_content)
  File ""C:\Users\miniconda3\lib\encodings\cp1252.py"", line 19, in encode
    return codecs.charmap_encode(input,self.errors,encoding_table)[0]
UnicodeEncodeError: 'charmap' codec can't encode character '\u03bc' in position 205764: character maps to <undefined>
```",closed,2022-12-02T16:40:08Z,2022-12-07T17:29:32Z,[],https://github.com/openai/openai-python/issues/143,120.82,False,,,,low,
142,Different request methods lead to different results.,"Hi,

I requested through python-client-library ""openai"" and restful request—all with default parameters.

With identical input text, it returned different results. Besides the difference in the output text, the other difference appears in ""finish_reason"". One is ""stop"", and the other is ""length"".

Could you please let me know the reason why the results were different?
",closed,2022-12-02T05:38:30Z,2022-12-05T02:02:59Z,[],https://github.com/openai/openai-python/issues/142,68.41,True,missing_option_violation,medium,"The different request methods (python-client-library vs. RESTful request) may not adhere to the same contract options, leading to variations in output and finish_reason.",high,
144,"openai.Image.create_edit() requires a mask image to be passed, but the API endpoint list this parameter as optional","The Image Edit endpoint of the API can be utilized to create an entirely new, but similar image when supplied with both an image and a prompt. This feature is functionally different than the variant creation endpoint. The Edit image endpoint also has the Mask parameter, which can be used to supply an existing image mask to be edited.  

The Python function for this feature is limited, in that the Mask parameter is not optional, therefore the creation of a new, non-mask-based image is not possible. Only half of the image edit functionality can be utilized. 

![Screen Shot 2022-12-04 at 5 40 30 PM](https://user-images.githubusercontent.com/6175652/205523107-65677ec8-9c6a-4695-bcbb-ebd7c245ff46.png)
![Screen Shot 2022-12-04 at 5 40 51 PM](https://user-images.githubusercontent.com/6175652/205523108-a69c9b98-02aa-4d86-897e-b1d4dd5c9ab6.png)
",closed,2022-12-04T23:46:34Z,2022-12-05T00:02:17Z,[],https://github.com/openai/openai-python/issues/144,0.26,True,missing_option_violation,high,"The API documentation states that the Mask parameter is optional, but the implementation requires it, limiting functionality.",high,
132,"How are the text-search models {doc,query} different?","Seems to me, these two models should be the same?
After all, its the same vector space.

```
text-search-ada-doc-001
text-search-ada-query-001
```
How would they be different to get a different nomenclature?
",closed,2022-10-21T20:49:05Z,2022-10-30T10:15:33Z,[],https://github.com/openai/openai-python/issues/132,205.44,False,,,,,
130,can't find finetuning example,"hello

Clicking on the olympics example shows code has moved but I cannot access it either in the cookbook nor in openai-python
This code example has moved. You can now find it in the [OpenAI Cookbook](https://github.com/openai/openai-cookbook) at [examples/fine-tuned_qa/](https://github.com/openai/openai-cookbook/tree/main/examples/fine-tuned_qa).",closed,2022-10-18T01:33:47Z,2022-10-26T04:12:58Z,[],https://github.com/openai/openai-python/issues/130,194.65,True,missing_dependency_violation,medium,"The issue indicates a missing reference to the updated location of the finetuning example, which may lead to confusion and hinder users from accessing necessary resources.",high,
121,KeyError when removing long examples after removing duplicate rows,"Error: 
```
openai tools fine_tunes.prepare_data -f training_data_2022-09-14.jsonl
Analyzing...

- Your file contains 2446 prompt-completion pairs
Based on the analysis we will perform the following actions:
- [Recommended] Remove 1155 duplicate rows [Y/n]: y
- [Recommended] Remove 49 long examples [Y/n]: y
Traceback (most recent call last):
  File ""/Users/ser/project/project-venv/bin/openai"", line 8, in <module>
    sys.exit(main())
  File ""/Users/ser/project/project-venv/lib/python3.10/site-packages/openai/_openai_scripts.py"", line 63, in main
    args.func(args)
  File ""/Users/ser/project/project-venv/lib/python3.10/site-packages/openai/cli.py"", line 531, in prepare_data
    apply_validators(
  File ""/Users/ser/project/project-venv/lib/python3.10/site-packages/openai/validators.py"", line 851, in apply_validators
    df, optional_applied = apply_optional_remediation(
  File ""/Users/ser/project/project-venv/lib/python3.10/site-packages/openai/validators.py"", line 578, in apply_optional_remediation
    df = remediation.optional_fn(df)
  File ""/Users/ser/project/project-venv/lib/python3.10/site-packages/openai/validators.py"", line 171, in optional_fn
    return x.drop(long_indexes)
  File ""/Users/ser/project/project-venv/lib/python3.10/site-packages/pandas/util/_decorators.py"", line 311, in wrapper
    return func(*args, **kwargs)
  File ""/Users/ser/project/project-venv/lib/python3.10/site-packages/pandas/core/frame.py"", line 4957, in drop
    return super().drop(
  File ""/Users/ser/project/project-venv/lib/python3.10/site-packages/pandas/core/generic.py"", line 4267, in drop
    obj = obj._drop_axis(labels, axis, level=level, errors=errors)
  File ""/Users/ser/project/project-venv/lib/python3.10/site-packages/pandas/core/generic.py"", line 4311, in _drop_axis
    new_axis = axis.drop(labels, errors=errors)
  File ""/Users/ser/project/project-venv/lib/python3.10/site-packages/pandas/core/indexes/base.py"", line 6661, in drop
    raise KeyError(f""{list(labels[mask])} not found in axis"")
KeyError: '[330, 352, 377, 378, 422, 424, 435, 1172, 1194, 1219, 1220, 1264, 1266, 1277, 1468, 1498, 1549, 1641, 1648, 1714, 1741, 1816, 1859, 1984] not found in axis'
```

I believe that since the duplicate rows were removed, many of the long examples are missing, throwing this error. And thus I end up needing to apply the first recommendation and not the second one, and then use the resulting file to apply the second recommendation.

It would be great to be able to apply both changes to the same file.",closed,2022-09-14T23:31:07Z,2022-10-16T23:01:54Z,['bug'],https://github.com/openai/openai-python/issues/121,767.51,True,missing_dependency_violation,High,"The removal of duplicate rows created a dependency issue where the long examples could not be removed due to missing indices, leading to a KeyError.",high,
122,Support api_key parameter in Moderation.create method,"## Description
`create` method in `Moderation` class initializes a new instance by calling parent constructor without any parameters. That's why it's impossible make `Moderation`endpoint calls without setting API key as environment variable or setting it through `openai.api_key` in contrast to `Completion` class.

## How to reproduce
```
import openai

openai.api_key = None
moderator = openai.Moderation(api_key=api_key)

text = ""hello world""
response = moderator.create(input=text, model=""text-moderation-stable"")

> AuthenticationError: No API key provided. You can set your API key in code using 'openai.api_key = <API-KEY>',
or you can set the environment variable OPENAI_API_KEY=<API-KEY>).
If your API key is stored in a file, you can point the openai module at it with 'openai.api_key_path = <PATH>'.
You can generate API keys in the OpenAI web interface. See https://onboard.openai.com for details, or email support@openai.com if you have any questions.
```

## Potential solution
- [x] Accept `api_key` parameter in `Moderation.create`
- [x] Pass `api_key` to super class",closed,2022-09-16T23:22:46Z,2022-09-20T15:56:07Z,[],https://github.com/openai/openai-python/issues/122,88.56,True,missing_dependency_violation,high,"The Moderation.create method lacks the ability to accept an api_key parameter, leading to a dependency on external environment variables for authentication.",high,
113,Add a return-type flag to the CLI fine-tuning command,"I'm using the OpenAI CLI fine-tuning command `openai api fine_tunes.create` within an automated data-collection + training loop. Given that this process is automated, it would be really helpful to have the ability to specify the output format from this command - specifically the ability to just return the new fine-tuned model ID rather than the whole text block (which then needs to be parsed).

",closed,2022-07-25T21:55:49Z,2022-09-18T01:09:59Z,['enhancement'],https://github.com/openai/openai-python/issues/113,1299.24,True,missing_option_violation,medium,"The API does not provide an option to specify the output format, which can lead to inefficiencies in automated processes that require parsing of the entire text block.",high,
117,Confusing error message,"In the function `openai.embeddings_utils.get_embeddings` ([code](https://github.com/openai/openai-python/blob/3edecbee24102299dd6e4a35af031780e9ad0f9a/openai/embeddings_utils.py#L30)), there is a check for the length of the input texts list:

```python
assert len(list_of_text) < 2048, ""The batch size should not be larger than 2048.""
```

From the error message `The batch size should not be larger than 2048`, I expect that this function accepts a list of length 2048. However, the assertion `assert len(list_of_text) < 2048` allows a maximum length of 2047. I find this quite confusing.",closed,2022-08-26T07:17:26Z,2022-08-26T17:32:24Z,[],https://github.com/openai/openai-python/issues/117,10.25,True,input_value_violation,medium,"The error message suggests that a list of length 2048 is acceptable, but the assertion restricts it to 2047, leading to confusion.",high,
86,Continue fine tuning a model e.g. text-davinci-002,"I am only able to define the base model for fine-tuning as ada, babbage, curie or davinci. I would like to be able to (1) fine tune on the official fine tuned models, e.g. text-davinci-002 to allow me to compare my fine tuning to the default latest and greatest. And of lesser importance: (2) continue a fine-tuning on my own models.
",closed,2022-03-30T22:29:47Z,2022-07-29T17:49:33Z,['enhancement'],https://github.com/openai/openai-python/issues/86,2899.33,True,missing_option_violation,medium,"The API does not allow fine-tuning on official fine-tuned models like text-davinci-002, which may limit user flexibility and performance comparison.",high,
110,Bug in the clustering jupyter example,"In the following code in the clustering notebook example, at the first cell:

```
df['text-similarity-babbage-001'] = df.babbage_similarity.apply(eval).apply(np.array)
matrix = np.vstack(df.babbage_similarity.values)
```

the second  line should be the following: `matrix = np.vstack(df['text-similarity-babbage-001'].values)`",closed,2022-07-11T20:27:59Z,2022-07-29T07:51:35Z,['bug'],https://github.com/openai/openai-python/issues/110,419.39,True,missing_dependency_violation,medium,"The second line of code relies on the output of the first line, which is not correctly referenced, leading to potential incorrect behavior.",high,
90,Completion is created only for the last prompt in the list of prompts,"Hello!

Here is a sample code where the model is expected to return 4 choices for 2 prompts (2 for each prompt), according to the documentation - https://beta.openai.com/docs/api-reference/completions/create#completions/create-prompt:

```python
import pandas as pd
import openai

openai.api_key = ""YOUR_API_KEY""

prompts = [
    ""example 1"",
    ""example 2"",
]
generated = openai.Completion.create(model='davinci',
                                     prompt=prompts,
                                     max_tokens=10,
                                     best_of=4,
                                     n=2,
                                     echo=True,
                                     )
print(generated)
```

But get only 2 choices for last prompt (pay attention to indices, they do not start from 0):

```
<OpenAIObject text_completion id=cmpl-4zR7IQzUzg8ZQQAluCUkLfkJo3DmO at 0x1885f37c630> JSON: {
  ""choices"": [
    {
      ""finish_reason"": ""length"",
      ""index"": 4,
      ""logprobs"": null,
      ""text"": ""example 2.10.1\"" , \""subtitle\""""
    },
    {
      ""finish_reason"": ""length"",
      ""index"": 5,
      ""logprobs"": null,
      ""text"": ""example 2). 5 Add meat, cover pot with tight-""
    }
  ],
  ""created"": 1650547592,
  ""id"": ""cmpl-4zR7IQzUzg8ZQQAluCUkLfkJo3DmO"",
  ""model"": ""davinci"",
  ""object"": ""text_completion""
}
```

Similar code, but using JS lib works as expected. 


Versions:
```
openai==0.18.1
python==3.9.7
```

Does anyone have any idea what went wrong? I apologize if the error is banal, but I spent quite a lot of time looking for it and failed. Thank you!",closed,2022-04-21T14:06:58Z,2022-07-28T15:35:18Z,['bug'],https://github.com/openai/openai-python/issues/90,2353.47,True,missing_dependency_violation,high,"The API call does not handle multiple prompts correctly, leading to incomplete responses for the provided inputs.",high,
106,Finetuning classifier giving recommendations which are not from any of the specified categories used to train the models,"Hello!

I made a finetuned classifier with 7 categories, similar to the one shown at `examples/finetuning/finetuning-classification.ipynb` and used the following command to create the classifier.

```shell
openai api fine_tunes.create -t ""data_prepared_train.jsonl"" -v ""data_prepared_valid.jsonl"" --compute_classification_metrics --classification_n_classes 7 -m ada
```

I use the following command in python when querying from the classifier

```python
res = openai.Completion.create(model=ft_model, prompt = prompt + ""\n\n###\n\n"", max_tokens=1, temperature=0, logprobs=5)
```

However, when querying a completion from the classifier, the `log_probs` returns values which does not fall under any of the predefined categories. I have tried these with both numerical categories, and string based categories, and the same behavior is still exhibited. Was wondering if there was any way to stop these other non-predefined categories in the `log_probs` as they do not aid with classification in my use case. Just having the probabilities for the predefined categories would suffice. Additionally, there is also the concern that the fine-tuning model could classify the prompt into one of the undefined categories.

Here are a few examples of such occurrences:

**Numerical Categories**
Predefined categories are from 0-6 inclusive

```python
<OpenAIObject at 0x174ab8f9450> JSON: {
  ""text_offset"": [
    57
  ],
  ""token_logprobs"": [
    -0.08814435
  ],
  ""tokens"": [
    "" 1""
  ],
  ""top_logprobs"": [
    {
      "" 1"": -0.08814435,
      "" 2"": -8.10889,
      "" 5"": -7.6156697,
      "" 6"": -2.4937658,
      "" 7"": -9.455983
    }
  ]
}
```
Here it suggests the value `"" 7""` which has never been specified in the dataset

```python
<OpenAIObject at 0x174ab8cf360> JSON: {
  ""text_offset"": [
    28
  ],
  ""token_logprobs"": [
    -0.028152594
  ],
  ""tokens"": [
    "" 5""
  ],
  ""top_logprobs"": [
    {
      "" 0"": -8.047939,
      "" 4"": -6.4044333,
      "" 5"": -0.028152594,
      "" 6"": -3.8101103,
      "" very"": -8.138523
    }
  ]
}
```
Here it suggests the value `"" very""` which is not an integer

```Python
<OpenAIObject at 0x174ab8cac70> JSON: {
  ""text_offset"": [
    60
  ],
  ""token_logprobs"": [
    -0.07207727
  ],
  ""tokens"": [
    "" 2""
  ],
  ""top_logprobs"": [
    {
      "" 2"": -0.07207727,
      "" 3"": -6.8346596,
      "" 4"": -6.0346746,
      "" 6"": -2.7322218,
      ""2"": -8.06412
    }
  ]
}
```
Here it suggests `""2""` which is different from `"" 2""`

**String categories**
Where categories are [""background"", ""bug"", ""feature-request"" (shows up as feature), ""follow-up"", ""null"", ""painpoint"" (shows up as pain), ""usability""]


```Python
<OpenAIObject at 0x174ab91d630> JSON: {
  ""text_offset"": [
    60
  ],
  ""token_logprobs"": [
    -0.2158865
  ],
  ""tokens"": [
    "" feature""
  ],
  ""top_logprobs"": [
    {
      "" UX"": -8.75913,
      "" feature"": -0.2158865,
      "" features"": -7.479752,
      "" functionality"": -9.452176,
      "" usability"": -1.6482441
    }
  ]
}
```
Here it is returning other options such as `"" UX""`, `"" features""`, `"" functionality""`, etc over other predefined categories.

```Python
<OpenAIObject at 0x174ab8ed900> JSON: {
  ""text_offset"": [
    52
  ],
  ""token_logprobs"": [
    -0.001725924
  ],
  ""tokens"": [
    "" pain""
  ],
  ""top_logprobs"": [
    {
      "" Pain"": -6.667007,
      "" bug"": -9.389308,
      "" pain"": -0.001725924,
      "" painful"": -8.79043,
      ""pain"": -9.397688
    }
  ]
}
```

Here it is returning many different  versions of ""pain"".

Would really appreciate any advice/support over this, as such completions which are not part of the predefined categories are not expected.

",closed,2022-06-29T07:08:22Z,2022-07-12T02:49:35Z,[],https://github.com/openai/openai-python/issues/106,307.69,True,input_value_violation,high,"The classifier is returning log probabilities for categories that are not part of the predefined set, which can lead to incorrect classifications and undermine the model's intended functionality.",high,
108,Requests timing out ,"I am running a Python backend service using fastapi and a docker container and every once in a while all requests start timing out for no apparent reason. The same request can time out and suddenly work an hour later. The service is under development so my team is the only one currently using our keys and service. I have created a simple call with no fancy logic and using the ada model but this still times out (see code below). I get 

""error.APIConnectionerror(""error communicating with OpenAI"")"" 

coming from 

""requests.exceptions.ConnectionError: HTTPSConnectionPool(host='api.openai.com', port=443): Max retries exceeded with url: /v1/engines/text-ada-001/completions (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f3052475310>: Failed to establish a new connection: [Errno 110] Connection timed out'))""

Code: 

```
def simple_call():
    openai.organization = config('ORGANIZATION')
    openai.api_key = config('API_KEY')
    print(""WARNING: performing simple call"")
    prompt = ""Tell me a fun story to cheer me up""
    completion = openai.Completion.create(**{""engine"": ""text-ada-001"", ""max_tokens"": 500, ""prompt"": prompt})
    response = {'generations': [{'prompt': prompt, 'question': ""MOCK"", 'answer': completion[""choices""][0][""text""]}]}
    return response
```

Any advice is appreciated",closed,2022-07-01T13:14:55Z,2022-07-12T02:47:39Z,[],https://github.com/openai/openai-python/issues/108,253.55,True,method_order_violation,hang,"The requests are timing out intermittently, indicating potential issues with the order of API calls or resource availability.",medium,
100,Error when installing on kali-linux,"For some reason I'm getting a build error when pip installing on Kali Linux. Correct me if I'm wrong, but I assume this is a build error. Attached is the output from my build.
[txt.log](https://github.com/openai/openai-python/files/8904264/txt.log)
",closed,2022-06-14T22:28:41Z,2022-06-21T16:16:26Z,['wontfix'],https://github.com/openai/openai-python/issues/100,161.8,True,missing_dependency_violation,High,"The build error may indicate a missing dependency required for the installation process, leading to potential instability or failure during the setup.",high,
96,openaipython,,closed,2022-05-21T16:44:31Z,2022-05-23T13:36:17Z,[],https://github.com/openai/openai-python/issues/96,44.86,True,missing_dependency_violation,high,"The issue lacks sufficient context or details regarding the API usage, indicating a potential missing dependency on required information.",high,
95,HTTP Connection is not reused in stream mode,"Requesting api in stream mode is not reusing HTTP Connection

Logs:
```
...
DEBUG:urllib3.connectionpool:Resetting dropped connection: api.openai.com
...
```

wireshark screenshot
<img width=""1206"" alt=""image"" src=""https://user-images.githubusercontent.com/22409414/169264490-3ce7bc77-edee-4870-a0ea-ed013a6b83be.png"">


minimal code to reproduce:
```python
import openai
from http.client import HTTPConnection
import logging


HTTPConnection.debuglevel = 10
logging.basicConfig()
logging.getLogger().setLevel(logging.DEBUG)
requests_log = logging.getLogger(""requests.packages.urllib3"")
requests_log.setLevel(logging.DEBUG)
requests_log.propagate = True

def test():
    openai_response = openai.Completion.create(
        engine=""text-davinci-002"",
        prompt=""Test prompt"",
        temperature=1.0,
        max_tokens=120,
        top_p=1.0,
        stream=True,
    )

    for resp in openai_response:
        text = resp[""choices""][0][""text""]

test()
test()

```

Why need to fix?
TCP and TLS handshake can be slow and it's better to reuse connections

",closed,2022-05-19T09:45:44Z,2022-05-19T21:47:00Z,[],https://github.com/openai/openai-python/issues/95,12.02,True,missing_dependency_violation,performance_degradation,"The API does not reuse HTTP connections in stream mode, leading to unnecessary TCP and TLS handshakes, which can slow down performance.",high,
89,Calculating tokens prior to prompt,"hey all,

I'm often making few-shot classification models using completions and I need to be able to check how many tokens exactly are inside my prompt prior to sending it off. Right now I do something like:

```
# sudo code
def send_request(prompt)
    try:
        ret = make_request(prompt)
    except:
        prompt = reduce_tokens(prompt)
        ret = send_request(prompt)
    return ret
```

but I'd rather just have the right number of tokens calculated in my preprocessing stage. Is there a way to use OpenAI's tokenizing tool to calculate this ahead of time?
",closed,2022-04-20T19:04:46Z,2022-04-23T04:24:26Z,[],https://github.com/openai/openai-python/issues/89,57.33,True,input_value_violation,medium,"The user is attempting to send a prompt that may exceed token limits without pre-validation, leading to potential performance degradation.",high,
73,Unable to use custom certs with OpenAI client,"I'm unable to get custom cert bundle path (client side) working with OpenAI. For background, I'm on a work laptop which is behind a VPN, so we use a custom SSL cert (.pem file) for any outbound requests. The Python `requests` library seems to honor this with the use of the `REQUESTS_CA_BUNDLE` env variable, but was unable to get it working with the OpenAI client.

Here's what I tried so far:

```
import os

import openai

print(os.environ['REQUESTS_CA_BUNDLE'])  # confirm that it prints out correct path to .pem file

openai.verify_ssl_certs = False # disable verify but this does nothing, only logs a warning
openai.api_key = 'my-api-key'

file = '/path/to/my/file'
with open(file, 'rb') as in_file:
    openai.File.create(
        file=in_file, purpose='search')
```

This results in the below error being printed to console:

```
/Users/rnag/path/to/pem/file
/Users/rnag/.pyenv/versions/3.10.0/lib/python3.10/site-packages/openai/api_requestor.py:47: UserWarning: verify_ssl_certs is ignored; openai always verifies.
  warnings.warn(""verify_ssl_certs is ignored; openai always verifies."")
Traceback (most recent call last):
  File ""/Users/rnag/.pyenv/versions/3.10.0/lib/python3.10/site-packages/urllib3/connectionpool.py"", line 699, in urlopen
    httplib_response = self._make_request(
  File ""/Users/rnag/.pyenv/versions/3.10.0/lib/python3.10/site-packages/urllib3/connectionpool.py"", line 382, in _make_request
    self._validate_conn(conn)
  File ""/Users/rnag/.pyenv/versions/3.10.0/lib/python3.10/site-packages/urllib3/connectionpool.py"", line 1010, in _validate_conn
    conn.connect()
  File ""/Users/rnag/.pyenv/versions/3.10.0/lib/python3.10/site-packages/urllib3/connection.py"", line 416, in connect
    self.sock = ssl_wrap_socket(
  File ""/Users/rnag/.pyenv/versions/3.10.0/lib/python3.10/site-packages/urllib3/util/ssl_.py"", line 449, in ssl_wrap_socket
    ssl_sock = _ssl_wrap_socket_impl(
  File ""/Users/rnag/.pyenv/versions/3.10.0/lib/python3.10/site-packages/urllib3/util/ssl_.py"", line 493, in _ssl_wrap_socket_impl
    return ssl_context.wrap_socket(sock, server_hostname=server_hostname)
  File ""/Users/rnag/.pyenv/versions/3.10.0/lib/python3.10/ssl.py"", line 512, in wrap_socket
    return self.sslsocket_class._create(
  File ""/Users/rnag/.pyenv/versions/3.10.0/lib/python3.10/ssl.py"", line 1070, in _create
    self.do_handshake()
  File ""/Users/rnag/.pyenv/versions/3.10.0/lib/python3.10/ssl.py"", line 1341, in do_handshake
    self._sslobj.do_handshake()
ssl.SSLCertVerificationError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:997)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/Users/rnag/.pyenv/versions/3.10.0/lib/python3.10/site-packages/requests/adapters.py"", line 439, in send
    resp = conn.urlopen(
  File ""/Users/rnag/.pyenv/versions/3.10.0/lib/python3.10/site-packages/urllib3/connectionpool.py"", line 783, in urlopen
    return self.urlopen(
  File ""/Users/rnag/.pyenv/versions/3.10.0/lib/python3.10/site-packages/urllib3/connectionpool.py"", line 783, in urlopen
    return self.urlopen(
  File ""/Users/rnag/.pyenv/versions/3.10.0/lib/python3.10/site-packages/urllib3/connectionpool.py"", line 755, in urlopen
    retries = retries.increment(
  File ""/Users/rnag/.pyenv/versions/3.10.0/lib/python3.10/site-packages/urllib3/util/retry.py"", line 574, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='api.openai.com', port=443): Max retries exceeded with url: /v1/files (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:997)')))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/Users/rnag/.pyenv/versions/3.10.0/lib/python3.10/site-packages/openai/api_requestor.py"", line 255, in request_raw
    result = _thread_context.session.request(
  File ""/Users/rnag/.pyenv/versions/3.10.0/lib/python3.10/site-packages/requests/sessions.py"", line 542, in request
    resp = self.send(prep, **send_kwargs)
  File ""/Users/rnag/.pyenv/versions/3.10.0/lib/python3.10/site-packages/requests/sessions.py"", line 655, in send
    r = adapter.send(request, **kwargs)
  File ""/Users/rnag/.pyenv/versions/3.10.0/lib/python3.10/site-packages/requests/adapters.py"", line 514, in send
    raise SSLError(e, request=request)
requests.exceptions.SSLError: HTTPSConnectionPool(host='api.openai.com', port=443): Max retries exceeded with url: /v1/files (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:997)')))

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File ""/path/to/py/file"", line 11, in <module>
    openai.File.create(
  File ""/Users/rnag/.pyenv/versions/3.10.0/lib/python3.10/site-packages/openai/api_resources/file.py"", line 43, in create
    response, _, api_key = requestor.request(""post"", url, files=files)
  File ""/Users/rnag/.pyenv/versions/3.10.0/lib/python3.10/site-packages/openai/api_requestor.py"", line 98, in request
    result = self.request_raw(
  File ""/Users/rnag/.pyenv/versions/3.10.0/lib/python3.10/site-packages/openai/api_requestor.py"", line 266, in request_raw
    raise error.APIConnectionError(""Error communicating with OpenAI"") from e
openai.error.APIConnectionError: Error communicating with OpenAI
```

### My workaround

Its not ideal but current workaround I'm using is to manually update the [api_requestor.py](https://github.com/openai/openai-python/blob/main/openai/api_requestor.py#L279) script to pass in `verify=False` to `request` method:
```
timeout=TIMEOUT_SECS,
verify=False,
````",closed,2022-02-14T15:46:32Z,2022-02-22T18:35:32Z,[],https://github.com/openai/openai-python/issues/73,194.82,True,input_value_violation,high,"The API fails to handle custom SSL certificates properly, leading to SSL verification errors when the client attempts to use a custom cert bundle.",high,
70,openai: error: invalid choice: 'tools' (choose from 'api'),"```sh
$ openai tools fine_tunes.prepare_data -f ./threat_impact_prediction/data.jsonl 
usage: openai [-h] [-v] [-b API_BASE] [-k API_KEY] [-o ORGANIZATION] {api} ...
openai: error: invalid choice: 'tools' (choose from 'api')
```

Anaconda Python==3.6.9",closed,2022-01-31T04:57:32Z,2022-01-31T04:59:41Z,[],https://github.com/openai/openai-python/issues/70,0.04,True,missing_option_violation,medium,"The command 'openai tools' is not recognized, indicating that the user did not follow the correct usage options provided by the API's contract.",high,
17,Building openai into an exe with pyInstaller causes a certificate error (can't find file),"```
Error Ranking: Unexpected error communicating with OpenAI. It looks like there's
probably a configuration issue locally.  If this problem persists, let
us know at support@openai.com.

(Network error: A OSError was raised with error message Could not find a suitable TLS CA certificate bundle, invalid path: [MASKED]\AppData\Local\Temp\_MEI122122\openai\data/ca-certificates.crt)
```
Is there a reason @gdb added this file in the first place to the repo? There's not commit history, so it's not clear on the context. 

The file is added in to setup.py here https://github.com/openai/openai-python/blob/5f8c4a842674eda7aeb7a81f87686fdd4123f797/setup.py#L24",closed,2021-06-03T22:35:03Z,2022-01-27T05:28:09Z,[],https://github.com/openai/openai-python/issues/17,5694.89,True,missing_dependency_violation,high,"The API cannot locate the required TLS CA certificate bundle, indicating a missing dependency that is essential for secure communication.",high,
48,Use `entry_points console_scripts `,"Hi,

I was wondering if it might make sense to switch from `scripts` to `entry_points console_scripts ` in _setup.py_. This is the more modern and preferred approach and recommended by PyPA nowadays.

Example:
```
entry_points={
    'console_scripts': [
        'sample=sample:main',
    ],
},
```


Guide: https://packaging.python.org/guides/distributing-packages-using-setuptools/#scripts

Ref: 
* https://github.com/openai/openai-python/blob/main/bin/openai
* https://github.com/openai/openai-python/blob/62f8d40ffc6d3a723f5f1d99fee7febfd47026b5/setup.py#L25",closed,2021-11-30T19:27:53Z,2022-01-25T20:56:55Z,[],https://github.com/openai/openai-python/issues/48,1345.48,False,,,,,
10,Ability to check the amount of tokens remaining using the API?,"I couldn't find any reference to this in the docs and the slack chatroom is quite dead, so I'm opening an issue here to suggest it if it's not implemented yet.",closed,2021-03-22T20:54:55Z,2022-01-24T21:54:01Z,[],https://github.com/openai/openai-python/issues/10,7392.98,True,missing_option_violation,medium,"The API does not provide a method to check the remaining tokens, which is a recommended feature for users to manage their usage effectively.",high,
65,Can't send files - missing purpose,"Hi I am trying to send files for finetuning from my api service (not command line)
I tried:
`return json.loads(requests.post(
            'https://api.openai.com/v1/files',
            headers={'Authorization': 'Bearer '+ os.getenv(""OPENAI_API_KEY"")},
            json={""purpose"":""fine-tune""},
            files={'file': ""\n"".join(data)}
        ).text)`
and
`return json.loads(requests.post(
            'https://api.openai.com/v1/files',
            headers={'Authorization': 'Bearer '+ os.getenv(""OPENAI_API_KEY"")},
            params={""purpose"":""fine-tune""},
            files={'file': ""\n"".join(data)}
        ).text)`  

but I got `""'purpose' is a required property""` error

and when I try:
`return json.loads(requests.post(
            'https://api.openai.com/v1/files',
            headers={'Authorization': 'Bearer '+ os.getenv(""OPENAI_API_KEY"")},
            files={""purpose"":""fine-tune"", 'file': ""\n"".join(data)}
        ).text)`

I got ` ""'' is not one of ['fine-tune', 'answers', 'search', 'classifications'] - 'purpose'""` error

What am I missing?",closed,2022-01-18T07:42:06Z,2022-01-19T01:54:20Z,[],https://github.com/openai/openai-python/issues/65,18.2,True,missing_option_violation,high,"The user is not providing the 'purpose' in the correct format or location as required by the API contract, leading to errors.",high,
2,All Playground Config Options are not available via API,"There are a few config options that are available in the Playground, but not via the API. For example: Frequency Penalty, Presence Penalty, Best Of, etc. Is there a way to use these in the library now or will these eventually be added?",closed,2021-01-30T03:42:10Z,2022-01-04T01:10:29Z,[],https://github.com/openai/openai-python/issues/2,8133.47,True,missing_option_violation,medium,"Certain configuration options available in the Playground are not accessible via the API, leading to incomplete functionality.",high,
46,How to work with maximum context length is 2049 tokens?,"I'd like to send the OpenAI's API, the text from various PDF's. Specifically, the [Summarize for a 2nd grader](https://beta.openai.com/examples/default-summarize) or the [TL;DR summarization](https://beta.openai.com/examples/default-tldr-summary) API's.

I can extract the text from PDF's using `PyMuPDF` and prepare the OpenAI prompt.

**Question:** How best to prepare the `prompt` when the token count is longer than the allowed 2049?

- Do I just truncate the text?
- Or is there a way to sample the text to ""compress"" it to lose key points?",closed,2021-11-22T04:12:01Z,2022-01-04T01:09:45Z,[],https://github.com/openai/openai-python/issues/46,1028.96,True,input_value_violation,medium,"The user is inquiring about handling prompts that exceed the maximum token limit of 2049, which indicates potential input value violations related to prompt length.",high,
38,Can only get a limited number of logprobs from completions?,"Hi, 

I am specifying that I want the logprobs of the top 100 tokens but the API seems to give me at most 5:

```
(Pdb) response = openai.Completion.create(engine='ada', prompt='test', max_tokens=1, logprobs=100)
(Pdb) response
<OpenAIObject text_completion id=cmpl-3qtEQ4P8iNmTHqV8Eh4Wq59vzqnbm at 0x7fcf271ebb30> JSON: {
  ""choices"": [
    {
      ""finish_reason"": ""length"",
      ""index"": 0,
      ""logprobs"": {
        ""text_offset"": [
          4
        ],
        ""token_logprobs"": [
          -10.029023
        ],
        ""tokens"": [
          "" goal""
        ],
        ""top_logprobs"": [
          {
            "","": -3.721809,
            ""-"": -2.6293674,
            ""."": -2.6775126,
            ""/"": -2.527664,
            ""_"": -2.10408
          }
        ]
      },
      ""text"": "" goal""
    }
  ],
  ""created"": 1633734378,
  ""id"": ""cmpl-3qtEQ4P8iNmTHqV8Eh4Wq59vzqnbm"",
  ""model"": ""ada:2020-05-03"",
  ""object"": ""text_completion""
}
```

Is there any way I can get around this?",closed,2021-10-08T23:07:52Z,2021-12-23T04:56:46Z,[],https://github.com/openai/openai-python/issues/38,1805.82,True,input_value_violation,incorrect_functionality,"The API is not returning the expected number of logprobs (100) as specified by the user, instead returning only 5.",high,
40,Python 3.10 not support???,"When I try to use python 3.10 to run the openai library, it returns 

PS C:\Users\USER\Desktop\Gpt-3> pip install openai
Collecting openai
  Using cached openai-0.10.5.tar.gz (157 kB)
Collecting requests>=2.20
  Using cached requests-2.26.0-py2.py3-none-any.whl (62 kB)
Collecting tqdm
  Using cached tqdm-4.62.3-py2.py3-none-any.whl (76 kB)
Collecting pandas>=1.2.3
  Using cached pandas-1.3.3.tar.gz (4.7 MB)
  Installing build dependencies ... error
  ERROR: Command errored out with exit status 1:
   command: 'C:\Users\USER\Desktop\Gpt-3\venv\Scripts\python.exe' 'C:\Users\USER\AppData\Local\Temp\pip-standalone-pip-ip8y9guu\__env_pip__.zip\pip' install --ignore-i
nstalled --no-user --prefix 'C:\Users\USER\AppData\Local\Temp\pip-build-env-djvlomh4\overlay' --no-warn-script-location --no-binary :none: --only-binary :none: -i http
s://pypi.org/simple -- 'setuptools>=51.0.0' wheel 'Cython>=0.29.21,<3' 'numpy==1.17.3; python_version=='""'""'3.7'""'""' and (platform_machine!='""'""'arm64'""'""' or platform
_system!='""'""'Darwin'""'""') and platform_machine!='""'""'aarch64'""'""'' 'numpy==1.18.3; python_version=='""'""'3.8'""'""' and (platform_machine!='""'""'arm64'""'""' or platform_sy
stem!='""'""'Darwin'""'""') and platform_machine!='""'""'aarch64'""'""'' 'numpy==1.19.3; python_version>='""'""'3.9'""'""' and (platform_machine!='""'""'arm64'""'""' or platform_syste
m!='""'""'Darwin'""'""') and platform_machine!='""'""'aarch64'""'""'' 'numpy==1.19.2; python_version=='""'""'3.7'""'""' and platform_machine=='""'""'aarch64'""'""'' 'numpy==1.19.2; py
thon_version=='""'""'3.8'""'""' and platform_machine=='""'""'aarch64'""'""'' 'numpy>=1.20.0; python_version=='""'""'3.8'""'""' and platform_machine=='""'""'arm64'""'""' and platform_s
ystem=='""'""'Darwin'""'""'' 'numpy>=1.20.0; python_version=='""'""'3.9'""'""' and platform_machine=='""'""'arm64'""'""' and platform_system=='""'""'Darwin'""'""''

 ",closed,2021-10-15T20:33:27Z,2021-12-23T04:53:09Z,[],https://github.com/openai/openai-python/issues/40,1640.33,True,input_type_violation,unknown,"The openai library may not support Python 3.10, leading to compatibility issues during installation.",high,
53,Embedding api not working ,"[https://beta.openai.com/docs/guides/embeddings/what-are-embeddings](url) documented the newly available embedding api, but the examples are not working:

```
response = openai.Engine(id=""babbage-similarity"").embeddings(
    input=""Sample document text goes here""
)
embeddings = response['data'][0]['embedding']
```

This is the error message:
```

Traceback (most recent call last):
  File ""/home/whao/.virtualenv/openai/lib/python3.6/site-packages/openai/openai_object.py"", line 96, in __getattr__
    return self[k]
  File ""/home/whao/.virtualenv/openai/lib/python3.6/site-packages/openai/openai_object.py"", line 136, in __getitem__
    raise err
  File ""/home/whao/.virtualenv/openai/lib/python3.6/site-packages/openai/openai_object.py"", line 124, in __getitem__
    return super(OpenAIObject, self).__getitem__(k)
KeyError: 'embeddings'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/whao/.virtualenv/openai/lib/python3.6/site-packages/openai/openai_object.py"", line 98, in __getattr__
    raise AttributeError(*err.args)
AttributeError: embeddings

```",closed,2021-12-11T02:30:09Z,2021-12-23T04:50:34Z,[],https://github.com/openai/openai-python/issues/53,290.34,True,missing_dependency_violation,high,"The API call to retrieve embeddings is failing due to a missing or incorrect response structure, indicating a dependency on prior configuration or input that was not met.",high,
11,File api ignores api_base,"cc @hallacy 
https://github.com/openai/openai-python/blob/cf9c04a24ea6d3d2bea2e2c7ef3d67fe5264a1e4/openai/api_resources/file.py#L26",closed,2021-03-26T19:44:57Z,2021-12-23T04:50:02Z,[],https://github.com/openai/openai-python/issues/11,6513.08,True,missing_dependency_violation,high,"The file API does not respect the api_base configuration, leading to potential incorrect API calls and responses.",high,
61,configure retry behavior,"Is there a way to configure this SDK to retry failed API calls? 

My experiments are breaking half way through because of occasional errors such as 
```bash
openai.error.APIError: The server had an error while processing your request. Sorry about that! You can retry your request, or contact support@openai.com if the error persists. (Please include the request ID [...] in your email.)
```

These are costing money and preventing experiments from running to completion, so I'd love to find a robust way to retry failed requests 2-3 times. Thank you for your input.
",closed,2021-12-21T20:40:16Z,2021-12-23T04:49:01Z,[],https://github.com/openai/openai-python/issues/61,32.15,True,missing_dependency_violation,medium,"The issue indicates a missing mechanism to handle retry logic for failed API calls, which is essential for robust API interaction.",high,
59,"Engine Not found Error, when trying to get embeddings  "," I am trying to get embeddings using this example here -> [Get_embeddings](https://github.com/openai/openai-python/blob/main/examples/embeddings/Get_embeddings.ipynb) 
getting error at below line,
```
df['babbage_similarity'] = df.combined.apply(lambda x: get_embedding(x, engine='babbage-similarity'))
```

```
InvalidRequestError: Engine not found

The above exception was the direct cause of the following exception:

RetryError                                Traceback (most recent call last)
<ipython-input-14-7b798ddc3611> in <module>
      2 
      3 # This will take just under 10 minutes
----> 4 df['babbage_similarity'] = df.combined.apply(lambda x: get_embedding(x, engine='babbage-similarity'))
```",closed,2021-12-21T13:34:09Z,2021-12-23T04:43:53Z,[],https://github.com/openai/openai-python/issues/59,39.16,False,,,,low,
31,Example of using fine-tuned model in fine-tuning docs doesn't work with current release. ,"Currently using openai-0.10.2 from pypi and gettting

```InvalidRequestError: Must provide an 'engine' parameter to create a <class 'openai.api_resources.completion.Completion'>```

even though docs say explicitly not to provide engine. ",closed,2021-08-31T08:00:30Z,2021-08-31T08:04:54Z,[],https://github.com/openai/openai-python/issues/31,0.07,True,missing_option_violation,high,"The API documentation states that the 'engine' parameter should not be provided, but the current implementation requires it, leading to an invalid request error.",high,
30,Now requires Python 3.7?,"I am getting the following error pushing an app to Heroku with openai-python:

```
remote:        ERROR: Could not find a version that satisfies the requirement pandas>=1.2.3 (from openai->-r /tmp/build_0e7253a3/requirements.txt (line 21)) (from versions: 0.1, 0.2b0, 0.2b1, 0.2, 0.3.0b0, 0.3.0b2, 0.3.0, 0.4.0, 0.4.1, 0.4.2, 0.4.3, 0.5.0, 0.6.0, 0.6.1, 0.7.0, 0.7.1, 0.7.2, 0.7.3, 0.8.0rc1, 0.8.0, 0.8.1, 0.9.0, 0.9.1, 0.10.0, 0.10.1, 0.11.0, 0.12.0, 0.13.0, 0.13.1, 0.14.0, 0.14.1, 0.15.0, 0.15.1, 0.15.2, 0.16.0, 0.16.1, 0.16.2, 0.17.0, 0.17.1, 0.18.0, 0.18.1, 0.19.0, 0.19.1, 0.19.2, 0.20.0, 0.20.1, 0.20.2, 0.20.3, 0.21.0, 0.21.1, 0.22.0, 0.23.0, 0.23.1, 0.23.2, 0.23.3, 0.23.4, 0.24.0, 0.24.1, 0.24.2, 0.25.0, 0.25.1, 0.25.2, 0.25.3, 1.0.0, 1.0.1, 1.0.2, 1.0.3, 1.0.4, 1.0.5, 1.1.0, 1.1.1, 1.1.2, 1.1.3, 1.1.4, 1.1.5) 

remote:        ERROR: No matching distribution found for pandas>=1.2.3 (from openai->-r /tmp/build_0e7253a3/requirements.txt (line 21)) 
```



This is on py 3.6.10. This stopped working without any changes to the `requirements.txt` for the app. ",closed,2021-08-06T08:35:22Z,2021-08-06T08:44:41Z,[],https://github.com/openai/openai-python/issues/30,0.16,True,input_type_violation,unknown,"The application requires a version of pandas that is incompatible with Python 3.6.10, indicating a potential type mismatch in the expected Python version for the library.",high,
6,Api key,"I'm very excited with the idea of using this technology.

I've applied and fulfilled the form to get the API key.

Just need to know how long it will take to actually get one.

Thanks
",closed,2021-03-04T10:45:31Z,2021-04-01T16:54:54Z,[],https://github.com/openai/openai-python/issues/6,678.16,False,,,,,
4,Added to conda,"Hi, this is more an FYI than an issue but I just wanted to let you guys know that I added this to conda-forge (https://github.com/conda-forge/openai-feedstock) so that it can be downloaded using conda. 
Let me know if someone from here wants to be added as a maintainer there, otherwise, feel free to just close this issue.",closed,2021-02-08T22:16:21Z,2021-02-15T20:59:36Z,[],https://github.com/openai/openai-python/issues/4,166.72,False,,,,,
1,Commit history was lost,"Why was this repo's history reset with a new ""Initial commit"" 3 days ago? There's nothing documenting why this was done.",closed,2020-10-28T16:47:55Z,2020-10-31T20:38:36Z,[],https://github.com/openai/openai-python/issues/1,75.84,True,data_corruption,High,"The commit history being reset indicates a potential loss of data integrity and consistency, which can lead to incorrect system behavior or data loss.",high,
