[
  {
    "repository": "QwenLM/Qwen",
    "issue_number": 1346,
    "title": "问题：用Ollama运行你自己的GGUF文件里面，关于qwen2.5-0.5b-instuctor的Modelfile应该是什么",
    "body": "\n教程：https://qwen.readthedocs.io/zh-cn/latest/run_locally/ollama.html#\n问题：用Ollama运行你自己的GGUF文件里面，关于qwen2.5-0.5b-instuctor的Modelfile应该是什么\n\n我使用官网的qwen2.5-7b-instruct的Modelfile放入到我训练完成的qwen2.5-0.5b-instuctor里面是错误的，麻烦告知一下qwen2.5-0.5b-instuctor\n是什么，感谢\n",
    "state": "closed",
    "created_at": "2025-01-24T07:43:03+00:00",
    "closed_at": "2025-01-24T08:07:42+00:00",
    "updated_at": "2025-01-24T08:07:42+00:00",
    "author": "Wheeeeeeeeels",
    "author_type": "User",
    "comments_count": 1,
    "reactions_count": 0,
    "labels": "",
    "milestone": null,
    "closed_by": "Wheeeeeeeeels",
    "resolution_time_hours": 0.41083333333333333,
    "first_comments": [
      {
        "author": "Wheeeeeeeeels",
        "created_at": "2025-01-24T08:07:40+00:00",
        "body": "明白了，是这个，找到了\n\n\nFROM /mnt/workspace/qwen2-0.5b-instruct-q8_0.gguf\n\n# set the temperature to 0.7 [higher is more creative, lower is more coherent]\nPARAMETER temperature 0.7\nPARAMETER top_p 0.8\nPARAMETER repeat_penalty 1.05\nTEMPLATE \"\"\"{{ if .System }}<|im_start|>system\n{{ .System }}<|im_end|>\n{{ end }}{{ if .Prompt }}<|im_start|>user\n{{ .Prompt }}<|im_end|>\n{{ end }}<|im_start|>assistant\n{{ .Response }}<|im_end|>\"\"\"\n# set the system message\nSYSTEM \"\"\"\nYou are a helpful assistant.\n\"\"\""
      }
    ],
    "url": "https://github.com/QwenLM/Qwen/issues/1346"
  },
  {
    "repository": "QwenLM/Qwen",
    "issue_number": 1336,
    "title": "本地进行qwen2-vl-2b-instruct推理，增加max_pixels，但是占用内存依然不变",
    "body": "hellohello，我想问个问题，我在本地进行qwen2-vl-2b-instruct的多图推理，我把max_picels设置成3000x28x28之后，占用内存11g（我用的是3090）。但是回答的很不好，之后我想着显存还够，可以继续增大。于是我把3000改成5000，但是占用内存依然不变，而且回答结果也不变，这是啥原因呀？好奇怪，因为我图像的分辨率很高可能有10000x28x28那样，按理说应该会显存会继续增大，并且回答的会更好呀？\r\n<img width=\"727\" alt=\"8f056643390ef9a01caf94a6e20144a\" src=\"https://github.com/user-attachments/assets/82f948e3-008c-4378-9c9a-9734929533b4\" />\r\n我用的是jupyterlab。我不太懂这个问题的可能原因，有没有人可以帮帮我，非常感谢。",
    "state": "closed",
    "created_at": "2024-12-13T06:53:18+00:00",
    "closed_at": "2025-01-20T08:07:49+00:00",
    "updated_at": "2025-01-20T08:07:49+00:00",
    "author": "hm123450",
    "author_type": "User",
    "comments_count": 1,
    "reactions_count": 0,
    "labels": "inactive",
    "milestone": null,
    "closed_by": "github-actions[bot]",
    "resolution_time_hours": 913.2419444444445,
    "first_comments": [
      {
        "author": "github-actions[bot]",
        "created_at": "2025-01-12T08:06:23+00:00",
        "body": "This issue has been automatically marked as inactive due to lack of recent activity. Should you believe it remains unresolved and warrants attention, kindly leave a comment on this thread.\n此问题由于长期未有新进展而被系统自动标记为不活跃。如果您认为它仍有待解决，请在此帖下方留言以补充信息。"
      }
    ],
    "url": "https://github.com/QwenLM/Qwen/issues/1336"
  },
  {
    "repository": "QwenLM/Qwen",
    "issue_number": 1335,
    "title": "[BUG] <title>微信群人数已满",
    "body": "### 是否已有关于该错误的issue或讨论？ | Is there an existing issue / discussion for this?\n\n- [X] 我已经搜索过已有的issues和讨论 | I have searched the existing issues / discussions\n\n### 该问题是否在FAQ中有解答？ | Is there an existing answer for this in FAQ?\n\n- [X] 我已经搜索过FAQ | I have searched FAQ\n\n### 当前行为 | Current Behavior\n\n微信群人数已满\n\n### 期望行为 | Expected Behavior\n\n微信群人数已满\n\n### 复现方法 | Steps To Reproduce\n\n微信群人数已满\n\n### 运行环境 | Environment\n\n```Markdown\n- OS:\r\n- Python:\r\n- Transformers:\r\n- PyTorch:\r\n- CUDA (`python -c 'import torch; print(torch.version.cuda)'`):\n```\n\n\n### 备注 | Anything else?\n\n微信群人数已满",
    "state": "closed",
    "created_at": "2024-12-09T10:00:31+00:00",
    "closed_at": "2025-01-17T08:06:59+00:00",
    "updated_at": "2025-01-17T08:06:59+00:00",
    "author": "balcklive",
    "author_type": "User",
    "comments_count": 1,
    "reactions_count": 0,
    "labels": "inactive",
    "milestone": null,
    "closed_by": "github-actions[bot]",
    "resolution_time_hours": 934.1077777777778,
    "first_comments": [
      {
        "author": "github-actions[bot]",
        "created_at": "2025-01-09T08:07:13+00:00",
        "body": "This issue has been automatically marked as inactive due to lack of recent activity. Should you believe it remains unresolved and warrants attention, kindly leave a comment on this thread.\n此问题由于长期未有新进展而被系统自动标记为不活跃。如果您认为它仍有待解决，请在此帖下方留言以补充信息。"
      }
    ],
    "url": "https://github.com/QwenLM/Qwen/issues/1335"
  },
  {
    "repository": "QwenLM/Qwen",
    "issue_number": 1334,
    "title": "A100/A800 显卡去运行不同参数的模型，想确认一下算力的需求",
    "body": "### 起始日期 | Start Date\n\n2024/12/05\n\n### 实现PR | Implementation PR\n\n无\n\n### 相关Issues | Reference Issues\n\n我现在有A100的显卡，想试试8B 32B 72B 模型的运行效率和每一分钟可以生成多少toekn，想确认一下这个模型的使用上限在哪里\n\n### 摘要 | Summary\n\n模型的推演能力\n\n### 基本示例 | Basic Example\n\n无\n\n### 缺陷 | Drawbacks\n\n无\n\n### 未解决问题 | Unresolved questions\n\n无",
    "state": "closed",
    "created_at": "2024-12-05T08:14:54+00:00",
    "closed_at": "2025-01-08T07:46:04+00:00",
    "updated_at": "2025-01-08T07:46:04+00:00",
    "author": "guoshiyin-666",
    "author_type": "User",
    "comments_count": 1,
    "reactions_count": 0,
    "labels": "question,inactive",
    "milestone": null,
    "closed_by": "guoshiyin-666",
    "resolution_time_hours": 815.5194444444444,
    "first_comments": [
      {
        "author": "github-actions[bot]",
        "created_at": "2025-01-05T08:06:23+00:00",
        "body": "This issue has been automatically marked as inactive due to lack of recent activity. Should you believe it remains unresolved and warrants attention, kindly leave a comment on this thread.\n此问题由于长期未有新进展而被系统自动标记为不活跃。如果您认为它仍有待解决，请在此帖下方留言以补充信息。"
      }
    ],
    "url": "https://github.com/QwenLM/Qwen/issues/1334"
  },
  {
    "repository": "QwenLM/Qwen",
    "issue_number": 1333,
    "title": "💡 [REQUEST] - <title>onnx导出教程",
    "body": "### 起始日期 | Start Date\n\n_No response_\n\n### 实现PR | Implementation PR\n\n_No response_\n\n### 相关Issues | Reference Issues\n\n暂无\n\n### 摘要 | Summary\n\n求一个官方的onnx的export 教程\n\n### 基本示例 | Basic Example\n\n需要能够将模型从pth或safetensor导出onnx的export 教程\n\n### 缺陷 | Drawbacks\n\n暂无\n\n### 未解决问题 | Unresolved questions\n\n_No response_",
    "state": "closed",
    "created_at": "2024-12-05T02:41:08+00:00",
    "closed_at": "2025-01-12T08:06:25+00:00",
    "updated_at": "2025-01-12T08:06:25+00:00",
    "author": "1826133674",
    "author_type": "User",
    "comments_count": 1,
    "reactions_count": 0,
    "labels": "question,inactive",
    "milestone": null,
    "closed_by": "github-actions[bot]",
    "resolution_time_hours": 917.4213888888889,
    "first_comments": [
      {
        "author": "github-actions[bot]",
        "created_at": "2025-01-04T08:06:25+00:00",
        "body": "This issue has been automatically marked as inactive due to lack of recent activity. Should you believe it remains unresolved and warrants attention, kindly leave a comment on this thread.\n此问题由于长期未有新进展而被系统自动标记为不活跃。如果您认为它仍有待解决，请在此帖下方留言以补充信息。"
      }
    ],
    "url": "https://github.com/QwenLM/Qwen/issues/1333"
  },
  {
    "repository": "QwenLM/Qwen",
    "issue_number": 1332,
    "title": "请教关于算法原理",
    "body": "请问Qwen大模型中数据shape次序是什么样的。\r\n[s, b, h] 形状还是[b, s, h] 形状\r\n为什么要这样设计\r\nhidden_states: 输入到这一层的隐藏状态张量，形状为 [s, b, h]，其中 s 是序列长度，b 是批量大小，h 是隐藏层维度。\r\n谢谢",
    "state": "closed",
    "created_at": "2024-12-04T08:18:15+00:00",
    "closed_at": "2025-01-12T08:06:25+00:00",
    "updated_at": "2025-01-12T08:06:26+00:00",
    "author": "elesun2018",
    "author_type": "User",
    "comments_count": 1,
    "reactions_count": 0,
    "labels": "inactive",
    "milestone": null,
    "closed_by": "github-actions[bot]",
    "resolution_time_hours": 935.8027777777778,
    "first_comments": [
      {
        "author": "github-actions[bot]",
        "created_at": "2025-01-04T08:06:27+00:00",
        "body": "This issue has been automatically marked as inactive due to lack of recent activity. Should you believe it remains unresolved and warrants attention, kindly leave a comment on this thread.\n此问题由于长期未有新进展而被系统自动标记为不活跃。如果您认为它仍有待解决，请在此帖下方留言以补充信息。"
      }
    ],
    "url": "https://github.com/QwenLM/Qwen/issues/1332"
  },
  {
    "repository": "QwenLM/Qwen",
    "issue_number": 1331,
    "title": "找不到设立了脚本",
    "body": "### 起始日期 | Start Date\n\n_No response_\n\n### 实现PR | Implementation PR\n\n_No response_\n\n### 相关Issues | Reference Issues\n\n_No response_\n\n### 摘要 | Summary\n\n能给一个shell脚本吗\n\n### 基本示例 | Basic Example\n\n1\n\n### 缺陷 | Drawbacks\n\n1\n\n### 未解决问题 | Unresolved questions\n\n_No response_",
    "state": "closed",
    "created_at": "2024-11-29T14:19:01+00:00",
    "closed_at": "2025-01-06T08:07:31+00:00",
    "updated_at": "2025-01-06T08:07:31+00:00",
    "author": "belo-belove",
    "author_type": "User",
    "comments_count": 1,
    "reactions_count": 0,
    "labels": "question,inactive",
    "milestone": null,
    "closed_by": "github-actions[bot]",
    "resolution_time_hours": 905.8083333333333,
    "first_comments": [
      {
        "author": "github-actions[bot]",
        "created_at": "2024-12-30T08:07:24+00:00",
        "body": "This issue has been automatically marked as inactive due to lack of recent activity. Should you believe it remains unresolved and warrants attention, kindly leave a comment on this thread.\n此问题由于长期未有新进展而被系统自动标记为不活跃。如果您认为它仍有待解决，请在此帖下方留言以补充信息。"
      }
    ],
    "url": "https://github.com/QwenLM/Qwen/issues/1331"
  },
  {
    "repository": "QwenLM/Qwen",
    "issue_number": 1330,
    "title": "[BUG] <title>An AssertionError occurs when docker run(docker容器运行如下错误)：",
    "body": "### 是否已有关于该错误的issue或讨论？ | Is there an existing issue / discussion for this?\n\n- [X] 我已经搜索过已有的issues和讨论 | I have searched the existing issues / discussions\n\n### 该问题是否在FAQ中有解答？ | Is there an existing answer for this in FAQ?\n\n- [X] 我已经搜索过FAQ | I have searched FAQ\n\n### 当前行为 | Current Behavior\n\n==========\r\n== CUDA ==\r\n==========\r\n\r\nCUDA Version 11.7.1\r\n\r\nContainer image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved.\r\n\r\nThis container image and its contents are governed by the NVIDIA Deep Learning Container License.\r\nBy pulling and using the container, you accept the terms and conditions of this license:\r\nhttps://developer.nvidia.com/ngc/nvidia-deep-learning-container-license\r\n\r\nA copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience.\r\n\r\nThe model is automatically converting to bf16 for faster inference. If you want to disable the automatic precision, please manually add bf16/fp16/fp32=True to \"AutoModelForCausalLM.from_pretrained\".\r\nTry importing flash-attention for faster inference...\r\nLoading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 8/8 [00:10<00:00,  1.35s/it]\r\nRunning on local URL:  http://0.0.0.0:80\r\n\r\nTo create a public link, set `share=True` in `launch()`.\r\nUser: 柔柔弱弱\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 21, in rotary_kernel\r\nKeyError: ('2-.-0-.-0-d82511111ad128294e9d31a6ac684238-d6252949da17ceb5f3a278a70250af13-3b85c7bef5f0a641282f3b73af50f599-14de7de5c4da5794c8ca14e7e41a122d-3498c340fd4b6ee7805fd54b882a04f5-e1f133f98d04093da2078dfc51c36b72-b26258bf01f839199e39d64851821f26-d7c06e3b46e708006c15224aac7a1378-f585402118c8a136948ce0a49cfe122c', (torch.float32, torch.float32, torch.float32, torch.float32, None, 'i32', 'i32', 'i32', 'i32', 'i32', 'i32', 'i32', 'i32', 'i32', 'i32', 'i32', 'i32', 'i32', 'i32'), (128, False, False, False, False, 4), (True, True, True, True, (False,), (True, False), (False, False), (True, False), (True, False), (False, False), (True, False), (True, False), (True, False), (True, False), (False, True), (True, False), (True, False), (True, False), (False, True)))\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.8/dist-packages/gradio/queueing.py\", line 536, in process_events\r\n    response = await route_utils.call_process_api(\r\n  File \"/usr/local/lib/python3.8/dist-packages/gradio/route_utils.py\", line 322, in call_process_api\r\n    output = await app.get_blocks().process_api(\r\n  File \"/usr/local/lib/python3.8/dist-packages/gradio/blocks.py\", line 1935, in process_api\r\n    result = await self.call_function(\r\n  File \"/usr/local/lib/python3.8/dist-packages/gradio/blocks.py\", line 1532, in call_function\r\n    prediction = await utils.async_iteration(iterator)\r\n  File \"/usr/local/lib/python3.8/dist-packages/gradio/utils.py\", line 671, in async_iteration\r\n    return await iterator.__anext__()\r\n  File \"/usr/local/lib/python3.8/dist-packages/gradio/utils.py\", line 664, in __anext__\r\n    return await anyio.to_thread.run_sync(\r\n  File \"/usr/local/lib/python3.8/dist-packages/anyio/to_thread.py\", line 33, in run_sync\r\n    return await get_asynclib().run_sync_in_worker_thread(\r\n  File \"/usr/local/lib/python3.8/dist-packages/anyio/_backends/_asyncio.py\", line 877, in run_sync_in_worker_thread\r\n    return await future\r\n  File \"/usr/local/lib/python3.8/dist-packages/anyio/_backends/_asyncio.py\", line 807, in run\r\n    result = context.run(func, *args)\r\n  File \"/usr/local/lib/python3.8/dist-packages/gradio/utils.py\", line 647, in run_sync_iterator_async\r\n    return next(iterator)\r\n  File \"/usr/local/lib/python3.8/dist-packages/gradio/utils.py\", line 809, in gen_wrapper\r\n    response = next(iterator)\r\n  File \"web_demo.py\", line 126, in predict\r\n    for response in model.chat_stream(tokenizer, _query, history=_task_history, generation_config=config):\r\n  File \"/root/.cache/huggingface/modules/transformers_modules/modeling_qwen.py\", line 1216, in stream_generator\r\n    for token in self.generate_stream(\r\n  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/_contextlib.py\", line 35, in generator_context\r\n    response = gen.send(None)\r\n  File \"/usr/local/lib/python3.8/dist-packages/transformers_stream_generator/main.py\", line 931, in sample_stream\r\n    outputs = self(\r\n  File \"/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.8/dist-packages/accelerate/hooks.py\", line 164, in new_forward\r\n    output = module._old_forward(*args, **kwargs)\r\n  File \"/root/.cache/huggingface/modules/transformers_modules/modeling_qwen.py\", line 1045, in forward\r\n    transformer_outputs = self.transformer(\r\n  File \"/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/root/.cache/huggingface/modules/transformers_modules/modeling_qwen.py\", line 893, in forward\r\n    outputs = block(\r\n  File \"/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.8/dist-packages/accelerate/hooks.py\", line 164, in new_forward\r\n    output = module._old_forward(*args, **kwargs)\r\n  File \"/root/.cache/huggingface/modules/transformers_modules/modeling_qwen.py\", line 612, in forward\r\n    attn_outputs = self.attn(\r\n  File \"/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.8/dist-packages/accelerate/hooks.py\", line 164, in new_forward\r\n    output = module._old_forward(*args, **kwargs)\r\n  File \"/root/.cache/huggingface/modules/transformers_modules/modeling_qwen.py\", line 432, in forward\r\n    query = apply_rotary_pos_emb(query, q_pos_emb)\r\n  File \"/root/.cache/huggingface/modules/transformers_modules/modeling_qwen.py\", line 1344, in apply_rotary_pos_emb\r\n    return apply_rotary_emb_func(t_float, cos, sin).type_as(t)\r\n  File \"/usr/local/lib/python3.8/dist-packages/flash_attn/layers/rotary.py\", line 122, in apply_rotary_emb\r\n    return ApplyRotaryEmb.apply(\r\n  File \"/usr/local/lib/python3.8/dist-packages/torch/autograd/function.py\", line 506, in apply\r\n    return super().apply(*args, **kwargs)  # type: ignore[misc]\r\n  File \"/usr/local/lib/python3.8/dist-packages/flash_attn/layers/rotary.py\", line 48, in forward\r\n    out = apply_rotary(\r\n  File \"/usr/local/lib/python3.8/dist-packages/flash_attn/ops/triton/rotary.py\", line 213, in apply_rotary\r\n    rotary_kernel[grid](\r\n  File \"<string>\", line 41, in rotary_kernel\r\n  File \"/usr/local/lib/python3.8/dist-packages/triton/compiler.py\", line 1629, in compile\r\n    metadata[\"name\"] = ptx_get_kernel_name(next_module)\r\n  File \"/usr/local/lib/python3.8/dist-packages/triton/compiler.py\", line 1040, in ptx_get_kernel_name\r\n    assert ptx\r\nAssertionError\r\n\r\n![Uploading 微信图片_20241118164724.png…]()\r\n\r\n\n\n### 期望行为 | Expected Behavior\n\n_No response_\n\n### 复现方法 | Steps To Reproduce\n\n_No response_\n\n### 运行环境 | Environment\n\n```Markdown\n- OS:\r\n- Python:\r\n- Transformers:\r\n- PyTorch:\r\n- CUDA (`python -c 'import torch; print(torch.version.cuda)'`):\n```\n\n\n### 备注 | Anything else?\n\n_No response_",
    "state": "closed",
    "created_at": "2024-11-18T08:50:04+00:00",
    "closed_at": "2024-12-27T08:07:04+00:00",
    "updated_at": "2024-12-27T08:07:04+00:00",
    "author": "jydsun",
    "author_type": "User",
    "comments_count": 2,
    "reactions_count": 0,
    "labels": "inactive",
    "milestone": null,
    "closed_by": "github-actions[bot]",
    "resolution_time_hours": 935.2833333333333,
    "first_comments": [
      {
        "author": "jydsun",
        "created_at": "2024-11-18T08:55:23+00:00",
        "body": "![微信图片_20241118164724](https://github.com/user-attachments/assets/cb7257c1-f6a5-4b51-b8e7-6061b8bbc25f)\r\n![微信图片_20241118165342](https://github.com/user-attachments/assets/e0d4782c-54e6-47a4-b2a3-cb1af7aa706a)\r\n\r\n补充信息，一天都没找到原因，请高手指导，如何解决"
      },
      {
        "author": "github-actions[bot]",
        "created_at": "2024-12-19T08:07:36+00:00",
        "body": "This issue has been automatically marked as inactive due to lack of recent activity. Should you believe it remains unresolved and warrants attention, kindly leave a comment on this thread.\n此问题由于长期未有新进展而被系统自动标记为不活跃。如果您认为它仍有待解决，请在此帖下方留言以补充信息。"
      }
    ],
    "url": "https://github.com/QwenLM/Qwen/issues/1330"
  },
  {
    "repository": "QwenLM/Qwen",
    "issue_number": 1329,
    "title": "1",
    "body": null,
    "state": "closed",
    "created_at": "2024-11-08T08:40:56+00:00",
    "closed_at": "2024-11-08T09:01:03+00:00",
    "updated_at": "2024-11-08T09:01:03+00:00",
    "author": "dragonfqg",
    "author_type": "User",
    "comments_count": 1,
    "reactions_count": 0,
    "labels": "",
    "milestone": null,
    "closed_by": "dragonfqg",
    "resolution_time_hours": 0.3352777777777778,
    "first_comments": [
      {
        "author": "dragonfqg",
        "created_at": "2024-11-08T09:01:03+00:00",
        "body": "> _No description provided. 未提供描述。_\r\n\r\n"
      }
    ],
    "url": "https://github.com/QwenLM/Qwen/issues/1329"
  },
  {
    "repository": "QwenLM/Qwen",
    "issue_number": 1327,
    "title": "[BUG] <title> File \"/home/.cache/huggingface/modules/transformers_modules/Qwen-7B-Chat/modeling_qwen.py\", line 352, in _attn     attn_weights = torch.where( RuntimeError: expected scalar type c10::Half but found double",
    "body": "### 是否已有关于该错误的issue或讨论？ | Is there an existing issue / discussion for this?\n\n- [X] 我已经搜索过已有的issues和讨论 | I have searched the existing issues / discussions\n\n### 该问题是否在FAQ中有解答？ | Is there an existing answer for this in FAQ?\n\n- [X] 我已经搜索过FAQ | I have searched FAQ\n\n### 当前行为 | Current Behavior\n\n_No response_\n\n### 期望行为 | Expected Behavior\n\n_No response_\n\n### 复现方法 | Steps To Reproduce\n\n_No response_\n\n### 运行环境 | Environment\n\n```Markdown\n- OS:\r\n- Python:\r\n- Transformers:\r\n- PyTorch:\r\n- CUDA (`python -c 'import torch; print(torch.version.cuda)'`):\n```\n\n\n### 备注 | Anything else?\n\n_No response_",
    "state": "closed",
    "created_at": "2024-10-30T06:35:51+00:00",
    "closed_at": "2024-12-06T08:07:34+00:00",
    "updated_at": "2024-12-06T08:07:34+00:00",
    "author": "zkailinzhang",
    "author_type": "User",
    "comments_count": 1,
    "reactions_count": 0,
    "labels": "inactive",
    "milestone": null,
    "closed_by": "github-actions[bot]",
    "resolution_time_hours": 889.5286111111111,
    "first_comments": [
      {
        "author": "github-actions[bot]",
        "created_at": "2024-11-29T08:07:31+00:00",
        "body": "This issue has been automatically marked as inactive due to lack of recent activity. Should you believe it remains unresolved and warrants attention, kindly leave a comment on this thread.\n此问题由于长期未有新进展而被系统自动标记为不活跃。如果您认为它仍有待解决，请在此帖下方留言以补充信息。"
      }
    ],
    "url": "https://github.com/QwenLM/Qwen/issues/1327"
  },
  {
    "repository": "QwenLM/Qwen",
    "issue_number": 1326,
    "title": "[BUG] <title>",
    "body": "### 是否已有关于该错误的issue或讨论？ | Is there an existing issue / discussion for this?\r\n\r\n- [X] 我已经搜索过已有的issues和讨论 | I have searched the existing issues / discussions\r\n\r\n### 该问题是否在FAQ中有解答？ | Is there an existing answer for this in FAQ?\r\n\r\n- [X] 我已经搜索过FAQ | I have searched FAQ\r\n\r\n### 当前行为 | Current Behavior\r\n\r\nTraceback (most recent call last):\r\n  File \"/root/autodl-tmp/Qwen/finetune.py\", line 374, in <module>\r\n    train()\r\n  File \"/root/autodl-tmp/Qwen/finetune.py\", line 363, in train\r\n    trainer = Trainer(\r\n  File \"/root/miniconda3/envs/qwen/lib/python3.10/site-packages/transformers/trainer.py\", line 340, in __init__\r\n    self.create_accelerator_and_postprocess()\r\n  File \"/root/miniconda3/envs/qwen/lib/python3.10/site-packages/transformers/trainer.py\", line 3883, in create_accelerator_and_postprocess\r\n    self.accelerator = Accelerator(\r\nTypeError: Accelerator.__init__() got an unexpected keyword argument 'dispatch_batches'\r\n\r\n\r\n### 运行环境 | Environment\r\n\r\n```Markdown\r\n- OS: linux\r\n- Python: 3.10\r\n- Transformers: 4.32.0\r\n- PyTorch: 2.1.0\r\n- CUDA (`python -c 'import torch; print(torch.version.cuda)'`):12.1\r\n```\r\n\r\n\r\n### 备注 | Anything else?\r\n\r\n_No response_",
    "state": "closed",
    "created_at": "2024-10-24T13:03:50+00:00",
    "closed_at": "2024-12-03T08:07:45+00:00",
    "updated_at": "2024-12-03T08:07:45+00:00",
    "author": "bxhsort",
    "author_type": "User",
    "comments_count": 2,
    "reactions_count": 0,
    "labels": "inactive",
    "milestone": null,
    "closed_by": "github-actions[bot]",
    "resolution_time_hours": 955.0652777777777,
    "first_comments": [
      {
        "author": "guoping1127",
        "created_at": "2024-10-26T09:19:07+00:00",
        "body": "我也有这个错误\r\n"
      },
      {
        "author": "github-actions[bot]",
        "created_at": "2024-11-26T08:07:42+00:00",
        "body": "This issue has been automatically marked as inactive due to lack of recent activity. Should you believe it remains unresolved and warrants attention, kindly leave a comment on this thread.\n此问题由于长期未有新进展而被系统自动标记为不活跃。如果您认为它仍有待解决，请在此帖下方留言以补充信息。"
      }
    ],
    "url": "https://github.com/QwenLM/Qwen/issues/1326"
  },
  {
    "repository": "QwenLM/Qwen",
    "issue_number": 1324,
    "title": "💡 [REQUEST] - <title>qwen推理模型是否可以多机分布式部署？（单机单卡不足以支撑推理模型部署，需要多机多卡）",
    "body": "### 起始日期 | Start Date\n\n_No response_\n\n### 实现PR | Implementation PR\n\n我想布置qwen-14B-chat推理模型，但是我只有两台16G显存的机器，一张卡上运行不起来模型，我想问一下，是否可以两台机器分布式部署推理模型\n\n### 相关Issues | Reference Issues\n\n_No response_\n\n### 摘要 | Summary\n\n多台机器分布式部署推理模型\n\n### 基本示例 | Basic Example\n\n如何使用两台tesla-16G进行14B-chat的推理\n\n### 缺陷 | Drawbacks\n\n单机单卡不足以支撑\n\n### 未解决问题 | Unresolved questions\n\n_No response_",
    "state": "closed",
    "created_at": "2024-10-22T02:22:13+00:00",
    "closed_at": "2024-12-01T08:07:00+00:00",
    "updated_at": "2024-12-01T08:07:00+00:00",
    "author": "feifaxiaoming",
    "author_type": "User",
    "comments_count": 8,
    "reactions_count": 0,
    "labels": "question,inactive",
    "milestone": null,
    "closed_by": "github-actions[bot]",
    "resolution_time_hours": 965.7463888888889,
    "first_comments": [
      {
        "author": "xuanhua",
        "created_at": "2024-10-23T06:50:43+00:00",
        "body": "目前transformer默认都是使用自家的accelerate这个库来提供推理的，从目前来看这个库只支持单机多卡。多节点的推理其实应该也很简单，都是类似。\r\nhttps://github.com/huggingface/accelerate/issues/1890"
      },
      {
        "author": "feifaxiaoming",
        "created_at": "2024-10-23T08:00:17+00:00",
        "body": "> 目前transformer默认都是使用自家的accelerate这个库来提供推理的，从目前来看这个库只支持单机多卡。多节点的推理其实应该也很简单，都是类似。 [huggingface/accelerate#1890](https://github.com/huggingface/accelerate/issues/1890)\r\n\r\n您提供的连接中，也是咨询多节点得，但是我发现里面并没有给出解决答案，您那有能用的方案吗？"
      },
      {
        "author": "xuanhua",
        "created_at": "2024-10-23T15:30:32+00:00",
        "body": "@feifaxiaoming 对，我给你的链接就说明目前huggingface这边还没有实现这块儿功能，建议你单步跟踪一下huggingface的模型的token的生成过程。尤其是将 device_map 参数设置为 ”auto“ 的时候。transformers库使用了huggingface家的另外一个叫做accelerate的库，这个库会把模型的各个sumodule分散放到不同的设备上（优先gpu，gpu显存不够了，再往cpu的memory上放）。然后在模型进行forward的过程时候，修改了原有模块的forward函数，使得原有的foward函数执行之后还会执行一个post的操作，就是把foward的输出结果放到下一submodule的forward操作之后，首先将上一轮的计算结果防止到该submodule所在设备上。同理，如果是跨节点，那么就需要将某一个节点上最后执行完的结果通过网络发送到下一个节点上。从更高效率的角度上说，似乎还需要一个mpi，或者nccl来做一个调度器，让整个pipeline更加有效率。"
      },
      {
        "author": "xuanhua",
        "created_at": "2024-10-23T18:56:33+00:00",
        "body": "@feifaxiaoming 另外，训练和推理还是不太一样，推理的时候，如果是分成两台机器，生成一个token需要两次数据传输；\r\n假设机器A上存储模型的0\\~15层，机器B上存储模型16\\~29层（假设模型一共30层transformer结构），那么在完成计算所有prompt中的token对应的hidden states （假设为`H_0`）的计算之后，计算真正所需要生成的第一个token需要经历下面的过程：\r\n1） ``H_0``在机器A上作为输入，经过0\\~15层的计算，产生第一个token所对应的 ``h_1_A`` （形状为 batch_size \\times hidden_size）\r\n2)   ``h_1_A``通过A和B之间的网络到达B机器\r\n3）B机器使用``h_1_A``作为输入，经历16\\~29层的计算，产生``h_1_B``作为最后的输出，``h_1_B``在机器B上进行采样（假设使用采样的方法生成token）生成token `` t_1``\r\n4） ``t_1``从B传回到A\r\n5）A将现有的context长度加1，继续回到第 1)步\r\n所以，每生成一个token，实际上两台机器之间就需要进行两次数据交换；假设最快耗时为1秒进行网络传输，那么一个token就需要多2秒针计算时间。对于一个prompt，整体计算下来的时间消耗基本上很难接受。例如生成120字，你可能光在网络传输上就需要等待120 x 2 = 4 分钟；所以你的两台机器之间网络传输速度要非常快，例如10ms，那么 120 x 20ms = 2.4s； 所以需要两台机器之间的网络延时非常的小，并且估计需要把batch size设置的很大，充分利用带宽。 总之训练的时候使用的并行预测token，多机也还行，但如果是推理，多机基本上对网络要求太高了。大部分的时间都是在网络传数据上。"
      },
      {
        "author": "feifaxiaoming",
        "created_at": "2024-10-24T02:26:54+00:00",
        "body": "accelerate这个库只能单机多卡部署，不能实现多机并行，还有您说的这个层的，我试验了使用device_map拆分了80层，在单机上是可用的，但是多机的时候并不好用，您那边是否有可以直接使用的方案呢？就是可以多机分布式部署的模型方案呢》？"
      }
    ],
    "url": "https://github.com/QwenLM/Qwen/issues/1324"
  },
  {
    "repository": "QwenLM/Qwen",
    "issue_number": 1323,
    "title": "[BUG] <title>mtbench在qwen2-7b-instruct的得分",
    "body": "### 是否已有关于该错误的issue或讨论？ | Is there an existing issue / discussion for this?\n\n- [X] 我已经搜索过已有的issues和讨论 | I have searched the existing issues / discussions\n\n### 该问题是否在FAQ中有解答？ | Is there an existing answer for this in FAQ?\n\n- [X] 我已经搜索过FAQ | I have searched FAQ\n\n### 当前行为 | Current Behavior\n\n请教一下，qwen2-7b-instruct在mtbench上是如何进行评测得分的，可以具体一点吗？比如超参数设置\n\n### 期望行为 | Expected Behavior\n\n_No response_\n\n### 复现方法 | Steps To Reproduce\n\n_No response_\n\n### 运行环境 | Environment\n\n```Markdown\n- OS:\r\n- Python:\r\n- Transformers:\r\n- PyTorch:\r\n- CUDA (`python -c 'import torch; print(torch.version.cuda)'`):\n```\n\n\n### 备注 | Anything else?\n\n_No response_",
    "state": "closed",
    "created_at": "2024-10-21T13:01:37+00:00",
    "closed_at": "2024-11-28T08:07:41+00:00",
    "updated_at": "2024-11-28T08:07:42+00:00",
    "author": "zhang-junjian",
    "author_type": "User",
    "comments_count": 1,
    "reactions_count": 0,
    "labels": "inactive",
    "milestone": null,
    "closed_by": "github-actions[bot]",
    "resolution_time_hours": 907.1011111111111,
    "first_comments": [
      {
        "author": "github-actions[bot]",
        "created_at": "2024-11-21T08:07:29+00:00",
        "body": "This issue has been automatically marked as inactive due to lack of recent activity. Should you believe it remains unresolved and warrants attention, kindly leave a comment on this thread.\n此问题由于长期未有新进展而被系统自动标记为不活跃。如果您认为它仍有待解决，请在此帖下方留言以补充信息。"
      }
    ],
    "url": "https://github.com/QwenLM/Qwen/issues/1323"
  },
  {
    "repository": "QwenLM/Qwen",
    "issue_number": 1322,
    "title": "[BUG] <title> No heartbeat received from MQLLMEngine",
    "body": "### 是否已有关于该错误的issue或讨论？ | Is there an existing issue / discussion for this?\n\n- [X] 我已经搜索过已有的issues和讨论 | I have searched the existing issues / discussions\n\n### 该问题是否在FAQ中有解答？ | Is there an existing answer for this in FAQ?\n\n- [X] 我已经搜索过FAQ | I have searched FAQ\n\n### 当前行为 | Current Behavior\n\nPyTorch version: 2.4.0+cu121\r\n Python version: 3.10.12\r\n启动模型服务：\r\npython3 -m vllm.entrypoints.openai.api_server --host [0.0.0.0](http://0.0.0.0/) \\\r\n--port 10023 --seed 1024 \\\r\n--served-model qwen1.5-4b \\\r\n--model \"/home/work/ssd1/Qwen1.5-4B-Chat\" --trust-remote-code \\\r\n--tokenizer-mode slow --max-model-len 128 \\\r\n--dtype float16 \r\n进行模型推理：\r\ncurl http://127.0.0.1:10023/v1/completions \\\r\n-H \"Content-Type: application/json\" \\\r\n-d '{\r\n\"model\":\"qwen1.5-4b\", \"max_tokens\":64, \"seed\":0, \"top_k\":1, \"top_p\":0.8, \"temperature\":0.3, \r\n\"prompt\": \"谈谈你对大模型的理解\"\r\n}'\r\n报错信息：\r\nNo heartbeat received from MQLLMEngine\n\n### 期望行为 | Expected Behavior\n\n_No response_\n\n### 复现方法 | Steps To Reproduce\n\n_No response_\n\n### 运行环境 | Environment\n\n```Markdown\n- OS:\r\n- Python:\r\n- Transformers:\r\n- PyTorch:\r\n- CUDA (`python -c 'import torch; print(torch.version.cuda)'`):\n```\n\n\n### 备注 | Anything else?\n\n_No response_",
    "state": "closed",
    "created_at": "2024-10-14T11:23:08+00:00",
    "closed_at": "2024-11-21T08:07:31+00:00",
    "updated_at": "2024-11-21T08:07:31+00:00",
    "author": "hulk-zhk",
    "author_type": "User",
    "comments_count": 1,
    "reactions_count": 0,
    "labels": "inactive",
    "milestone": null,
    "closed_by": "github-actions[bot]",
    "resolution_time_hours": 908.7397222222222,
    "first_comments": [
      {
        "author": "github-actions[bot]",
        "created_at": "2024-11-14T08:07:20+00:00",
        "body": "This issue has been automatically marked as inactive due to lack of recent activity. Should you believe it remains unresolved and warrants attention, kindly leave a comment on this thread.\n此问题由于长期未有新进展而被系统自动标记为不活跃。如果您认为它仍有待解决，请在此帖下方留言以补充信息。"
      }
    ],
    "url": "https://github.com/QwenLM/Qwen/issues/1322"
  },
  {
    "repository": "QwenLM/Qwen",
    "issue_number": 1321,
    "title": "[BUG] <title>Cannot reproduce Qwen1.5-7B base model's reported score 62.5 on gsm8k",
    "body": "### 是否已有关于该错误的issue或讨论？ | Is there an existing issue / discussion for this?\n\n- [X] 我已经搜索过已有的issues和讨论 | I have searched the existing issues / discussions\n\n### 该问题是否在FAQ中有解答？ | Is there an existing answer for this in FAQ?\n\n- [X] 我已经搜索过FAQ | I have searched FAQ\n\n### 当前行为 | Current Behavior\n\nI used the code eval/evaluate_gsm8k.py to evaluate Qwen1.5-7B base model downloaded from huggingface. \r\nThe results shows that Qwen1.5-7B base got Acc:  0.4457922668 on gsm8k, which is much lower than the reported score 62.5 (https://huggingface.co/Qwen/Qwen2-7B).\r\nBut the  Qwen1.5-1.8B base got Acc: 0.382865807 which is similar to the reported score 38.4 (https://huggingface.co/Qwen/Qwen2-1.5B)\r\n\r\nAnother strange thing is that the Qwen1.5-7B-Chat model got 60.3 on gsm8k (https://huggingface.co/Qwen/Qwen2-7B-Instruct), which is lower than the base model.\r\n\r\nHope to know if there is any typo or the base model is finetuned with the gsm8k training set before the evaluation on the test set?\r\n\r\n\n\n### 期望行为 | Expected Behavior\n\nreproduce Qwen1.5-7B base model's reported score 62.5 on gsm8k\n\n### 复现方法 | Steps To Reproduce\n\nI downloaded the gsm8k test set from https://github.com/openai/grade-school-math/tree/master/grade_school_math/data and checked its content is as same as the huggingface parquet https://huggingface.co/datasets/openai/gsm8k/tree/main/main\r\n\r\nThe few-shot prompt (from https://github.com/QwenLM/Qwen/blob/main/eval/gsm8k_prompt.txt) is correctly added.\r\n\r\nI only modified these three lines:\r\nsent = tokenizer.tokenizer.decode(tokens[raw_text_len:])   ->   sent = tokenizer.decode(tokens[raw_text_len:])\r\ninput_ids = tokenizer.tokenizer.encode(input_txt)   ->    input_ids = tokenizer.encode(input_txt)\r\ndataset = load_from_disk(args.sample_input_file)  ->   data_files = {'train': args.sample_input_file+'train.json', 'test': args.sample_input_file+'test.json'}\r\n        dataset = load_dataset('json', data_files=data_files)\r\n\r\n\r\n\r\n\n\n### 运行环境 | Environment\n\n```Markdown\nMy environment: ubuntu 18.04\r\nTesla V100-SXM2-32GB * 8\r\n\r\npython                    3.10.14\r\ntorch                     2.2.0\r\ntransformers              4.41.2\r\nCUDA: 12.1\n```\n\n\n### 备注 | Anything else?\n\n_No response_",
    "state": "closed",
    "created_at": "2024-10-09T12:11:03+00:00",
    "closed_at": "2024-11-17T08:06:27+00:00",
    "updated_at": "2024-11-17T08:06:27+00:00",
    "author": "StevenLau6",
    "author_type": "User",
    "comments_count": 2,
    "reactions_count": 0,
    "labels": "inactive",
    "milestone": null,
    "closed_by": "github-actions[bot]",
    "resolution_time_hours": 931.9233333333333,
    "first_comments": [
      {
        "author": "TanateT",
        "created_at": "2024-10-10T09:25:24+00:00",
        "body": "I also encounter the same question. what do I need to modify?"
      },
      {
        "author": "github-actions[bot]",
        "created_at": "2024-11-10T08:06:24+00:00",
        "body": "This issue has been automatically marked as inactive due to lack of recent activity. Should you believe it remains unresolved and warrants attention, kindly leave a comment on this thread.\n此问题由于长期未有新进展而被系统自动标记为不活跃。如果您认为它仍有待解决，请在此帖下方留言以补充信息。"
      }
    ],
    "url": "https://github.com/QwenLM/Qwen/issues/1321"
  },
  {
    "repository": "QwenLM/Qwen",
    "issue_number": 1320,
    "title": "[BUG] 通过web_demo 运行微调模型",
    "body": "### 是否已有关于该错误的issue或讨论？ | Is there an existing issue / discussion for this?\r\n\r\n- [X] 我已经搜索过已有的issues和讨论 | I have searched the existing issues / discussions\r\n\r\n### 该问题是否在FAQ中有解答？ | Is there an existing answer for this in FAQ?\r\n\r\n- [X] 我已经搜索过FAQ | I have searched FAQ\r\n\r\n### 当前行为 | Current Behavior\r\n\r\n\r\n\r\n### 期望行为 | Expected Behavior\r\n\r\n_No response_\r\n\r\n### 复现方法 | Steps To Reproduce\r\n\r\n_No response_\r\n\r\n### 运行环境 | Environment\r\n\r\n```Markdown\r\n- OS: Ubuntu 22.04\r\n- Python: 3.12.5\r\n- Transformers:   4.45.1\r\n- PyTorch:\r\n- CUDA (`python -c 'import torch; print(torch.version.cuda)'`):\r\n```\r\n\r\n\r\n### 备注 | Anything else?\r\n\r\n_No response_",
    "state": "closed",
    "created_at": "2024-10-09T00:06:43+00:00",
    "closed_at": "2024-10-09T03:00:27+00:00",
    "updated_at": "2024-10-09T03:00:27+00:00",
    "author": "YANGLEDUO1",
    "author_type": "User",
    "comments_count": 0,
    "reactions_count": 0,
    "labels": "",
    "milestone": null,
    "closed_by": "YANGLEDUO1",
    "resolution_time_hours": 2.8955555555555557,
    "first_comments": [],
    "url": "https://github.com/QwenLM/Qwen/issues/1320"
  },
  {
    "repository": "QwenLM/Qwen",
    "issue_number": 1319,
    "title": "[BUG] <title>",
    "body": "### 是否已有关于该错误的issue或讨论？ | Is there an existing issue / discussion for this?\n\n- [X] 我已经搜索过已有的issues和讨论 | I have searched the existing issues / discussions\n\n### 该问题是否在FAQ中有解答？ | Is there an existing answer for this in FAQ?\n\n- [X] 我已经搜索过FAQ | I have searched FAQ\n\n### 当前行为 | Current Behavior\n\nWhen using the Qianwen 2B model, there were very few cases of abnormal allocation of video memory. What is the problem?\r\n![image](https://github.com/user-attachments/assets/b8b3e33d-503a-4b12-8fc7-5594655b82ff)\r\n\n\n### 期望行为 | Expected Behavior\n\n I hope it can be resolved\n\n### 复现方法 | Steps To Reproduce\n\n_No response_\n\n### 运行环境 | Environment\n\n```Markdown\n- OS:\r\n- Python:\r\n- Transformers:\r\n- PyTorch:\r\n- CUDA (`python -c 'import torch; print(torch.version.cuda)'`):\n```\n\n\n### 备注 | Anything else?\n\n_No response_",
    "state": "closed",
    "created_at": "2024-09-24T09:27:29+00:00",
    "closed_at": "2024-11-02T08:06:49+00:00",
    "updated_at": "2024-11-02T08:06:49+00:00",
    "author": "Timeqaq",
    "author_type": "User",
    "comments_count": 1,
    "reactions_count": 0,
    "labels": "inactive",
    "milestone": null,
    "closed_by": "github-actions[bot]",
    "resolution_time_hours": 934.6555555555556,
    "first_comments": [
      {
        "author": "github-actions[bot]",
        "created_at": "2024-10-25T08:07:14+00:00",
        "body": "This issue has been automatically marked as inactive due to lack of recent activity. Should you believe it remains unresolved and warrants attention, kindly leave a comment on this thread.\n此问题由于长期未有新进展而被系统自动标记为不活跃。如果您认为它仍有待解决，请在此帖下方留言以补充信息。"
      }
    ],
    "url": "https://github.com/QwenLM/Qwen/issues/1319"
  },
  {
    "repository": "QwenLM/Qwen",
    "issue_number": 1318,
    "title": "ValueError: Cannot merge LORA layers when the model is gptq quantized",
    "body": "### 是否已有关于该错误的issue或讨论？ | Is there an existing issue / discussion for this?\n\n- [X] 我已经搜索过已有的issues和讨论 | I have searched the existing issues / discussions\n\n### 该问题是否在FAQ中有解答？ | Is there an existing answer for this in FAQ?\n\n- [X] 我已经搜索过FAQ | I have searched FAQ\n\n### 当前行为 | Current Behavior\n\n微调Qwen_1.8b_chat_int4模型，分别使用lora和qlora方法，合并模型时报错\r\nValueError: Cannot merge LORA layers when the model is gptq quantized\n\n### 期望行为 | Expected Behavior\n\n解决该问题\n\n### 复现方法 | Steps To Reproduce\n\npython qwen_lora_merge.py\r\n\r\nfrom peft import AutoPeftModelForCausalLM\r\nfrom transformers import AutoTokenizer\r\n\r\npath_to_adapter=\"/home/ren/Finetuning/Qwen-1.8-chat/\"\r\nnew_model_directory=\"/home/ren/Finetuning/llm_model/Qwen-1_8B-Chat-Int4_law\"\r\n\r\nmodel = AutoPeftModelForCausalLM.from_pretrained(\r\n\r\n   path_to_adapter, # path to the output directory\r\n\r\n   device_map=\"auto\",\r\n\r\n   trust_remote_code=True\r\n\r\n).eval()merged_model = model.merge_and_unload()\r\n\r\n# max_shard_size and safe serialization are not necessary.\r\n\r\n# They respectively work for sharding checkpoint and save the model to safetensors\r\n\r\nmerged_model.save_pretrained(new_model_directory, max_shard_size=\"2048MB\", safe_serialization=True)\n\n### 运行环境 | Environment\n\n```Markdown\n- OS:Ubuntu 20.04\r\n- Python:3.10\r\n- Transformers:4.37.2\r\n- PyTorch:2.2.1\r\n- CUDA (`python -c 'import torch; print(torch.version.cuda)'`):11.8\n```\n\n\n### 备注 | Anything else?\n\nno",
    "state": "closed",
    "created_at": "2024-09-23T01:58:44+00:00",
    "closed_at": "2024-11-02T08:06:49+00:00",
    "updated_at": "2024-11-02T08:06:49+00:00",
    "author": "goy-jin",
    "author_type": "User",
    "comments_count": 2,
    "reactions_count": 0,
    "labels": "inactive",
    "milestone": null,
    "closed_by": "github-actions[bot]",
    "resolution_time_hours": 966.1347222222222,
    "first_comments": [
      {
        "author": "danyow-cheung",
        "created_at": "2024-09-24T15:38:50+00:00",
        "body": "如果你觉得这样一步到位的方式让你很不安心或者影响你接入下游应用，你可以选择先合并并存储模型（LoRA支持合并，Q-LoRA不支持），再用常规方式读取你的新模型，示例如下：\r\n\r\n-----\r\nqlora不支持合并"
      },
      {
        "author": "github-actions[bot]",
        "created_at": "2024-10-25T08:07:15+00:00",
        "body": "This issue has been automatically marked as inactive due to lack of recent activity. Should you believe it remains unresolved and warrants attention, kindly leave a comment on this thread.\n此问题由于长期未有新进展而被系统自动标记为不活跃。如果您认为它仍有待解决，请在此帖下方留言以补充信息。"
      }
    ],
    "url": "https://github.com/QwenLM/Qwen/issues/1318"
  },
  {
    "repository": "QwenLM/Qwen",
    "issue_number": 1317,
    "title": "ollama qwen2.5:72b-instruct 不断循环生成问题和答案",
    "body": "### 是否已有关于该错误的issue或讨论？ | Is there an existing issue / discussion for this?\n\n- [X] 我已经搜索过已有的issues和讨论 | I have searched the existing issues / discussions\n\n### 该问题是否在FAQ中有解答？ | Is there an existing answer for this in FAQ?\n\n- [X] 我已经搜索过FAQ | I have searched FAQ\n\n### 当前行为 | Current Behavior\n\n使用ollama run qwen2.5:72b-instruct，交互式问问题时出现了系统自动生成下一个问题，自己回答再自动生成下一个这样循环的情况，持续了1-2小时，被我终止了，第一次出现是这样\n\n### 期望行为 | Expected Behavior\n\n_No response_\n\n### 复现方法 | Steps To Reproduce\n\n_No response_\n\n### 运行环境 | Environment\n\n```Markdown\n- OS:\r\n- Python:\r\n- Transformers:\r\n- PyTorch:\r\n- CUDA (`python -c 'import torch; print(torch.version.cuda)'`):\n```\n\n\n### 备注 | Anything else?\n\n_No response_",
    "state": "closed",
    "created_at": "2024-09-22T19:47:08+00:00",
    "closed_at": "2024-10-30T08:07:28+00:00",
    "updated_at": "2024-10-30T08:07:28+00:00",
    "author": "jaychj",
    "author_type": "User",
    "comments_count": 4,
    "reactions_count": 0,
    "labels": "inactive",
    "milestone": null,
    "closed_by": "github-actions[bot]",
    "resolution_time_hours": 900.3388888888888,
    "first_comments": [
      {
        "author": "jaychj",
        "created_at": "2024-09-22T19:52:48+00:00",
        "body": "check every <|endoftext|>Human:\r\n[issue.md](https://github.com/user-attachments/files/17091005/issue.md)\r\n"
      },
      {
        "author": "jaychj",
        "created_at": "2024-09-22T20:03:35+00:00",
        "body": "复现了，似乎如果我让它写代码，就会触发问题，这次是这样的\r\ncat Desktop/input.md | ollama run qwen2.5:72b-instruct \"\\n\\nplease tell me how to do it by showing code, database schema, NLP etc. all the techs\" > Desktop/output.md\r\ninput.md是我前面一个问题，我做附件也上传了，只是想给更复杂的prompt而已，而且这个input.md也是这个模型生成的答案\r\n[input.md](https://github.com/user-attachments/files/17091037/input.md)\r\n\r\n目前这个output.md已经包含很多循环的问题了，检索每一个<|endoftext|>Human:可以看到\r\n[output.md](https://github.com/user-attachments/files/17091041/output.md)\r\n然后我被动停止了程序，似乎自己无法停止"
      },
      {
        "author": "jaychj",
        "created_at": "2024-09-22T20:08:05+00:00",
        "body": "os：macOS15.0\r\n芯片是M3 Max ，128G内存\r\npython：3.12.6"
      },
      {
        "author": "github-actions[bot]",
        "created_at": "2024-10-23T08:07:18+00:00",
        "body": "This issue has been automatically marked as inactive due to lack of recent activity. Should you believe it remains unresolved and warrants attention, kindly leave a comment on this thread.\n此问题由于长期未有新进展而被系统自动标记为不活跃。如果您认为它仍有待解决，请在此帖下方留言以补充信息。"
      }
    ],
    "url": "https://github.com/QwenLM/Qwen/issues/1317"
  },
  {
    "repository": "QwenLM/Qwen",
    "issue_number": 1315,
    "title": "qwen-7b-int4用vllm推理，为什么结果是乱码?",
    "body": "### 是否已有关于该错误的issue或讨论？ | Is there an existing issue / discussion for this?\n\n- [X] 我已经搜索过已有的issues和讨论 | I have searched the existing issues / discussions\n\n### 该问题是否在FAQ中有解答？ | Is there an existing answer for this in FAQ?\n\n- [X] 我已经搜索过FAQ | I have searched FAQ\n\n### 当前行为 | Current Behavior\n\n我的加载模型代码是:` model = vLLMWrapper(args.checkpoint_path, tensor_parallel_size=1)`,运行结果是\r\n![Screenshot from 2024-09-04 16-16-13](https://github.com/user-attachments/assets/d3ba9207-daeb-4e30-a4b2-9ed3a3e036a6)\r\nvllm推理fp16的结果是正确的，但是推理int4的结果是错误的。\n\n### 期望行为 | Expected Behavior\n\n_No response_\n\n### 复现方法 | Steps To Reproduce\n\n_No response_\n\n### 运行环境 | Environment\n\n```Markdown\n- OS:\r\n- Python:\r\n- Transformers:\r\n- PyTorch:\r\n- CUDA (`python -c 'import torch; print(torch.version.cuda)'`):\n```\n\n\n### 备注 | Anything else?\n\n_No response_",
    "state": "closed",
    "created_at": "2024-09-04T08:18:42+00:00",
    "closed_at": "2024-10-13T08:06:37+00:00",
    "updated_at": "2024-10-13T08:06:37+00:00",
    "author": "shaqing",
    "author_type": "User",
    "comments_count": 1,
    "reactions_count": 0,
    "labels": "inactive",
    "milestone": null,
    "closed_by": "github-actions[bot]",
    "resolution_time_hours": 935.7986111111111,
    "first_comments": [
      {
        "author": "github-actions[bot]",
        "created_at": "2024-10-05T08:06:31+00:00",
        "body": "This issue has been automatically marked as inactive due to lack of recent activity. Should you believe it remains unresolved and warrants attention, kindly leave a comment on this thread.\n此问题由于长期未有新进展而被系统自动标记为不活跃。如果您认为它仍有待解决，请在此帖下方留言以补充信息。"
      }
    ],
    "url": "https://github.com/QwenLM/Qwen/issues/1315"
  },
  {
    "repository": "QwenLM/Qwen",
    "issue_number": 1314,
    "title": "请问可以支持加入本地知识库进行微调大模型吗",
    "body": "### 是否已有关于该错误的issue或讨论？ | Is there an existing issue / discussion for this?\n\n- [X] 我已经搜索过已有的issues和讨论 | I have searched the existing issues / discussions\n\n### 该问题是否在FAQ中有解答？ | Is there an existing answer for this in FAQ?\n\n- [X] 我已经搜索过FAQ | I have searched FAQ\n\n### 当前行为 | Current Behavior\n\n请问可以支持加入本地知识库进行微调大模型吗\n\n### 期望行为 | Expected Behavior\n\n请问可以支持加入本地知识库进行微调大模型吗\n\n### 复现方法 | Steps To Reproduce\n\n请问可以支持加入本地知识库进行微调大模型吗\n\n### 运行环境 | Environment\n\n```Markdown\n- OS:\r\n- Python:\r\n- Transformers:\r\n- PyTorch:\r\n- CUDA (`python -c 'import torch; print(torch.version.cuda)'`):\n```\n\n\n### 备注 | Anything else?\n\n请问可以支持加入本地知识库进行微调大模型吗",
    "state": "closed",
    "created_at": "2024-08-23T03:31:00+00:00",
    "closed_at": "2024-10-05T08:06:33+00:00",
    "updated_at": "2024-10-05T08:06:33+00:00",
    "author": "jingmingtao",
    "author_type": "User",
    "comments_count": 3,
    "reactions_count": 0,
    "labels": "inactive",
    "milestone": null,
    "closed_by": "github-actions[bot]",
    "resolution_time_hours": 1036.5925,
    "first_comments": [
      {
        "author": "jingmingtao",
        "created_at": "2024-08-23T03:46:02+00:00",
        "body": "请问我想要微调，让模型回答同学的名字与出生日期，我的微调数据可以这么设计吗：[\r\n  {\r\n    \"id\": \"identity_0\",\r\n    \"conversations\": [\r\n      {\r\n        \"from\": \"user\",\r\n        \"value\": \"人名与生日\"\r\n      },\r\n      {\r\n        \"from\": \"assistant\",\r\n        \"value\": \"人名:小明,出生日期: 2021年5月11日\"\r\n      },\r\n      {\r\n        \"from\": \"assistant\",\r\n        \"value\": \"人名:小涛,出生日期: 2008年6月23日\"\r\n      }\r\n    ]\r\n  }\r\n]\r\n我想要达到这样的效果：问大模型，请问2021年5月11日出生的人名字是谁"
      },
      {
        "author": "zpge",
        "created_at": "2024-08-28T10:03:00+00:00",
        "body": "> ### 是否已有关于该错误的issue或讨论？ | Is there an existing issue / discussion for this?\r\n> * [x]  我已经搜索过已有的issues和讨论 | I have searched the existing issues / discussions\r\n> \r\n> ### 该问题是否在FAQ中有解答？ | Is there an existing answer for this in FAQ?\r\n> * [x]  我已经搜索过FAQ | I have searched FAQ\r\n> \r\n> ### 当前行为 | Current Behavior\r\n> 请问可以支持加入本地知识库进行微调大模型吗\r\n> \r\n> ### 期望行为 | Expected Behavior\r\n> 请问可以支持加入本地知识库进行微调大模型吗\r\n> \r\n> ### 复现方法 | Steps To Reproduce\r\n> 请问可以支持加入本地知识库进行微调大模型吗\r\n> \r\n> ### 运行环境 | Environment\r\n> ```\r\n> - OS:\r\n> - Python:\r\n> - Transformers:\r\n> - PyTorch:\r\n> - CUDA (`python -c 'import torch; print(torch.version.cuda)'`):\r\n> ```\r\n> \r\n> ### 备注 | Anything else?\r\n> 请问可以支持加入本地知识库进行微调大模型吗\r\n\r\n同问，目前提供的微调方式都是构建对话的，如果我有很多文档，想先让模型学一些领域知识，应该怎么微调？"
      },
      {
        "author": "github-actions[bot]",
        "created_at": "2024-09-28T08:06:30+00:00",
        "body": "This issue has been automatically marked as inactive due to lack of recent activity. Should you believe it remains unresolved and warrants attention, kindly leave a comment on this thread.\n此问题由于长期未有新进展而被系统自动标记为不活跃。如果您认为它仍有待解决，请在此帖下方留言以补充信息。"
      }
    ],
    "url": "https://github.com/QwenLM/Qwen/issues/1314"
  },
  {
    "repository": "QwenLM/Qwen",
    "issue_number": 1313,
    "title": "AWQ量化后，输出不能正常停止，不量化推理正常",
    "body": "### 是否已有关于该错误的issue或讨论？ | Is there an existing issue / discussion for this?\r\n\r\n- [X] 我已经搜索过已有的issues和讨论 | I have searched the existing issues / discussions\r\n\r\n### 该问题是否在FAQ中有解答？ | Is there an existing answer for this in FAQ?\r\n\r\n- [X] 我已经搜索过FAQ | I have searched FAQ\r\n\r\n### 当前行为 | Current Behavior\r\n\r\n**现象：不能正常停止输出**\r\n![image](https://github.com/user-attachments/assets/c6368c1c-fbfc-40cd-a63c-42620b69a047)\r\n\r\n**AWQ量化代码如下**\r\n输入我是从template.py中复制过来的，也对比过tokenizer_config.json，输入一致\r\n![image](https://github.com/user-attachments/assets/83a2f905-8cb2-49d3-ad27-6b887c5f50e0)\r\n\r\n```\r\nimport datasets\r\nimport json\r\nfrom awq import AutoAWQForCausalLM\r\nfrom transformers import AwqConfig, AutoConfig\r\nfrom huggingface_hub import HfApi\r\nfrom transformers import (\r\n    AutoTokenizer,\r\n    TrainingArguments,\r\n    Trainer,\r\n    DataCollatorForLanguageModeling\r\n)\r\nfrom peft import get_peft_model, LoraConfig, TaskType\r\n\r\n\r\n# _register_template(\r\n#     name=\"qwen\",\r\n#     format_user=StringFormatter(slots=[\"<|im_start|>user\\n{{content}}<|im_end|>\\n<|im_start|>assistant\\n\"]),\r\n#     format_system=StringFormatter(slots=[\"<|im_start|>system\\n{{content}}<|im_end|>\\n\"]),\r\n#     format_observation=StringFormatter(slots=[\"<|im_start|>tool\\n{{content}}<|im_end|>\\n<|im_start|>assistant\\n\"]),\r\n#     format_separator=EmptyFormatter(slots=[\"\\n\"]),\r\n#     default_system=\"You are a helpful assistant.\",\r\n#     stop_words=[\"<|im_end|>\"],\r\n#     replace_eos=True,\r\n# )\r\n\r\n\r\ndef format_text(item):\r\n    Q = item['input']\r\n    A = item['output']\r\n    system_input = \"<|im_start|>system\\n你是一个信息抽取专家，根据文本内容抽取最重要的5个实体和3个具备争议性的话题。<|im_end|>\\n\"\r\n    user = f\"<|im_start|>user\\n{Q}<|im_end|>\\n<|im_start|>assistant\\n{A}\"\r\n    res = system_input + user\r\n    return res\r\n\r\n\r\nmodel_path = \"/root/autodl-tmp/checkpoints/qwen2/entity_cls_all_clear_new/freeze/checkpoint-70\"\r\ndata_path = \"awq_eval.json\"\r\nquant_path = \"/root/autodl-tmp/qwen15-7b-awq\"\r\nquant_config = {\"zero_point\": True, \"q_group_size\": 128, \"w_bit\": 4, \"version\": \"GEMM\"}\r\n# Load model\r\nprint(\"Load model-------------\")\r\nmodel = AutoAWQForCausalLM.from_pretrained(model_path, safetensors=True)\r\ntokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\r\nf = open(data_path, 'r', encoding=\"utf-8\")\r\ndata = json.load(f)\r\ntext_list = [format_text(item) for item in data]\r\nprint('first:', text_list[0])\r\n# Quantize\r\nprint(\"Quantize-------------\")\r\nmodel.quantize(tokenizer, quant_config=quant_config, calib_data=text_list)\r\n\r\n# quantization_config = AwqConfig(\r\n#     bits=quant_config[\"w_bit\"],\r\n#     group_size=quant_config[\"q_group_size\"],\r\n#     zero_point=quant_config[\"zero_point\"],\r\n#     version=quant_config[\"version\"].lower(),\r\n# ).to_dict()\r\n# model.model.config.quantization_config = quantization_config\r\n# Save quantized model\r\nprint(\"Save quantized model-------------\")\r\nmodel.save_quantized(quant_path)\r\ntokenizer.save_pretrained(quant_path)\r\nprint(f'Model is quantized and saved at \"{quant_path}\"')\r\n```\r\n\r\n**AWQ量化后的推理脚本**\r\n```\r\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\r\nmodel_path = \"/root/autodl-tmp/qwen15-7b-awq-v2\"\r\ntokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\r\nmodel = AutoModelForCausalLM.from_pretrained(model_path, trust_remote_code=True).to(0)\r\ntext = \"酒驾、无证，还拉着1老4小…北海一男子因多项交通违法行为被交警查处 交通安全不容忽视，很多严重的道路交通事故都是因为不遵守交通法律法规导致的。近日，一男子就因多项交通违法行为被北海公安交警查处，该男子酒后、无证驾驶三轮车摩托车，还搭载5人出行，存在极大的安全隐患。7月16日晚上，北海市公安局交警支队第二大队执勤民警在北部湾路开展夏季交通安全突出违法专项整治行动时，发现一辆无牌三轮摩托车，车上除了驾驶人竟然还坐着五个人（一名老人和四名小孩）。民警赶忙喊话示意驾驶人靠边停车，并下车接受检查。在此过程中，民警发现该车驾驶人朱某身上散发出的酒味，随即对朱某进行呼吸式酒精测试，结果显示其体内酒精含量为22mg/100ml，达到饮酒后驾驶机动车标准。此外，民警在核查朱某身份信息时，还发现其并未取得机动车驾驶证。据朱某交代，当天晚上他在家吃饭时喝了点酒，家里小孩闹着要出去玩，他自认为晚上这段路应该不会有交警查车，心存侥幸便驾车上路，没想到被逮了个正着。随后，民警对朱某及其家人进行了严肃的批评教育，告知朱某违法的法律后果，告诫其要严格遵守交通法律法规，确保安全出行。最后，民警针对朱某饮酒后驾驶机动车、未取得机动车驾驶证驾驶机动车上道路行驶、驾驶未悬挂机动车号牌的机动车上道路行驶、三轮车违反规定载人的违法行为，处以罚款 1850 元、拖移机动车的行政处罚；此外，朱某还将面临15日以下行政拘留的处罚。北海公安交警提示：请广大交通参与者在驾驶车辆时切勿酒驾、醉驾、超员、超速、超载、违法载人等，切实增强自我安全防护意识，严格遵守交通法规，谨慎驾驶，安全出行。\"\r\n\r\nsystem_input = \"<|im_start|>system\\n你是一个信息抽取专家，根据文本内容抽取最重要的5个实体和3个具备争议性的话题。<|im_end|>\\n\"\r\nuser = f\"<|im_start|>user\\n{text}<|im_end|>\\n<|im_start|>assistant\\n\"\r\nres = system_input + user\r\ninputs = tokenizer(res, return_tensors=\"pt\").to(0)\r\nout = model.generate(**inputs, max_new_tokens=2048)\r\nprint(tokenizer.decode(out[0], skip_special_tokens=True))\r\n```\r\n\r\n### 期望行为 | Expected Behavior\r\n\r\n_No response_\r\n\r\n### 复现方法 | Steps To Reproduce\r\n\r\n_No response_\r\n\r\n### 运行环境 | Environment\r\n\r\n```Markdown\r\n模型版本：qwen1.5-7b\r\nPython 3.10.8\r\ncuda 12.1\r\ntorch 2.3.1\r\ntransformers 4.42.3\r\n```\r\n\r\n\r\n### 备注 | Anything else?\r\n\r\n_No response_",
    "state": "closed",
    "created_at": "2024-08-20T09:04:03+00:00",
    "closed_at": "2024-09-27T08:07:34+00:00",
    "updated_at": "2024-09-27T08:07:34+00:00",
    "author": "angelOnly",
    "author_type": "User",
    "comments_count": 1,
    "reactions_count": 0,
    "labels": "inactive",
    "milestone": null,
    "closed_by": "github-actions[bot]",
    "resolution_time_hours": 911.0586111111111,
    "first_comments": [
      {
        "author": "github-actions[bot]",
        "created_at": "2024-09-20T08:07:02+00:00",
        "body": "This issue has been automatically marked as inactive due to lack of recent activity. Should you believe it remains unresolved and warrants attention, kindly leave a comment on this thread.\n此问题由于长期未有新进展而被系统自动标记为不活跃。如果您认为它仍有待解决，请在此帖下方留言以补充信息。"
      }
    ],
    "url": "https://github.com/QwenLM/Qwen/issues/1313"
  },
  {
    "repository": "QwenLM/Qwen",
    "issue_number": 1312,
    "title": "[BUG] <title>Nvidia Jetson Orin NX开发板上推理运行qlora微调之后的模型，报错：不支持QuantLinear()",
    "body": "### 是否已有关于该错误的issue或讨论？ | Is there an existing issue / discussion for this?\n\n- [X] 我已经搜索过已有的issues和讨论 | I have searched the existing issues / discussions\n\n### 该问题是否在FAQ中有解答？ | Is there an existing answer for this in FAQ?\n\n- [X] 我已经搜索过FAQ | I have searched FAQ\n\n### 当前行为 | Current Behavior\n\n按官网步骤进行qlora微调之后，在开发板上推理运行报错：\r\nValueError: Target module QuantLinear() is not supported. Currently, only the following modules are supported: `torch.nn.Linear`, `torch.nn.Embedding`, `torch.nn.Conv2d`, `transformers.pytorch_utils.Conv1D`.\r\n\n\n### 期望行为 | Expected Behavior\n\n_No response_\n\n### 复现方法 | Steps To Reproduce\n\n_No response_\n\n### 运行环境 | Environment\n\n```Markdown\n- OS: Ubuntu 22.04\r\n- Python: 3.10\r\n- Transformers: 4.39.0\r\n- PyTorch: 2.4.0\r\n- CUDA (`python -c 'import torch; print(torch.version.cuda)'`): 12.2\n```\n\n\n### 备注 | Anything else?\n\n_No response_",
    "state": "closed",
    "created_at": "2024-08-14T03:22:08+00:00",
    "closed_at": "2024-09-20T08:07:04+00:00",
    "updated_at": "2024-09-20T08:07:04+00:00",
    "author": "lollipopyu",
    "author_type": "User",
    "comments_count": 1,
    "reactions_count": 0,
    "labels": "inactive",
    "milestone": null,
    "closed_by": "github-actions[bot]",
    "resolution_time_hours": 892.7488888888889,
    "first_comments": [
      {
        "author": "github-actions[bot]",
        "created_at": "2024-09-13T08:06:47+00:00",
        "body": "This issue has been automatically marked as inactive due to lack of recent activity. Should you believe it remains unresolved and warrants attention, kindly leave a comment on this thread.\n此问题由于长期未有新进展而被系统自动标记为不活跃。如果您认为它仍有待解决，请在此帖下方留言以补充信息。"
      }
    ],
    "url": "https://github.com/QwenLM/Qwen/issues/1312"
  },
  {
    "repository": "QwenLM/Qwen",
    "issue_number": 1311,
    "title": "[BUG] 增加上下文长度后输出乱码",
    "body": "### 是否已有关于该错误的issue或讨论？ | Is there an existing issue / discussion for this?\n\n- [X] 我已经搜索过已有的issues和讨论 | I have searched the existing issues / discussions\n\n### 该问题是否在FAQ中有解答？ | Is there an existing answer for this in FAQ?\n\n- [X] 我已经搜索过FAQ | I have searched FAQ\n\n### 当前行为 | Current Behavior\n\n使用官网教程中的YARN想扩充上下文长度，尝试扩充两倍，使用的config增加了：\r\n\"rope_scaling\": {\r\n            \"factor\": 2.0,\r\n            \"original_max_position_embeddings\": 32768,\r\n            \"type\": \"yarn\"\r\n        }\r\n        这一部分。\r\n        现在结果可以输出，但是会乱码显示类似这种：2,\r\n\r\n\r\n\r\n\r\n\r\n42 42\r\n2\r\n\r\n 92\r\n2e\r\n\r\n42 62,-2\r\n\r\n\"\r\n is92,1,22,,\r\n\r\n\r\n\r\n32,1I,H162 62 H2\r\n\r\n\r\n2 32 92\r\n2,h2 42, 42  2 42,h32 92,62y\r\n\r\n\r\n\r\n\r\n2,\r\n2, T\r\n\r\n2621: 11,5,\r\n\r\n42 2,62 622\r\n2-42\r\n\r\n82h61.，\r\n\r\n\r\n\r\n/2,,0 62 2h\r\n\r\n\r\n        \n\n### 期望行为 | Expected Behavior\n\n希望可以扩充上下文长度后还可以正确输出文本。\n\n### 复现方法 | Steps To Reproduce\n\n1.修改config文件\r\n2.使用两张a100运行原始代码\n\n### 运行环境 | Environment\n\n```Markdown\n- OS:\r\n- Python:\r\n- Transformers:\r\n- PyTorch:\r\n- CUDA (`python -c 'import torch; print(torch.version.cuda)'`):\n```\n\n\n### 备注 | Anything else?\n\n_No response_",
    "state": "closed",
    "created_at": "2024-08-13T03:16:11+00:00",
    "closed_at": "2024-09-19T08:07:30+00:00",
    "updated_at": "2024-09-19T08:07:30+00:00",
    "author": "kitty-nami",
    "author_type": "User",
    "comments_count": 1,
    "reactions_count": 0,
    "labels": "inactive",
    "milestone": null,
    "closed_by": "github-actions[bot]",
    "resolution_time_hours": 892.8552777777778,
    "first_comments": [
      {
        "author": "github-actions[bot]",
        "created_at": "2024-09-12T08:06:53+00:00",
        "body": "This issue has been automatically marked as inactive due to lack of recent activity. Should you believe it remains unresolved and warrants attention, kindly leave a comment on this thread.\n此问题由于长期未有新进展而被系统自动标记为不活跃。如果您认为它仍有待解决，请在此帖下方留言以补充信息。"
      }
    ],
    "url": "https://github.com/QwenLM/Qwen/issues/1311"
  },
  {
    "repository": "QwenLM/Qwen",
    "issue_number": 1310,
    "title": "Qwen-Chat-RLHF和Qwen-Chat的区别",
    "body": "`We then use SFT and RLHF to align QWEN to human preference and thus we have QWEN-CHAT and specifically its improved version QWEN-CHAT-RLHF.`\r\n\r\n技术报告里有提到`QWEN-CHAT-RLHF`，但在 huggingface 和 modelscope 上都没有看到 RLHF 相关的模型，我理解 QWEN-CHAT 模型应该包括了 RLHF 阶段的训练，那么技术报告中提到的 Qwen-Chat-RLHF 和 Qwen-Chat 的区别是什么？",
    "state": "closed",
    "created_at": "2024-08-08T09:14:29+00:00",
    "closed_at": "2024-09-15T08:06:29+00:00",
    "updated_at": "2024-09-15T08:06:29+00:00",
    "author": "Tramac",
    "author_type": "User",
    "comments_count": 1,
    "reactions_count": 0,
    "labels": "inactive",
    "milestone": null,
    "closed_by": "github-actions[bot]",
    "resolution_time_hours": 910.8666666666667,
    "first_comments": [
      {
        "author": "github-actions[bot]",
        "created_at": "2024-09-08T08:06:17+00:00",
        "body": "This issue has been automatically marked as inactive due to lack of recent activity. Should you believe it remains unresolved and warrants attention, kindly leave a comment on this thread.\n此问题由于长期未有新进展而被系统自动标记为不活跃。如果您认为它仍有待解决，请在此帖下方留言以补充信息。"
      }
    ],
    "url": "https://github.com/QwenLM/Qwen/issues/1310"
  },
  {
    "repository": "QwenLM/Qwen",
    "issue_number": 1309,
    "title": "官方推理脚本和模型文件中的pad_token不一致",
    "body": "请问官方的推理脚本里的pad_token是'<|extra_0|>'，模型文件中的pad_token是\"<|endoftext|>\"，跑推理时是否需要对齐？\r\n![image](https://github.com/user-attachments/assets/8eb71d56-e15d-47e2-9328-75db8ffa01c6)\r\n![image](https://github.com/user-attachments/assets/0891b80c-6be9-42d6-87cd-7ace3fe86959)",
    "state": "closed",
    "created_at": "2024-08-07T07:47:15+00:00",
    "closed_at": "2024-09-13T08:06:50+00:00",
    "updated_at": "2024-09-13T08:06:50+00:00",
    "author": "DemingCheng",
    "author_type": "User",
    "comments_count": 1,
    "reactions_count": 0,
    "labels": "inactive",
    "milestone": null,
    "closed_by": "github-actions[bot]",
    "resolution_time_hours": 888.3263888888889,
    "first_comments": [
      {
        "author": "github-actions[bot]",
        "created_at": "2024-09-06T08:06:39+00:00",
        "body": "This issue has been automatically marked as inactive due to lack of recent activity. Should you believe it remains unresolved and warrants attention, kindly leave a comment on this thread.\n此问题由于长期未有新进展而被系统自动标记为不活跃。如果您认为它仍有待解决，请在此帖下方留言以补充信息。"
      }
    ],
    "url": "https://github.com/QwenLM/Qwen/issues/1309"
  },
  {
    "repository": "QwenLM/Qwen",
    "issue_number": 1308,
    "title": "[BUG] <title> 请问QWenLMHeadModel中的QWenModel模块是处理文本信息吗？",
    "body": "### 是否已有关于该错误的issue或讨论？ | Is there an existing issue / discussion for this?\n\n- [X] 我已经搜索过已有的issues和讨论 | I have searched the existing issues / discussions\n\n### 该问题是否在FAQ中有解答？ | Is there an existing answer for this in FAQ?\n\n- [X] 我已经搜索过FAQ | I have searched FAQ\n\n### 当前行为 | Current Behavior\n\n_No response_\n\n### 期望行为 | Expected Behavior\n\n_No response_\n\n### 复现方法 | Steps To Reproduce\n\n_No response_\n\n### 运行环境 | Environment\n\n```Markdown\n- OS:\r\n- Python:\r\n- Transformers:\r\n- PyTorch:\r\n- CUDA (`python -c 'import torch; print(torch.version.cuda)'`):\n```\n\n\n### 备注 | Anything else?\n\n请问QWenLMHeadModel中的QWenModel模块是处理文本信息吗？",
    "state": "closed",
    "created_at": "2024-08-03T15:03:09+00:00",
    "closed_at": "2024-09-10T08:07:06+00:00",
    "updated_at": "2024-09-10T08:07:06+00:00",
    "author": "a1wj1",
    "author_type": "User",
    "comments_count": 1,
    "reactions_count": 0,
    "labels": "inactive",
    "milestone": null,
    "closed_by": "github-actions[bot]",
    "resolution_time_hours": 905.0658333333333,
    "first_comments": [
      {
        "author": "github-actions[bot]",
        "created_at": "2024-09-03T08:07:01+00:00",
        "body": "This issue has been automatically marked as inactive due to lack of recent activity. Should you believe it remains unresolved and warrants attention, kindly leave a comment on this thread.\n此问题由于长期未有新进展而被系统自动标记为不活跃。如果您认为它仍有待解决，请在此帖下方留言以补充信息。"
      }
    ],
    "url": "https://github.com/QwenLM/Qwen/issues/1308"
  },
  {
    "repository": "QwenLM/Qwen",
    "issue_number": 1307,
    "title": "[BUG] <title> model_max_length 32768 not work",
    "body": "### 是否已有关于该错误的issue或讨论？ | Is there an existing issue / discussion for this?\r\n\r\n- [X] 我已经搜索过已有的issues和讨论 | I have searched the existing issues / discussions\r\n\r\n### 该问题是否在FAQ中有解答？ | Is there an existing answer for this in FAQ?\r\n\r\n- [X] 我已经搜索过FAQ | I have searched FAQ\r\n\r\n### 当前行为 | Current Behavior\r\n\r\nTraceback (most recent call last):\r\n[rank0]:   File \"……site-packages/transformers/tokenization_utils_base.py\", line 762, in convert_to_tensors\r\n[rank0]:     tensor = as_tensor(value)\r\n[rank0]:   File \"……site-packages/transformers/tokenization_utils_base.py\", line 724, in as_tensor\r\n[rank0]:     return torch.tensor(value)\r\n[rank0]: ValueError: expected sequence of length 8034 at dim 1 (got 14276)\r\n\r\n### 期望行为 | Expected Behavior\r\n\r\n期望可以支持 32K 的长度，或者自动截断\r\n\r\n### 复现方法 | Steps To Reproduce\r\n\r\nfinetune.py --model_max_length 32768 --use_lora True\r\n\r\n### 运行环境 | Environment\r\n\r\n```Markdown\r\n- OS: Ubuntu 20.04\r\n- Python: 3.8.19\r\n- Transformers: 4.42.3\r\n- PyTorch: 2.3.0\r\n- CUDA 12.1\r\n```\r\n\r\n\r\n### 备注 | Anything else?\r\n\r\n_No response_",
    "state": "closed",
    "created_at": "2024-08-01T08:09:09+00:00",
    "closed_at": "2024-09-08T08:06:19+00:00",
    "updated_at": "2024-09-08T08:06:19+00:00",
    "author": "chansonzhang",
    "author_type": "User",
    "comments_count": 4,
    "reactions_count": 0,
    "labels": "inactive",
    "milestone": null,
    "closed_by": "github-actions[bot]",
    "resolution_time_hours": 911.9527777777778,
    "first_comments": [
      {
        "author": "chansonzhang",
        "created_at": "2024-08-01T08:20:49+00:00",
        "body": "according to [README](https://github.com/QwenLM/Qwen/blob/main/README.md), Qwen-7B does support 32K Max Length.\r\n![image](https://github.com/user-attachments/assets/6652569c-e872-4e67-9eed-a598682c64cb)\r\n"
      },
      {
        "author": "chansonzhang",
        "created_at": "2024-08-02T02:35:56+00:00",
        "body": "I noticed input_ids are padded to the max_length in batch, but labels are not. \r\nThe checkpoint is at `site-packages\\transformers\\tokenization_utils_base.py`, line 3450\r\n\r\n![image](https://github.com/user-attachments/assets/e9e74430-24c1-4492-8c1f-3d1f3d64cd99)\r\n![image](https://github.com/user-attachments/assets/848dd737-0608-4c10-a536-b82763d53246)\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"C:\\ProgramData\\mambaforge\\envs\\py38\\lib\\site-packages\\transformers\\tokenization_utils_base.py\", line 762, in convert_to_tensors\r\n    tensor = as_tensor(value)\r\n  File \"C:\\ProgramData\\mambaforge\\envs\\py38\\lib\\site-packages\\transformers\\tokenization_utils_base.py\", line 724, in as_tensor\r\n    return torch.tensor(value)\r\nValueError: expected sequence of length 6035 at dim 1 (got 6977)\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\ProgramData\\mambaforge\\envs\\py38\\lib\\site-packages\\accelerate\\data_loader.py\", line 454, in __iter__\r\n    current_batch = next(dataloader_iter)\r\n  File \"C:\\ProgramData\\mambaforge\\envs\\py38\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 631, in __next__\r\n    data = self._next_data()\r\n  File \"C:\\ProgramData\\mambaforge\\envs\\py38\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 675, in _next_data\r\n    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\r\n  File \"C:\\ProgramData\\mambaforge\\envs\\py38\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 54, in fetch\r\n    return self.collate_fn(data)\r\n  File \"C:\\ProgramData\\mambaforge\\envs\\py38\\lib\\site-packages\\transformers\\data\\data_collator.py\", line 271, in __call__\r\n    batch = pad_without_fast_tokenizer_warning(\r\n  File \"C:\\ProgramData\\mambaforge\\envs\\py38\\lib\\site-packages\\transformers\\data\\data_collator.py\", line 66, in pad_without_fast_tokenizer_warning\r\n    padded = tokenizer.pad(*pad_args, **pad_kwargs)\r\n  File \"C:\\ProgramData\\mambaforge\\envs\\py38\\lib\\site-packages\\transformers\\tokenization_utils_base.py\", line 3450, in pad\r\n    return BatchEncoding(batch_outputs, tensor_type=return_tensors)\r\n  File \"C:\\ProgramData\\mambaforge\\envs\\py38\\lib\\site-packages\\transformers\\tokenization_utils_base.py\", line 227, in __init__\r\n    self.convert_to_tensors(tensor_type=tensor_type, prepend_batch_axis=prepend_batch_axis)\r\n  File \"C:\\ProgramData\\mambaforge\\envs\\py38\\lib\\site-packages\\transformers\\tokenization_utils_base.py\", line 778, in convert_to_tensors\r\n    raise ValueError(\r\nValueError: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`labels` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\r\npython-BaseException\r\n```\r\n\r\nI "
      },
      {
        "author": "chansonzhang",
        "created_at": "2024-08-02T06:29:33+00:00",
        "body": "When I change the model from `Qwen1.5-7B-Chat` to `Qwen2-7B-Instruct`, the same error is still there."
      },
      {
        "author": "github-actions[bot]",
        "created_at": "2024-09-01T08:06:12+00:00",
        "body": "This issue has been automatically marked as inactive due to lack of recent activity. Should you believe it remains unresolved and warrants attention, kindly leave a comment on this thread.\n此问题由于长期未有新进展而被系统自动标记为不活跃。如果您认为它仍有待解决，请在此帖下方留言以补充信息。"
      }
    ],
    "url": "https://github.com/QwenLM/Qwen/issues/1307"
  },
  {
    "repository": "QwenLM/Qwen",
    "issue_number": 1300,
    "title": "大模型function call对比传统nlp方式有什么优势？",
    "body": "function call通过识别要调用的工具+抽取工具所需的参数来实现工具调用。想问下这样做对比传统的意图识别（调用什么工具）+信息抽取（参数抽取）有什么优势呢？都能实现相似的功能。",
    "state": "closed",
    "created_at": "2024-07-15T10:31:52+00:00",
    "closed_at": "2024-08-31T08:05:52+00:00",
    "updated_at": "2024-08-31T08:05:52+00:00",
    "author": "LawlightXY",
    "author_type": "User",
    "comments_count": 2,
    "reactions_count": 0,
    "labels": "inactive",
    "milestone": null,
    "closed_by": "github-actions[bot]",
    "resolution_time_hours": 1125.5666666666666,
    "first_comments": [
      {
        "author": "xueyouluo",
        "created_at": "2024-07-23T08:42:51+00:00",
        "body": "个人感觉：\r\n优点：\r\n\r\n1. 接入效率高，无需去训练模型判断意图和做槽位提取，可以方便的通过修改工具描述和系统说明来调整、加减工具；（当然，效果不好还是得微调）；\r\n2. 可能可以省掉workflow，模型自己判断需要调用哪些工具，或者只需要在系统描述中使用自然语言描述；\r\n3. 语义理解更好，比如经典case：帮我查附近的美食，不要日料，基于意图和槽位容易翻车。\r\n4. 开发简单一些，不用维护太多模块\r\n\r\n缺点：\r\n\r\n1. 黑盒，你也不知道它什么时候会调用工具，什么时候不调用，或者什么时候停止调用\r\n2. 慢，如果是多步骤，还得调多次\r\n3. 如果模型能力不行，听不懂指令，或者业务有些复杂逻辑，模型理解不了就很难受，只能靠微调，代价大"
      },
      {
        "author": "github-actions[bot]",
        "created_at": "2024-08-23T08:07:23+00:00",
        "body": "This issue has been automatically marked as inactive due to lack of recent activity. Should you believe it remains unresolved and warrants attention, kindly leave a comment on this thread.\n此问题由于长期未有新进展而被系统自动标记为不活跃。如果您认为它仍有待解决，请在此帖下方留言以补充信息。"
      }
    ],
    "url": "https://github.com/QwenLM/Qwen/issues/1300"
  },
  {
    "repository": "QwenLM/Qwen",
    "issue_number": 1299,
    "title": "[BUG] <title>Qwen2-7b-instruct使用SFT-FT，loss变为0，如何解决？",
    "body": "### 是否已有关于该错误的issue或讨论？ | Is there an existing issue / discussion for this?\r\n\r\n- [X] 我已经搜索过已有的issues和讨论 | I have searched the existing issues / discussions\r\n\r\n### 该问题是否在FAQ中有解答？ | Is there an existing answer for this in FAQ?\r\n\r\n- [X] 我已经搜索过FAQ | I have searched FAQ\r\n\r\n### 当前行为 | Current Behavior\r\n\r\n我的训练参数如下：\r\nexport CUDA_VISIBLE_DEVICES=\"1,6,7\" && DS_SKIP_CUDA_CHECK=1 OMP_NUM_THREADS=8 torchrun --standalone --nproc_per_node=gpu train_qwen.py\r\n--model_name_or_path $model_path\r\n--deepspeed $deepspeed_config\r\n--data_path $data_path\r\n--bf16 True\r\n--output_dir $output_path\r\n--num_train_epochs 1\r\n--max_steps 376\r\n--per_device_train_batch_size 1\r\n--gradient_accumulation_steps 16\r\n--evaluation_strategy \"no\"\r\n--save_strategy \"steps\"\r\n--save_steps 47\r\n--save_total_limit 1\r\n--learning_rate 1e-5\r\n--weight_decay 0.1\r\n--adam_beta2 0.95\r\n--warmup_ratio 0.01\r\n--lr_scheduler_type \"cosine\"\r\n--logging_steps 8\r\n--model_max_length 8192\r\n--gradient_checkpointing True\r\n--lazy_preprocess False\r\n--iterable True\r\n--tf32 0\r\n\r\n如下是我的运行日志，第二步loss就变为0了。不知道问题是什么\r\n{'loss': 1.7328, 'grad_norm': 1.9622441768332994e-05, 'learning_rate': 9.919354838709679e-06, 'epoch': 0.02}\r\n{'loss': 0.0, 'grad_norm': 3.913150697629098e-06, 'learning_rate': 9.704301075268819e-06, 'epoch': 0.04}\r\n4%|████▉ | 16/376 [17:36<6:36:00, 66.00s/it]\r\n\r\n\r\n\r\n### 期望行为 | Expected Behavior\r\n\r\n能够正常训练\r\n\r\n### 复现方法 | Steps To Reproduce\r\n\r\n使用的fscaht微调代码，chaglm可正常训练，Qwen系列模型无法正常训练，特殊token使用的是qwen系列的<|im_start|>、<|im_end|>\r\n\r\n### 运行环境 | Environment\r\n\r\n```Markdown\r\n- OS: ubuntu20.04\r\n- Python:3.10\r\n- Transformers: 4.34\r\n- PyTorch: 2.3\r\n- CUDA (`python -c 'import torch; print(torch.version.cuda)'`):  12.2\r\n```\r\n\r\n\r\n### 备注 | Anything else?\r\n\r\n使用Qwen2微调也一样的现象。",
    "state": "closed",
    "created_at": "2024-07-15T05:27:51+00:00",
    "closed_at": "2024-09-01T08:06:13+00:00",
    "updated_at": "2024-09-01T08:06:13+00:00",
    "author": "ming-shy",
    "author_type": "User",
    "comments_count": 2,
    "reactions_count": 0,
    "labels": "inactive",
    "milestone": null,
    "closed_by": "github-actions[bot]",
    "resolution_time_hours": 1154.6394444444445,
    "first_comments": [
      {
        "author": "jesuswa",
        "created_at": "2024-07-26T06:29:16+00:00",
        "body": "你这个问题是怎么解决的\r\n"
      },
      {
        "author": "github-actions[bot]",
        "created_at": "2024-08-25T08:06:12+00:00",
        "body": "This issue has been automatically marked as inactive due to lack of recent activity. Should you believe it remains unresolved and warrants attention, kindly leave a comment on this thread.\n此问题由于长期未有新进展而被系统自动标记为不活跃。如果您认为它仍有待解决，请在此帖下方留言以补充信息。"
      }
    ],
    "url": "https://github.com/QwenLM/Qwen/issues/1299"
  },
  {
    "repository": "QwenLM/Qwen",
    "issue_number": 1294,
    "title": "Qwen 的开源模型能输出 logprobs吗？",
    "body": "### 是否已有关于该错误的issue或讨论？ | Is there an existing issue / discussion for this?\n\n- [X] 我已经搜索过已有的issues和讨论 | I have searched the existing issues / discussions\n\n### 该问题是否在FAQ中有解答？ | Is there an existing answer for this in FAQ?\n\n- [X] 我已经搜索过FAQ | I have searched FAQ\n\n### 当前行为 | Current Behavior\n\n_No response_\n\n### 期望行为 | Expected Behavior\n\n_No response_\n\n### 复现方法 | Steps To Reproduce\n\n_No response_\n\n### 运行环境 | Environment\n\n```Markdown\n- OS:\r\n- Python:\r\n- Transformers:\r\n- PyTorch:\r\n- CUDA (`python -c 'import torch; print(torch.version.cuda)'`):\n```\n\n\n### 备注 | Anything else?\n\n_No response_",
    "state": "closed",
    "created_at": "2024-06-27T07:33:43+00:00",
    "closed_at": "2024-08-04T08:06:00+00:00",
    "updated_at": "2024-08-16T14:04:28+00:00",
    "author": "Miracle1207",
    "author_type": "User",
    "comments_count": 3,
    "reactions_count": 0,
    "labels": "inactive",
    "milestone": null,
    "closed_by": "github-actions[bot]",
    "resolution_time_hours": 912.5380555555555,
    "first_comments": [
      {
        "author": "github-actions[bot]",
        "created_at": "2024-07-27T08:05:33+00:00",
        "body": "This issue has been automatically marked as inactive due to lack of recent activity. Should you believe it remains unresolved and warrants attention, kindly leave a comment on this thread.\n此问题由于长期未有新进展而被系统自动标记为不活跃。如果您认为它仍有待解决，请在此帖下方留言以补充信息。"
      },
      {
        "author": "Artessay",
        "created_at": "2024-08-07T16:20:40+00:00",
        "body": "调用dashscope的qwen-turbo，qwen-plus等模型时，可以正常获取logprob，但是调用qwen2-7b-instruct等模型时却无法获取。是不是开源的模型没有提供相关输出呢？"
      },
      {
        "author": "bigbrother001",
        "created_at": "2024-08-16T14:04:27+00:00",
        "body": "我也想知道怎么获得logits\r\n"
      }
    ],
    "url": "https://github.com/QwenLM/Qwen/issues/1294"
  }
]