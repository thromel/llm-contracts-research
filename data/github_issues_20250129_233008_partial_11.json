[
  {
    "repository": "QwenLM/Qwen",
    "issue_number": 1346,
    "title": "é—®é¢˜ï¼šç”¨Ollamaè¿è¡Œä½ è‡ªå·±çš„GGUFæ–‡ä»¶é‡Œé¢ï¼Œå…³äºqwen2.5-0.5b-instuctorçš„Modelfileåº”è¯¥æ˜¯ä»€ä¹ˆ",
    "body": "\næ•™ç¨‹ï¼šhttps://qwen.readthedocs.io/zh-cn/latest/run_locally/ollama.html#\né—®é¢˜ï¼šç”¨Ollamaè¿è¡Œä½ è‡ªå·±çš„GGUFæ–‡ä»¶é‡Œé¢ï¼Œå…³äºqwen2.5-0.5b-instuctorçš„Modelfileåº”è¯¥æ˜¯ä»€ä¹ˆ\n\næˆ‘ä½¿ç”¨å®˜ç½‘çš„qwen2.5-7b-instructçš„Modelfileæ”¾å…¥åˆ°æˆ‘è®­ç»ƒå®Œæˆçš„qwen2.5-0.5b-instuctoré‡Œé¢æ˜¯é”™è¯¯çš„ï¼Œéº»çƒ¦å‘ŠçŸ¥ä¸€ä¸‹qwen2.5-0.5b-instuctor\næ˜¯ä»€ä¹ˆï¼Œæ„Ÿè°¢\n",
    "state": "closed",
    "created_at": "2025-01-24T07:43:03+00:00",
    "closed_at": "2025-01-24T08:07:42+00:00",
    "updated_at": "2025-01-24T08:07:42+00:00",
    "author": "Wheeeeeeeeels",
    "author_type": "User",
    "comments_count": 1,
    "reactions_count": 0,
    "labels": "",
    "milestone": null,
    "closed_by": "Wheeeeeeeeels",
    "resolution_time_hours": 0.41083333333333333,
    "first_comments": [
      {
        "author": "Wheeeeeeeeels",
        "created_at": "2025-01-24T08:07:40+00:00",
        "body": "æ˜ç™½äº†ï¼Œæ˜¯è¿™ä¸ªï¼Œæ‰¾åˆ°äº†\n\n\nFROM /mnt/workspace/qwen2-0.5b-instruct-q8_0.gguf\n\n# set the temperature to 0.7 [higher is more creative, lower is more coherent]\nPARAMETER temperature 0.7\nPARAMETER top_p 0.8\nPARAMETER repeat_penalty 1.05\nTEMPLATE \"\"\"{{ if .System }}<|im_start|>system\n{{ .System }}<|im_end|>\n{{ end }}{{ if .Prompt }}<|im_start|>user\n{{ .Prompt }}<|im_end|>\n{{ end }}<|im_start|>assistant\n{{ .Response }}<|im_end|>\"\"\"\n# set the system message\nSYSTEM \"\"\"\nYou are a helpful assistant.\n\"\"\""
      }
    ],
    "url": "https://github.com/QwenLM/Qwen/issues/1346"
  },
  {
    "repository": "QwenLM/Qwen",
    "issue_number": 1336,
    "title": "æœ¬åœ°è¿›è¡Œqwen2-vl-2b-instructæ¨ç†ï¼Œå¢åŠ max_pixelsï¼Œä½†æ˜¯å ç”¨å†…å­˜ä¾ç„¶ä¸å˜",
    "body": "hellohelloï¼Œæˆ‘æƒ³é—®ä¸ªé—®é¢˜ï¼Œæˆ‘åœ¨æœ¬åœ°è¿›è¡Œqwen2-vl-2b-instructçš„å¤šå›¾æ¨ç†ï¼Œæˆ‘æŠŠmax_picelsè®¾ç½®æˆ3000x28x28ä¹‹åï¼Œå ç”¨å†…å­˜11gï¼ˆæˆ‘ç”¨çš„æ˜¯3090ï¼‰ã€‚ä½†æ˜¯å›ç­”çš„å¾ˆä¸å¥½ï¼Œä¹‹åæˆ‘æƒ³ç€æ˜¾å­˜è¿˜å¤Ÿï¼Œå¯ä»¥ç»§ç»­å¢å¤§ã€‚äºæ˜¯æˆ‘æŠŠ3000æ”¹æˆ5000ï¼Œä½†æ˜¯å ç”¨å†…å­˜ä¾ç„¶ä¸å˜ï¼Œè€Œä¸”å›ç­”ç»“æœä¹Ÿä¸å˜ï¼Œè¿™æ˜¯å•¥åŸå› å‘€ï¼Ÿå¥½å¥‡æ€ªï¼Œå› ä¸ºæˆ‘å›¾åƒçš„åˆ†è¾¨ç‡å¾ˆé«˜å¯èƒ½æœ‰10000x28x28é‚£æ ·ï¼ŒæŒ‰ç†è¯´åº”è¯¥ä¼šæ˜¾å­˜ä¼šç»§ç»­å¢å¤§ï¼Œå¹¶ä¸”å›ç­”çš„ä¼šæ›´å¥½å‘€ï¼Ÿ\r\n<img width=\"727\" alt=\"8f056643390ef9a01caf94a6e20144a\" src=\"https://github.com/user-attachments/assets/82f948e3-008c-4378-9c9a-9734929533b4\" />\r\næˆ‘ç”¨çš„æ˜¯jupyterlabã€‚æˆ‘ä¸å¤ªæ‡‚è¿™ä¸ªé—®é¢˜çš„å¯èƒ½åŸå› ï¼Œæœ‰æ²¡æœ‰äººå¯ä»¥å¸®å¸®æˆ‘ï¼Œéå¸¸æ„Ÿè°¢ã€‚",
    "state": "closed",
    "created_at": "2024-12-13T06:53:18+00:00",
    "closed_at": "2025-01-20T08:07:49+00:00",
    "updated_at": "2025-01-20T08:07:49+00:00",
    "author": "hm123450",
    "author_type": "User",
    "comments_count": 1,
    "reactions_count": 0,
    "labels": "inactive",
    "milestone": null,
    "closed_by": "github-actions[bot]",
    "resolution_time_hours": 913.2419444444445,
    "first_comments": [
      {
        "author": "github-actions[bot]",
        "created_at": "2025-01-12T08:06:23+00:00",
        "body": "This issue has been automatically marked as inactive due to lack of recent activity. Should you believe it remains unresolved and warrants attention, kindly leave a comment on this thread.\næ­¤é—®é¢˜ç”±äºé•¿æœŸæœªæœ‰æ–°è¿›å±•è€Œè¢«ç³»ç»Ÿè‡ªåŠ¨æ ‡è®°ä¸ºä¸æ´»è·ƒã€‚å¦‚æœæ‚¨è®¤ä¸ºå®ƒä»æœ‰å¾…è§£å†³ï¼Œè¯·åœ¨æ­¤å¸–ä¸‹æ–¹ç•™è¨€ä»¥è¡¥å……ä¿¡æ¯ã€‚"
      }
    ],
    "url": "https://github.com/QwenLM/Qwen/issues/1336"
  },
  {
    "repository": "QwenLM/Qwen",
    "issue_number": 1335,
    "title": "[BUG] <title>å¾®ä¿¡ç¾¤äººæ•°å·²æ»¡",
    "body": "### æ˜¯å¦å·²æœ‰å…³äºè¯¥é”™è¯¯çš„issueæˆ–è®¨è®ºï¼Ÿ | Is there an existing issue / discussion for this?\n\n- [X] æˆ‘å·²ç»æœç´¢è¿‡å·²æœ‰çš„issueså’Œè®¨è®º | I have searched the existing issues / discussions\n\n### è¯¥é—®é¢˜æ˜¯å¦åœ¨FAQä¸­æœ‰è§£ç­”ï¼Ÿ | Is there an existing answer for this in FAQ?\n\n- [X] æˆ‘å·²ç»æœç´¢è¿‡FAQ | I have searched FAQ\n\n### å½“å‰è¡Œä¸º | Current Behavior\n\nå¾®ä¿¡ç¾¤äººæ•°å·²æ»¡\n\n### æœŸæœ›è¡Œä¸º | Expected Behavior\n\nå¾®ä¿¡ç¾¤äººæ•°å·²æ»¡\n\n### å¤ç°æ–¹æ³• | Steps To Reproduce\n\nå¾®ä¿¡ç¾¤äººæ•°å·²æ»¡\n\n### è¿è¡Œç¯å¢ƒ | Environment\n\n```Markdown\n- OS:\r\n- Python:\r\n- Transformers:\r\n- PyTorch:\r\n- CUDA (`python -c 'import torch; print(torch.version.cuda)'`):\n```\n\n\n### å¤‡æ³¨ | Anything else?\n\nå¾®ä¿¡ç¾¤äººæ•°å·²æ»¡",
    "state": "closed",
    "created_at": "2024-12-09T10:00:31+00:00",
    "closed_at": "2025-01-17T08:06:59+00:00",
    "updated_at": "2025-01-17T08:06:59+00:00",
    "author": "balcklive",
    "author_type": "User",
    "comments_count": 1,
    "reactions_count": 0,
    "labels": "inactive",
    "milestone": null,
    "closed_by": "github-actions[bot]",
    "resolution_time_hours": 934.1077777777778,
    "first_comments": [
      {
        "author": "github-actions[bot]",
        "created_at": "2025-01-09T08:07:13+00:00",
        "body": "This issue has been automatically marked as inactive due to lack of recent activity. Should you believe it remains unresolved and warrants attention, kindly leave a comment on this thread.\næ­¤é—®é¢˜ç”±äºé•¿æœŸæœªæœ‰æ–°è¿›å±•è€Œè¢«ç³»ç»Ÿè‡ªåŠ¨æ ‡è®°ä¸ºä¸æ´»è·ƒã€‚å¦‚æœæ‚¨è®¤ä¸ºå®ƒä»æœ‰å¾…è§£å†³ï¼Œè¯·åœ¨æ­¤å¸–ä¸‹æ–¹ç•™è¨€ä»¥è¡¥å……ä¿¡æ¯ã€‚"
      }
    ],
    "url": "https://github.com/QwenLM/Qwen/issues/1335"
  },
  {
    "repository": "QwenLM/Qwen",
    "issue_number": 1334,
    "title": "A100/A800 æ˜¾å¡å»è¿è¡Œä¸åŒå‚æ•°çš„æ¨¡å‹ï¼Œæƒ³ç¡®è®¤ä¸€ä¸‹ç®—åŠ›çš„éœ€æ±‚",
    "body": "### èµ·å§‹æ—¥æœŸ | Start Date\n\n2024/12/05\n\n### å®ç°PR | Implementation PR\n\næ— \n\n### ç›¸å…³Issues | Reference Issues\n\næˆ‘ç°åœ¨æœ‰A100çš„æ˜¾å¡ï¼Œæƒ³è¯•è¯•8B 32B 72B æ¨¡å‹çš„è¿è¡Œæ•ˆç‡å’Œæ¯ä¸€åˆ†é’Ÿå¯ä»¥ç”Ÿæˆå¤šå°‘toeknï¼Œæƒ³ç¡®è®¤ä¸€ä¸‹è¿™ä¸ªæ¨¡å‹çš„ä½¿ç”¨ä¸Šé™åœ¨å“ªé‡Œ\n\n### æ‘˜è¦ | Summary\n\næ¨¡å‹çš„æ¨æ¼”èƒ½åŠ›\n\n### åŸºæœ¬ç¤ºä¾‹ | Basic Example\n\næ— \n\n### ç¼ºé™· | Drawbacks\n\næ— \n\n### æœªè§£å†³é—®é¢˜ | Unresolved questions\n\næ— ",
    "state": "closed",
    "created_at": "2024-12-05T08:14:54+00:00",
    "closed_at": "2025-01-08T07:46:04+00:00",
    "updated_at": "2025-01-08T07:46:04+00:00",
    "author": "guoshiyin-666",
    "author_type": "User",
    "comments_count": 1,
    "reactions_count": 0,
    "labels": "question,inactive",
    "milestone": null,
    "closed_by": "guoshiyin-666",
    "resolution_time_hours": 815.5194444444444,
    "first_comments": [
      {
        "author": "github-actions[bot]",
        "created_at": "2025-01-05T08:06:23+00:00",
        "body": "This issue has been automatically marked as inactive due to lack of recent activity. Should you believe it remains unresolved and warrants attention, kindly leave a comment on this thread.\næ­¤é—®é¢˜ç”±äºé•¿æœŸæœªæœ‰æ–°è¿›å±•è€Œè¢«ç³»ç»Ÿè‡ªåŠ¨æ ‡è®°ä¸ºä¸æ´»è·ƒã€‚å¦‚æœæ‚¨è®¤ä¸ºå®ƒä»æœ‰å¾…è§£å†³ï¼Œè¯·åœ¨æ­¤å¸–ä¸‹æ–¹ç•™è¨€ä»¥è¡¥å……ä¿¡æ¯ã€‚"
      }
    ],
    "url": "https://github.com/QwenLM/Qwen/issues/1334"
  },
  {
    "repository": "QwenLM/Qwen",
    "issue_number": 1333,
    "title": "ğŸ’¡ [REQUEST] - <title>onnxå¯¼å‡ºæ•™ç¨‹",
    "body": "### èµ·å§‹æ—¥æœŸ | Start Date\n\n_No response_\n\n### å®ç°PR | Implementation PR\n\n_No response_\n\n### ç›¸å…³Issues | Reference Issues\n\næš‚æ— \n\n### æ‘˜è¦ | Summary\n\næ±‚ä¸€ä¸ªå®˜æ–¹çš„onnxçš„export æ•™ç¨‹\n\n### åŸºæœ¬ç¤ºä¾‹ | Basic Example\n\néœ€è¦èƒ½å¤Ÿå°†æ¨¡å‹ä»pthæˆ–safetensorå¯¼å‡ºonnxçš„export æ•™ç¨‹\n\n### ç¼ºé™· | Drawbacks\n\næš‚æ— \n\n### æœªè§£å†³é—®é¢˜ | Unresolved questions\n\n_No response_",
    "state": "closed",
    "created_at": "2024-12-05T02:41:08+00:00",
    "closed_at": "2025-01-12T08:06:25+00:00",
    "updated_at": "2025-01-12T08:06:25+00:00",
    "author": "1826133674",
    "author_type": "User",
    "comments_count": 1,
    "reactions_count": 0,
    "labels": "question,inactive",
    "milestone": null,
    "closed_by": "github-actions[bot]",
    "resolution_time_hours": 917.4213888888889,
    "first_comments": [
      {
        "author": "github-actions[bot]",
        "created_at": "2025-01-04T08:06:25+00:00",
        "body": "This issue has been automatically marked as inactive due to lack of recent activity. Should you believe it remains unresolved and warrants attention, kindly leave a comment on this thread.\næ­¤é—®é¢˜ç”±äºé•¿æœŸæœªæœ‰æ–°è¿›å±•è€Œè¢«ç³»ç»Ÿè‡ªåŠ¨æ ‡è®°ä¸ºä¸æ´»è·ƒã€‚å¦‚æœæ‚¨è®¤ä¸ºå®ƒä»æœ‰å¾…è§£å†³ï¼Œè¯·åœ¨æ­¤å¸–ä¸‹æ–¹ç•™è¨€ä»¥è¡¥å……ä¿¡æ¯ã€‚"
      }
    ],
    "url": "https://github.com/QwenLM/Qwen/issues/1333"
  },
  {
    "repository": "QwenLM/Qwen",
    "issue_number": 1332,
    "title": "è¯·æ•™å…³äºç®—æ³•åŸç†",
    "body": "è¯·é—®Qwenå¤§æ¨¡å‹ä¸­æ•°æ®shapeæ¬¡åºæ˜¯ä»€ä¹ˆæ ·çš„ã€‚\r\n[s, b, h]Â å½¢çŠ¶è¿˜æ˜¯[b, s, h]Â å½¢çŠ¶\r\nä¸ºä»€ä¹ˆè¦è¿™æ ·è®¾è®¡\r\nhidden_states: è¾“å…¥åˆ°è¿™ä¸€å±‚çš„éšè—çŠ¶æ€å¼ é‡ï¼Œå½¢çŠ¶ä¸ºÂ [s, b, h]ï¼Œå…¶ä¸­Â sÂ æ˜¯åºåˆ—é•¿åº¦ï¼ŒbÂ æ˜¯æ‰¹é‡å¤§å°ï¼ŒhÂ æ˜¯éšè—å±‚ç»´åº¦ã€‚\r\nè°¢è°¢",
    "state": "closed",
    "created_at": "2024-12-04T08:18:15+00:00",
    "closed_at": "2025-01-12T08:06:25+00:00",
    "updated_at": "2025-01-12T08:06:26+00:00",
    "author": "elesun2018",
    "author_type": "User",
    "comments_count": 1,
    "reactions_count": 0,
    "labels": "inactive",
    "milestone": null,
    "closed_by": "github-actions[bot]",
    "resolution_time_hours": 935.8027777777778,
    "first_comments": [
      {
        "author": "github-actions[bot]",
        "created_at": "2025-01-04T08:06:27+00:00",
        "body": "This issue has been automatically marked as inactive due to lack of recent activity. Should you believe it remains unresolved and warrants attention, kindly leave a comment on this thread.\næ­¤é—®é¢˜ç”±äºé•¿æœŸæœªæœ‰æ–°è¿›å±•è€Œè¢«ç³»ç»Ÿè‡ªåŠ¨æ ‡è®°ä¸ºä¸æ´»è·ƒã€‚å¦‚æœæ‚¨è®¤ä¸ºå®ƒä»æœ‰å¾…è§£å†³ï¼Œè¯·åœ¨æ­¤å¸–ä¸‹æ–¹ç•™è¨€ä»¥è¡¥å……ä¿¡æ¯ã€‚"
      }
    ],
    "url": "https://github.com/QwenLM/Qwen/issues/1332"
  },
  {
    "repository": "QwenLM/Qwen",
    "issue_number": 1331,
    "title": "æ‰¾ä¸åˆ°è®¾ç«‹äº†è„šæœ¬",
    "body": "### èµ·å§‹æ—¥æœŸ | Start Date\n\n_No response_\n\n### å®ç°PR | Implementation PR\n\n_No response_\n\n### ç›¸å…³Issues | Reference Issues\n\n_No response_\n\n### æ‘˜è¦ | Summary\n\nèƒ½ç»™ä¸€ä¸ªshellè„šæœ¬å—\n\n### åŸºæœ¬ç¤ºä¾‹ | Basic Example\n\n1\n\n### ç¼ºé™· | Drawbacks\n\n1\n\n### æœªè§£å†³é—®é¢˜ | Unresolved questions\n\n_No response_",
    "state": "closed",
    "created_at": "2024-11-29T14:19:01+00:00",
    "closed_at": "2025-01-06T08:07:31+00:00",
    "updated_at": "2025-01-06T08:07:31+00:00",
    "author": "belo-belove",
    "author_type": "User",
    "comments_count": 1,
    "reactions_count": 0,
    "labels": "question,inactive",
    "milestone": null,
    "closed_by": "github-actions[bot]",
    "resolution_time_hours": 905.8083333333333,
    "first_comments": [
      {
        "author": "github-actions[bot]",
        "created_at": "2024-12-30T08:07:24+00:00",
        "body": "This issue has been automatically marked as inactive due to lack of recent activity. Should you believe it remains unresolved and warrants attention, kindly leave a comment on this thread.\næ­¤é—®é¢˜ç”±äºé•¿æœŸæœªæœ‰æ–°è¿›å±•è€Œè¢«ç³»ç»Ÿè‡ªåŠ¨æ ‡è®°ä¸ºä¸æ´»è·ƒã€‚å¦‚æœæ‚¨è®¤ä¸ºå®ƒä»æœ‰å¾…è§£å†³ï¼Œè¯·åœ¨æ­¤å¸–ä¸‹æ–¹ç•™è¨€ä»¥è¡¥å……ä¿¡æ¯ã€‚"
      }
    ],
    "url": "https://github.com/QwenLM/Qwen/issues/1331"
  },
  {
    "repository": "QwenLM/Qwen",
    "issue_number": 1330,
    "title": "[BUG] <title>An AssertionError occurs when docker run(dockerå®¹å™¨è¿è¡Œå¦‚ä¸‹é”™è¯¯)ï¼š",
    "body": "### æ˜¯å¦å·²æœ‰å…³äºè¯¥é”™è¯¯çš„issueæˆ–è®¨è®ºï¼Ÿ | Is there an existing issue / discussion for this?\n\n- [X] æˆ‘å·²ç»æœç´¢è¿‡å·²æœ‰çš„issueså’Œè®¨è®º | I have searched the existing issues / discussions\n\n### è¯¥é—®é¢˜æ˜¯å¦åœ¨FAQä¸­æœ‰è§£ç­”ï¼Ÿ | Is there an existing answer for this in FAQ?\n\n- [X] æˆ‘å·²ç»æœç´¢è¿‡FAQ | I have searched FAQ\n\n### å½“å‰è¡Œä¸º | Current Behavior\n\n==========\r\n== CUDA ==\r\n==========\r\n\r\nCUDA Version 11.7.1\r\n\r\nContainer image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved.\r\n\r\nThis container image and its contents are governed by the NVIDIA Deep Learning Container License.\r\nBy pulling and using the container, you accept the terms and conditions of this license:\r\nhttps://developer.nvidia.com/ngc/nvidia-deep-learning-container-license\r\n\r\nA copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience.\r\n\r\nThe model is automatically converting to bf16 for faster inference. If you want to disable the automatic precision, please manually add bf16/fp16/fp32=True to \"AutoModelForCausalLM.from_pretrained\".\r\nTry importing flash-attention for faster inference...\r\nLoading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:10<00:00,  1.35s/it]\r\nRunning on local URL:  http://0.0.0.0:80\r\n\r\nTo create a public link, set `share=True` in `launch()`.\r\nUser: æŸ”æŸ”å¼±å¼±\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 21, in rotary_kernel\r\nKeyError: ('2-.-0-.-0-d82511111ad128294e9d31a6ac684238-d6252949da17ceb5f3a278a70250af13-3b85c7bef5f0a641282f3b73af50f599-14de7de5c4da5794c8ca14e7e41a122d-3498c340fd4b6ee7805fd54b882a04f5-e1f133f98d04093da2078dfc51c36b72-b26258bf01f839199e39d64851821f26-d7c06e3b46e708006c15224aac7a1378-f585402118c8a136948ce0a49cfe122c', (torch.float32, torch.float32, torch.float32, torch.float32, None, 'i32', 'i32', 'i32', 'i32', 'i32', 'i32', 'i32', 'i32', 'i32', 'i32', 'i32', 'i32', 'i32', 'i32'), (128, False, False, False, False, 4), (True, True, True, True, (False,), (True, False), (False, False), (True, False), (True, False), (False, False), (True, False), (True, False), (True, False), (True, False), (False, True), (True, False), (True, False), (True, False), (False, True)))\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.8/dist-packages/gradio/queueing.py\", line 536, in process_events\r\n    response = await route_utils.call_process_api(\r\n  File \"/usr/local/lib/python3.8/dist-packages/gradio/route_utils.py\", line 322, in call_process_api\r\n    output = await app.get_blocks().process_api(\r\n  File \"/usr/local/lib/python3.8/dist-packages/gradio/blocks.py\", line 1935, in process_api\r\n    result = await self.call_function(\r\n  File \"/usr/local/lib/python3.8/dist-packages/gradio/blocks.py\", line 1532, in call_function\r\n    prediction = await utils.async_iteration(iterator)\r\n  File \"/usr/local/lib/python3.8/dist-packages/gradio/utils.py\", line 671, in async_iteration\r\n    return await iterator.__anext__()\r\n  File \"/usr/local/lib/python3.8/dist-packages/gradio/utils.py\", line 664, in __anext__\r\n    return await anyio.to_thread.run_sync(\r\n  File \"/usr/local/lib/python3.8/dist-packages/anyio/to_thread.py\", line 33, in run_sync\r\n    return await get_asynclib().run_sync_in_worker_thread(\r\n  File \"/usr/local/lib/python3.8/dist-packages/anyio/_backends/_asyncio.py\", line 877, in run_sync_in_worker_thread\r\n    return await future\r\n  File \"/usr/local/lib/python3.8/dist-packages/anyio/_backends/_asyncio.py\", line 807, in run\r\n    result = context.run(func, *args)\r\n  File \"/usr/local/lib/python3.8/dist-packages/gradio/utils.py\", line 647, in run_sync_iterator_async\r\n    return next(iterator)\r\n  File \"/usr/local/lib/python3.8/dist-packages/gradio/utils.py\", line 809, in gen_wrapper\r\n    response = next(iterator)\r\n  File \"web_demo.py\", line 126, in predict\r\n    for response in model.chat_stream(tokenizer, _query, history=_task_history, generation_config=config):\r\n  File \"/root/.cache/huggingface/modules/transformers_modules/modeling_qwen.py\", line 1216, in stream_generator\r\n    for token in self.generate_stream(\r\n  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/_contextlib.py\", line 35, in generator_context\r\n    response = gen.send(None)\r\n  File \"/usr/local/lib/python3.8/dist-packages/transformers_stream_generator/main.py\", line 931, in sample_stream\r\n    outputs = self(\r\n  File \"/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.8/dist-packages/accelerate/hooks.py\", line 164, in new_forward\r\n    output = module._old_forward(*args, **kwargs)\r\n  File \"/root/.cache/huggingface/modules/transformers_modules/modeling_qwen.py\", line 1045, in forward\r\n    transformer_outputs = self.transformer(\r\n  File \"/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/root/.cache/huggingface/modules/transformers_modules/modeling_qwen.py\", line 893, in forward\r\n    outputs = block(\r\n  File \"/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.8/dist-packages/accelerate/hooks.py\", line 164, in new_forward\r\n    output = module._old_forward(*args, **kwargs)\r\n  File \"/root/.cache/huggingface/modules/transformers_modules/modeling_qwen.py\", line 612, in forward\r\n    attn_outputs = self.attn(\r\n  File \"/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.8/dist-packages/accelerate/hooks.py\", line 164, in new_forward\r\n    output = module._old_forward(*args, **kwargs)\r\n  File \"/root/.cache/huggingface/modules/transformers_modules/modeling_qwen.py\", line 432, in forward\r\n    query = apply_rotary_pos_emb(query, q_pos_emb)\r\n  File \"/root/.cache/huggingface/modules/transformers_modules/modeling_qwen.py\", line 1344, in apply_rotary_pos_emb\r\n    return apply_rotary_emb_func(t_float, cos, sin).type_as(t)\r\n  File \"/usr/local/lib/python3.8/dist-packages/flash_attn/layers/rotary.py\", line 122, in apply_rotary_emb\r\n    return ApplyRotaryEmb.apply(\r\n  File \"/usr/local/lib/python3.8/dist-packages/torch/autograd/function.py\", line 506, in apply\r\n    return super().apply(*args, **kwargs)  # type: ignore[misc]\r\n  File \"/usr/local/lib/python3.8/dist-packages/flash_attn/layers/rotary.py\", line 48, in forward\r\n    out = apply_rotary(\r\n  File \"/usr/local/lib/python3.8/dist-packages/flash_attn/ops/triton/rotary.py\", line 213, in apply_rotary\r\n    rotary_kernel[grid](\r\n  File \"<string>\", line 41, in rotary_kernel\r\n  File \"/usr/local/lib/python3.8/dist-packages/triton/compiler.py\", line 1629, in compile\r\n    metadata[\"name\"] = ptx_get_kernel_name(next_module)\r\n  File \"/usr/local/lib/python3.8/dist-packages/triton/compiler.py\", line 1040, in ptx_get_kernel_name\r\n    assert ptx\r\nAssertionError\r\n\r\n![Uploading å¾®ä¿¡å›¾ç‰‡_20241118164724.pngâ€¦]()\r\n\r\n\n\n### æœŸæœ›è¡Œä¸º | Expected Behavior\n\n_No response_\n\n### å¤ç°æ–¹æ³• | Steps To Reproduce\n\n_No response_\n\n### è¿è¡Œç¯å¢ƒ | Environment\n\n```Markdown\n- OS:\r\n- Python:\r\n- Transformers:\r\n- PyTorch:\r\n- CUDA (`python -c 'import torch; print(torch.version.cuda)'`):\n```\n\n\n### å¤‡æ³¨ | Anything else?\n\n_No response_",
    "state": "closed",
    "created_at": "2024-11-18T08:50:04+00:00",
    "closed_at": "2024-12-27T08:07:04+00:00",
    "updated_at": "2024-12-27T08:07:04+00:00",
    "author": "jydsun",
    "author_type": "User",
    "comments_count": 2,
    "reactions_count": 0,
    "labels": "inactive",
    "milestone": null,
    "closed_by": "github-actions[bot]",
    "resolution_time_hours": 935.2833333333333,
    "first_comments": [
      {
        "author": "jydsun",
        "created_at": "2024-11-18T08:55:23+00:00",
        "body": "![å¾®ä¿¡å›¾ç‰‡_20241118164724](https://github.com/user-attachments/assets/cb7257c1-f6a5-4b51-b8e7-6061b8bbc25f)\r\n![å¾®ä¿¡å›¾ç‰‡_20241118165342](https://github.com/user-attachments/assets/e0d4782c-54e6-47a4-b2a3-cb1af7aa706a)\r\n\r\nè¡¥å……ä¿¡æ¯ï¼Œä¸€å¤©éƒ½æ²¡æ‰¾åˆ°åŸå› ï¼Œè¯·é«˜æ‰‹æŒ‡å¯¼ï¼Œå¦‚ä½•è§£å†³"
      },
      {
        "author": "github-actions[bot]",
        "created_at": "2024-12-19T08:07:36+00:00",
        "body": "This issue has been automatically marked as inactive due to lack of recent activity. Should you believe it remains unresolved and warrants attention, kindly leave a comment on this thread.\næ­¤é—®é¢˜ç”±äºé•¿æœŸæœªæœ‰æ–°è¿›å±•è€Œè¢«ç³»ç»Ÿè‡ªåŠ¨æ ‡è®°ä¸ºä¸æ´»è·ƒã€‚å¦‚æœæ‚¨è®¤ä¸ºå®ƒä»æœ‰å¾…è§£å†³ï¼Œè¯·åœ¨æ­¤å¸–ä¸‹æ–¹ç•™è¨€ä»¥è¡¥å……ä¿¡æ¯ã€‚"
      }
    ],
    "url": "https://github.com/QwenLM/Qwen/issues/1330"
  },
  {
    "repository": "QwenLM/Qwen",
    "issue_number": 1329,
    "title": "1",
    "body": null,
    "state": "closed",
    "created_at": "2024-11-08T08:40:56+00:00",
    "closed_at": "2024-11-08T09:01:03+00:00",
    "updated_at": "2024-11-08T09:01:03+00:00",
    "author": "dragonfqg",
    "author_type": "User",
    "comments_count": 1,
    "reactions_count": 0,
    "labels": "",
    "milestone": null,
    "closed_by": "dragonfqg",
    "resolution_time_hours": 0.3352777777777778,
    "first_comments": [
      {
        "author": "dragonfqg",
        "created_at": "2024-11-08T09:01:03+00:00",
        "body": "> _No description provided.Â æœªæä¾›æè¿°ã€‚_\r\n\r\n"
      }
    ],
    "url": "https://github.com/QwenLM/Qwen/issues/1329"
  },
  {
    "repository": "QwenLM/Qwen",
    "issue_number": 1327,
    "title": "[BUG] <title> File \"/home/.cache/huggingface/modules/transformers_modules/Qwen-7B-Chat/modeling_qwen.py\", line 352, in _attn     attn_weights = torch.where( RuntimeError: expected scalar type c10::Half but found double",
    "body": "### æ˜¯å¦å·²æœ‰å…³äºè¯¥é”™è¯¯çš„issueæˆ–è®¨è®ºï¼Ÿ | Is there an existing issue / discussion for this?\n\n- [X] æˆ‘å·²ç»æœç´¢è¿‡å·²æœ‰çš„issueså’Œè®¨è®º | I have searched the existing issues / discussions\n\n### è¯¥é—®é¢˜æ˜¯å¦åœ¨FAQä¸­æœ‰è§£ç­”ï¼Ÿ | Is there an existing answer for this in FAQ?\n\n- [X] æˆ‘å·²ç»æœç´¢è¿‡FAQ | I have searched FAQ\n\n### å½“å‰è¡Œä¸º | Current Behavior\n\n_No response_\n\n### æœŸæœ›è¡Œä¸º | Expected Behavior\n\n_No response_\n\n### å¤ç°æ–¹æ³• | Steps To Reproduce\n\n_No response_\n\n### è¿è¡Œç¯å¢ƒ | Environment\n\n```Markdown\n- OS:\r\n- Python:\r\n- Transformers:\r\n- PyTorch:\r\n- CUDA (`python -c 'import torch; print(torch.version.cuda)'`):\n```\n\n\n### å¤‡æ³¨ | Anything else?\n\n_No response_",
    "state": "closed",
    "created_at": "2024-10-30T06:35:51+00:00",
    "closed_at": "2024-12-06T08:07:34+00:00",
    "updated_at": "2024-12-06T08:07:34+00:00",
    "author": "zkailinzhang",
    "author_type": "User",
    "comments_count": 1,
    "reactions_count": 0,
    "labels": "inactive",
    "milestone": null,
    "closed_by": "github-actions[bot]",
    "resolution_time_hours": 889.5286111111111,
    "first_comments": [
      {
        "author": "github-actions[bot]",
        "created_at": "2024-11-29T08:07:31+00:00",
        "body": "This issue has been automatically marked as inactive due to lack of recent activity. Should you believe it remains unresolved and warrants attention, kindly leave a comment on this thread.\næ­¤é—®é¢˜ç”±äºé•¿æœŸæœªæœ‰æ–°è¿›å±•è€Œè¢«ç³»ç»Ÿè‡ªåŠ¨æ ‡è®°ä¸ºä¸æ´»è·ƒã€‚å¦‚æœæ‚¨è®¤ä¸ºå®ƒä»æœ‰å¾…è§£å†³ï¼Œè¯·åœ¨æ­¤å¸–ä¸‹æ–¹ç•™è¨€ä»¥è¡¥å……ä¿¡æ¯ã€‚"
      }
    ],
    "url": "https://github.com/QwenLM/Qwen/issues/1327"
  },
  {
    "repository": "QwenLM/Qwen",
    "issue_number": 1326,
    "title": "[BUG] <title>",
    "body": "### æ˜¯å¦å·²æœ‰å…³äºè¯¥é”™è¯¯çš„issueæˆ–è®¨è®ºï¼Ÿ | Is there an existing issue / discussion for this?\r\n\r\n- [X] æˆ‘å·²ç»æœç´¢è¿‡å·²æœ‰çš„issueså’Œè®¨è®º | I have searched the existing issues / discussions\r\n\r\n### è¯¥é—®é¢˜æ˜¯å¦åœ¨FAQä¸­æœ‰è§£ç­”ï¼Ÿ | Is there an existing answer for this in FAQ?\r\n\r\n- [X] æˆ‘å·²ç»æœç´¢è¿‡FAQ | I have searched FAQ\r\n\r\n### å½“å‰è¡Œä¸º | Current Behavior\r\n\r\nTraceback (most recent call last):\r\n  File \"/root/autodl-tmp/Qwen/finetune.py\", line 374, in <module>\r\n    train()\r\n  File \"/root/autodl-tmp/Qwen/finetune.py\", line 363, in train\r\n    trainer = Trainer(\r\n  File \"/root/miniconda3/envs/qwen/lib/python3.10/site-packages/transformers/trainer.py\", line 340, in __init__\r\n    self.create_accelerator_and_postprocess()\r\n  File \"/root/miniconda3/envs/qwen/lib/python3.10/site-packages/transformers/trainer.py\", line 3883, in create_accelerator_and_postprocess\r\n    self.accelerator = Accelerator(\r\nTypeError: Accelerator.__init__() got an unexpected keyword argument 'dispatch_batches'\r\n\r\n\r\n### è¿è¡Œç¯å¢ƒ | Environment\r\n\r\n```Markdown\r\n- OS: linux\r\n- Python: 3.10\r\n- Transformers: 4.32.0\r\n- PyTorch: 2.1.0\r\n- CUDA (`python -c 'import torch; print(torch.version.cuda)'`):12.1\r\n```\r\n\r\n\r\n### å¤‡æ³¨ | Anything else?\r\n\r\n_No response_",
    "state": "closed",
    "created_at": "2024-10-24T13:03:50+00:00",
    "closed_at": "2024-12-03T08:07:45+00:00",
    "updated_at": "2024-12-03T08:07:45+00:00",
    "author": "bxhsort",
    "author_type": "User",
    "comments_count": 2,
    "reactions_count": 0,
    "labels": "inactive",
    "milestone": null,
    "closed_by": "github-actions[bot]",
    "resolution_time_hours": 955.0652777777777,
    "first_comments": [
      {
        "author": "guoping1127",
        "created_at": "2024-10-26T09:19:07+00:00",
        "body": "æˆ‘ä¹Ÿæœ‰è¿™ä¸ªé”™è¯¯\r\n"
      },
      {
        "author": "github-actions[bot]",
        "created_at": "2024-11-26T08:07:42+00:00",
        "body": "This issue has been automatically marked as inactive due to lack of recent activity. Should you believe it remains unresolved and warrants attention, kindly leave a comment on this thread.\næ­¤é—®é¢˜ç”±äºé•¿æœŸæœªæœ‰æ–°è¿›å±•è€Œè¢«ç³»ç»Ÿè‡ªåŠ¨æ ‡è®°ä¸ºä¸æ´»è·ƒã€‚å¦‚æœæ‚¨è®¤ä¸ºå®ƒä»æœ‰å¾…è§£å†³ï¼Œè¯·åœ¨æ­¤å¸–ä¸‹æ–¹ç•™è¨€ä»¥è¡¥å……ä¿¡æ¯ã€‚"
      }
    ],
    "url": "https://github.com/QwenLM/Qwen/issues/1326"
  },
  {
    "repository": "QwenLM/Qwen",
    "issue_number": 1324,
    "title": "ğŸ’¡ [REQUEST] - <title>qwenæ¨ç†æ¨¡å‹æ˜¯å¦å¯ä»¥å¤šæœºåˆ†å¸ƒå¼éƒ¨ç½²ï¼Ÿï¼ˆå•æœºå•å¡ä¸è¶³ä»¥æ”¯æ’‘æ¨ç†æ¨¡å‹éƒ¨ç½²ï¼Œéœ€è¦å¤šæœºå¤šå¡ï¼‰",
    "body": "### èµ·å§‹æ—¥æœŸ | Start Date\n\n_No response_\n\n### å®ç°PR | Implementation PR\n\næˆ‘æƒ³å¸ƒç½®qwen-14B-chatæ¨ç†æ¨¡å‹ï¼Œä½†æ˜¯æˆ‘åªæœ‰ä¸¤å°16Gæ˜¾å­˜çš„æœºå™¨ï¼Œä¸€å¼ å¡ä¸Šè¿è¡Œä¸èµ·æ¥æ¨¡å‹ï¼Œæˆ‘æƒ³é—®ä¸€ä¸‹ï¼Œæ˜¯å¦å¯ä»¥ä¸¤å°æœºå™¨åˆ†å¸ƒå¼éƒ¨ç½²æ¨ç†æ¨¡å‹\n\n### ç›¸å…³Issues | Reference Issues\n\n_No response_\n\n### æ‘˜è¦ | Summary\n\nå¤šå°æœºå™¨åˆ†å¸ƒå¼éƒ¨ç½²æ¨ç†æ¨¡å‹\n\n### åŸºæœ¬ç¤ºä¾‹ | Basic Example\n\nå¦‚ä½•ä½¿ç”¨ä¸¤å°tesla-16Gè¿›è¡Œ14B-chatçš„æ¨ç†\n\n### ç¼ºé™· | Drawbacks\n\nå•æœºå•å¡ä¸è¶³ä»¥æ”¯æ’‘\n\n### æœªè§£å†³é—®é¢˜ | Unresolved questions\n\n_No response_",
    "state": "closed",
    "created_at": "2024-10-22T02:22:13+00:00",
    "closed_at": "2024-12-01T08:07:00+00:00",
    "updated_at": "2024-12-01T08:07:00+00:00",
    "author": "feifaxiaoming",
    "author_type": "User",
    "comments_count": 8,
    "reactions_count": 0,
    "labels": "question,inactive",
    "milestone": null,
    "closed_by": "github-actions[bot]",
    "resolution_time_hours": 965.7463888888889,
    "first_comments": [
      {
        "author": "xuanhua",
        "created_at": "2024-10-23T06:50:43+00:00",
        "body": "ç›®å‰transformeré»˜è®¤éƒ½æ˜¯ä½¿ç”¨è‡ªå®¶çš„accelerateè¿™ä¸ªåº“æ¥æä¾›æ¨ç†çš„ï¼Œä»ç›®å‰æ¥çœ‹è¿™ä¸ªåº“åªæ”¯æŒå•æœºå¤šå¡ã€‚å¤šèŠ‚ç‚¹çš„æ¨ç†å…¶å®åº”è¯¥ä¹Ÿå¾ˆç®€å•ï¼Œéƒ½æ˜¯ç±»ä¼¼ã€‚\r\nhttps://github.com/huggingface/accelerate/issues/1890"
      },
      {
        "author": "feifaxiaoming",
        "created_at": "2024-10-23T08:00:17+00:00",
        "body": "> ç›®å‰transformeré»˜è®¤éƒ½æ˜¯ä½¿ç”¨è‡ªå®¶çš„accelerateè¿™ä¸ªåº“æ¥æä¾›æ¨ç†çš„ï¼Œä»ç›®å‰æ¥çœ‹è¿™ä¸ªåº“åªæ”¯æŒå•æœºå¤šå¡ã€‚å¤šèŠ‚ç‚¹çš„æ¨ç†å…¶å®åº”è¯¥ä¹Ÿå¾ˆç®€å•ï¼Œéƒ½æ˜¯ç±»ä¼¼ã€‚ [huggingface/accelerate#1890](https://github.com/huggingface/accelerate/issues/1890)\r\n\r\næ‚¨æä¾›çš„è¿æ¥ä¸­ï¼Œä¹Ÿæ˜¯å’¨è¯¢å¤šèŠ‚ç‚¹å¾—ï¼Œä½†æ˜¯æˆ‘å‘ç°é‡Œé¢å¹¶æ²¡æœ‰ç»™å‡ºè§£å†³ç­”æ¡ˆï¼Œæ‚¨é‚£æœ‰èƒ½ç”¨çš„æ–¹æ¡ˆå—ï¼Ÿ"
      },
      {
        "author": "xuanhua",
        "created_at": "2024-10-23T15:30:32+00:00",
        "body": "@feifaxiaoming å¯¹ï¼Œæˆ‘ç»™ä½ çš„é“¾æ¥å°±è¯´æ˜ç›®å‰huggingfaceè¿™è¾¹è¿˜æ²¡æœ‰å®ç°è¿™å—å„¿åŠŸèƒ½ï¼Œå»ºè®®ä½ å•æ­¥è·Ÿè¸ªä¸€ä¸‹huggingfaceçš„æ¨¡å‹çš„tokençš„ç”Ÿæˆè¿‡ç¨‹ã€‚å°¤å…¶æ˜¯å°† device_map å‚æ•°è®¾ç½®ä¸º â€autoâ€œ çš„æ—¶å€™ã€‚transformersåº“ä½¿ç”¨äº†huggingfaceå®¶çš„å¦å¤–ä¸€ä¸ªå«åšaccelerateçš„åº“ï¼Œè¿™ä¸ªåº“ä¼šæŠŠæ¨¡å‹çš„å„ä¸ªsumoduleåˆ†æ•£æ”¾åˆ°ä¸åŒçš„è®¾å¤‡ä¸Šï¼ˆä¼˜å…ˆgpuï¼Œgpuæ˜¾å­˜ä¸å¤Ÿäº†ï¼Œå†å¾€cpuçš„memoryä¸Šæ”¾ï¼‰ã€‚ç„¶ååœ¨æ¨¡å‹è¿›è¡Œforwardçš„è¿‡ç¨‹æ—¶å€™ï¼Œä¿®æ”¹äº†åŸæœ‰æ¨¡å—çš„forwardå‡½æ•°ï¼Œä½¿å¾—åŸæœ‰çš„fowardå‡½æ•°æ‰§è¡Œä¹‹åè¿˜ä¼šæ‰§è¡Œä¸€ä¸ªpostçš„æ“ä½œï¼Œå°±æ˜¯æŠŠfowardçš„è¾“å‡ºç»“æœæ”¾åˆ°ä¸‹ä¸€submoduleçš„forwardæ“ä½œä¹‹åï¼Œé¦–å…ˆå°†ä¸Šä¸€è½®çš„è®¡ç®—ç»“æœé˜²æ­¢åˆ°è¯¥submoduleæ‰€åœ¨è®¾å¤‡ä¸Šã€‚åŒç†ï¼Œå¦‚æœæ˜¯è·¨èŠ‚ç‚¹ï¼Œé‚£ä¹ˆå°±éœ€è¦å°†æŸä¸€ä¸ªèŠ‚ç‚¹ä¸Šæœ€åæ‰§è¡Œå®Œçš„ç»“æœé€šè¿‡ç½‘ç»œå‘é€åˆ°ä¸‹ä¸€ä¸ªèŠ‚ç‚¹ä¸Šã€‚ä»æ›´é«˜æ•ˆç‡çš„è§’åº¦ä¸Šè¯´ï¼Œä¼¼ä¹è¿˜éœ€è¦ä¸€ä¸ªmpiï¼Œæˆ–è€…ncclæ¥åšä¸€ä¸ªè°ƒåº¦å™¨ï¼Œè®©æ•´ä¸ªpipelineæ›´åŠ æœ‰æ•ˆç‡ã€‚"
      },
      {
        "author": "xuanhua",
        "created_at": "2024-10-23T18:56:33+00:00",
        "body": "@feifaxiaoming å¦å¤–ï¼Œè®­ç»ƒå’Œæ¨ç†è¿˜æ˜¯ä¸å¤ªä¸€æ ·ï¼Œæ¨ç†çš„æ—¶å€™ï¼Œå¦‚æœæ˜¯åˆ†æˆä¸¤å°æœºå™¨ï¼Œç”Ÿæˆä¸€ä¸ªtokenéœ€è¦ä¸¤æ¬¡æ•°æ®ä¼ è¾“ï¼›\r\nå‡è®¾æœºå™¨Aä¸Šå­˜å‚¨æ¨¡å‹çš„0\\~15å±‚ï¼Œæœºå™¨Bä¸Šå­˜å‚¨æ¨¡å‹16\\~29å±‚ï¼ˆå‡è®¾æ¨¡å‹ä¸€å…±30å±‚transformerç»“æ„ï¼‰ï¼Œé‚£ä¹ˆåœ¨å®Œæˆè®¡ç®—æ‰€æœ‰promptä¸­çš„tokenå¯¹åº”çš„hidden states ï¼ˆå‡è®¾ä¸º`H_0`ï¼‰çš„è®¡ç®—ä¹‹åï¼Œè®¡ç®—çœŸæ­£æ‰€éœ€è¦ç”Ÿæˆçš„ç¬¬ä¸€ä¸ªtokenéœ€è¦ç»å†ä¸‹é¢çš„è¿‡ç¨‹ï¼š\r\n1ï¼‰ ``H_0``åœ¨æœºå™¨Aä¸Šä½œä¸ºè¾“å…¥ï¼Œç»è¿‡0\\~15å±‚çš„è®¡ç®—ï¼Œäº§ç”Ÿç¬¬ä¸€ä¸ªtokenæ‰€å¯¹åº”çš„ ``h_1_A`` ï¼ˆå½¢çŠ¶ä¸º batch_size \\times hidden_sizeï¼‰\r\n2)   ``h_1_A``é€šè¿‡Aå’ŒBä¹‹é—´çš„ç½‘ç»œåˆ°è¾¾Bæœºå™¨\r\n3ï¼‰Bæœºå™¨ä½¿ç”¨``h_1_A``ä½œä¸ºè¾“å…¥ï¼Œç»å†16\\~29å±‚çš„è®¡ç®—ï¼Œäº§ç”Ÿ``h_1_B``ä½œä¸ºæœ€åçš„è¾“å‡ºï¼Œ``h_1_B``åœ¨æœºå™¨Bä¸Šè¿›è¡Œé‡‡æ ·ï¼ˆå‡è®¾ä½¿ç”¨é‡‡æ ·çš„æ–¹æ³•ç”Ÿæˆtokenï¼‰ç”Ÿæˆtoken `` t_1``\r\n4ï¼‰ ``t_1``ä»Bä¼ å›åˆ°A\r\n5ï¼‰Aå°†ç°æœ‰çš„contexté•¿åº¦åŠ 1ï¼Œç»§ç»­å›åˆ°ç¬¬ 1)æ­¥\r\næ‰€ä»¥ï¼Œæ¯ç”Ÿæˆä¸€ä¸ªtokenï¼Œå®é™…ä¸Šä¸¤å°æœºå™¨ä¹‹é—´å°±éœ€è¦è¿›è¡Œä¸¤æ¬¡æ•°æ®äº¤æ¢ï¼›å‡è®¾æœ€å¿«è€—æ—¶ä¸º1ç§’è¿›è¡Œç½‘ç»œä¼ è¾“ï¼Œé‚£ä¹ˆä¸€ä¸ªtokenå°±éœ€è¦å¤š2ç§’é’ˆè®¡ç®—æ—¶é—´ã€‚å¯¹äºä¸€ä¸ªpromptï¼Œæ•´ä½“è®¡ç®—ä¸‹æ¥çš„æ—¶é—´æ¶ˆè€—åŸºæœ¬ä¸Šå¾ˆéš¾æ¥å—ã€‚ä¾‹å¦‚ç”Ÿæˆ120å­—ï¼Œä½ å¯èƒ½å…‰åœ¨ç½‘ç»œä¼ è¾“ä¸Šå°±éœ€è¦ç­‰å¾…120 x 2 = 4 åˆ†é’Ÿï¼›æ‰€ä»¥ä½ çš„ä¸¤å°æœºå™¨ä¹‹é—´ç½‘ç»œä¼ è¾“é€Ÿåº¦è¦éå¸¸å¿«ï¼Œä¾‹å¦‚10msï¼Œé‚£ä¹ˆ 120 x 20ms = 2.4sï¼› æ‰€ä»¥éœ€è¦ä¸¤å°æœºå™¨ä¹‹é—´çš„ç½‘ç»œå»¶æ—¶éå¸¸çš„å°ï¼Œå¹¶ä¸”ä¼°è®¡éœ€è¦æŠŠbatch sizeè®¾ç½®çš„å¾ˆå¤§ï¼Œå……åˆ†åˆ©ç”¨å¸¦å®½ã€‚ æ€»ä¹‹è®­ç»ƒçš„æ—¶å€™ä½¿ç”¨çš„å¹¶è¡Œé¢„æµ‹tokenï¼Œå¤šæœºä¹Ÿè¿˜è¡Œï¼Œä½†å¦‚æœæ˜¯æ¨ç†ï¼Œå¤šæœºåŸºæœ¬ä¸Šå¯¹ç½‘ç»œè¦æ±‚å¤ªé«˜äº†ã€‚å¤§éƒ¨åˆ†çš„æ—¶é—´éƒ½æ˜¯åœ¨ç½‘ç»œä¼ æ•°æ®ä¸Šã€‚"
      },
      {
        "author": "feifaxiaoming",
        "created_at": "2024-10-24T02:26:54+00:00",
        "body": "accelerateè¿™ä¸ªåº“åªèƒ½å•æœºå¤šå¡éƒ¨ç½²ï¼Œä¸èƒ½å®ç°å¤šæœºå¹¶è¡Œï¼Œè¿˜æœ‰æ‚¨è¯´çš„è¿™ä¸ªå±‚çš„ï¼Œæˆ‘è¯•éªŒäº†ä½¿ç”¨device_mapæ‹†åˆ†äº†80å±‚ï¼Œåœ¨å•æœºä¸Šæ˜¯å¯ç”¨çš„ï¼Œä½†æ˜¯å¤šæœºçš„æ—¶å€™å¹¶ä¸å¥½ç”¨ï¼Œæ‚¨é‚£è¾¹æ˜¯å¦æœ‰å¯ä»¥ç›´æ¥ä½¿ç”¨çš„æ–¹æ¡ˆå‘¢ï¼Ÿå°±æ˜¯å¯ä»¥å¤šæœºåˆ†å¸ƒå¼éƒ¨ç½²çš„æ¨¡å‹æ–¹æ¡ˆå‘¢ã€‹ï¼Ÿ"
      }
    ],
    "url": "https://github.com/QwenLM/Qwen/issues/1324"
  },
  {
    "repository": "QwenLM/Qwen",
    "issue_number": 1323,
    "title": "[BUG] <title>mtbenchåœ¨qwen2-7b-instructçš„å¾—åˆ†",
    "body": "### æ˜¯å¦å·²æœ‰å…³äºè¯¥é”™è¯¯çš„issueæˆ–è®¨è®ºï¼Ÿ | Is there an existing issue / discussion for this?\n\n- [X] æˆ‘å·²ç»æœç´¢è¿‡å·²æœ‰çš„issueså’Œè®¨è®º | I have searched the existing issues / discussions\n\n### è¯¥é—®é¢˜æ˜¯å¦åœ¨FAQä¸­æœ‰è§£ç­”ï¼Ÿ | Is there an existing answer for this in FAQ?\n\n- [X] æˆ‘å·²ç»æœç´¢è¿‡FAQ | I have searched FAQ\n\n### å½“å‰è¡Œä¸º | Current Behavior\n\nè¯·æ•™ä¸€ä¸‹ï¼Œqwen2-7b-instructåœ¨mtbenchä¸Šæ˜¯å¦‚ä½•è¿›è¡Œè¯„æµ‹å¾—åˆ†çš„ï¼Œå¯ä»¥å…·ä½“ä¸€ç‚¹å—ï¼Ÿæ¯”å¦‚è¶…å‚æ•°è®¾ç½®\n\n### æœŸæœ›è¡Œä¸º | Expected Behavior\n\n_No response_\n\n### å¤ç°æ–¹æ³• | Steps To Reproduce\n\n_No response_\n\n### è¿è¡Œç¯å¢ƒ | Environment\n\n```Markdown\n- OS:\r\n- Python:\r\n- Transformers:\r\n- PyTorch:\r\n- CUDA (`python -c 'import torch; print(torch.version.cuda)'`):\n```\n\n\n### å¤‡æ³¨ | Anything else?\n\n_No response_",
    "state": "closed",
    "created_at": "2024-10-21T13:01:37+00:00",
    "closed_at": "2024-11-28T08:07:41+00:00",
    "updated_at": "2024-11-28T08:07:42+00:00",
    "author": "zhang-junjian",
    "author_type": "User",
    "comments_count": 1,
    "reactions_count": 0,
    "labels": "inactive",
    "milestone": null,
    "closed_by": "github-actions[bot]",
    "resolution_time_hours": 907.1011111111111,
    "first_comments": [
      {
        "author": "github-actions[bot]",
        "created_at": "2024-11-21T08:07:29+00:00",
        "body": "This issue has been automatically marked as inactive due to lack of recent activity. Should you believe it remains unresolved and warrants attention, kindly leave a comment on this thread.\næ­¤é—®é¢˜ç”±äºé•¿æœŸæœªæœ‰æ–°è¿›å±•è€Œè¢«ç³»ç»Ÿè‡ªåŠ¨æ ‡è®°ä¸ºä¸æ´»è·ƒã€‚å¦‚æœæ‚¨è®¤ä¸ºå®ƒä»æœ‰å¾…è§£å†³ï¼Œè¯·åœ¨æ­¤å¸–ä¸‹æ–¹ç•™è¨€ä»¥è¡¥å……ä¿¡æ¯ã€‚"
      }
    ],
    "url": "https://github.com/QwenLM/Qwen/issues/1323"
  },
  {
    "repository": "QwenLM/Qwen",
    "issue_number": 1322,
    "title": "[BUG] <title> No heartbeat received from MQLLMEngine",
    "body": "### æ˜¯å¦å·²æœ‰å…³äºè¯¥é”™è¯¯çš„issueæˆ–è®¨è®ºï¼Ÿ | Is there an existing issue / discussion for this?\n\n- [X] æˆ‘å·²ç»æœç´¢è¿‡å·²æœ‰çš„issueså’Œè®¨è®º | I have searched the existing issues / discussions\n\n### è¯¥é—®é¢˜æ˜¯å¦åœ¨FAQä¸­æœ‰è§£ç­”ï¼Ÿ | Is there an existing answer for this in FAQ?\n\n- [X] æˆ‘å·²ç»æœç´¢è¿‡FAQ | I have searched FAQ\n\n### å½“å‰è¡Œä¸º | Current Behavior\n\nPyTorch version: 2.4.0+cu121\r\n Python version: 3.10.12\r\nå¯åŠ¨æ¨¡å‹æœåŠ¡ï¼š\r\npython3 -m vllm.entrypoints.openai.api_server --host [0.0.0.0](http://0.0.0.0/) \\\r\n--port 10023 --seed 1024 \\\r\n--served-model qwen1.5-4b \\\r\n--model \"/home/work/ssd1/Qwen1.5-4B-Chat\" --trust-remote-code \\\r\n--tokenizer-mode slow --max-model-len 128 \\\r\n--dtype float16 \r\nè¿›è¡Œæ¨¡å‹æ¨ç†ï¼š\r\ncurl http://127.0.0.1:10023/v1/completions \\\r\n-H \"Content-Type: application/json\" \\\r\n-d '{\r\n\"model\":\"qwen1.5-4b\", \"max_tokens\":64, \"seed\":0, \"top_k\":1, \"top_p\":0.8, \"temperature\":0.3, \r\n\"prompt\": \"è°ˆè°ˆä½ å¯¹å¤§æ¨¡å‹çš„ç†è§£\"\r\n}'\r\næŠ¥é”™ä¿¡æ¯ï¼š\r\nNo heartbeat received from MQLLMEngine\n\n### æœŸæœ›è¡Œä¸º | Expected Behavior\n\n_No response_\n\n### å¤ç°æ–¹æ³• | Steps To Reproduce\n\n_No response_\n\n### è¿è¡Œç¯å¢ƒ | Environment\n\n```Markdown\n- OS:\r\n- Python:\r\n- Transformers:\r\n- PyTorch:\r\n- CUDA (`python -c 'import torch; print(torch.version.cuda)'`):\n```\n\n\n### å¤‡æ³¨ | Anything else?\n\n_No response_",
    "state": "closed",
    "created_at": "2024-10-14T11:23:08+00:00",
    "closed_at": "2024-11-21T08:07:31+00:00",
    "updated_at": "2024-11-21T08:07:31+00:00",
    "author": "hulk-zhk",
    "author_type": "User",
    "comments_count": 1,
    "reactions_count": 0,
    "labels": "inactive",
    "milestone": null,
    "closed_by": "github-actions[bot]",
    "resolution_time_hours": 908.7397222222222,
    "first_comments": [
      {
        "author": "github-actions[bot]",
        "created_at": "2024-11-14T08:07:20+00:00",
        "body": "This issue has been automatically marked as inactive due to lack of recent activity. Should you believe it remains unresolved and warrants attention, kindly leave a comment on this thread.\næ­¤é—®é¢˜ç”±äºé•¿æœŸæœªæœ‰æ–°è¿›å±•è€Œè¢«ç³»ç»Ÿè‡ªåŠ¨æ ‡è®°ä¸ºä¸æ´»è·ƒã€‚å¦‚æœæ‚¨è®¤ä¸ºå®ƒä»æœ‰å¾…è§£å†³ï¼Œè¯·åœ¨æ­¤å¸–ä¸‹æ–¹ç•™è¨€ä»¥è¡¥å……ä¿¡æ¯ã€‚"
      }
    ],
    "url": "https://github.com/QwenLM/Qwen/issues/1322"
  },
  {
    "repository": "QwenLM/Qwen",
    "issue_number": 1321,
    "title": "[BUG] <title>Cannot reproduce Qwen1.5-7B base model's reported score 62.5 on gsm8k",
    "body": "### æ˜¯å¦å·²æœ‰å…³äºè¯¥é”™è¯¯çš„issueæˆ–è®¨è®ºï¼Ÿ | Is there an existing issue / discussion for this?\n\n- [X] æˆ‘å·²ç»æœç´¢è¿‡å·²æœ‰çš„issueså’Œè®¨è®º | I have searched the existing issues / discussions\n\n### è¯¥é—®é¢˜æ˜¯å¦åœ¨FAQä¸­æœ‰è§£ç­”ï¼Ÿ | Is there an existing answer for this in FAQ?\n\n- [X] æˆ‘å·²ç»æœç´¢è¿‡FAQ | I have searched FAQ\n\n### å½“å‰è¡Œä¸º | Current Behavior\n\nI used the code eval/evaluate_gsm8k.py to evaluate Qwen1.5-7B base model downloaded from huggingface. \r\nThe results shows that Qwen1.5-7B base got Acc:  0.4457922668 on gsm8k, which is much lower than the reported score 62.5 (https://huggingface.co/Qwen/Qwen2-7B).\r\nBut the  Qwen1.5-1.8B base got Acc: 0.382865807 which is similar to the reported score 38.4 (https://huggingface.co/Qwen/Qwen2-1.5B)\r\n\r\nAnother strange thing is that the Qwen1.5-7B-Chat model got 60.3 on gsm8k (https://huggingface.co/Qwen/Qwen2-7B-Instruct), which is lower than the base model.\r\n\r\nHope to know if there is any typo or the base model is finetuned with the gsm8k training set before the evaluation on the test set?\r\n\r\n\n\n### æœŸæœ›è¡Œä¸º | Expected Behavior\n\nreproduce Qwen1.5-7B base model's reported score 62.5 on gsm8k\n\n### å¤ç°æ–¹æ³• | Steps To Reproduce\n\nI downloaded the gsm8k test set from https://github.com/openai/grade-school-math/tree/master/grade_school_math/data and checked its content is as same as the huggingface parquet https://huggingface.co/datasets/openai/gsm8k/tree/main/main\r\n\r\nThe few-shot prompt (from https://github.com/QwenLM/Qwen/blob/main/eval/gsm8k_prompt.txt) is correctly added.\r\n\r\nI only modified these three lines:\r\nsent = tokenizer.tokenizer.decode(tokens[raw_text_len:])   ->   sent = tokenizer.decode(tokens[raw_text_len:])\r\ninput_ids = tokenizer.tokenizer.encode(input_txt)   ->    input_ids = tokenizer.encode(input_txt)\r\ndataset = load_from_disk(args.sample_input_file)  ->   data_files = {'train': args.sample_input_file+'train.json', 'test': args.sample_input_file+'test.json'}\r\n        dataset = load_dataset('json', data_files=data_files)\r\n\r\n\r\n\r\n\n\n### è¿è¡Œç¯å¢ƒ | Environment\n\n```Markdown\nMy environment: ubuntu 18.04\r\nTesla V100-SXM2-32GB * 8\r\n\r\npython                    3.10.14\r\ntorch                     2.2.0\r\ntransformers              4.41.2\r\nCUDA: 12.1\n```\n\n\n### å¤‡æ³¨ | Anything else?\n\n_No response_",
    "state": "closed",
    "created_at": "2024-10-09T12:11:03+00:00",
    "closed_at": "2024-11-17T08:06:27+00:00",
    "updated_at": "2024-11-17T08:06:27+00:00",
    "author": "StevenLau6",
    "author_type": "User",
    "comments_count": 2,
    "reactions_count": 0,
    "labels": "inactive",
    "milestone": null,
    "closed_by": "github-actions[bot]",
    "resolution_time_hours": 931.9233333333333,
    "first_comments": [
      {
        "author": "TanateT",
        "created_at": "2024-10-10T09:25:24+00:00",
        "body": "I also encounter the same question. what do I need to modify?"
      },
      {
        "author": "github-actions[bot]",
        "created_at": "2024-11-10T08:06:24+00:00",
        "body": "This issue has been automatically marked as inactive due to lack of recent activity. Should you believe it remains unresolved and warrants attention, kindly leave a comment on this thread.\næ­¤é—®é¢˜ç”±äºé•¿æœŸæœªæœ‰æ–°è¿›å±•è€Œè¢«ç³»ç»Ÿè‡ªåŠ¨æ ‡è®°ä¸ºä¸æ´»è·ƒã€‚å¦‚æœæ‚¨è®¤ä¸ºå®ƒä»æœ‰å¾…è§£å†³ï¼Œè¯·åœ¨æ­¤å¸–ä¸‹æ–¹ç•™è¨€ä»¥è¡¥å……ä¿¡æ¯ã€‚"
      }
    ],
    "url": "https://github.com/QwenLM/Qwen/issues/1321"
  },
  {
    "repository": "QwenLM/Qwen",
    "issue_number": 1320,
    "title": "[BUG] é€šè¿‡web_demo è¿è¡Œå¾®è°ƒæ¨¡å‹",
    "body": "### æ˜¯å¦å·²æœ‰å…³äºè¯¥é”™è¯¯çš„issueæˆ–è®¨è®ºï¼Ÿ | Is there an existing issue / discussion for this?\r\n\r\n- [X] æˆ‘å·²ç»æœç´¢è¿‡å·²æœ‰çš„issueså’Œè®¨è®º | I have searched the existing issues / discussions\r\n\r\n### è¯¥é—®é¢˜æ˜¯å¦åœ¨FAQä¸­æœ‰è§£ç­”ï¼Ÿ | Is there an existing answer for this in FAQ?\r\n\r\n- [X] æˆ‘å·²ç»æœç´¢è¿‡FAQ | I have searched FAQ\r\n\r\n### å½“å‰è¡Œä¸º | Current Behavior\r\n\r\n\r\n\r\n### æœŸæœ›è¡Œä¸º | Expected Behavior\r\n\r\n_No response_\r\n\r\n### å¤ç°æ–¹æ³• | Steps To Reproduce\r\n\r\n_No response_\r\n\r\n### è¿è¡Œç¯å¢ƒ | Environment\r\n\r\n```Markdown\r\n- OS: Ubuntu 22.04\r\n- Python: 3.12.5\r\n- Transformers:   4.45.1\r\n- PyTorch:\r\n- CUDA (`python -c 'import torch; print(torch.version.cuda)'`):\r\n```\r\n\r\n\r\n### å¤‡æ³¨ | Anything else?\r\n\r\n_No response_",
    "state": "closed",
    "created_at": "2024-10-09T00:06:43+00:00",
    "closed_at": "2024-10-09T03:00:27+00:00",
    "updated_at": "2024-10-09T03:00:27+00:00",
    "author": "YANGLEDUO1",
    "author_type": "User",
    "comments_count": 0,
    "reactions_count": 0,
    "labels": "",
    "milestone": null,
    "closed_by": "YANGLEDUO1",
    "resolution_time_hours": 2.8955555555555557,
    "first_comments": [],
    "url": "https://github.com/QwenLM/Qwen/issues/1320"
  },
  {
    "repository": "QwenLM/Qwen",
    "issue_number": 1319,
    "title": "[BUG] <title>",
    "body": "### æ˜¯å¦å·²æœ‰å…³äºè¯¥é”™è¯¯çš„issueæˆ–è®¨è®ºï¼Ÿ | Is there an existing issue / discussion for this?\n\n- [X] æˆ‘å·²ç»æœç´¢è¿‡å·²æœ‰çš„issueså’Œè®¨è®º | I have searched the existing issues / discussions\n\n### è¯¥é—®é¢˜æ˜¯å¦åœ¨FAQä¸­æœ‰è§£ç­”ï¼Ÿ | Is there an existing answer for this in FAQ?\n\n- [X] æˆ‘å·²ç»æœç´¢è¿‡FAQ | I have searched FAQ\n\n### å½“å‰è¡Œä¸º | Current Behavior\n\nWhen using the Qianwen 2B model, there were very few cases of abnormal allocation of video memory. What is the problem?\r\n![image](https://github.com/user-attachments/assets/b8b3e33d-503a-4b12-8fc7-5594655b82ff)\r\n\n\n### æœŸæœ›è¡Œä¸º | Expected Behavior\n\n I hope it can be resolved\n\n### å¤ç°æ–¹æ³• | Steps To Reproduce\n\n_No response_\n\n### è¿è¡Œç¯å¢ƒ | Environment\n\n```Markdown\n- OS:\r\n- Python:\r\n- Transformers:\r\n- PyTorch:\r\n- CUDA (`python -c 'import torch; print(torch.version.cuda)'`):\n```\n\n\n### å¤‡æ³¨ | Anything else?\n\n_No response_",
    "state": "closed",
    "created_at": "2024-09-24T09:27:29+00:00",
    "closed_at": "2024-11-02T08:06:49+00:00",
    "updated_at": "2024-11-02T08:06:49+00:00",
    "author": "Timeqaq",
    "author_type": "User",
    "comments_count": 1,
    "reactions_count": 0,
    "labels": "inactive",
    "milestone": null,
    "closed_by": "github-actions[bot]",
    "resolution_time_hours": 934.6555555555556,
    "first_comments": [
      {
        "author": "github-actions[bot]",
        "created_at": "2024-10-25T08:07:14+00:00",
        "body": "This issue has been automatically marked as inactive due to lack of recent activity. Should you believe it remains unresolved and warrants attention, kindly leave a comment on this thread.\næ­¤é—®é¢˜ç”±äºé•¿æœŸæœªæœ‰æ–°è¿›å±•è€Œè¢«ç³»ç»Ÿè‡ªåŠ¨æ ‡è®°ä¸ºä¸æ´»è·ƒã€‚å¦‚æœæ‚¨è®¤ä¸ºå®ƒä»æœ‰å¾…è§£å†³ï¼Œè¯·åœ¨æ­¤å¸–ä¸‹æ–¹ç•™è¨€ä»¥è¡¥å……ä¿¡æ¯ã€‚"
      }
    ],
    "url": "https://github.com/QwenLM/Qwen/issues/1319"
  },
  {
    "repository": "QwenLM/Qwen",
    "issue_number": 1318,
    "title": "ValueError: Cannot merge LORA layers when the model is gptq quantized",
    "body": "### æ˜¯å¦å·²æœ‰å…³äºè¯¥é”™è¯¯çš„issueæˆ–è®¨è®ºï¼Ÿ | Is there an existing issue / discussion for this?\n\n- [X] æˆ‘å·²ç»æœç´¢è¿‡å·²æœ‰çš„issueså’Œè®¨è®º | I have searched the existing issues / discussions\n\n### è¯¥é—®é¢˜æ˜¯å¦åœ¨FAQä¸­æœ‰è§£ç­”ï¼Ÿ | Is there an existing answer for this in FAQ?\n\n- [X] æˆ‘å·²ç»æœç´¢è¿‡FAQ | I have searched FAQ\n\n### å½“å‰è¡Œä¸º | Current Behavior\n\nå¾®è°ƒQwen_1.8b_chat_int4æ¨¡å‹ï¼Œåˆ†åˆ«ä½¿ç”¨loraå’Œqloraæ–¹æ³•ï¼Œåˆå¹¶æ¨¡å‹æ—¶æŠ¥é”™\r\nValueError: Cannot merge LORA layers when the model is gptq quantized\n\n### æœŸæœ›è¡Œä¸º | Expected Behavior\n\nè§£å†³è¯¥é—®é¢˜\n\n### å¤ç°æ–¹æ³• | Steps To Reproduce\n\npython qwen_lora_merge.py\r\n\r\nfrom peft import AutoPeftModelForCausalLM\r\nfrom transformers import AutoTokenizer\r\n\r\npath_to_adapter=\"/home/ren/Finetuning/Qwen-1.8-chat/\"\r\nnew_model_directory=\"/home/ren/Finetuning/llm_model/Qwen-1_8B-Chat-Int4_law\"\r\n\r\nmodel = AutoPeftModelForCausalLM.from_pretrained(\r\n\r\n   path_to_adapter, # path to the output directory\r\n\r\n   device_map=\"auto\",\r\n\r\n   trust_remote_code=True\r\n\r\n).eval()merged_model = model.merge_and_unload()\r\n\r\n# max_shard_size and safe serialization are not necessary.\r\n\r\n# They respectively work for sharding checkpoint and save the model to safetensors\r\n\r\nmerged_model.save_pretrained(new_model_directory, max_shard_size=\"2048MB\", safe_serialization=True)\n\n### è¿è¡Œç¯å¢ƒ | Environment\n\n```Markdown\n- OS:Ubuntu 20.04\r\n- Python:3.10\r\n- Transformers:4.37.2\r\n- PyTorch:2.2.1\r\n- CUDA (`python -c 'import torch; print(torch.version.cuda)'`):11.8\n```\n\n\n### å¤‡æ³¨ | Anything else?\n\nno",
    "state": "closed",
    "created_at": "2024-09-23T01:58:44+00:00",
    "closed_at": "2024-11-02T08:06:49+00:00",
    "updated_at": "2024-11-02T08:06:49+00:00",
    "author": "goy-jin",
    "author_type": "User",
    "comments_count": 2,
    "reactions_count": 0,
    "labels": "inactive",
    "milestone": null,
    "closed_by": "github-actions[bot]",
    "resolution_time_hours": 966.1347222222222,
    "first_comments": [
      {
        "author": "danyow-cheung",
        "created_at": "2024-09-24T15:38:50+00:00",
        "body": "å¦‚æœä½ è§‰å¾—è¿™æ ·ä¸€æ­¥åˆ°ä½çš„æ–¹å¼è®©ä½ å¾ˆä¸å®‰å¿ƒæˆ–è€…å½±å“ä½ æ¥å…¥ä¸‹æ¸¸åº”ç”¨ï¼Œä½ å¯ä»¥é€‰æ‹©å…ˆåˆå¹¶å¹¶å­˜å‚¨æ¨¡å‹ï¼ˆLoRAæ”¯æŒåˆå¹¶ï¼ŒQ-LoRAä¸æ”¯æŒï¼‰ï¼Œå†ç”¨å¸¸è§„æ–¹å¼è¯»å–ä½ çš„æ–°æ¨¡å‹ï¼Œç¤ºä¾‹å¦‚ä¸‹ï¼š\r\n\r\n-----\r\nqloraä¸æ”¯æŒåˆå¹¶"
      },
      {
        "author": "github-actions[bot]",
        "created_at": "2024-10-25T08:07:15+00:00",
        "body": "This issue has been automatically marked as inactive due to lack of recent activity. Should you believe it remains unresolved and warrants attention, kindly leave a comment on this thread.\næ­¤é—®é¢˜ç”±äºé•¿æœŸæœªæœ‰æ–°è¿›å±•è€Œè¢«ç³»ç»Ÿè‡ªåŠ¨æ ‡è®°ä¸ºä¸æ´»è·ƒã€‚å¦‚æœæ‚¨è®¤ä¸ºå®ƒä»æœ‰å¾…è§£å†³ï¼Œè¯·åœ¨æ­¤å¸–ä¸‹æ–¹ç•™è¨€ä»¥è¡¥å……ä¿¡æ¯ã€‚"
      }
    ],
    "url": "https://github.com/QwenLM/Qwen/issues/1318"
  },
  {
    "repository": "QwenLM/Qwen",
    "issue_number": 1317,
    "title": "ollama qwen2.5:72b-instruct ä¸æ–­å¾ªç¯ç”Ÿæˆé—®é¢˜å’Œç­”æ¡ˆ",
    "body": "### æ˜¯å¦å·²æœ‰å…³äºè¯¥é”™è¯¯çš„issueæˆ–è®¨è®ºï¼Ÿ | Is there an existing issue / discussion for this?\n\n- [X] æˆ‘å·²ç»æœç´¢è¿‡å·²æœ‰çš„issueså’Œè®¨è®º | I have searched the existing issues / discussions\n\n### è¯¥é—®é¢˜æ˜¯å¦åœ¨FAQä¸­æœ‰è§£ç­”ï¼Ÿ | Is there an existing answer for this in FAQ?\n\n- [X] æˆ‘å·²ç»æœç´¢è¿‡FAQ | I have searched FAQ\n\n### å½“å‰è¡Œä¸º | Current Behavior\n\nä½¿ç”¨ollama run qwen2.5:72b-instructï¼Œäº¤äº’å¼é—®é—®é¢˜æ—¶å‡ºç°äº†ç³»ç»Ÿè‡ªåŠ¨ç”Ÿæˆä¸‹ä¸€ä¸ªé—®é¢˜ï¼Œè‡ªå·±å›ç­”å†è‡ªåŠ¨ç”Ÿæˆä¸‹ä¸€ä¸ªè¿™æ ·å¾ªç¯çš„æƒ…å†µï¼ŒæŒç»­äº†1-2å°æ—¶ï¼Œè¢«æˆ‘ç»ˆæ­¢äº†ï¼Œç¬¬ä¸€æ¬¡å‡ºç°æ˜¯è¿™æ ·\n\n### æœŸæœ›è¡Œä¸º | Expected Behavior\n\n_No response_\n\n### å¤ç°æ–¹æ³• | Steps To Reproduce\n\n_No response_\n\n### è¿è¡Œç¯å¢ƒ | Environment\n\n```Markdown\n- OS:\r\n- Python:\r\n- Transformers:\r\n- PyTorch:\r\n- CUDA (`python -c 'import torch; print(torch.version.cuda)'`):\n```\n\n\n### å¤‡æ³¨ | Anything else?\n\n_No response_",
    "state": "closed",
    "created_at": "2024-09-22T19:47:08+00:00",
    "closed_at": "2024-10-30T08:07:28+00:00",
    "updated_at": "2024-10-30T08:07:28+00:00",
    "author": "jaychj",
    "author_type": "User",
    "comments_count": 4,
    "reactions_count": 0,
    "labels": "inactive",
    "milestone": null,
    "closed_by": "github-actions[bot]",
    "resolution_time_hours": 900.3388888888888,
    "first_comments": [
      {
        "author": "jaychj",
        "created_at": "2024-09-22T19:52:48+00:00",
        "body": "check every <|endoftext|>Human:\r\n[issue.md](https://github.com/user-attachments/files/17091005/issue.md)\r\n"
      },
      {
        "author": "jaychj",
        "created_at": "2024-09-22T20:03:35+00:00",
        "body": "å¤ç°äº†ï¼Œä¼¼ä¹å¦‚æœæˆ‘è®©å®ƒå†™ä»£ç ï¼Œå°±ä¼šè§¦å‘é—®é¢˜ï¼Œè¿™æ¬¡æ˜¯è¿™æ ·çš„\r\ncat Desktop/input.md | ollama run qwen2.5:72b-instruct \"\\n\\nplease tell me how to do it by showing code, database schema, NLP etc. all the techs\" > Desktop/output.md\r\ninput.mdæ˜¯æˆ‘å‰é¢ä¸€ä¸ªé—®é¢˜ï¼Œæˆ‘åšé™„ä»¶ä¹Ÿä¸Šä¼ äº†ï¼Œåªæ˜¯æƒ³ç»™æ›´å¤æ‚çš„promptè€Œå·²ï¼Œè€Œä¸”è¿™ä¸ªinput.mdä¹Ÿæ˜¯è¿™ä¸ªæ¨¡å‹ç”Ÿæˆçš„ç­”æ¡ˆ\r\n[input.md](https://github.com/user-attachments/files/17091037/input.md)\r\n\r\nç›®å‰è¿™ä¸ªoutput.mdå·²ç»åŒ…å«å¾ˆå¤šå¾ªç¯çš„é—®é¢˜äº†ï¼Œæ£€ç´¢æ¯ä¸€ä¸ª<|endoftext|>Human:å¯ä»¥çœ‹åˆ°\r\n[output.md](https://github.com/user-attachments/files/17091041/output.md)\r\nç„¶åæˆ‘è¢«åŠ¨åœæ­¢äº†ç¨‹åºï¼Œä¼¼ä¹è‡ªå·±æ— æ³•åœæ­¢"
      },
      {
        "author": "jaychj",
        "created_at": "2024-09-22T20:08:05+00:00",
        "body": "osï¼šmacOS15.0\r\nèŠ¯ç‰‡æ˜¯M3 Max ï¼Œ128Gå†…å­˜\r\npythonï¼š3.12.6"
      },
      {
        "author": "github-actions[bot]",
        "created_at": "2024-10-23T08:07:18+00:00",
        "body": "This issue has been automatically marked as inactive due to lack of recent activity. Should you believe it remains unresolved and warrants attention, kindly leave a comment on this thread.\næ­¤é—®é¢˜ç”±äºé•¿æœŸæœªæœ‰æ–°è¿›å±•è€Œè¢«ç³»ç»Ÿè‡ªåŠ¨æ ‡è®°ä¸ºä¸æ´»è·ƒã€‚å¦‚æœæ‚¨è®¤ä¸ºå®ƒä»æœ‰å¾…è§£å†³ï¼Œè¯·åœ¨æ­¤å¸–ä¸‹æ–¹ç•™è¨€ä»¥è¡¥å……ä¿¡æ¯ã€‚"
      }
    ],
    "url": "https://github.com/QwenLM/Qwen/issues/1317"
  },
  {
    "repository": "QwenLM/Qwen",
    "issue_number": 1315,
    "title": "qwen-7b-int4ç”¨vllmæ¨ç†ï¼Œä¸ºä»€ä¹ˆç»“æœæ˜¯ä¹±ç ?",
    "body": "### æ˜¯å¦å·²æœ‰å…³äºè¯¥é”™è¯¯çš„issueæˆ–è®¨è®ºï¼Ÿ | Is there an existing issue / discussion for this?\n\n- [X] æˆ‘å·²ç»æœç´¢è¿‡å·²æœ‰çš„issueså’Œè®¨è®º | I have searched the existing issues / discussions\n\n### è¯¥é—®é¢˜æ˜¯å¦åœ¨FAQä¸­æœ‰è§£ç­”ï¼Ÿ | Is there an existing answer for this in FAQ?\n\n- [X] æˆ‘å·²ç»æœç´¢è¿‡FAQ | I have searched FAQ\n\n### å½“å‰è¡Œä¸º | Current Behavior\n\næˆ‘çš„åŠ è½½æ¨¡å‹ä»£ç æ˜¯:` model = vLLMWrapper(args.checkpoint_path, tensor_parallel_size=1)`,è¿è¡Œç»“æœæ˜¯\r\n![Screenshot from 2024-09-04 16-16-13](https://github.com/user-attachments/assets/d3ba9207-daeb-4e30-a4b2-9ed3a3e036a6)\r\nvllmæ¨ç†fp16çš„ç»“æœæ˜¯æ­£ç¡®çš„ï¼Œä½†æ˜¯æ¨ç†int4çš„ç»“æœæ˜¯é”™è¯¯çš„ã€‚\n\n### æœŸæœ›è¡Œä¸º | Expected Behavior\n\n_No response_\n\n### å¤ç°æ–¹æ³• | Steps To Reproduce\n\n_No response_\n\n### è¿è¡Œç¯å¢ƒ | Environment\n\n```Markdown\n- OS:\r\n- Python:\r\n- Transformers:\r\n- PyTorch:\r\n- CUDA (`python -c 'import torch; print(torch.version.cuda)'`):\n```\n\n\n### å¤‡æ³¨ | Anything else?\n\n_No response_",
    "state": "closed",
    "created_at": "2024-09-04T08:18:42+00:00",
    "closed_at": "2024-10-13T08:06:37+00:00",
    "updated_at": "2024-10-13T08:06:37+00:00",
    "author": "shaqing",
    "author_type": "User",
    "comments_count": 1,
    "reactions_count": 0,
    "labels": "inactive",
    "milestone": null,
    "closed_by": "github-actions[bot]",
    "resolution_time_hours": 935.7986111111111,
    "first_comments": [
      {
        "author": "github-actions[bot]",
        "created_at": "2024-10-05T08:06:31+00:00",
        "body": "This issue has been automatically marked as inactive due to lack of recent activity. Should you believe it remains unresolved and warrants attention, kindly leave a comment on this thread.\næ­¤é—®é¢˜ç”±äºé•¿æœŸæœªæœ‰æ–°è¿›å±•è€Œè¢«ç³»ç»Ÿè‡ªåŠ¨æ ‡è®°ä¸ºä¸æ´»è·ƒã€‚å¦‚æœæ‚¨è®¤ä¸ºå®ƒä»æœ‰å¾…è§£å†³ï¼Œè¯·åœ¨æ­¤å¸–ä¸‹æ–¹ç•™è¨€ä»¥è¡¥å……ä¿¡æ¯ã€‚"
      }
    ],
    "url": "https://github.com/QwenLM/Qwen/issues/1315"
  },
  {
    "repository": "QwenLM/Qwen",
    "issue_number": 1314,
    "title": "è¯·é—®å¯ä»¥æ”¯æŒåŠ å…¥æœ¬åœ°çŸ¥è¯†åº“è¿›è¡Œå¾®è°ƒå¤§æ¨¡å‹å—",
    "body": "### æ˜¯å¦å·²æœ‰å…³äºè¯¥é”™è¯¯çš„issueæˆ–è®¨è®ºï¼Ÿ | Is there an existing issue / discussion for this?\n\n- [X] æˆ‘å·²ç»æœç´¢è¿‡å·²æœ‰çš„issueså’Œè®¨è®º | I have searched the existing issues / discussions\n\n### è¯¥é—®é¢˜æ˜¯å¦åœ¨FAQä¸­æœ‰è§£ç­”ï¼Ÿ | Is there an existing answer for this in FAQ?\n\n- [X] æˆ‘å·²ç»æœç´¢è¿‡FAQ | I have searched FAQ\n\n### å½“å‰è¡Œä¸º | Current Behavior\n\nè¯·é—®å¯ä»¥æ”¯æŒåŠ å…¥æœ¬åœ°çŸ¥è¯†åº“è¿›è¡Œå¾®è°ƒå¤§æ¨¡å‹å—\n\n### æœŸæœ›è¡Œä¸º | Expected Behavior\n\nè¯·é—®å¯ä»¥æ”¯æŒåŠ å…¥æœ¬åœ°çŸ¥è¯†åº“è¿›è¡Œå¾®è°ƒå¤§æ¨¡å‹å—\n\n### å¤ç°æ–¹æ³• | Steps To Reproduce\n\nè¯·é—®å¯ä»¥æ”¯æŒåŠ å…¥æœ¬åœ°çŸ¥è¯†åº“è¿›è¡Œå¾®è°ƒå¤§æ¨¡å‹å—\n\n### è¿è¡Œç¯å¢ƒ | Environment\n\n```Markdown\n- OS:\r\n- Python:\r\n- Transformers:\r\n- PyTorch:\r\n- CUDA (`python -c 'import torch; print(torch.version.cuda)'`):\n```\n\n\n### å¤‡æ³¨ | Anything else?\n\nè¯·é—®å¯ä»¥æ”¯æŒåŠ å…¥æœ¬åœ°çŸ¥è¯†åº“è¿›è¡Œå¾®è°ƒå¤§æ¨¡å‹å—",
    "state": "closed",
    "created_at": "2024-08-23T03:31:00+00:00",
    "closed_at": "2024-10-05T08:06:33+00:00",
    "updated_at": "2024-10-05T08:06:33+00:00",
    "author": "jingmingtao",
    "author_type": "User",
    "comments_count": 3,
    "reactions_count": 0,
    "labels": "inactive",
    "milestone": null,
    "closed_by": "github-actions[bot]",
    "resolution_time_hours": 1036.5925,
    "first_comments": [
      {
        "author": "jingmingtao",
        "created_at": "2024-08-23T03:46:02+00:00",
        "body": "è¯·é—®æˆ‘æƒ³è¦å¾®è°ƒï¼Œè®©æ¨¡å‹å›ç­”åŒå­¦çš„åå­—ä¸å‡ºç”Ÿæ—¥æœŸï¼Œæˆ‘çš„å¾®è°ƒæ•°æ®å¯ä»¥è¿™ä¹ˆè®¾è®¡å—ï¼š[\r\n  {\r\n    \"id\": \"identity_0\",\r\n    \"conversations\": [\r\n      {\r\n        \"from\": \"user\",\r\n        \"value\": \"äººåä¸ç”Ÿæ—¥\"\r\n      },\r\n      {\r\n        \"from\": \"assistant\",\r\n        \"value\": \"äººå:å°æ˜,å‡ºç”Ÿæ—¥æœŸ: 2021å¹´5æœˆ11æ—¥\"\r\n      },\r\n      {\r\n        \"from\": \"assistant\",\r\n        \"value\": \"äººå:å°æ¶›,å‡ºç”Ÿæ—¥æœŸ: 2008å¹´6æœˆ23æ—¥\"\r\n      }\r\n    ]\r\n  }\r\n]\r\næˆ‘æƒ³è¦è¾¾åˆ°è¿™æ ·çš„æ•ˆæœï¼šé—®å¤§æ¨¡å‹ï¼Œè¯·é—®2021å¹´5æœˆ11æ—¥å‡ºç”Ÿçš„äººåå­—æ˜¯è°"
      },
      {
        "author": "zpge",
        "created_at": "2024-08-28T10:03:00+00:00",
        "body": "> ### æ˜¯å¦å·²æœ‰å…³äºè¯¥é”™è¯¯çš„issueæˆ–è®¨è®ºï¼Ÿ | Is there an existing issue / discussion for this?\r\n> * [x]  æˆ‘å·²ç»æœç´¢è¿‡å·²æœ‰çš„issueså’Œè®¨è®º | I have searched the existing issues / discussions\r\n> \r\n> ### è¯¥é—®é¢˜æ˜¯å¦åœ¨FAQä¸­æœ‰è§£ç­”ï¼Ÿ | Is there an existing answer for this in FAQ?\r\n> * [x]  æˆ‘å·²ç»æœç´¢è¿‡FAQ | I have searched FAQ\r\n> \r\n> ### å½“å‰è¡Œä¸º | Current Behavior\r\n> è¯·é—®å¯ä»¥æ”¯æŒåŠ å…¥æœ¬åœ°çŸ¥è¯†åº“è¿›è¡Œå¾®è°ƒå¤§æ¨¡å‹å—\r\n> \r\n> ### æœŸæœ›è¡Œä¸º | Expected Behavior\r\n> è¯·é—®å¯ä»¥æ”¯æŒåŠ å…¥æœ¬åœ°çŸ¥è¯†åº“è¿›è¡Œå¾®è°ƒå¤§æ¨¡å‹å—\r\n> \r\n> ### å¤ç°æ–¹æ³• | Steps To Reproduce\r\n> è¯·é—®å¯ä»¥æ”¯æŒåŠ å…¥æœ¬åœ°çŸ¥è¯†åº“è¿›è¡Œå¾®è°ƒå¤§æ¨¡å‹å—\r\n> \r\n> ### è¿è¡Œç¯å¢ƒ | Environment\r\n> ```\r\n> - OS:\r\n> - Python:\r\n> - Transformers:\r\n> - PyTorch:\r\n> - CUDA (`python -c 'import torch; print(torch.version.cuda)'`):\r\n> ```\r\n> \r\n> ### å¤‡æ³¨ | Anything else?\r\n> è¯·é—®å¯ä»¥æ”¯æŒåŠ å…¥æœ¬åœ°çŸ¥è¯†åº“è¿›è¡Œå¾®è°ƒå¤§æ¨¡å‹å—\r\n\r\nåŒé—®ï¼Œç›®å‰æä¾›çš„å¾®è°ƒæ–¹å¼éƒ½æ˜¯æ„å»ºå¯¹è¯çš„ï¼Œå¦‚æœæˆ‘æœ‰å¾ˆå¤šæ–‡æ¡£ï¼Œæƒ³å…ˆè®©æ¨¡å‹å­¦ä¸€äº›é¢†åŸŸçŸ¥è¯†ï¼Œåº”è¯¥æ€ä¹ˆå¾®è°ƒï¼Ÿ"
      },
      {
        "author": "github-actions[bot]",
        "created_at": "2024-09-28T08:06:30+00:00",
        "body": "This issue has been automatically marked as inactive due to lack of recent activity. Should you believe it remains unresolved and warrants attention, kindly leave a comment on this thread.\næ­¤é—®é¢˜ç”±äºé•¿æœŸæœªæœ‰æ–°è¿›å±•è€Œè¢«ç³»ç»Ÿè‡ªåŠ¨æ ‡è®°ä¸ºä¸æ´»è·ƒã€‚å¦‚æœæ‚¨è®¤ä¸ºå®ƒä»æœ‰å¾…è§£å†³ï¼Œè¯·åœ¨æ­¤å¸–ä¸‹æ–¹ç•™è¨€ä»¥è¡¥å……ä¿¡æ¯ã€‚"
      }
    ],
    "url": "https://github.com/QwenLM/Qwen/issues/1314"
  },
  {
    "repository": "QwenLM/Qwen",
    "issue_number": 1313,
    "title": "AWQé‡åŒ–åï¼Œè¾“å‡ºä¸èƒ½æ­£å¸¸åœæ­¢ï¼Œä¸é‡åŒ–æ¨ç†æ­£å¸¸",
    "body": "### æ˜¯å¦å·²æœ‰å…³äºè¯¥é”™è¯¯çš„issueæˆ–è®¨è®ºï¼Ÿ | Is there an existing issue / discussion for this?\r\n\r\n- [X] æˆ‘å·²ç»æœç´¢è¿‡å·²æœ‰çš„issueså’Œè®¨è®º | I have searched the existing issues / discussions\r\n\r\n### è¯¥é—®é¢˜æ˜¯å¦åœ¨FAQä¸­æœ‰è§£ç­”ï¼Ÿ | Is there an existing answer for this in FAQ?\r\n\r\n- [X] æˆ‘å·²ç»æœç´¢è¿‡FAQ | I have searched FAQ\r\n\r\n### å½“å‰è¡Œä¸º | Current Behavior\r\n\r\n**ç°è±¡ï¼šä¸èƒ½æ­£å¸¸åœæ­¢è¾“å‡º**\r\n![image](https://github.com/user-attachments/assets/c6368c1c-fbfc-40cd-a63c-42620b69a047)\r\n\r\n**AWQé‡åŒ–ä»£ç å¦‚ä¸‹**\r\nè¾“å…¥æˆ‘æ˜¯ä»template.pyä¸­å¤åˆ¶è¿‡æ¥çš„ï¼Œä¹Ÿå¯¹æ¯”è¿‡tokenizer_config.jsonï¼Œè¾“å…¥ä¸€è‡´\r\n![image](https://github.com/user-attachments/assets/83a2f905-8cb2-49d3-ad27-6b887c5f50e0)\r\n\r\n```\r\nimport datasets\r\nimport json\r\nfrom awq import AutoAWQForCausalLM\r\nfrom transformers import AwqConfig, AutoConfig\r\nfrom huggingface_hub import HfApi\r\nfrom transformers import (\r\n    AutoTokenizer,\r\n    TrainingArguments,\r\n    Trainer,\r\n    DataCollatorForLanguageModeling\r\n)\r\nfrom peft import get_peft_model, LoraConfig, TaskType\r\n\r\n\r\n# _register_template(\r\n#     name=\"qwen\",\r\n#     format_user=StringFormatter(slots=[\"<|im_start|>user\\n{{content}}<|im_end|>\\n<|im_start|>assistant\\n\"]),\r\n#     format_system=StringFormatter(slots=[\"<|im_start|>system\\n{{content}}<|im_end|>\\n\"]),\r\n#     format_observation=StringFormatter(slots=[\"<|im_start|>tool\\n{{content}}<|im_end|>\\n<|im_start|>assistant\\n\"]),\r\n#     format_separator=EmptyFormatter(slots=[\"\\n\"]),\r\n#     default_system=\"You are a helpful assistant.\",\r\n#     stop_words=[\"<|im_end|>\"],\r\n#     replace_eos=True,\r\n# )\r\n\r\n\r\ndef format_text(item):\r\n    Q = item['input']\r\n    A = item['output']\r\n    system_input = \"<|im_start|>system\\nä½ æ˜¯ä¸€ä¸ªä¿¡æ¯æŠ½å–ä¸“å®¶ï¼Œæ ¹æ®æ–‡æœ¬å†…å®¹æŠ½å–æœ€é‡è¦çš„5ä¸ªå®ä½“å’Œ3ä¸ªå…·å¤‡äº‰è®®æ€§çš„è¯é¢˜ã€‚<|im_end|>\\n\"\r\n    user = f\"<|im_start|>user\\n{Q}<|im_end|>\\n<|im_start|>assistant\\n{A}\"\r\n    res = system_input + user\r\n    return res\r\n\r\n\r\nmodel_path = \"/root/autodl-tmp/checkpoints/qwen2/entity_cls_all_clear_new/freeze/checkpoint-70\"\r\ndata_path = \"awq_eval.json\"\r\nquant_path = \"/root/autodl-tmp/qwen15-7b-awq\"\r\nquant_config = {\"zero_point\": True, \"q_group_size\": 128, \"w_bit\": 4, \"version\": \"GEMM\"}\r\n# Load model\r\nprint(\"Load model-------------\")\r\nmodel = AutoAWQForCausalLM.from_pretrained(model_path, safetensors=True)\r\ntokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\r\nf = open(data_path, 'r', encoding=\"utf-8\")\r\ndata = json.load(f)\r\ntext_list = [format_text(item) for item in data]\r\nprint('first:', text_list[0])\r\n# Quantize\r\nprint(\"Quantize-------------\")\r\nmodel.quantize(tokenizer, quant_config=quant_config, calib_data=text_list)\r\n\r\n# quantization_config = AwqConfig(\r\n#     bits=quant_config[\"w_bit\"],\r\n#     group_size=quant_config[\"q_group_size\"],\r\n#     zero_point=quant_config[\"zero_point\"],\r\n#     version=quant_config[\"version\"].lower(),\r\n# ).to_dict()\r\n# model.model.config.quantization_config = quantization_config\r\n# Save quantized model\r\nprint(\"Save quantized model-------------\")\r\nmodel.save_quantized(quant_path)\r\ntokenizer.save_pretrained(quant_path)\r\nprint(f'Model is quantized and saved at \"{quant_path}\"')\r\n```\r\n\r\n**AWQé‡åŒ–åçš„æ¨ç†è„šæœ¬**\r\n```\r\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\r\nmodel_path = \"/root/autodl-tmp/qwen15-7b-awq-v2\"\r\ntokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\r\nmodel = AutoModelForCausalLM.from_pretrained(model_path, trust_remote_code=True).to(0)\r\ntext = \"é…’é©¾ã€æ— è¯ï¼Œè¿˜æ‹‰ç€1è€4å°â€¦åŒ—æµ·ä¸€ç”·å­å› å¤šé¡¹äº¤é€šè¿æ³•è¡Œä¸ºè¢«äº¤è­¦æŸ¥å¤„ äº¤é€šå®‰å…¨ä¸å®¹å¿½è§†ï¼Œå¾ˆå¤šä¸¥é‡çš„é“è·¯äº¤é€šäº‹æ•…éƒ½æ˜¯å› ä¸ºä¸éµå®ˆäº¤é€šæ³•å¾‹æ³•è§„å¯¼è‡´çš„ã€‚è¿‘æ—¥ï¼Œä¸€ç”·å­å°±å› å¤šé¡¹äº¤é€šè¿æ³•è¡Œä¸ºè¢«åŒ—æµ·å…¬å®‰äº¤è­¦æŸ¥å¤„ï¼Œè¯¥ç”·å­é…’åã€æ— è¯é©¾é©¶ä¸‰è½®è½¦æ‘©æ‰˜è½¦ï¼Œè¿˜æ­è½½5äººå‡ºè¡Œï¼Œå­˜åœ¨æå¤§çš„å®‰å…¨éšæ‚£ã€‚7æœˆ16æ—¥æ™šä¸Šï¼ŒåŒ—æµ·å¸‚å…¬å®‰å±€äº¤è­¦æ”¯é˜Ÿç¬¬äºŒå¤§é˜Ÿæ‰§å‹¤æ°‘è­¦åœ¨åŒ—éƒ¨æ¹¾è·¯å¼€å±•å¤å­£äº¤é€šå®‰å…¨çªå‡ºè¿æ³•ä¸“é¡¹æ•´æ²»è¡ŒåŠ¨æ—¶ï¼Œå‘ç°ä¸€è¾†æ— ç‰Œä¸‰è½®æ‘©æ‰˜è½¦ï¼Œè½¦ä¸Šé™¤äº†é©¾é©¶äººç«Ÿç„¶è¿˜åç€äº”ä¸ªäººï¼ˆä¸€åè€äººå’Œå››åå°å­©ï¼‰ã€‚æ°‘è­¦èµ¶å¿™å–Šè¯ç¤ºæ„é©¾é©¶äººé è¾¹åœè½¦ï¼Œå¹¶ä¸‹è½¦æ¥å—æ£€æŸ¥ã€‚åœ¨æ­¤è¿‡ç¨‹ä¸­ï¼Œæ°‘è­¦å‘ç°è¯¥è½¦é©¾é©¶äººæœ±æŸèº«ä¸Šæ•£å‘å‡ºçš„é…’å‘³ï¼Œéšå³å¯¹æœ±æŸè¿›è¡Œå‘¼å¸å¼é…’ç²¾æµ‹è¯•ï¼Œç»“æœæ˜¾ç¤ºå…¶ä½“å†…é…’ç²¾å«é‡ä¸º22mg/100mlï¼Œè¾¾åˆ°é¥®é…’åé©¾é©¶æœºåŠ¨è½¦æ ‡å‡†ã€‚æ­¤å¤–ï¼Œæ°‘è­¦åœ¨æ ¸æŸ¥æœ±æŸèº«ä»½ä¿¡æ¯æ—¶ï¼Œè¿˜å‘ç°å…¶å¹¶æœªå–å¾—æœºåŠ¨è½¦é©¾é©¶è¯ã€‚æ®æœ±æŸäº¤ä»£ï¼Œå½“å¤©æ™šä¸Šä»–åœ¨å®¶åƒé¥­æ—¶å–äº†ç‚¹é…’ï¼Œå®¶é‡Œå°å­©é—¹ç€è¦å‡ºå»ç©ï¼Œä»–è‡ªè®¤ä¸ºæ™šä¸Šè¿™æ®µè·¯åº”è¯¥ä¸ä¼šæœ‰äº¤è­¦æŸ¥è½¦ï¼Œå¿ƒå­˜ä¾¥å¹¸ä¾¿é©¾è½¦ä¸Šè·¯ï¼Œæ²¡æƒ³åˆ°è¢«é€®äº†ä¸ªæ­£ç€ã€‚éšåï¼Œæ°‘è­¦å¯¹æœ±æŸåŠå…¶å®¶äººè¿›è¡Œäº†ä¸¥è‚ƒçš„æ‰¹è¯„æ•™è‚²ï¼Œå‘ŠçŸ¥æœ±æŸè¿æ³•çš„æ³•å¾‹åæœï¼Œå‘Šè¯«å…¶è¦ä¸¥æ ¼éµå®ˆäº¤é€šæ³•å¾‹æ³•è§„ï¼Œç¡®ä¿å®‰å…¨å‡ºè¡Œã€‚æœ€åï¼Œæ°‘è­¦é’ˆå¯¹æœ±æŸé¥®é…’åé©¾é©¶æœºåŠ¨è½¦ã€æœªå–å¾—æœºåŠ¨è½¦é©¾é©¶è¯é©¾é©¶æœºåŠ¨è½¦ä¸Šé“è·¯è¡Œé©¶ã€é©¾é©¶æœªæ‚¬æŒ‚æœºåŠ¨è½¦å·ç‰Œçš„æœºåŠ¨è½¦ä¸Šé“è·¯è¡Œé©¶ã€ä¸‰è½®è½¦è¿åè§„å®šè½½äººçš„è¿æ³•è¡Œä¸ºï¼Œå¤„ä»¥ç½šæ¬¾ 1850 å…ƒã€æ‹–ç§»æœºåŠ¨è½¦çš„è¡Œæ”¿å¤„ç½šï¼›æ­¤å¤–ï¼Œæœ±æŸè¿˜å°†é¢ä¸´15æ—¥ä»¥ä¸‹è¡Œæ”¿æ‹˜ç•™çš„å¤„ç½šã€‚åŒ—æµ·å…¬å®‰äº¤è­¦æç¤ºï¼šè¯·å¹¿å¤§äº¤é€šå‚ä¸è€…åœ¨é©¾é©¶è½¦è¾†æ—¶åˆ‡å‹¿é…’é©¾ã€é†‰é©¾ã€è¶…å‘˜ã€è¶…é€Ÿã€è¶…è½½ã€è¿æ³•è½½äººç­‰ï¼Œåˆ‡å®å¢å¼ºè‡ªæˆ‘å®‰å…¨é˜²æŠ¤æ„è¯†ï¼Œä¸¥æ ¼éµå®ˆäº¤é€šæ³•è§„ï¼Œè°¨æ…é©¾é©¶ï¼Œå®‰å…¨å‡ºè¡Œã€‚\"\r\n\r\nsystem_input = \"<|im_start|>system\\nä½ æ˜¯ä¸€ä¸ªä¿¡æ¯æŠ½å–ä¸“å®¶ï¼Œæ ¹æ®æ–‡æœ¬å†…å®¹æŠ½å–æœ€é‡è¦çš„5ä¸ªå®ä½“å’Œ3ä¸ªå…·å¤‡äº‰è®®æ€§çš„è¯é¢˜ã€‚<|im_end|>\\n\"\r\nuser = f\"<|im_start|>user\\n{text}<|im_end|>\\n<|im_start|>assistant\\n\"\r\nres = system_input + user\r\ninputs = tokenizer(res, return_tensors=\"pt\").to(0)\r\nout = model.generate(**inputs, max_new_tokens=2048)\r\nprint(tokenizer.decode(out[0], skip_special_tokens=True))\r\n```\r\n\r\n### æœŸæœ›è¡Œä¸º | Expected Behavior\r\n\r\n_No response_\r\n\r\n### å¤ç°æ–¹æ³• | Steps To Reproduce\r\n\r\n_No response_\r\n\r\n### è¿è¡Œç¯å¢ƒ | Environment\r\n\r\n```Markdown\r\næ¨¡å‹ç‰ˆæœ¬ï¼šqwen1.5-7b\r\nPython 3.10.8\r\ncuda 12.1\r\ntorch 2.3.1\r\ntransformers 4.42.3\r\n```\r\n\r\n\r\n### å¤‡æ³¨ | Anything else?\r\n\r\n_No response_",
    "state": "closed",
    "created_at": "2024-08-20T09:04:03+00:00",
    "closed_at": "2024-09-27T08:07:34+00:00",
    "updated_at": "2024-09-27T08:07:34+00:00",
    "author": "angelOnly",
    "author_type": "User",
    "comments_count": 1,
    "reactions_count": 0,
    "labels": "inactive",
    "milestone": null,
    "closed_by": "github-actions[bot]",
    "resolution_time_hours": 911.0586111111111,
    "first_comments": [
      {
        "author": "github-actions[bot]",
        "created_at": "2024-09-20T08:07:02+00:00",
        "body": "This issue has been automatically marked as inactive due to lack of recent activity. Should you believe it remains unresolved and warrants attention, kindly leave a comment on this thread.\næ­¤é—®é¢˜ç”±äºé•¿æœŸæœªæœ‰æ–°è¿›å±•è€Œè¢«ç³»ç»Ÿè‡ªåŠ¨æ ‡è®°ä¸ºä¸æ´»è·ƒã€‚å¦‚æœæ‚¨è®¤ä¸ºå®ƒä»æœ‰å¾…è§£å†³ï¼Œè¯·åœ¨æ­¤å¸–ä¸‹æ–¹ç•™è¨€ä»¥è¡¥å……ä¿¡æ¯ã€‚"
      }
    ],
    "url": "https://github.com/QwenLM/Qwen/issues/1313"
  },
  {
    "repository": "QwenLM/Qwen",
    "issue_number": 1312,
    "title": "[BUG] <title>Nvidia Jetson Orin NXå¼€å‘æ¿ä¸Šæ¨ç†è¿è¡Œqloraå¾®è°ƒä¹‹åçš„æ¨¡å‹ï¼ŒæŠ¥é”™ï¼šä¸æ”¯æŒQuantLinear()",
    "body": "### æ˜¯å¦å·²æœ‰å…³äºè¯¥é”™è¯¯çš„issueæˆ–è®¨è®ºï¼Ÿ | Is there an existing issue / discussion for this?\n\n- [X] æˆ‘å·²ç»æœç´¢è¿‡å·²æœ‰çš„issueså’Œè®¨è®º | I have searched the existing issues / discussions\n\n### è¯¥é—®é¢˜æ˜¯å¦åœ¨FAQä¸­æœ‰è§£ç­”ï¼Ÿ | Is there an existing answer for this in FAQ?\n\n- [X] æˆ‘å·²ç»æœç´¢è¿‡FAQ | I have searched FAQ\n\n### å½“å‰è¡Œä¸º | Current Behavior\n\næŒ‰å®˜ç½‘æ­¥éª¤è¿›è¡Œqloraå¾®è°ƒä¹‹åï¼Œåœ¨å¼€å‘æ¿ä¸Šæ¨ç†è¿è¡ŒæŠ¥é”™ï¼š\r\nValueError: Target module QuantLinear() is not supported. Currently, only the following modules are supported: `torch.nn.Linear`, `torch.nn.Embedding`, `torch.nn.Conv2d`, `transformers.pytorch_utils.Conv1D`.\r\n\n\n### æœŸæœ›è¡Œä¸º | Expected Behavior\n\n_No response_\n\n### å¤ç°æ–¹æ³• | Steps To Reproduce\n\n_No response_\n\n### è¿è¡Œç¯å¢ƒ | Environment\n\n```Markdown\n- OS: Ubuntu 22.04\r\n- Python: 3.10\r\n- Transformers: 4.39.0\r\n- PyTorch: 2.4.0\r\n- CUDA (`python -c 'import torch; print(torch.version.cuda)'`): 12.2\n```\n\n\n### å¤‡æ³¨ | Anything else?\n\n_No response_",
    "state": "closed",
    "created_at": "2024-08-14T03:22:08+00:00",
    "closed_at": "2024-09-20T08:07:04+00:00",
    "updated_at": "2024-09-20T08:07:04+00:00",
    "author": "lollipopyu",
    "author_type": "User",
    "comments_count": 1,
    "reactions_count": 0,
    "labels": "inactive",
    "milestone": null,
    "closed_by": "github-actions[bot]",
    "resolution_time_hours": 892.7488888888889,
    "first_comments": [
      {
        "author": "github-actions[bot]",
        "created_at": "2024-09-13T08:06:47+00:00",
        "body": "This issue has been automatically marked as inactive due to lack of recent activity. Should you believe it remains unresolved and warrants attention, kindly leave a comment on this thread.\næ­¤é—®é¢˜ç”±äºé•¿æœŸæœªæœ‰æ–°è¿›å±•è€Œè¢«ç³»ç»Ÿè‡ªåŠ¨æ ‡è®°ä¸ºä¸æ´»è·ƒã€‚å¦‚æœæ‚¨è®¤ä¸ºå®ƒä»æœ‰å¾…è§£å†³ï¼Œè¯·åœ¨æ­¤å¸–ä¸‹æ–¹ç•™è¨€ä»¥è¡¥å……ä¿¡æ¯ã€‚"
      }
    ],
    "url": "https://github.com/QwenLM/Qwen/issues/1312"
  },
  {
    "repository": "QwenLM/Qwen",
    "issue_number": 1311,
    "title": "[BUG] å¢åŠ ä¸Šä¸‹æ–‡é•¿åº¦åè¾“å‡ºä¹±ç ",
    "body": "### æ˜¯å¦å·²æœ‰å…³äºè¯¥é”™è¯¯çš„issueæˆ–è®¨è®ºï¼Ÿ | Is there an existing issue / discussion for this?\n\n- [X] æˆ‘å·²ç»æœç´¢è¿‡å·²æœ‰çš„issueså’Œè®¨è®º | I have searched the existing issues / discussions\n\n### è¯¥é—®é¢˜æ˜¯å¦åœ¨FAQä¸­æœ‰è§£ç­”ï¼Ÿ | Is there an existing answer for this in FAQ?\n\n- [X] æˆ‘å·²ç»æœç´¢è¿‡FAQ | I have searched FAQ\n\n### å½“å‰è¡Œä¸º | Current Behavior\n\nä½¿ç”¨å®˜ç½‘æ•™ç¨‹ä¸­çš„YARNæƒ³æ‰©å……ä¸Šä¸‹æ–‡é•¿åº¦ï¼Œå°è¯•æ‰©å……ä¸¤å€ï¼Œä½¿ç”¨çš„configå¢åŠ äº†ï¼š\r\n\"rope_scaling\": {\r\n            \"factor\": 2.0,\r\n            \"original_max_position_embeddings\": 32768,\r\n            \"type\": \"yarn\"\r\n        }\r\n        è¿™ä¸€éƒ¨åˆ†ã€‚\r\n        ç°åœ¨ç»“æœå¯ä»¥è¾“å‡ºï¼Œä½†æ˜¯ä¼šä¹±ç æ˜¾ç¤ºç±»ä¼¼è¿™ç§ï¼š2,\r\n\r\n\r\n\r\n\r\n\r\n42 42\r\n2\r\n\r\n 92\r\n2e\r\n\r\n42 62,-2\r\n\r\n\"\r\n is92,1,22,,\r\n\r\n\r\n\r\n32,1I,H162 62 H2\r\n\r\n\r\n2 32 92\r\n2,h2 42, 42  2 42,h32 92,62y\r\n\r\n\r\n\r\n\r\n2,\r\n2, T\r\n\r\n2621: 11,5,\r\n\r\n42 2,62 622\r\n2-42\r\n\r\n82h61.ï¼Œ\r\n\r\n\r\n\r\n/2,,0 62 2h\r\n\r\n\r\n        \n\n### æœŸæœ›è¡Œä¸º | Expected Behavior\n\nå¸Œæœ›å¯ä»¥æ‰©å……ä¸Šä¸‹æ–‡é•¿åº¦åè¿˜å¯ä»¥æ­£ç¡®è¾“å‡ºæ–‡æœ¬ã€‚\n\n### å¤ç°æ–¹æ³• | Steps To Reproduce\n\n1.ä¿®æ”¹configæ–‡ä»¶\r\n2.ä½¿ç”¨ä¸¤å¼ a100è¿è¡ŒåŸå§‹ä»£ç \n\n### è¿è¡Œç¯å¢ƒ | Environment\n\n```Markdown\n- OS:\r\n- Python:\r\n- Transformers:\r\n- PyTorch:\r\n- CUDA (`python -c 'import torch; print(torch.version.cuda)'`):\n```\n\n\n### å¤‡æ³¨ | Anything else?\n\n_No response_",
    "state": "closed",
    "created_at": "2024-08-13T03:16:11+00:00",
    "closed_at": "2024-09-19T08:07:30+00:00",
    "updated_at": "2024-09-19T08:07:30+00:00",
    "author": "kitty-nami",
    "author_type": "User",
    "comments_count": 1,
    "reactions_count": 0,
    "labels": "inactive",
    "milestone": null,
    "closed_by": "github-actions[bot]",
    "resolution_time_hours": 892.8552777777778,
    "first_comments": [
      {
        "author": "github-actions[bot]",
        "created_at": "2024-09-12T08:06:53+00:00",
        "body": "This issue has been automatically marked as inactive due to lack of recent activity. Should you believe it remains unresolved and warrants attention, kindly leave a comment on this thread.\næ­¤é—®é¢˜ç”±äºé•¿æœŸæœªæœ‰æ–°è¿›å±•è€Œè¢«ç³»ç»Ÿè‡ªåŠ¨æ ‡è®°ä¸ºä¸æ´»è·ƒã€‚å¦‚æœæ‚¨è®¤ä¸ºå®ƒä»æœ‰å¾…è§£å†³ï¼Œè¯·åœ¨æ­¤å¸–ä¸‹æ–¹ç•™è¨€ä»¥è¡¥å……ä¿¡æ¯ã€‚"
      }
    ],
    "url": "https://github.com/QwenLM/Qwen/issues/1311"
  },
  {
    "repository": "QwenLM/Qwen",
    "issue_number": 1310,
    "title": "Qwen-Chat-RLHFå’ŒQwen-Chatçš„åŒºåˆ«",
    "body": "`We then use SFT and RLHF to align QWEN to human preference and thus we have QWEN-CHAT and specifically its improved version QWEN-CHAT-RLHF.`\r\n\r\næŠ€æœ¯æŠ¥å‘Šé‡Œæœ‰æåˆ°`QWEN-CHAT-RLHF`ï¼Œä½†åœ¨ huggingface å’Œ modelscope ä¸Šéƒ½æ²¡æœ‰çœ‹åˆ° RLHF ç›¸å…³çš„æ¨¡å‹ï¼Œæˆ‘ç†è§£ QWEN-CHAT æ¨¡å‹åº”è¯¥åŒ…æ‹¬äº† RLHF é˜¶æ®µçš„è®­ç»ƒï¼Œé‚£ä¹ˆæŠ€æœ¯æŠ¥å‘Šä¸­æåˆ°çš„ Qwen-Chat-RLHF å’Œ Qwen-Chat çš„åŒºåˆ«æ˜¯ä»€ä¹ˆï¼Ÿ",
    "state": "closed",
    "created_at": "2024-08-08T09:14:29+00:00",
    "closed_at": "2024-09-15T08:06:29+00:00",
    "updated_at": "2024-09-15T08:06:29+00:00",
    "author": "Tramac",
    "author_type": "User",
    "comments_count": 1,
    "reactions_count": 0,
    "labels": "inactive",
    "milestone": null,
    "closed_by": "github-actions[bot]",
    "resolution_time_hours": 910.8666666666667,
    "first_comments": [
      {
        "author": "github-actions[bot]",
        "created_at": "2024-09-08T08:06:17+00:00",
        "body": "This issue has been automatically marked as inactive due to lack of recent activity. Should you believe it remains unresolved and warrants attention, kindly leave a comment on this thread.\næ­¤é—®é¢˜ç”±äºé•¿æœŸæœªæœ‰æ–°è¿›å±•è€Œè¢«ç³»ç»Ÿè‡ªåŠ¨æ ‡è®°ä¸ºä¸æ´»è·ƒã€‚å¦‚æœæ‚¨è®¤ä¸ºå®ƒä»æœ‰å¾…è§£å†³ï¼Œè¯·åœ¨æ­¤å¸–ä¸‹æ–¹ç•™è¨€ä»¥è¡¥å……ä¿¡æ¯ã€‚"
      }
    ],
    "url": "https://github.com/QwenLM/Qwen/issues/1310"
  },
  {
    "repository": "QwenLM/Qwen",
    "issue_number": 1309,
    "title": "å®˜æ–¹æ¨ç†è„šæœ¬å’Œæ¨¡å‹æ–‡ä»¶ä¸­çš„pad_tokenä¸ä¸€è‡´",
    "body": "è¯·é—®å®˜æ–¹çš„æ¨ç†è„šæœ¬é‡Œçš„pad_tokenæ˜¯'<|extra_0|>'ï¼Œæ¨¡å‹æ–‡ä»¶ä¸­çš„pad_tokenæ˜¯\"<|endoftext|>\"ï¼Œè·‘æ¨ç†æ—¶æ˜¯å¦éœ€è¦å¯¹é½ï¼Ÿ\r\n![image](https://github.com/user-attachments/assets/8eb71d56-e15d-47e2-9328-75db8ffa01c6)\r\n![image](https://github.com/user-attachments/assets/0891b80c-6be9-42d6-87cd-7ace3fe86959)",
    "state": "closed",
    "created_at": "2024-08-07T07:47:15+00:00",
    "closed_at": "2024-09-13T08:06:50+00:00",
    "updated_at": "2024-09-13T08:06:50+00:00",
    "author": "DemingCheng",
    "author_type": "User",
    "comments_count": 1,
    "reactions_count": 0,
    "labels": "inactive",
    "milestone": null,
    "closed_by": "github-actions[bot]",
    "resolution_time_hours": 888.3263888888889,
    "first_comments": [
      {
        "author": "github-actions[bot]",
        "created_at": "2024-09-06T08:06:39+00:00",
        "body": "This issue has been automatically marked as inactive due to lack of recent activity. Should you believe it remains unresolved and warrants attention, kindly leave a comment on this thread.\næ­¤é—®é¢˜ç”±äºé•¿æœŸæœªæœ‰æ–°è¿›å±•è€Œè¢«ç³»ç»Ÿè‡ªåŠ¨æ ‡è®°ä¸ºä¸æ´»è·ƒã€‚å¦‚æœæ‚¨è®¤ä¸ºå®ƒä»æœ‰å¾…è§£å†³ï¼Œè¯·åœ¨æ­¤å¸–ä¸‹æ–¹ç•™è¨€ä»¥è¡¥å……ä¿¡æ¯ã€‚"
      }
    ],
    "url": "https://github.com/QwenLM/Qwen/issues/1309"
  },
  {
    "repository": "QwenLM/Qwen",
    "issue_number": 1308,
    "title": "[BUG] <title> è¯·é—®QWenLMHeadModelä¸­çš„QWenModelæ¨¡å—æ˜¯å¤„ç†æ–‡æœ¬ä¿¡æ¯å—ï¼Ÿ",
    "body": "### æ˜¯å¦å·²æœ‰å…³äºè¯¥é”™è¯¯çš„issueæˆ–è®¨è®ºï¼Ÿ | Is there an existing issue / discussion for this?\n\n- [X] æˆ‘å·²ç»æœç´¢è¿‡å·²æœ‰çš„issueså’Œè®¨è®º | I have searched the existing issues / discussions\n\n### è¯¥é—®é¢˜æ˜¯å¦åœ¨FAQä¸­æœ‰è§£ç­”ï¼Ÿ | Is there an existing answer for this in FAQ?\n\n- [X] æˆ‘å·²ç»æœç´¢è¿‡FAQ | I have searched FAQ\n\n### å½“å‰è¡Œä¸º | Current Behavior\n\n_No response_\n\n### æœŸæœ›è¡Œä¸º | Expected Behavior\n\n_No response_\n\n### å¤ç°æ–¹æ³• | Steps To Reproduce\n\n_No response_\n\n### è¿è¡Œç¯å¢ƒ | Environment\n\n```Markdown\n- OS:\r\n- Python:\r\n- Transformers:\r\n- PyTorch:\r\n- CUDA (`python -c 'import torch; print(torch.version.cuda)'`):\n```\n\n\n### å¤‡æ³¨ | Anything else?\n\nè¯·é—®QWenLMHeadModelä¸­çš„QWenModelæ¨¡å—æ˜¯å¤„ç†æ–‡æœ¬ä¿¡æ¯å—ï¼Ÿ",
    "state": "closed",
    "created_at": "2024-08-03T15:03:09+00:00",
    "closed_at": "2024-09-10T08:07:06+00:00",
    "updated_at": "2024-09-10T08:07:06+00:00",
    "author": "a1wj1",
    "author_type": "User",
    "comments_count": 1,
    "reactions_count": 0,
    "labels": "inactive",
    "milestone": null,
    "closed_by": "github-actions[bot]",
    "resolution_time_hours": 905.0658333333333,
    "first_comments": [
      {
        "author": "github-actions[bot]",
        "created_at": "2024-09-03T08:07:01+00:00",
        "body": "This issue has been automatically marked as inactive due to lack of recent activity. Should you believe it remains unresolved and warrants attention, kindly leave a comment on this thread.\næ­¤é—®é¢˜ç”±äºé•¿æœŸæœªæœ‰æ–°è¿›å±•è€Œè¢«ç³»ç»Ÿè‡ªåŠ¨æ ‡è®°ä¸ºä¸æ´»è·ƒã€‚å¦‚æœæ‚¨è®¤ä¸ºå®ƒä»æœ‰å¾…è§£å†³ï¼Œè¯·åœ¨æ­¤å¸–ä¸‹æ–¹ç•™è¨€ä»¥è¡¥å……ä¿¡æ¯ã€‚"
      }
    ],
    "url": "https://github.com/QwenLM/Qwen/issues/1308"
  },
  {
    "repository": "QwenLM/Qwen",
    "issue_number": 1307,
    "title": "[BUG] <title> model_max_length 32768 not work",
    "body": "### æ˜¯å¦å·²æœ‰å…³äºè¯¥é”™è¯¯çš„issueæˆ–è®¨è®ºï¼Ÿ | Is there an existing issue / discussion for this?\r\n\r\n- [X] æˆ‘å·²ç»æœç´¢è¿‡å·²æœ‰çš„issueså’Œè®¨è®º | I have searched the existing issues / discussions\r\n\r\n### è¯¥é—®é¢˜æ˜¯å¦åœ¨FAQä¸­æœ‰è§£ç­”ï¼Ÿ | Is there an existing answer for this in FAQ?\r\n\r\n- [X] æˆ‘å·²ç»æœç´¢è¿‡FAQ | I have searched FAQ\r\n\r\n### å½“å‰è¡Œä¸º | Current Behavior\r\n\r\nTraceback (most recent call last):\r\n[rank0]:   File \"â€¦â€¦site-packages/transformers/tokenization_utils_base.py\", line 762, in convert_to_tensors\r\n[rank0]:     tensor = as_tensor(value)\r\n[rank0]:   File \"â€¦â€¦site-packages/transformers/tokenization_utils_base.py\", line 724, in as_tensor\r\n[rank0]:     return torch.tensor(value)\r\n[rank0]: ValueError: expected sequence of length 8034 at dim 1 (got 14276)\r\n\r\n### æœŸæœ›è¡Œä¸º | Expected Behavior\r\n\r\næœŸæœ›å¯ä»¥æ”¯æŒ 32K çš„é•¿åº¦ï¼Œæˆ–è€…è‡ªåŠ¨æˆªæ–­\r\n\r\n### å¤ç°æ–¹æ³• | Steps To Reproduce\r\n\r\nfinetune.py --model_max_length 32768 --use_lora True\r\n\r\n### è¿è¡Œç¯å¢ƒ | Environment\r\n\r\n```Markdown\r\n- OS: Ubuntu 20.04\r\n- Python: 3.8.19\r\n- Transformers: 4.42.3\r\n- PyTorch: 2.3.0\r\n- CUDA 12.1\r\n```\r\n\r\n\r\n### å¤‡æ³¨ | Anything else?\r\n\r\n_No response_",
    "state": "closed",
    "created_at": "2024-08-01T08:09:09+00:00",
    "closed_at": "2024-09-08T08:06:19+00:00",
    "updated_at": "2024-09-08T08:06:19+00:00",
    "author": "chansonzhang",
    "author_type": "User",
    "comments_count": 4,
    "reactions_count": 0,
    "labels": "inactive",
    "milestone": null,
    "closed_by": "github-actions[bot]",
    "resolution_time_hours": 911.9527777777778,
    "first_comments": [
      {
        "author": "chansonzhang",
        "created_at": "2024-08-01T08:20:49+00:00",
        "body": "according to [README](https://github.com/QwenLM/Qwen/blob/main/README.md), Qwen-7B does support 32K Max Length.\r\n![image](https://github.com/user-attachments/assets/6652569c-e872-4e67-9eed-a598682c64cb)\r\n"
      },
      {
        "author": "chansonzhang",
        "created_at": "2024-08-02T02:35:56+00:00",
        "body": "I noticed input_ids are padded to the max_length in batch, but labels are not. \r\nThe checkpoint is at `site-packages\\transformers\\tokenization_utils_base.py`, line 3450\r\n\r\n![image](https://github.com/user-attachments/assets/e9e74430-24c1-4492-8c1f-3d1f3d64cd99)\r\n![image](https://github.com/user-attachments/assets/848dd737-0608-4c10-a536-b82763d53246)\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"C:\\ProgramData\\mambaforge\\envs\\py38\\lib\\site-packages\\transformers\\tokenization_utils_base.py\", line 762, in convert_to_tensors\r\n    tensor = as_tensor(value)\r\n  File \"C:\\ProgramData\\mambaforge\\envs\\py38\\lib\\site-packages\\transformers\\tokenization_utils_base.py\", line 724, in as_tensor\r\n    return torch.tensor(value)\r\nValueError: expected sequence of length 6035 at dim 1 (got 6977)\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\ProgramData\\mambaforge\\envs\\py38\\lib\\site-packages\\accelerate\\data_loader.py\", line 454, in __iter__\r\n    current_batch = next(dataloader_iter)\r\n  File \"C:\\ProgramData\\mambaforge\\envs\\py38\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 631, in __next__\r\n    data = self._next_data()\r\n  File \"C:\\ProgramData\\mambaforge\\envs\\py38\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 675, in _next_data\r\n    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\r\n  File \"C:\\ProgramData\\mambaforge\\envs\\py38\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 54, in fetch\r\n    return self.collate_fn(data)\r\n  File \"C:\\ProgramData\\mambaforge\\envs\\py38\\lib\\site-packages\\transformers\\data\\data_collator.py\", line 271, in __call__\r\n    batch = pad_without_fast_tokenizer_warning(\r\n  File \"C:\\ProgramData\\mambaforge\\envs\\py38\\lib\\site-packages\\transformers\\data\\data_collator.py\", line 66, in pad_without_fast_tokenizer_warning\r\n    padded = tokenizer.pad(*pad_args, **pad_kwargs)\r\n  File \"C:\\ProgramData\\mambaforge\\envs\\py38\\lib\\site-packages\\transformers\\tokenization_utils_base.py\", line 3450, in pad\r\n    return BatchEncoding(batch_outputs, tensor_type=return_tensors)\r\n  File \"C:\\ProgramData\\mambaforge\\envs\\py38\\lib\\site-packages\\transformers\\tokenization_utils_base.py\", line 227, in __init__\r\n    self.convert_to_tensors(tensor_type=tensor_type, prepend_batch_axis=prepend_batch_axis)\r\n  File \"C:\\ProgramData\\mambaforge\\envs\\py38\\lib\\site-packages\\transformers\\tokenization_utils_base.py\", line 778, in convert_to_tensors\r\n    raise ValueError(\r\nValueError: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`labels` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\r\npython-BaseException\r\n```\r\n\r\nI "
      },
      {
        "author": "chansonzhang",
        "created_at": "2024-08-02T06:29:33+00:00",
        "body": "When I change the model from `Qwen1.5-7B-Chat` to `Qwen2-7B-Instruct`, the same error is still there."
      },
      {
        "author": "github-actions[bot]",
        "created_at": "2024-09-01T08:06:12+00:00",
        "body": "This issue has been automatically marked as inactive due to lack of recent activity. Should you believe it remains unresolved and warrants attention, kindly leave a comment on this thread.\næ­¤é—®é¢˜ç”±äºé•¿æœŸæœªæœ‰æ–°è¿›å±•è€Œè¢«ç³»ç»Ÿè‡ªåŠ¨æ ‡è®°ä¸ºä¸æ´»è·ƒã€‚å¦‚æœæ‚¨è®¤ä¸ºå®ƒä»æœ‰å¾…è§£å†³ï¼Œè¯·åœ¨æ­¤å¸–ä¸‹æ–¹ç•™è¨€ä»¥è¡¥å……ä¿¡æ¯ã€‚"
      }
    ],
    "url": "https://github.com/QwenLM/Qwen/issues/1307"
  },
  {
    "repository": "QwenLM/Qwen",
    "issue_number": 1300,
    "title": "å¤§æ¨¡å‹function callå¯¹æ¯”ä¼ ç»Ÿnlpæ–¹å¼æœ‰ä»€ä¹ˆä¼˜åŠ¿ï¼Ÿ",
    "body": "function callé€šè¿‡è¯†åˆ«è¦è°ƒç”¨çš„å·¥å…·+æŠ½å–å·¥å…·æ‰€éœ€çš„å‚æ•°æ¥å®ç°å·¥å…·è°ƒç”¨ã€‚æƒ³é—®ä¸‹è¿™æ ·åšå¯¹æ¯”ä¼ ç»Ÿçš„æ„å›¾è¯†åˆ«ï¼ˆè°ƒç”¨ä»€ä¹ˆå·¥å…·ï¼‰+ä¿¡æ¯æŠ½å–ï¼ˆå‚æ•°æŠ½å–ï¼‰æœ‰ä»€ä¹ˆä¼˜åŠ¿å‘¢ï¼Ÿéƒ½èƒ½å®ç°ç›¸ä¼¼çš„åŠŸèƒ½ã€‚",
    "state": "closed",
    "created_at": "2024-07-15T10:31:52+00:00",
    "closed_at": "2024-08-31T08:05:52+00:00",
    "updated_at": "2024-08-31T08:05:52+00:00",
    "author": "LawlightXY",
    "author_type": "User",
    "comments_count": 2,
    "reactions_count": 0,
    "labels": "inactive",
    "milestone": null,
    "closed_by": "github-actions[bot]",
    "resolution_time_hours": 1125.5666666666666,
    "first_comments": [
      {
        "author": "xueyouluo",
        "created_at": "2024-07-23T08:42:51+00:00",
        "body": "ä¸ªäººæ„Ÿè§‰ï¼š\r\nä¼˜ç‚¹ï¼š\r\n\r\n1. æ¥å…¥æ•ˆç‡é«˜ï¼Œæ— éœ€å»è®­ç»ƒæ¨¡å‹åˆ¤æ–­æ„å›¾å’Œåšæ§½ä½æå–ï¼Œå¯ä»¥æ–¹ä¾¿çš„é€šè¿‡ä¿®æ”¹å·¥å…·æè¿°å’Œç³»ç»Ÿè¯´æ˜æ¥è°ƒæ•´ã€åŠ å‡å·¥å…·ï¼›ï¼ˆå½“ç„¶ï¼Œæ•ˆæœä¸å¥½è¿˜æ˜¯å¾—å¾®è°ƒï¼‰ï¼›\r\n2. å¯èƒ½å¯ä»¥çœæ‰workflowï¼Œæ¨¡å‹è‡ªå·±åˆ¤æ–­éœ€è¦è°ƒç”¨å“ªäº›å·¥å…·ï¼Œæˆ–è€…åªéœ€è¦åœ¨ç³»ç»Ÿæè¿°ä¸­ä½¿ç”¨è‡ªç„¶è¯­è¨€æè¿°ï¼›\r\n3. è¯­ä¹‰ç†è§£æ›´å¥½ï¼Œæ¯”å¦‚ç»å…¸caseï¼šå¸®æˆ‘æŸ¥é™„è¿‘çš„ç¾é£Ÿï¼Œä¸è¦æ—¥æ–™ï¼ŒåŸºäºæ„å›¾å’Œæ§½ä½å®¹æ˜“ç¿»è½¦ã€‚\r\n4. å¼€å‘ç®€å•ä¸€äº›ï¼Œä¸ç”¨ç»´æŠ¤å¤ªå¤šæ¨¡å—\r\n\r\nç¼ºç‚¹ï¼š\r\n\r\n1. é»‘ç›’ï¼Œä½ ä¹Ÿä¸çŸ¥é“å®ƒä»€ä¹ˆæ—¶å€™ä¼šè°ƒç”¨å·¥å…·ï¼Œä»€ä¹ˆæ—¶å€™ä¸è°ƒç”¨ï¼Œæˆ–è€…ä»€ä¹ˆæ—¶å€™åœæ­¢è°ƒç”¨\r\n2. æ…¢ï¼Œå¦‚æœæ˜¯å¤šæ­¥éª¤ï¼Œè¿˜å¾—è°ƒå¤šæ¬¡\r\n3. å¦‚æœæ¨¡å‹èƒ½åŠ›ä¸è¡Œï¼Œå¬ä¸æ‡‚æŒ‡ä»¤ï¼Œæˆ–è€…ä¸šåŠ¡æœ‰äº›å¤æ‚é€»è¾‘ï¼Œæ¨¡å‹ç†è§£ä¸äº†å°±å¾ˆéš¾å—ï¼Œåªèƒ½é å¾®è°ƒï¼Œä»£ä»·å¤§"
      },
      {
        "author": "github-actions[bot]",
        "created_at": "2024-08-23T08:07:23+00:00",
        "body": "This issue has been automatically marked as inactive due to lack of recent activity. Should you believe it remains unresolved and warrants attention, kindly leave a comment on this thread.\næ­¤é—®é¢˜ç”±äºé•¿æœŸæœªæœ‰æ–°è¿›å±•è€Œè¢«ç³»ç»Ÿè‡ªåŠ¨æ ‡è®°ä¸ºä¸æ´»è·ƒã€‚å¦‚æœæ‚¨è®¤ä¸ºå®ƒä»æœ‰å¾…è§£å†³ï¼Œè¯·åœ¨æ­¤å¸–ä¸‹æ–¹ç•™è¨€ä»¥è¡¥å……ä¿¡æ¯ã€‚"
      }
    ],
    "url": "https://github.com/QwenLM/Qwen/issues/1300"
  },
  {
    "repository": "QwenLM/Qwen",
    "issue_number": 1299,
    "title": "[BUG] <title>Qwen2-7b-instructä½¿ç”¨SFT-FTï¼Œlosså˜ä¸º0ï¼Œå¦‚ä½•è§£å†³ï¼Ÿ",
    "body": "### æ˜¯å¦å·²æœ‰å…³äºè¯¥é”™è¯¯çš„issueæˆ–è®¨è®ºï¼Ÿ | Is there an existing issue / discussion for this?\r\n\r\n- [X] æˆ‘å·²ç»æœç´¢è¿‡å·²æœ‰çš„issueså’Œè®¨è®º | I have searched the existing issues / discussions\r\n\r\n### è¯¥é—®é¢˜æ˜¯å¦åœ¨FAQä¸­æœ‰è§£ç­”ï¼Ÿ | Is there an existing answer for this in FAQ?\r\n\r\n- [X] æˆ‘å·²ç»æœç´¢è¿‡FAQ | I have searched FAQ\r\n\r\n### å½“å‰è¡Œä¸º | Current Behavior\r\n\r\næˆ‘çš„è®­ç»ƒå‚æ•°å¦‚ä¸‹ï¼š\r\nexport CUDA_VISIBLE_DEVICES=\"1,6,7\" && DS_SKIP_CUDA_CHECK=1 OMP_NUM_THREADS=8 torchrun --standalone --nproc_per_node=gpu train_qwen.py\r\n--model_name_or_path $model_path\r\n--deepspeed $deepspeed_config\r\n--data_path $data_path\r\n--bf16 True\r\n--output_dir $output_path\r\n--num_train_epochs 1\r\n--max_steps 376\r\n--per_device_train_batch_size 1\r\n--gradient_accumulation_steps 16\r\n--evaluation_strategy \"no\"\r\n--save_strategy \"steps\"\r\n--save_steps 47\r\n--save_total_limit 1\r\n--learning_rate 1e-5\r\n--weight_decay 0.1\r\n--adam_beta2 0.95\r\n--warmup_ratio 0.01\r\n--lr_scheduler_type \"cosine\"\r\n--logging_steps 8\r\n--model_max_length 8192\r\n--gradient_checkpointing True\r\n--lazy_preprocess False\r\n--iterable True\r\n--tf32 0\r\n\r\nå¦‚ä¸‹æ˜¯æˆ‘çš„è¿è¡Œæ—¥å¿—ï¼Œç¬¬äºŒæ­¥losså°±å˜ä¸º0äº†ã€‚ä¸çŸ¥é“é—®é¢˜æ˜¯ä»€ä¹ˆ\r\n{'loss': 1.7328, 'grad_norm': 1.9622441768332994e-05, 'learning_rate': 9.919354838709679e-06, 'epoch': 0.02}\r\n{'loss': 0.0, 'grad_norm': 3.913150697629098e-06, 'learning_rate': 9.704301075268819e-06, 'epoch': 0.04}\r\n4%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 16/376 [17:36<6:36:00, 66.00s/it]\r\n\r\n\r\n\r\n### æœŸæœ›è¡Œä¸º | Expected Behavior\r\n\r\nèƒ½å¤Ÿæ­£å¸¸è®­ç»ƒ\r\n\r\n### å¤ç°æ–¹æ³• | Steps To Reproduce\r\n\r\nä½¿ç”¨çš„fscahtå¾®è°ƒä»£ç ï¼Œchaglmå¯æ­£å¸¸è®­ç»ƒï¼ŒQwenç³»åˆ—æ¨¡å‹æ— æ³•æ­£å¸¸è®­ç»ƒï¼Œç‰¹æ®Štokenä½¿ç”¨çš„æ˜¯qwenç³»åˆ—çš„<|im_start|>ã€<|im_end|>\r\n\r\n### è¿è¡Œç¯å¢ƒ | Environment\r\n\r\n```Markdown\r\n- OS: ubuntu20.04\r\n- Python:3.10\r\n- Transformers: 4.34\r\n- PyTorch: 2.3\r\n- CUDA (`python -c 'import torch; print(torch.version.cuda)'`):  12.2\r\n```\r\n\r\n\r\n### å¤‡æ³¨ | Anything else?\r\n\r\nä½¿ç”¨Qwen2å¾®è°ƒä¹Ÿä¸€æ ·çš„ç°è±¡ã€‚",
    "state": "closed",
    "created_at": "2024-07-15T05:27:51+00:00",
    "closed_at": "2024-09-01T08:06:13+00:00",
    "updated_at": "2024-09-01T08:06:13+00:00",
    "author": "ming-shy",
    "author_type": "User",
    "comments_count": 2,
    "reactions_count": 0,
    "labels": "inactive",
    "milestone": null,
    "closed_by": "github-actions[bot]",
    "resolution_time_hours": 1154.6394444444445,
    "first_comments": [
      {
        "author": "jesuswa",
        "created_at": "2024-07-26T06:29:16+00:00",
        "body": "ä½ è¿™ä¸ªé—®é¢˜æ˜¯æ€ä¹ˆè§£å†³çš„\r\n"
      },
      {
        "author": "github-actions[bot]",
        "created_at": "2024-08-25T08:06:12+00:00",
        "body": "This issue has been automatically marked as inactive due to lack of recent activity. Should you believe it remains unresolved and warrants attention, kindly leave a comment on this thread.\næ­¤é—®é¢˜ç”±äºé•¿æœŸæœªæœ‰æ–°è¿›å±•è€Œè¢«ç³»ç»Ÿè‡ªåŠ¨æ ‡è®°ä¸ºä¸æ´»è·ƒã€‚å¦‚æœæ‚¨è®¤ä¸ºå®ƒä»æœ‰å¾…è§£å†³ï¼Œè¯·åœ¨æ­¤å¸–ä¸‹æ–¹ç•™è¨€ä»¥è¡¥å……ä¿¡æ¯ã€‚"
      }
    ],
    "url": "https://github.com/QwenLM/Qwen/issues/1299"
  },
  {
    "repository": "QwenLM/Qwen",
    "issue_number": 1294,
    "title": "Qwen çš„å¼€æºæ¨¡å‹èƒ½è¾“å‡º logprobså—ï¼Ÿ",
    "body": "### æ˜¯å¦å·²æœ‰å…³äºè¯¥é”™è¯¯çš„issueæˆ–è®¨è®ºï¼Ÿ | Is there an existing issue / discussion for this?\n\n- [X] æˆ‘å·²ç»æœç´¢è¿‡å·²æœ‰çš„issueså’Œè®¨è®º | I have searched the existing issues / discussions\n\n### è¯¥é—®é¢˜æ˜¯å¦åœ¨FAQä¸­æœ‰è§£ç­”ï¼Ÿ | Is there an existing answer for this in FAQ?\n\n- [X] æˆ‘å·²ç»æœç´¢è¿‡FAQ | I have searched FAQ\n\n### å½“å‰è¡Œä¸º | Current Behavior\n\n_No response_\n\n### æœŸæœ›è¡Œä¸º | Expected Behavior\n\n_No response_\n\n### å¤ç°æ–¹æ³• | Steps To Reproduce\n\n_No response_\n\n### è¿è¡Œç¯å¢ƒ | Environment\n\n```Markdown\n- OS:\r\n- Python:\r\n- Transformers:\r\n- PyTorch:\r\n- CUDA (`python -c 'import torch; print(torch.version.cuda)'`):\n```\n\n\n### å¤‡æ³¨ | Anything else?\n\n_No response_",
    "state": "closed",
    "created_at": "2024-06-27T07:33:43+00:00",
    "closed_at": "2024-08-04T08:06:00+00:00",
    "updated_at": "2024-08-16T14:04:28+00:00",
    "author": "Miracle1207",
    "author_type": "User",
    "comments_count": 3,
    "reactions_count": 0,
    "labels": "inactive",
    "milestone": null,
    "closed_by": "github-actions[bot]",
    "resolution_time_hours": 912.5380555555555,
    "first_comments": [
      {
        "author": "github-actions[bot]",
        "created_at": "2024-07-27T08:05:33+00:00",
        "body": "This issue has been automatically marked as inactive due to lack of recent activity. Should you believe it remains unresolved and warrants attention, kindly leave a comment on this thread.\næ­¤é—®é¢˜ç”±äºé•¿æœŸæœªæœ‰æ–°è¿›å±•è€Œè¢«ç³»ç»Ÿè‡ªåŠ¨æ ‡è®°ä¸ºä¸æ´»è·ƒã€‚å¦‚æœæ‚¨è®¤ä¸ºå®ƒä»æœ‰å¾…è§£å†³ï¼Œè¯·åœ¨æ­¤å¸–ä¸‹æ–¹ç•™è¨€ä»¥è¡¥å……ä¿¡æ¯ã€‚"
      },
      {
        "author": "Artessay",
        "created_at": "2024-08-07T16:20:40+00:00",
        "body": "è°ƒç”¨dashscopeçš„qwen-turboï¼Œqwen-plusç­‰æ¨¡å‹æ—¶ï¼Œå¯ä»¥æ­£å¸¸è·å–logprobï¼Œä½†æ˜¯è°ƒç”¨qwen2-7b-instructç­‰æ¨¡å‹æ—¶å´æ— æ³•è·å–ã€‚æ˜¯ä¸æ˜¯å¼€æºçš„æ¨¡å‹æ²¡æœ‰æä¾›ç›¸å…³è¾“å‡ºå‘¢ï¼Ÿ"
      },
      {
        "author": "bigbrother001",
        "created_at": "2024-08-16T14:04:27+00:00",
        "body": "æˆ‘ä¹Ÿæƒ³çŸ¥é“æ€ä¹ˆè·å¾—logits\r\n"
      }
    ],
    "url": "https://github.com/QwenLM/Qwen/issues/1294"
  }
]