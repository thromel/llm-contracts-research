[
  {
    "repository": "NVIDIA/NeMo",
    "issue_number": 11943,
    "title": "NeMo Git tag for patching a nvcr.io/nvidia/nemo:24.07 based Docker container",
    "body": "I am building a custom Docker container based off of nvcr.io/nvidia/nemo:24.07 which needs to add a patch to the NeMo/Megatron code. I do not see a matching git tag to ensure the NeMo code I am modifying is the same code version in my working copy as the nvcr.io/nvidia/nemo:24.07 container.\n\nCan you suggest a git tag I can use, or alternative approach for modifying the container NeMo code?\n\nMy desired workflow is something like this:\n\nLocal working dir:\n```\ngit clone --depth 1 --branch <tag matching 24.07> https://github.com/NVIDIA/NeMo.git\n# modify code locally\ngit diff > nemo.patch\n```\n\nDockerfile:\n```\nFROM nvcr.io/nvidia/nemo:24.07\nWORKDIR /workspace\n\n# Copy the NeMo git repo.\nRUN git clone --depth 1 --branch <tag matching 24.07> https://github.com/NVIDIA/NeMo.git\nWORKDIR NeMo\n\n# Apply patch. \nCOPY nemo.patch nemo.patch\nRUN git apply nemo.patch\n\nRUN cp ./path/to/modified/file /opt/NeMo/path/to/modified/file\n```",
    "state": "closed",
    "created_at": "2025-01-23T19:18:14+00:00",
    "closed_at": "2025-01-24T19:15:34+00:00",
    "updated_at": "2025-01-24T19:15:35+00:00",
    "author": "awonak",
    "author_type": "User",
    "comments_count": 1,
    "reactions_count": 0,
    "labels": "",
    "milestone": null,
    "closed_by": "awonak",
    "resolution_time_hours": 23.955555555555556,
    "first_comments": [
      {
        "author": "awonak",
        "created_at": "2025-01-24T19:15:34+00:00",
        "body": "I realized that the git metadata for NeMo exists in the docker container, so I was able to get the commit hash of the most recent commit with the following:\n\n```\nroot@cd6822df526b:/opt/NeMo# git log --pretty=format:\"%H\" -n1\ne033481e26e6ae32764d3e2b3f16afed00dc7218\n```"
      }
    ],
    "url": "https://github.com/NVIDIA/NeMo/issues/11943"
  },
  {
    "repository": "NVIDIA/NeMo",
    "issue_number": 11871,
    "title": "megatron version requirement",
    "body": "```\nTraceback (most recent call last):\n  File \"/user/panyinxu/workspace/NeMo-Aligner/examples/nlp/minicpm/train_minicpm3_4b_longrope_dpo.py\", line 164, in <module>\n    main()\n  File \"/user/panyinxu/workspace/NeMo/nemo/core/config/hydra_runner.py\", line 129, in wrapper\n    _run_hydra(\n  File \"/usr/local/lib/python3.10/dist-packages/hydra/_internal/utils.py\", line 394, in _run_hydra\n    _run_app(\n  File \"/usr/local/lib/python3.10/dist-packages/hydra/_internal/utils.py\", line 457, in _run_app\n    run_and_report(\n  File \"/usr/local/lib/python3.10/dist-packages/hydra/_internal/utils.py\", line 223, in run_and_report\n    raise ex\n  File \"/usr/local/lib/python3.10/dist-packages/hydra/_internal/utils.py\", line 220, in run_and_report\n    return func()\n  File \"/usr/local/lib/python3.10/dist-packages/hydra/_internal/utils.py\", line 458, in <lambda>\n    lambda: hydra.run(\n  File \"/usr/local/lib/python3.10/dist-packages/hydra/_internal/hydra.py\", line 132, in run\n    _ = ret.return_value\n  File \"/usr/local/lib/python3.10/dist-packages/hydra/core/utils.py\", line 260, in return_value\n    raise self._return_value\n  File \"/usr/local/lib/python3.10/dist-packages/hydra/core/utils.py\", line 186, in run_job\n    ret.return_value = task_function(task_cfg)\n  File \"/user/panyinxu/workspace/NeMo-Aligner/examples/nlp/minicpm/train_minicpm3_4b_longrope_dpo.py\", line 56, in main\n    ptl_model = load_from_nemo(\n  File \"/user/panyinxu/workspace/NeMo-Aligner/nemo_aligner/utils/utils.py\", line 118, in load_from_nemo\n    model = cls.restore_from(\n  File \"/user/panyinxu/workspace/NeMo/nemo/collections/nlp/models/nlp_model.py\", line 493, in restore_from\n    return super().restore_from(\n  File \"/user/panyinxu/workspace/NeMo/nemo/core/classes/modelPT.py\", line 474, in restore_from\n    instance = cls._save_restore_connector.restore_from(\n  File \"/user/panyinxu/workspace/NeMo-Aligner/nemo_aligner/utils/utils.py\", line 56, in restore_from\n    return super().restore_from(*args, replace_sharded_tensor_key=self.__replace_sharded_tensor_key, **kwargs)\n  File \"/user/panyinxu/workspace/NeMo/nemo/collections/nlp/parts/nlp_overrides.py\", line 1304, in restore_from\n    loaded_params = super().load_config_and_state_dict(\n  File \"/user/panyinxu/workspace/NeMo/nemo/core/connectors/save_restore_connector.py\", line 182, in load_config_and_state_dict\n    instance = calling_cls.from_config_dict(config=conf, trainer=trainer)\n  File \"/user/panyinxu/workspace/NeMo/nemo/core/classes/common.py\", line 530, in from_config_dict\n    raise e\n  File \"/user/panyinxu/workspace/NeMo/nemo/core/classes/common.py\", line 522, in from_config_dict\n    instance = cls(cfg=config, trainer=trainer)\n  File \"/user/panyinxu/workspace/NeMo-Aligner/nemo_aligner/models/nlp/gpt/megatron_gpt_dpo_model.py\", line 54, in __init__\n    super().__init__(cfg, trainer=trainer)\n  File \"/user/panyinxu/workspace/NeMo/nemo/collections/nlp/parts/mixins/nlp_adapter_mixins.py\", line 88, in __init__\n    super().__init__(*args, **kwargs)\n  File \"/user/panyinxu/workspace/NeMo/nemo/collections/nlp/models/language_modeling/megatron_gpt_model.py\", line 402, in __init__\n    self.model = build_model(\n  File \"/user/panyinxu/workspace/NeMo/nemo/collections/nlp/modules/common/megatron/build_model.py\", line 90, in build_model\n    model = model_provider_func(\n  File \"/user/panyinxu/workspace/NeMo/nemo/collections/nlp/models/language_modeling/megatron_gpt_model.py\", line 484, in model_provider_func\n    transformer_layer_spec=get_specs(\n  File \"/user/panyinxu/workspace/NeMo/nemo/collections/nlp/models/language_modeling/megatron_gpt_model.py\", line 172, in get_specs\n    \"modelopt\": get_gpt_layer_modelopt_spec(num_experts),\n  File \"/user/panyinxu/workspace/NeMo/nemo/collections/nlp/models/language_modeling/megatron/gpt_layer_modelopt_spec.py\", line 50, in get_gpt_layer_modelopt_spec\n    raise IMPORT_ERROR\n  File \"/user/panyinxu/workspace/NeMo/nemo/collections/nlp/models/language_modeling/megatron/gpt_layer_modelopt_spec.py\", line 23, in <module>\n    from megatron.core.transformer.moe.moe_layer import MoELayer, MoESubmodules\nImportError: cannot import name 'MoESubmodules' from 'megatron.core.transformer.moe.moe_layer' (/user/panyinxu/workspace/Megatron-LM/megatron/core/transformer/moe/moe_layer.py)\n```\n\nI am currently using NeMo==2.1.0 and NeMo-Aligner==0.6.0. It appears that there is an issue with Megatron version compatibility. Which version of Megatron do I need?",
    "state": "closed",
    "created_at": "2025-01-16T12:55:06+00:00",
    "closed_at": "2025-01-16T13:07:10+00:00",
    "updated_at": "2025-01-16T13:07:10+00:00",
    "author": "Cppowboy",
    "author_type": "User",
    "comments_count": 0,
    "reactions_count": 0,
    "labels": "",
    "milestone": null,
    "closed_by": "Cppowboy",
    "resolution_time_hours": 0.2011111111111111,
    "first_comments": [],
    "url": "https://github.com/NVIDIA/NeMo/issues/11871"
  },
  {
    "repository": "NVIDIA/NeMo",
    "issue_number": 11838,
    "title": "ModuleNotFoundError: No module named 'megatron.core.dist_checkpointing'",
    "body": "**ModuleNotFoundError: No module named 'megatron.core.dist_checkpointing'**\n\nI am getting `ModuleNotFoundError: No module named 'megatron.core.dist_checkpointing'`\n\n\n**Steps/Code to reproduce bug**\n```bash\npython /home/ubuntu/NeMo/scripts/checkpoint_converters/convert_llama_hf_to_nemo.py     --input_name_or_path meta-llama/Llama-3.2-1B-Instruct     --output_path Llama-3.2-1B-Instruct.nemo     --precision=bf16\n\nTraceback (most recent call last):\n  File \"/home/ubuntu/NeMo/scripts/checkpoint_converters/convert_llama_hf_to_nemo.py\", line 34, in <module>\n    from nemo.collections.nlp.models.language_modeling.megatron_gpt_model import MegatronGPTModel\n  File \"/home/ubuntu/NeMo/nemo/collections/nlp/__init__.py\", line 15, in <module>\n    from nemo.collections.nlp import data, losses, models, modules\n  File \"/home/ubuntu/NeMo/nemo/collections/nlp/models/__init__.py\", line 16, in <module>\n    from nemo.collections.nlp.models.duplex_text_normalization import (\n  File \"/home/ubuntu/NeMo/nemo/collections/nlp/models/duplex_text_normalization/__init__.py\", line 15, in <module>\n    from nemo.collections.nlp.models.duplex_text_normalization.duplex_decoder import DuplexDecoderModel\n  File \"/home/ubuntu/NeMo/nemo/collections/nlp/models/duplex_text_normalization/duplex_decoder.py\", line 34, in <module>\n    from nemo.collections.nlp.models.nlp_model import NLPModel\n  File \"/home/ubuntu/NeMo/nemo/collections/nlp/models/nlp_model.py\", line 40, in <module>\n    from nemo.collections.nlp.parts.nlp_overrides import NLPSaveRestoreConnector\n  File \"/home/ubuntu/NeMo/nemo/collections/nlp/parts/nlp_overrides.py\", line 68, in <module>\n    from nemo.collections.nlp.modules.common.megatron.transformer import AutocastTransformerLayer, ParallelTransformerLayer\n  File \"/home/ubuntu/NeMo/nemo/collections/nlp/modules/common/megatron/transformer.py\", line 28, in <module>\n    from nemo.collections.nlp.modules.common.megatron.adapters.parallel_adapters import (\n  File \"/home/ubuntu/NeMo/nemo/collections/nlp/modules/common/megatron/adapters/parallel_adapters.py\", line 27, in <module>\n    from megatron.core.dist_checkpointing.mapping import ShardedStateDict\nModuleNotFoundError: No module named 'megatron.core.dist_checkpointing'\n```\n\n**Expected behavior**\n\nA clear and concise description of what you expected to happen.\n\n**Environment overview (please complete the following information)**\n\n - Environment location: Cloud(LambdaLabs)\n - Method of NeMo install: from source\n\n**Environment details**\n\nIf NVIDIA docker image is used you don't need to specify these.\nOtherwise, please provide:\n- OS version: debian\n- PyTorch version: 2.4.1+cu124\n- Python version: Python 3.10.16\n\n**Additional context**\nGPU model: A100 SXM4\n",
    "state": "closed",
    "created_at": "2025-01-13T17:40:33+00:00",
    "closed_at": "2025-01-13T18:07:29+00:00",
    "updated_at": "2025-01-13T18:07:30+00:00",
    "author": "yonas-g",
    "author_type": "User",
    "comments_count": 1,
    "reactions_count": 0,
    "labels": "bug",
    "milestone": null,
    "closed_by": "yonas-g",
    "resolution_time_hours": 0.4488888888888889,
    "first_comments": [
      {
        "author": "yonas-g",
        "created_at": "2025-01-13T18:07:29+00:00",
        "body": "I found the issue. Instead of installing from `main` branch, it wokred when I reinstall from `r2.*`"
      }
    ],
    "url": "https://github.com/NVIDIA/NeMo/issues/11838"
  },
  {
    "repository": "NVIDIA/NeMo",
    "issue_number": 11836,
    "title": "Creating a hf to nemo converter for phi 3 mini instruct",
    "body": "Hi ,\n\nI am following the guidelines https://docs.nvidia.com/nemo-framework/user-guide/latest/nemotoolkit/checkpoints/dev_guide.html to create a converter. It looks like in addition to qkv reshaping and layer name mapping we need to load phi model into megatron_gpt_model. @cuichenx and @mayani-nv I noticed you have some PRs with phi 3 mini. Would you by any chance have a sample code. I've added code for \"_create_rename_keys_\" and \"_adjust_tensor_shapes_\" based on your code. I am struggling to load it into megatron_gpt_model. \n\nand I suspect its because I need something like https://github.com/NVIDIA/NeMo/blob/7f3ac6b88289048643d445ec752a4d21547f2788/nemo/collections/nlp/models/language_modeling/megatron/gemma2/gemma2_spec.py#L28\n\nIs there someone I could work with to improve the documentation or to simplify the process for this conversion ?\n\nthanks!",
    "state": "closed",
    "created_at": "2025-01-13T15:36:11+00:00",
    "closed_at": "2025-01-14T06:08:53+00:00",
    "updated_at": "2025-01-28T14:17:04+00:00",
    "author": "LopezGG",
    "author_type": "User",
    "comments_count": 9,
    "reactions_count": 0,
    "labels": "",
    "milestone": null,
    "closed_by": "LopezGG",
    "resolution_time_hours": 14.545,
    "first_comments": [
      {
        "author": "mayani-nv",
        "created_at": "2025-01-13T17:07:24+00:00",
        "body": "@LopezGG  the Phi3 mini has already been added [here](https://github.com/NVIDIA/NeMo/blob/main/nemo/collections/llm/gpt/model/phi3mini.py). The steps to do the fine-tuning with Phi3 are also described in the recipe as [well](https://github.com/NVIDIA/NeMo/blob/main/nemo/collections/llm/recipes/phi3_mini_4k_instruct.py). If you look at the first link, the qkv reshaping is performed to load Phi model into the megatron_gpt_model."
      },
      {
        "author": "cuichenx",
        "created_at": "2025-01-13T17:14:00+00:00",
        "body": "@LopezGG To add on to what @mayani-nv has said: Phi 3 mini is supported in NeMo 2.0, and we recommend you switch to 2.0 for new explorations with NeMo. You can find quick start instructions [here](https://docs.nvidia.com/nemo-framework/user-guide/latest/nemo-2.0/quickstart.html).  "
      },
      {
        "author": "mayani-nv",
        "created_at": "2025-01-14T05:03:53+00:00",
        "body": "@LopezGG  a dummy example to use this Phi3 model can be run as following steps\n\n```\n# start the NeMo container interactively\ndocker run -it --rm --gpus all nvcr.io/nvidia/nemo:24.12\n\n# download the ckpt \npython3 download_phi3ckpt.py\n\n# set the NEMO_HOME path\nexport NEMO_HOME=/root/.cache/nemo\n\n# run the inference to ensure parity with the HF format\npython3 phi3_generate.py\n```\nThe scripts used above are [download_phi3ckpkt.py](https://github.com/mayani-nv/NeMo/blob/phi3mini4k/scripts/llm/download_phi3ckpt.py) and [phi3_generate.py](https://github.com/mayani-nv/NeMo/blob/phi3mini4k/scripts/llm/phi3_generate.py)"
      },
      {
        "author": "LopezGG",
        "created_at": "2025-01-14T05:10:34+00:00",
        "body": "amazing. Thank you @mayani-nv. I was just trying to get this done by toying around with HFPhi3Importer & HFPhi3Exporter. This is straightforward"
      },
      {
        "author": "LopezGG",
        "created_at": "2025-01-14T13:39:00+00:00",
        "body": "@mayani-nv : I got my checkpoint but when I finetune I see that is missing some metadata\n\n<img width=\"635\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/584941fb-f1a3-4546-b2be-6f56401a0492\" />\n\nWhat would be a good way to debug this. I am attaching all the files except weights in case that might be helpful\n\n[nemo_phi_instruct.zip](https://github.com/user-attachments/files/18411032/nemo_phi_instruct.zip)"
      }
    ],
    "url": "https://github.com/NVIDIA/NeMo/issues/11836"
  },
  {
    "repository": "NVIDIA/NeMo",
    "issue_number": 11835,
    "title": "Annoyed NeMo User Rant",
    "body": "I have been using NeMo for the last 3 years mainly for speech. While the speech part is okay to use, the NLP/LLM side is an absolute hell. There has never been a single time that I was able to install without an issue.\n\nYou try to install NeMo then mamba-ssm fails, you spend another 3 hrs trying to resolve it, you give up and delete your server for a fresh installation to end up with the same issue. Even if you solve this with the grace of God, next you need to figure out the issue with Megatron, then transformer-engine then apex. Endless non stop horrible experience. :(\n",
    "state": "closed",
    "created_at": "2025-01-13T13:21:18+00:00",
    "closed_at": "2025-01-13T14:46:23+00:00",
    "updated_at": "2025-01-13T14:46:23+00:00",
    "author": "yonas-g",
    "author_type": "User",
    "comments_count": 0,
    "reactions_count": 0,
    "labels": "",
    "milestone": null,
    "closed_by": "yonas-g",
    "resolution_time_hours": 1.4180555555555556,
    "first_comments": [],
    "url": "https://github.com/NVIDIA/NeMo/issues/11835"
  },
  {
    "repository": "NVIDIA/NeMo",
    "issue_number": 11800,
    "title": "Error occurs when i try to pack my training.jsonl",
    "body": "**Describe the bug**\nWhen i follow this tutorial [https://docs.nvidia.com/nemo-framework/user-guide/latest/nemotoolkit/features/optimizations/sequence_packing.html#](url) with the training.jsonl generated by [https://docs.nvidia.com/nemo-framework/user-guide/24.07/playbooks/llama2sft.html](url) in Step 3.\n\nA clear and concise description of what the bug is.\n```shell\nroot@604d29b05e86:/opt/NeMo# export HYDRA_FULL_ERROR=1\nroot@604d29b05e86:/opt/NeMo# python scripts/nlp_language_modeling/prepare_packed_ft_dataset.py model.da\nta.train_ds.file_names=[/workspace/data/training.jsonl] model.data.train_ds.max_seq_length=2048 +tokeni\nzer_path=/workspace/models/llama2-7b/tokenizer.model +output_dir=/workspace/data +pack_sizes=[2024,4096\n,8192] +packing_algorithm=first_fit_shuffle +seed=0\n[NeMo W 2025-01-09 11:02:07 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n    See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n      ret = run_job(\n    \n[NeMo I 2025-01-09 11:02:07 prepare_packed_ft_dataset:82] Tokenizing dataset...\n[NeMo I 2025-01-09 11:02:07 tokenizer_utils:196] Getting SentencePiece with model: /workspace/models/llama2-7b/tokenizer.model\n[NeMo I 2025-01-09 11:02:07 text_memmap_dataset:116] Building data files\n[NeMo I 2025-01-09 11:02:07 text_memmap_dataset:525] Processing 1 data files using 2 workers\n[NeMo I 2025-01-09 11:02:07 text_memmap_dataset:535] Time building 0 / 1 mem-mapped files: 0:00:00.121457\n[NeMo I 2025-01-09 11:02:07 text_memmap_dataset:158] Loading data files\n[NeMo I 2025-01-09 11:02:07 text_memmap_dataset:249] Loading /workspace/data/training.jsonl\n[NeMo I 2025-01-09 11:02:07 text_memmap_dataset:161] Time loading 1 mem-mapped files: 0:00:00.003980\n[NeMo I 2025-01-09 11:02:07 text_memmap_dataset:165] Computing global indices\n[NeMo W 2025-01-09 11:02:08 gpt_sft_dataset:324] input is not long enough to truncate.\n[NeMo W 2025-01-09 11:02:09 gpt_sft_dataset:324] input is not long enough to truncate.\n[NeMo W 2025-01-09 11:02:09 gpt_sft_dataset:324] input is not long enough to truncate.\n[NeMo W 2025-01-09 11:02:10 gpt_sft_dataset:324] input is not long enough to truncate.\n[NeMo W 2025-01-09 11:02:12 gpt_sft_dataset:324] input is not long enough to truncate.\n[NeMo W 2025-01-09 11:02:13 gpt_sft_dataset:324] input is not long enough to truncate.\n[NeMo I 2025-01-09 11:02:14 sequence_packing_utils:115] Creating histogram from tokenized dataset...\n[NeMo I 2025-01-09 11:02:14 sequence_packing_utils:155] Packing sequences to length 2024...\n[NeMo I 2025-01-09 11:02:15 sequence_packing_utils:168] Packing is 99.79% efficient\n[NeMo I 2025-01-09 11:02:15 sequence_packing_utils:169] >>>>> For pack size 2024, average number of sequences per pack is n = 10.168 <<<<<\n100%|████████████████████████████████████████████████████████████| 2025/2025 [00:00<00:00, 3751.49it/s]\n  6%|███▍                                                         | 67/1181 [00:00<00:00, 11882.38it/s]\nError executing job with overrides: ['model.data.train_ds.file_names=[/workspace/data/training.jsonl]', 'model.data.train_ds.max_seq_length=2048', '+tokenizer_path=/workspace/models/llama2-7b/tokenizer.model', '+output_dir=/workspace/data', '+pack_sizes=[2024,4096,8192]', '+packing_algorithm=first_fit_shuffle', '+seed=0']\nTraceback (most recent call last):\n  File \"/opt/NeMo/scripts/nlp_language_modeling/prepare_packed_ft_dataset.py\", line 174, in <module>\n    main()\n  File \"/opt/NeMo/nemo/core/config/hydra_runner.py\", line 129, in wrapper\n    _run_hydra(\n  File \"/usr/local/lib/python3.10/dist-packages/hydra/_internal/utils.py\", line 394, in _run_hydra\n    _run_app(\n  File \"/usr/local/lib/python3.10/dist-packages/hydra/_internal/utils.py\", line 457, in _run_app\n    run_and_report(\n  File \"/usr/local/lib/python3.10/dist-packages/hydra/_internal/utils.py\", line 223, in run_and_report\n    raise ex\n  File \"/usr/local/lib/python3.10/dist-packages/hydra/_internal/utils.py\", line 220, in run_and_report\n    return func()\n  File \"/usr/local/lib/python3.10/dist-packages/hydra/_internal/utils.py\", line 458, in <lambda>\n    lambda: hydra.run(\n  File \"/usr/local/lib/python3.10/dist-packages/hydra/_internal/hydra.py\", line 132, in run\n    _ = ret.return_value\n  File \"/usr/local/lib/python3.10/dist-packages/hydra/core/utils.py\", line 260, in return_value\n    raise self._return_value\n  File \"/usr/local/lib/python3.10/dist-packages/hydra/core/utils.py\", line 186, in run_job\n    ret.return_value = task_function(task_cfg)\n  File \"/opt/NeMo/scripts/nlp_language_modeling/prepare_packed_ft_dataset.py\", line 142, in main\n    output_data = fill_packing_strategy(assignments, sequences, pack_size)\n  File \"/opt/NeMo/nemo/utils/sequence_packing_utils.py\", line 217, in fill_packing_strategy\n    _input_ids.extend(ifile_handles[seq_length][0].pop())\nKeyError: 2047\n``` \n**Steps/Code to reproduce bug**\nthis is my scripts \n```shell\npython scripts/nlp_language_modeling/prepare_packed_ft_\ndataset.py model.data.train_ds.file_names=[/workspace/data/training.jsonl] model.data.train_ds.max_seq_\nlength=2048 +tokenizer_path=/workspace/models/llama2-7b/tokenizer.model +output_dir=/workspace/data +pa\nck_sizes=[2024,4096,8192] +packing_algorithm=first_fit_shuffle +seed=0\n``` \n**Environment overview (please complete the following information)**\nDocker from here [https://catalog.ngc.nvidia.com/orgs/nvidia/containers/nemo/tags](url) version: nvcr.io/nvidia/nemo:24.05.llama3.1\n",
    "state": "closed",
    "created_at": "2025-01-09T11:03:23+00:00",
    "closed_at": "2025-01-09T11:19:29+00:00",
    "updated_at": "2025-01-09T11:19:29+00:00",
    "author": "Sun2018421",
    "author_type": "User",
    "comments_count": 1,
    "reactions_count": 0,
    "labels": "bug",
    "milestone": null,
    "closed_by": "Sun2018421",
    "resolution_time_hours": 0.2683333333333333,
    "first_comments": [
      {
        "author": "Sun2018421",
        "created_at": "2025-01-09T11:19:22+00:00",
        "body": "Sorry, i make a mistake about the param \"+pack_sizes [2024,...] -> [2048,...]\" "
      }
    ],
    "url": "https://github.com/NVIDIA/NeMo/issues/11800"
  },
  {
    "repository": "NVIDIA/NeMo",
    "issue_number": 11784,
    "title": "NeMo Llama 2 70B Pre-Training 20TB Dataset OOM",
    "body": "**Describe the bug**\n\nNeMo Llama 2 70B Pre-Training 20TB Dataset out-of-memory (CPU memory)\n\n**Steps/Code to reproduce bug**\n\nUsing the default YAML file here: https://github.com/NVIDIA/NeMo-Framework-Launcher/blob/main/launcher_scripts/conf/training/llama/llama2_70b.yaml\n\nThe dataset is the pile dataset replicated 35 times such that its size is 20TB\n\nWe run this on a GKE cluster with 2 A3 Mega nodes\n\n**Expected behavior**\n\nWe expect the training to run successfully with num_workers=4. But this runs into OOM error. Reducing num_workers to 2 worked. But this is problematic because we are using GCSFuse and the latency is high, so num_workers=2 is not able to saturate the training.\n\nSo we want to know if this is a known problem on Nvidia's end.\n\n**Environment overview (please complete the following information)**\n\n - Environment location: GCP GKE\n\n**Environment details**\n\nNeMo 24.07\n\n",
    "state": "closed",
    "created_at": "2025-01-07T22:14:17+00:00",
    "closed_at": "2025-01-14T21:30:54+00:00",
    "updated_at": "2025-01-14T21:30:55+00:00",
    "author": "shengshiqi-google",
    "author_type": "User",
    "comments_count": 6,
    "reactions_count": 0,
    "labels": "bug",
    "milestone": null,
    "closed_by": "terrykong",
    "resolution_time_hours": 167.27694444444444,
    "first_comments": [
      {
        "author": "terrykong",
        "created_at": "2025-01-10T18:01:20+00:00",
        "body": "Increasing num_workers can increase the host memory usage. Is my understanding correct that these A3 mega nodes have ~1.8TB of host memory? Do you have data points of how the process mem increase as you increase num_workers?\n\nDoes increasing the shared memory like in [this summary comment](https://github.com/pytorch/pytorch/issues/13246#issuecomment-905703662) help in your case?"
      },
      {
        "author": "terrykong",
        "created_at": "2025-01-10T18:16:09+00:00",
        "body": "Also, if you're not using [Nemo 2.0](https://docs.nvidia.com/nemo-framework/user-guide/latest/nemo-2.0/index.html), I think that would be worth a try. I have seen anecdotes of host OOMs going away after switching"
      },
      {
        "author": "awonak",
        "created_at": "2025-01-10T19:00:52+00:00",
        "body": "> Is my understanding correct that these A3 mega nodes have ~1.8TB of host memory?\n\nCorrect, our two-node cluster has a total of 3,760 GB\n\n>  Do you have data points of how the process mem increase as you increase num_workers?\n\nWhat would be the most helpful format to share this data?\n\nReducing num_nodes or reducing the dataset size will reduce the total amount of CPU memory used. We will experiment increasing shared memory."
      },
      {
        "author": "shengshiqi-google",
        "created_at": "2025-01-10T21:54:55+00:00",
        "body": "> Also, if you're not using [Nemo 2.0](https://docs.nvidia.com/nemo-framework/user-guide/latest/nemo-2.0/index.html), I think that would be worth a try. I have seen anecdotes of host OOMs going away after switching\n\nHi Terry, thank you for your help. We did try NeMo 2.0, and found that it does indeed resolve the OOM issue.\n\nHowever, it appears that NeMo 2.0 does not have Kubernetes support for it yet. Do you have any ideas on when that might happen?"
      },
      {
        "author": "terrykong",
        "created_at": "2025-01-10T22:58:25+00:00",
        "body": "> Hi Terry, thank you for your help. We did try NeMo 2.0, and found that it does indeed resolve the OOM issue.\n\nActually there is k8s support via skypilot: https://github.com/NVIDIA/NeMo-Run/blob/main/docs/source/guides/execution.md#execute-nemo-run\n\nPlease give that a try and feel free to leave feedback for the nemo-run team if something is missing."
      }
    ],
    "url": "https://github.com/NVIDIA/NeMo/issues/11784"
  },
  {
    "repository": "NVIDIA/NeMo",
    "issue_number": 11742,
    "title": "Want learn with model and optim config variable as float16 but i can't",
    "body": "Hello. \nI'm beginner of using NeMo framework.\n\nFollowing the tutorial_pretraining.py (https://docs.nvidia.com/nemo-framework/user-guide/latest/nemo-2.0/quickstart.html)\nI want to make small model (mamba7_780m) be learned with my old gpus ( Geforce RTX 2080 Ti -12GB * 2) .\nSo i wanted to chang model, optimizer variables like bellow. but bf16, fp16 variables are not changed. \nThe other variable are changed well (ex - adam_beta1) . \n\n  #Add\n  recipe.model.config.fp16 = True\n  recipe.model.config.bf16 = False\n  recipe.**optim.config.fp16 = True**\n  recipe.**optim.config.bf16 = False**\n  recipe.optim.config.adam_beta1=0.123\n  #Error\n   [NeMo I 2025-01-03 05:50:17 utils:302] Setting up optimizer with config OptimizerConfig(optimizer='adam', lr=0.0003, min_lr=None, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, **fp16=False, bf16=True**, params_dtype=torch.bfloat16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.123, adam_beta2=0.95, adam_eps=1e-05, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_param_gather_with_optimizer_step=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=False, timers=None, config_logger_dir='')\n\nHow can i solve it?\n\nThe code i edited\n```\ndef configure_recipe(nodes: int = 1, gpus_per_node: int = 1):\n    recipe = llm.mamba2_780m.pretrain_recipe(\n        dir=\"/checkpoints/nemotron\", # Path to store checkpoints\n        name=\"mamba_pretraining\",\n        num_nodes=nodes,\n        num_gpus_per_node=gpus_per_node,\n        tensor_parallelism = 1\n        \n    )\n    \n    recipe.trainer.val_check_interval = 100\n    recipe.data.global_batch_size = 2\n    recipe.data.micro_batch_size = 1\n    recipe.data.seq_length = 256\n    # recipe.model.config.num_layers = 24\n    recipe.model.config.num_attention_heads = 2\n    recipe.model.config.fp16 = True\n    recipe.model.config.bf16 = False\n    recipe.optim.config.fp16 = True\n    recipe.optim.config.bf16 = False\n    recipe.optim.config.use_distributed_optimizer = False\n    recipe.optim.config.params_dtype = torch.float16\n    recipe.optim.config.adam_beta1=0.123\n\n    return recipe\n\ndef local_executor_torchrun(nodes: int = 1, devices: int = 1) -> run.LocalExecutor:\n    # Env vars for jobs are configured here\n    env_vars = {\n        \"TORCH_NCCL_AVOID_RECORD_STREAMS\": \"1\",\n        \"NCCL_NVLS_ENABLE\": \"0\",\n        \"NVTE_DP_AMAX_REDUCE_INTERVAL\": \"0\",\n        \"NVTE_ASYNC_AMAX_REDUCTION\": \"1\",\n    }\n\n    executor = run.LocalExecutor(ntasks_per_node=devices, launcher=\"torchrun\", env_vars=env_vars)\n\n    return executor\n\ndef run_pretraining():\n    recipe = configure_recipe()\n    executor = local_executor_torchrun(nodes=recipe.trainer.num_nodes, devices=recipe.trainer.devices)\n\n    run.run(recipe, executor=executor, name=\"mamba2_780m_pretraining\")\n\n# This condition is necessary for the script to be compatible with Python's multiprocessing module.\nif __name__ == \"__main__\":\n    run_pretraining()\n```\nHere is the result of code\n```\n\n──────────────────────────────── Entering Experiment mamba2_780m_pretraining with id: mamba2_780m_pretraining_1735883405 ─────────────────────────────────\n[05:50:05] INFO     Log directory is:                                                                                               local_scheduler.py:777\n                    /root/.nemo_run/experiments/mamba2_780m_pretraining/mamba2_780m_pretraining_1735883405/mamba2_780m_pretraining                        \n[05:50:05] Launching job mamba2_780m_pretraining for experiment mamba2_780m_pretraining                                                  experiment.py:724\n           INFO     Log directory is:                                                                                               local_scheduler.py:777\n                    /root/.nemo_run/experiments/mamba2_780m_pretraining/mamba2_780m_pretraining_1735883405/mamba2_780m_pretraining                        \n           INFO     Launched app: local_persistent://nemo_run/mamba2_780m_pretraining-ht7tmrvqhgqrgd                                       launcher.py:111\n           INFO     AppStatus:                                                                                                             launcher.py:114\n                        State: RUNNING                                                                                                                    \n                        Num Restarts: 0                                                                                                                   \n                        Roles:                                                                                                                            \n                        Msg: <NONE>                                                                                                                       \n                        Structured Error Msg: <NONE>                                                                                                      \n                        UI URL:                                                                                                                           \n                    file:///root/.nemo_run/experiments/mamba2_780m_pretraining/mamba2_780m_pretraining_1735883405/mamba2_780m_pretraining/                \n                    nemo_run/mamba2_780m_pretraining-ht7tmrvqhgqrgd                                                                                       \n                                                                                                                                                          \n────────────────────────────────────────── Waiting for Experiment mamba2_780m_pretraining_1735883405 to finish ───────────────────────────────────────────\n\nExperiment Status for mamba2_780m_pretraining_1735883405\n\nTask 0: mamba2_780m_pretraining\n- Status: RUNNING\n- Executor: LocalExecutor\n- Job id: mamba2_780m_pretraining-ht7tmrvqhgqrgd\n- Local Directory: /root/.nemo_run/experiments/mamba2_780m_pretraining/mamba2_780m_pretraining_1735883405/mamba2_780m_pretraining\n\n           INFO     Waiting for job mamba2_780m_pretraining-ht7tmrvqhgqrgd to finish [log=True]...                                         launcher.py:132\nretraining/0 I0103 05:50:06.640000 17575 torch/distributed/launcher/api.py:194] Starting elastic_operator with launch configs:\nretraining/0 I0103 05:50:06.640000 17575 torch/distributed/launcher/api.py:194]   entrypoint       : nemo_run.core.runners.fdl_runner\nretraining/0 I0103 05:50:06.640000 17575 torch/distributed/launcher/api.py:194]   min_nodes        : 1\nretraining/0 I0103 05:50:06.640000 17575 torch/distributed/launcher/api.py:194]   max_nodes        : 1\nretraining/0 I0103 05:50:06.640000 17575 torch/distributed/launcher/api.py:194]   nproc_per_node   : 1\nretraining/0 I0103 05:50:06.640000 17575 torch/distributed/launcher/api.py:194]   run_id           : 868\nretraining/0 I0103 05:50:06.640000 17575 torch/distributed/launcher/api.py:194]   rdzv_backend     : c10d\nretraining/0 I0103 05:50:06.640000 17575 torch/distributed/launcher/api.py:194]   rdzv_endpoint    : localhost:0\nretraining/0 I0103 05:50:06.640000 17575 torch/distributed/launcher/api.py:194]   rdzv_configs     : {'timeout': 900}\nretraining/0 I0103 05:50:06.640000 17575 torch/distributed/launcher/api.py:194]   max_restarts     : 0\nretraining/0 I0103 05:50:06.640000 17575 torch/distributed/launcher/api.py:194]   monitor_interval : 0.1\nretraining/0 I0103 05:50:06.640000 17575 torch/distributed/launcher/api.py:194]   log_dir          : /root/.nemo_run/experiments/mamba2_780m_pretraining/mamba2_780m_pretraining_1735883405/mamba2_780m_pretraining/nemo_run/mamba2_780m_pretraining-ht7tmrvqhgqrgd/torchelastic/mamba2_780m_pretraining\nretraining/0 I0103 05:50:06.640000 17575 torch/distributed/launcher/api.py:194]   metrics_cfg      : {}\nretraining/0 I0103 05:50:06.640000 17575 torch/distributed/launcher/api.py:194] \nretraining/0 I0103 05:50:06.642000 17575 torch/distributed/elastic/agent/server/api.py:845] [default] starting workers for entrypoint: python\nretraining/0 I0103 05:50:06.642000 17575 torch/distributed/elastic/agent/server/api.py:662] [default] Rendezvous'ing worker group\nretraining/0 I0103 05:50:06.814000 17575 torch/distributed/elastic/agent/server/api.py:525] [default] Rendezvous complete for workers. Result:\nretraining/0 I0103 05:50:06.814000 17575 torch/distributed/elastic/agent/server/api.py:525]   restart_count=0\nretraining/0 I0103 05:50:06.814000 17575 torch/distributed/elastic/agent/server/api.py:525]   master_addr=localhost\nretraining/0 I0103 05:50:06.814000 17575 torch/distributed/elastic/agent/server/api.py:525]   master_port=36739\nretraining/0 I0103 05:50:06.814000 17575 torch/distributed/elastic/agent/server/api.py:525]   group_rank=0\nretraining/0 I0103 05:50:06.814000 17575 torch/distributed/elastic/agent/server/api.py:525]   group_world_size=1\nretraining/0 I0103 05:50:06.814000 17575 torch/distributed/elastic/agent/server/api.py:525]   local_ranks=[0]\nretraining/0 I0103 05:50:06.814000 17575 torch/distributed/elastic/agent/server/api.py:525]   role_ranks=[0]\nretraining/0 I0103 05:50:06.814000 17575 torch/distributed/elastic/agent/server/api.py:525]   global_ranks=[0]\nretraining/0 I0103 05:50:06.814000 17575 torch/distributed/elastic/agent/server/api.py:525]   role_world_sizes=[1]\nretraining/0 I0103 05:50:06.814000 17575 torch/distributed/elastic/agent/server/api.py:525]   global_world_sizes=[1]\nretraining/0 I0103 05:50:06.814000 17575 torch/distributed/elastic/agent/server/api.py:525] \nretraining/0 I0103 05:50:06.815000 17575 torch/distributed/elastic/agent/server/api.py:670] [default] Starting worker group\nretraining/0 I0103 05:50:06.815000 17575 torch/distributed/elastic/agent/server/local_elastic_agent.py:291] use_agent_store: True\nretraining/0 I0103 05:50:06.815000 17575 torch/distributed/elastic/agent/server/local_elastic_agent.py:192] Environment variable 'TORCHELASTIC_ENABLE_FILE_TIMER' not found. Do not start FileTimerServer.\nretraining/0 I0103 05:50:06.815000 17575 torch/distributed/elastic/agent/server/local_elastic_agent.py:229] Environment variable 'TORCHELASTIC_HEALTH_CHECK_PORT' not found. Do not start health check.\nretraining/0 [default0]:[NeMo W 2025-01-03 05:50:14 nemo_logging:361] /usr/local/lib/python3.10/dist-packages/pyannote/core/notebook.py:134: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap()`` or ``pyplot.get_cmap()`` instead.\nretraining/0 [default0]:      cm = get_cmap(\"Set1\")\nretraining/0 [default0]:    \nretraining/0 [default0]:[NeMo I 2025-01-03 05:50:16 tokenizer_utils:185] Getting HuggingFace AutoTokenizer with pretrained_model_name: EleutherAI/gpt-neox-20b\nretraining/0 [default0]:[NeMo I 2025-01-03 05:50:16 tokenizer_utils:185] Getting HuggingFace AutoTokenizer with pretrained_model_name: EleutherAI/gpt-neox-20b\nretraining/0 [default0]:[NeMo I 2025-01-03 05:50:16 nemo_logger:145] Experiments will be logged at /checkpoints/nemotron/mamba_pretraining\nretraining/0 [default0]:[NeMo I 2025-01-03 05:50:16 megatron_init:426] Rank 0 has data parallel group : [0]\nretraining/0 [default0]:[NeMo I 2025-01-03 05:50:16 megatron_init:432] Rank 0 has combined group of data parallel and context parallel : [0]\nretraining/0 [default0]:[NeMo I 2025-01-03 05:50:16 megatron_init:437] All data parallel group ranks with context parallel combined: [[0]]\nretraining/0 [default0]:[NeMo I 2025-01-03 05:50:16 megatron_init:440] Ranks 0 has data parallel rank: 0\nretraining/0 [default0]:[NeMo I 2025-01-03 05:50:16 megatron_init:448] Rank 0 has context parallel group: [0]\nretraining/0 [default0]:[NeMo I 2025-01-03 05:50:16 megatron_init:451] All context parallel group ranks: [[0]]\nretraining/0 [default0]:[NeMo I 2025-01-03 05:50:16 megatron_init:452] Ranks 0 has context parallel rank: 0\nretraining/0 [default0]:[NeMo I 2025-01-03 05:50:16 megatron_init:459] Rank 0 has model parallel group: [0]\nretraining/0 [default0]:[NeMo I 2025-01-03 05:50:16 megatron_init:460] All model parallel group ranks: [[0]]\nretraining/0 [default0]:[NeMo I 2025-01-03 05:50:16 megatron_init:469] Rank 0 has tensor model parallel group: [0]\nretraining/0 [default0]:[NeMo I 2025-01-03 05:50:16 megatron_init:473] All tensor model parallel group ranks: [[0]]\nretraining/0 [default0]:[NeMo I 2025-01-03 05:50:16 megatron_init:474] Rank 0 has tensor model parallel rank: 0\nretraining/0 [default0]:[NeMo I 2025-01-03 05:50:16 megatron_init:494] Rank 0 has pipeline model parallel group: [0]\nretraining/0 [default0]:[NeMo I 2025-01-03 05:50:16 megatron_init:506] Rank 0 has embedding group: [0]\nretraining/0 [default0]:[NeMo I 2025-01-03 05:50:16 megatron_init:512] All pipeline model parallel group ranks: [[0]]\nretraining/0 [default0]:[NeMo I 2025-01-03 05:50:16 megatron_init:513] Rank 0 has pipeline model parallel rank 0\nretraining/0 [default0]:[NeMo I 2025-01-03 05:50:16 megatron_init:514] All embedding group ranks: [[0]]\nretraining/0 [default0]:[NeMo I 2025-01-03 05:50:16 megatron_init:515] Rank 0 has embedding rank: 0\nretraining/0 [default0]:GPU available: True (cuda), used: True\nretraining/0 [default0]:TPU available: False, using: 0 TPU cores\nretraining/0 [default0]:HPU available: False, using: 0 HPUs\nretraining/0 [default0]:[NeMo W 2025-01-03 05:50:16 nemo_logger:123] No version folders would be created under the log folder as 'resume_if_exists' is enabled.\nretraining/0 [default0]:[NeMo W 2025-01-03 05:50:16 nemo_logger:173] \"update_logger_directory\" is True. Overwriting tensorboard logger \"save_dir\" to /checkpoints/nemotron/tb_logs\nretraining/0 [default0]:[NeMo W 2025-01-03 05:50:16 nemo_logger:189] The Trainer already contains a ModelCheckpoint callback. This will be overwritten.\nretraining/0 [default0]:[NeMo W 2025-01-03 05:50:16 nemo_logger:212] The checkpoint callback was told to monitor a validation value and trainer's max_steps was set to 100. Please ensure that max_steps will run for at least 1 epochs to ensure that checkpointing will not error out.\nretraining/0 [default0]:[NeMo W 2025-01-03 05:50:16 resume:228] There were no checkpoints found in checkpoint_dir or no checkpoint folder at checkpoint_dir :/checkpoints/nemotron/mamba_pretraining/checkpoints. Training from scratch.\nretraining/0 [default0]:Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1\nretraining/0 [default0]:----------------------------------------------------------------------------------------------------\nretraining/0 [default0]:distributed_backend=nccl\nretraining/0 [default0]:All distributed processes registered. Starting with 1 processes\nretraining/0 [default0]:----------------------------------------------------------------------------------------------------\nretraining/0 [default0]:\nretraining/0 [default0]:[NeMo I 2025-01-03 05:50:17 base:44] Padded vocab_size: 50288, original vocab_size: 50277, dummy tokens: 11.\nretraining/0 [default0]:[NeMo I 2025-01-03 05:50:17 utils:302] Using hybrid override pattern\nretraining/0 [default0]:[NeMo I 2025-01-03 05:50:17 utils:302] The override pattern matches the overridden pattern\nretraining/0 [default0]:[NeMo I 2025-01-03 05:50:17 utils:302] Hybrid allocation (M is mamba, * is attention, - is mlp):\nretraining/0 [default0]:[NeMo I 2025-01-03 05:50:17 utils:302] MMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMM\nretraining/0 [default0]:[NeMo I 2025-01-03 05:50:17 utils:302] 0 attention layers in 48 total layers.\nretraining/0 [default0]:[NeMo I 2025-01-03 05:50:17 utils:302] Target attention ratio: 0.00. Actual attention ratio: 0.00.\nretraining/0 [default0]:[NeMo I 2025-01-03 05:50:17 utils:302] 0 mlp layers in 48 total layers.\nretraining/0 [default0]:[NeMo I 2025-01-03 05:50:17 utils:302] Target mlp ratio: 0.00. Actual mlp ratio: 0.00.\nretraining/0 [default0]:[NeMo I 2025-01-03 05:50:17 megatron_strategy:327] Copying Trainer's 'max_steps' (100) to LR scheduler's 'max_steps'.\nretraining/0 [default0]:[NeMo I 2025-01-03 05:50:17 num_microbatches_calculator:228] setting number of microbatches to constant 2\nretraining/0 [default0]:[NeMo I 2025-01-03 05:50:17 megatron_parallel:549]  > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 857403648\nretraining/0 [default0]:[NeMo I 2025-01-03 05:50:17 utils:302] Setting up DistributedDataParallel with config DistributedDataParallelConfig(grad_reduce_in_fp32=True, overlap_grad_reduce=True, overlap_param_gather=True, align_param_gather=False, use_distributed_optimizer=False, num_distributed_optimizer_instances=1, check_for_nan_in_grad=True, bucket_size=40000000, average_in_collective=False, fp8_param_gather=False)\nretraining/0 [default0]:[NeMo I 2025-01-03 05:50:17 utils:323] Number of buckets for gradient all-reduce / reduce-scatter: 18\n\n[...]\n\nretraining/0 [default0]:[NeMo I 2025-01-03 05:50:17 utils:302] Setting up optimizer with config OptimizerConfig(optimizer='adam', lr=0.0003, min_lr=None, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=False, bf16=True, params_dtype=torch.bfloat16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.123, adam_beta2=0.95, adam_eps=1e-05, sgd_momentum=0.9, use_distributed_optimizer=False, overlap_param_gather_with_optimizer_step=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=False, timers=None, config_logger_dir='')\nretraining/0 [default0]:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\nretraining/0 [default0]:\nretraining/0 [default0]:  | Name   | Type | Params | Mode \nretraining/0 [default0]:----------------------------------------\nretraining/0 [default0]:0 | module | DDP  | 857 M  | train\nretraining/0 [default0]:----------------------------------------\nretraining/0 [default0]:857 M     Trainable params\nretraining/0 [default0]:0         Non-trainable params\nretraining/0 [default0]:857 M     Total params\nretraining/0 [default0]:3,429.615 Total estimated model params size (MB)\nretraining/0 [default0]:394       Modules in train mode\nretraining/0 [default0]:0         Modules in eval mode\nretraining/0 [default0]:[rank0]: Traceback (most recent call last):\nretraining/0 [default0]:[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/triton/backends/nvidia/compiler.py\", line 292, in make_cubin\nretraining/0 [default0]:[rank0]:     subprocess.run(cmd, shell=True, check=True)\nretraining/0 [default0]:[rank0]:   File \"/usr/lib/python3.10/subprocess.py\", line 526, in run\nretraining/0 [default0]:[rank0]:     raise CalledProcessError(retcode, process.args,\nretraining/0 [default0]:[rank0]: subprocess.CalledProcessError: Command '/usr/local/lib/python3.10/dist-packages/triton/backends/nvidia/bin/ptxas -lineinfo -v --gpu-name=sm_75 /tmp/tmppa4c09n_.ptx -o /tmp/tmppa4c09n_.ptx.o 2> /tmp/tmp39of6etg.log' returned non-zero exit status 255.\nretraining/0 [default0]:\nretraining/0 [default0]:[rank0]: During handling of the above exception, another exception occurred:\nretraining/0 [default0]:\nretraining/0 [default0]:[rank0]: Traceback (most recent call last):\nretraining/0 [default0]:[rank0]:   File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\nretraining/0 [default0]:[rank0]:     return _run_code(code, main_globals, None,\nretraining/0 [default0]:[rank0]:   File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\nretraining/0 [default0]:[rank0]:     exec(code, run_globals)\nretraining/0 [default0]:[rank0]:   File \"/opt/NeMo-Run/src/nemo_run/core/runners/fdl_runner.py\", line 66, in <module>\nretraining/0 [default0]:[rank0]:     fdl_runner_app()\nretraining/0 [default0]:[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/typer/main.py\", line 338, in __call__\nretraining/0 [default0]:[rank0]:     raise e\nretraining/0 [default0]:[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/typer/main.py\", line 321, in __call__\nretraining/0 [default0]:[rank0]:     return get_command(self)(*args, **kwargs)\nretraining/0 [default0]:[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/click/core.py\", line 1157, in __call__\nretraining/0 [default0]:[rank0]:     return self.main(*args, **kwargs)\nretraining/0 [default0]:[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/typer/core.py\", line 665, in main\nretraining/0 [default0]:[rank0]:     return _main(\nretraining/0 [default0]:[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/typer/core.py\", line 197, in _main\nretraining/0 [default0]:[rank0]:     rv = self.invoke(ctx)\nretraining/0 [default0]:[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/click/core.py\", line 1434, in invoke\nretraining/0 [default0]:[rank0]:     return ctx.invoke(self.callback, **ctx.params)\nretraining/0 [default0]:[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/click/core.py\", line 783, in invoke\nretraining/0 [default0]:[rank0]:     return __callback(*args, **kwargs)\nretraining/0 [default0]:[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/typer/main.py\", line 703, in wrapper\nretraining/0 [default0]:[rank0]:     return callback(**use_params)\nretraining/0 [default0]:[rank0]:   File \"/opt/NeMo-Run/src/nemo_run/core/runners/fdl_runner.py\", line 62, in fdl_direct_run\nretraining/0 [default0]:[rank0]:     fdl_fn()\nretraining/0 [default0]:[rank0]:   File \"/opt/NeMo/nemo/collections/llm/api.py\", line 150, in pretrain\nretraining/0 [default0]:[rank0]:     return train(\nretraining/0 [default0]:[rank0]:   File \"/opt/NeMo/nemo/collections/llm/api.py\", line 107, in train\nretraining/0 [default0]:[rank0]:     trainer.fit(model, data)\nretraining/0 [default0]:[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py\", line 538, in fit\nretraining/0 [default0]:[rank0]:     call._call_and_handle_interrupt(\nretraining/0 [default0]:[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/call.py\", line 46, in _call_and_handle_interrupt\nretraining/0 [default0]:[rank0]:     return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)\nretraining/0 [default0]:[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/strategies/launchers/subprocess_script.py\", line 105, in launch\nretraining/0 [default0]:[rank0]:     return function(*args, **kwargs)\nretraining/0 [default0]:[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py\", line 574, in _fit_impl\nretraining/0 [default0]:[rank0]:     self._run(model, ckpt_path=ckpt_path)\nretraining/0 [default0]:[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py\", line 981, in _run\nretraining/0 [default0]:[rank0]:     results = self._run_stage()\nretraining/0 [default0]:[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1023, in _run_stage\nretraining/0 [default0]:[rank0]:     self._run_sanity_check()\nretraining/0 [default0]:[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1052, in _run_sanity_check\nretraining/0 [default0]:[rank0]:     val_loop.run()\nretraining/0 [default0]:[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/utilities.py\", line 178, in _decorator\nretraining/0 [default0]:[rank0]:     return loop_run(self, *args, **kwargs)\nretraining/0 [default0]:[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 135, in run\nretraining/0 [default0]:[rank0]:     self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)\nretraining/0 [default0]:[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 396, in _evaluation_step\nretraining/0 [default0]:[rank0]:     output = call._call_strategy_hook(trainer, hook_name, *step_args)\nretraining/0 [default0]:[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/call.py\", line 319, in _call_strategy_hook\nretraining/0 [default0]:[rank0]:     output = fn(*args, **kwargs)\nretraining/0 [default0]:[rank0]:   File \"/opt/NeMo/nemo/lightning/pytorch/strategies/megatron_strategy.py\", line 607, in validation_step\nretraining/0 [default0]:[rank0]:     out = self.model.validation_step(dataloader_iter, *args, **kwargs)\nretraining/0 [default0]:[rank0]:   File \"/opt/NeMo/nemo/lightning/megatron_parallel.py\", line 342, in validation_step\nretraining/0 [default0]:[rank0]:     return self._step(\nretraining/0 [default0]:[rank0]:   File \"/opt/NeMo/nemo/lightning/megatron_parallel.py\", line 429, in _step\nretraining/0 [default0]:[rank0]:     return self.forward(\nretraining/0 [default0]:[rank0]:   File \"/opt/NeMo/nemo/lightning/megatron_parallel.py\", line 279, in forward\nretraining/0 [default0]:[rank0]:     microbatch_outputs = step()\nretraining/0 [default0]:[rank0]:   File \"/opt/NeMo/nemo/lightning/megatron_parallel.py\", line 1149, in __call__\nretraining/0 [default0]:[rank0]:     return self.forward_backward_func(\nretraining/0 [default0]:[rank0]:   File \"/opt/megatron-lm/megatron/core/pipeline_parallel/schedules.py\", line 452, in forward_backward_no_pipelining\nretraining/0 [default0]:[rank0]:     output_tensor, num_tokens = forward_step(\nretraining/0 [default0]:[rank0]:   File \"/opt/megatron-lm/megatron/core/pipeline_parallel/schedules.py\", line 274, in forward_step\nretraining/0 [default0]:[rank0]:     output_tensor, loss_func = forward_step_func(data_iterator, model)\nretraining/0 [default0]:[rank0]:   File \"/opt/NeMo/nemo/lightning/megatron_parallel.py\", line 492, in wrapped_forward_step_func\nretraining/0 [default0]:[rank0]:     output_tensor = _forward_step(model, batch)\nretraining/0 [default0]:[rank0]:   File \"/opt/NeMo/nemo/lightning/megatron_parallel.py\", line 760, in wrapped\nretraining/0 [default0]:[rank0]:     return attr(*args)\nretraining/0 [default0]:[rank0]:   File \"/opt/NeMo/nemo/collections/llm/gpt/model/base.py\", line 384, in validation_step\nretraining/0 [default0]:[rank0]:     return self.forward_step(batch)\nretraining/0 [default0]:[rank0]:   File \"/opt/NeMo/nemo/collections/llm/gpt/model/base.py\", line 375, in forward_step\nretraining/0 [default0]:[rank0]:     return self.config.forward_step_fn(self, batch)\nretraining/0 [default0]:[rank0]:   File \"/opt/NeMo/nemo/collections/llm/gpt/model/ssm.py\", line 47, in ssm_forward_step\nretraining/0 [default0]:[rank0]:     return model(**forward_args)\nretraining/0 [default0]:[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\nretraining/0 [default0]:[rank0]:     return self._call_impl(*args, **kwargs)\nretraining/0 [default0]:[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\nretraining/0 [default0]:[rank0]:     return forward_call(*args, **kwargs)\nretraining/0 [default0]:[rank0]:   File \"/opt/NeMo/nemo/collections/llm/gpt/model/base.py\", line 359, in forward\nretraining/0 [default0]:[rank0]:     output_tensor = self.module(\nretraining/0 [default0]:[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\nretraining/0 [default0]:[rank0]:     return self._call_impl(*args, **kwargs)\nretraining/0 [default0]:[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\nretraining/0 [default0]:[rank0]:     return forward_call(*args, **kwargs)\nretraining/0 [default0]:[rank0]:   File \"/opt/megatron-lm/megatron/core/distributed/data_parallel_base.py\", line 22, in forward\nretraining/0 [default0]:[rank0]:     return self.module(*inputs, **kwargs)\nretraining/0 [default0]:[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\nretraining/0 [default0]:[rank0]:     return self._call_impl(*args, **kwargs)\nretraining/0 [default0]:[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\nretraining/0 [default0]:[rank0]:     return forward_call(*args, **kwargs)\nretraining/0 [default0]:[rank0]:   File \"/opt/megatron-lm/megatron/core/transformer/module.py\", line 178, in forward\nretraining/0 [default0]:[rank0]:     outputs = self.module(*inputs, **kwargs)\nretraining/0 [default0]:[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\nretraining/0 [default0]:[rank0]:     return self._call_impl(*args, **kwargs)\nretraining/0 [default0]:[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\nretraining/0 [default0]:[rank0]:     return forward_call(*args, **kwargs)\nretraining/0 [default0]:[rank0]:   File \"/opt/megatron-lm/megatron/core/models/mamba/mamba_model.py\", line 206, in forward\nretraining/0 [default0]:[rank0]:     hidden_states = self.decoder(\nretraining/0 [default0]:[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\nretraining/0 [default0]:[rank0]:     return self._call_impl(*args, **kwargs)\nretraining/0 [default0]:[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\nretraining/0 [default0]:[rank0]:     return forward_call(*args, **kwargs)\nretraining/0 [default0]:[rank0]:   File \"/opt/megatron-lm/megatron/core/ssm/mamba_block.py\", line 262, in forward\nretraining/0 [default0]:[rank0]:     hidden_states = layer(\nretraining/0 [default0]:[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\nretraining/0 [default0]:[rank0]:     return self._call_impl(*args, **kwargs)\nretraining/0 [default0]:[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\nretraining/0 [default0]:[rank0]:     return forward_call(*args, **kwargs)\nretraining/0 [default0]:[rank0]:   File \"/opt/megatron-lm/megatron/core/ssm/mamba_layer.py\", line 105, in forward\nretraining/0 [default0]:[rank0]:     mixer_out_with_bias = self.mixer(hidden_states, inference_params=inference_params)\nretraining/0 [default0]:[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\nretraining/0 [default0]:[rank0]:     return self._call_impl(*args, **kwargs)\nretraining/0 [default0]:[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\nretraining/0 [default0]:[rank0]:     return forward_call(*args, **kwargs)\nretraining/0 [default0]:[rank0]:   File \"/opt/megatron-lm/megatron/core/ssm/mamba_mixer.py\", line 299, in forward\nretraining/0 [default0]:[rank0]:     y = mamba_split_conv1d_scan_combined(\nretraining/0 [default0]:[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/mamba_ssm/ops/triton/ssd_combined.py\", line 930, in mamba_split_conv1d_scan_combined\nretraining/0 [default0]:[rank0]:     return MambaSplitConv1dScanCombinedFn.apply(zxbcdt, conv1d_weight, conv1d_bias, dt_bias, A, D, chunk_size, initial_states, seq_idx, dt_limit, return_final_states, activation, rmsnorm_weight, rmsnorm_eps, outproj_weight, outproj_bias, headdim, ngroups, norm_before_gate)\nretraining/0 [default0]:[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/torch/autograd/function.py\", line 575, in apply\nretraining/0 [default0]:[rank0]:     return super().apply(*args, **kwargs)  # type: ignore[misc]\nretraining/0 [default0]:[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/torch/amp/autocast_mode.py\", line 456, in decorate_fwd\nretraining/0 [default0]:[rank0]:     return fwd(*args, **kwargs)\nretraining/0 [default0]:[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/mamba_ssm/ops/triton/ssd_combined.py\", line 789, in forward\nretraining/0 [default0]:[rank0]:     out, out_x, dt_out, dA_cumsum, states, final_states = _mamba_chunk_scan_combined_fwd(x, dt, A, B, C, chunk_size=chunk_size, D=D, z=z, dt_bias=dt_bias, initial_states=initial_states, seq_idx=seq_idx, dt_softplus=True, dt_limit=dt_limit)\nretraining/0 [default0]:[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/mamba_ssm/ops/triton/ssd_combined.py\", line 312, in _mamba_chunk_scan_combined_fwd\nretraining/0 [default0]:[rank0]:     dA_cumsum, dt = _chunk_cumsum_fwd(dt, A, chunk_size, dt_bias=dt_bias, dt_softplus=dt_softplus, dt_limit=dt_limit)\nretraining/0 [default0]:[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/mamba_ssm/ops/triton/ssd_chunk_state.py\", line 675, in _chunk_cumsum_fwd\nretraining/0 [default0]:[rank0]:     _chunk_cumsum_fwd_kernel[grid_chunk_cs](\nretraining/0 [default0]:[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/triton/runtime/jit.py\", line 345, in <lambda>\nretraining/0 [default0]:[rank0]:     return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\nretraining/0 [default0]:[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/triton/runtime/autotuner.py\", line 156, in run\nretraining/0 [default0]:[rank0]:     timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\nretraining/0 [default0]:[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/triton/runtime/autotuner.py\", line 156, in <dictcomp>\nretraining/0 [default0]:[rank0]:     timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\nretraining/0 [default0]:[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/triton/runtime/autotuner.py\", line 133, in _bench\nretraining/0 [default0]:[rank0]:     return do_bench(kernel_call, warmup=self.num_warmups, rep=self.num_reps, quantiles=(0.5, 0.2, 0.8))\nretraining/0 [default0]:[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/triton/testing.py\", line 106, in do_bench\nretraining/0 [default0]:[rank0]:     fn()\nretraining/0 [default0]:[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/triton/runtime/autotuner.py\", line 114, in kernel_call\nretraining/0 [default0]:[rank0]:     self.fn.run(\nretraining/0 [default0]:[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/triton/runtime/jit.py\", line 662, in run\nretraining/0 [default0]:[rank0]:     kernel = self.compile(\nretraining/0 [default0]:[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/triton/compiler/compiler.py\", line 282, in compile\nretraining/0 [default0]:[rank0]:     next_module = compile_ir(module, metadata)\nretraining/0 [default0]:[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/triton/backends/nvidia/compiler.py\", line 320, in <lambda>\nretraining/0 [default0]:[rank0]:     stages[\"cubin\"] = lambda src, metadata: self.make_cubin(src, metadata, options, self.capability)\nretraining/0 [default0]:[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/triton/backends/nvidia/compiler.py\", line 297, in make_cubin\nretraining/0 [default0]:[rank0]:     raise RuntimeError(f'Internal Triton PTX codegen error: \\n{log}')\nretraining/0 [default0]:[rank0]: RuntimeError: Internal Triton PTX codegen error: \nretraining/0 [default0]:[rank0]: ptxas /tmp/tmppa4c09n_.ptx, line 120; error   : Feature '.bf16' requires .target sm_80 or higher\nretraining/0 [default0]:[rank0]: ptxas /tmp/tmppa4c09n_.ptx, line 120; error   : Feature 'cvt with .f32.bf16' requires .target sm_80 or higher\nretraining/0 [default0]:[rank0]: ptxas fatal   : Ptx assembly aborted due to errors\nretraining/0 [default0]:\nretraining/0 E0103 05:50:22.565000 17575 torch/distributed/elastic/multiprocessing/api.py:862] failed (exitcode: 1) local_rank: 0 (pid: 17601) of binary: /usr/bin/python\nretraining/0 I0103 05:50:22.579000 17575 torch/distributed/elastic/multiprocessing/errors/__init__.py:368] ('local_rank %s FAILED with no error file. Decorate your entrypoint fn with @record for traceback info. See: https://pytorch.org/docs/stable/elastic/errors.html', 0)\nretraining/0 Traceback (most recent call last):\nretraining/0   File \"/usr/local/bin/torchrun\", line 33, in <module>\nretraining/0     sys.exit(load_entry_point('torch==2.5.0a0+e000cf0ad9.nv24.10', 'console_scripts', 'torchrun')())\nretraining/0   File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 355, in wrapper\nretraining/0     return f(*args, **kwargs)\nretraining/0   File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py\", line 919, in main\nretraining/0     run(args)\nretraining/0   File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py\", line 910, in run\nretraining/0     elastic_launch(\nretraining/0   File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py\", line 138, in __call__\nretraining/0     return launch_agent(self._config, self._entrypoint, list(args))\nretraining/0   File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py\", line 269, in launch_agent\nretraining/0     raise ChildFailedError(\nretraining/0 torch.distributed.elastic.multiprocessing.errors.ChildFailedError: \nretraining/0 ============================================================\nretraining/0 nemo_run.core.runners.fdl_runner FAILED\nretraining/0 ------------------------------------------------------------\nretraining/0 Failures:\nretraining/0   <NO_OTHER_FAILURES>\nretraining/0 ------------------------------------------------------------\nretraining/0 Root Cause (first observed failure):\nretraining/0 [0]:\nretraining/0   time      : 2025-01-03_05:50:22\nretraining/0   host      : bh-Precision-5820-Tower-X-Series\nretraining/0   rank      : 0 (local_rank: 0)\nretraining/0   exitcode  : 1 (pid: 17601)\nretraining/0   error_file: <N/A>\nretraining/0   traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\nretraining/0 ============================================================\n[05:50:23] INFO     Job mamba2_780m_pretraining-ht7tmrvqhgqrgd finished: FAILED                                                            launcher.py:162\n                                                                                                                                                          \n# The experiment was run with the following tasks: ['mamba2_780m_pretraining']                                                                            \n# You can inspect and reconstruct this experiment at a later point in time using:                                                                         \nexperiment = run.Experiment.from_id(\"mamba2_780m_pretraining_1735883405\")                                                                                 \nexperiment.status() # Gets the overall status                                                                                                             \nexperiment.logs(\"mamba2_780m_pretraining\") # Gets the log for the provided task                                                                           \nexperiment.cancel(\"mamba2_780m_pretraining\") # Cancels the provided task if still running \n``` \n\nAny tips would be great! Thank you in advance!\n\n\n",
    "state": "closed",
    "created_at": "2025-01-03T06:36:24+00:00",
    "closed_at": "2025-01-06T07:27:32+00:00",
    "updated_at": "2025-01-06T07:27:32+00:00",
    "author": "tolry418",
    "author_type": "User",
    "comments_count": 0,
    "reactions_count": 0,
    "labels": "",
    "milestone": null,
    "closed_by": "tolry418",
    "resolution_time_hours": 72.85222222222222,
    "first_comments": [],
    "url": "https://github.com/NVIDIA/NeMo/issues/11742"
  },
  {
    "repository": "NVIDIA/NeMo",
    "issue_number": 11690,
    "title": "docs.nvidia.com link to tutorial notebooks via `stable` tag, which fail to install dependencies",
    "body": "For new NeMo users following the documentation, the first linked NeMo primer fails.\n\nStarting from here: \nhttps://docs.nvidia.com/nemo-framework/user-guide/latest/nemotoolkit/starthere/tutorials.html#tutorial-overview\n\nThe links in the above page reference the `stable` tag from this repo:\nhttps://colab.research.google.com/github/NVIDIA/NeMo/blob/stable/tutorials/00_NeMo_Primer.ipynb\n\nThe commit tagged `stable` still suffers from #10368 causing a failure when trying to run the notebook.",
    "state": "closed",
    "created_at": "2024-12-20T18:35:01+00:00",
    "closed_at": "2025-01-27T01:57:55+00:00",
    "updated_at": "2025-01-27T01:57:56+00:00",
    "author": "jeffgreenca",
    "author_type": "User",
    "comments_count": 2,
    "reactions_count": 0,
    "labels": "stale",
    "milestone": null,
    "closed_by": "github-actions[bot]",
    "resolution_time_hours": 895.3816666666667,
    "first_comments": [
      {
        "author": "github-actions[bot]",
        "created_at": "2025-01-20T01:57:41+00:00",
        "body": "This issue is stale because it has been open for 30 days with no activity. Remove stale label or comment or this will be closed in 7 days."
      },
      {
        "author": "github-actions[bot]",
        "created_at": "2025-01-27T01:57:55+00:00",
        "body": "This issue was closed because it has been inactive for 7 days since being marked as stale."
      }
    ],
    "url": "https://github.com/NVIDIA/NeMo/issues/11690"
  },
  {
    "repository": "NVIDIA/NeMo",
    "issue_number": 11684,
    "title": "SymbolicValueError: STFT does not currently support complex types",
    "body": "**Describe the bug**\n\nI am trying to export the preprocessor class of Quartznet model to onnx. My requirement is to run preprocessing in a non-python environment. However, the export is failing.\n\n**Steps/Code to reproduce bug**\n\nThe bug can be reproduced using  the following code - \n\n```\nquartznet = nemo_asr.models.EncDecCTCModel.from_pretrained(model_name=\"QuartzNet15x5Base-En\")\n\npreprocessor = quartznet.preprocessor.to(device)\n\npreprocessor.eval()\n\npreprocessor.export(\"prep.onnx\", onnx_opset_version=17)\n\n``` \nThe following error is produced:\n\n```\n{\n\t\"name\": \"SymbolicValueError\",\n\t\"message\": \"STFT does not currently support complex types  [Caused by the value '97 defined in (%97 : Float(*, *, strides=[2946, 1], requires_grad=0, device=cpu) = onnx::Reshape[allowzero=0](%86, %96), scope: nemo.collections.asr.modules.audio_preprocessing.AudioToMelSpectrogramPreprocessor::/nemo.collections.asr.parts.preprocessing.features.FilterbankFeatures::featurizer # /home/ik/miniconda3/envs/nemo/lib/python3.11/site-packages/torch/functional.py:703:0\n)' (type 'Tensor') in the TorchScript graph. The containing node has kind 'onnx::Reshape'.] \n    (node defined in /home/ik/miniconda3/envs/nemo/lib/python3.11/site-packages/torch/functional.py(703): stft\n/home/ik/Documents/aakhor/asr/NeMo/nemo/collections/asr/parts/preprocessing/features.py(308): <lambda>\n/home/ik/Documents/aakhor/asr/NeMo/nemo/collections/asr/parts/preprocessing/features.py(423): forward\n/home/ik/miniconda3/envs/nemo/lib/python3.11/site-packages/torch/utils/_contextlib.py(116): decorate_context\n/home/ik/miniconda3/envs/nemo/lib/python3.11/site-packages/torch/nn/modules/module.py(1726): _slow_forward\n/home/ik/miniconda3/envs/nemo/lib/python3.11/site-packages/torch/nn/modules/module.py(1747): _call_impl\n/home/ik/miniconda3/envs/nemo/lib/python3.11/site-packages/torch/nn/modules/module.py(1736): _wrapped_call_impl\n/home/ik/Documents/aakhor/asr/NeMo/nemo/collections/asr/modules/audio_preprocessing.py(292): get_features\n/home/ik/Documents/aakhor/asr/NeMo/nemo/collections/asr/modules/audio_preprocessing.py(91): forward\n/home/ik/miniconda3/envs/nemo/lib/python3.11/site-packages/torch/utils/_contextlib.py(116): decorate_context\n/home/ik/miniconda3/envs/nemo/lib/python3.11/site-packages/torch/nn/modules/module.py(1726): _slow_forward\n/home/ik/miniconda3/envs/nemo/lib/python3.11/site-packages/torch/nn/modules/module.py(1747): _call_impl\n/home/ik/miniconda3/envs/nemo/lib/python3.11/site-packages/torch/nn/modules/module.py(1736): _wrapped_call_impl\n/home/ik/miniconda3/envs/nemo/lib/python3.11/site-packages/torch/jit/_trace.py(130): wrapper\n/home/ik/miniconda3/envs/nemo/lib/python3.11/site-packages/torch/jit/_trace.py(139): forward\n/home/ik/miniconda3/envs/nemo/lib/python3.11/site-packages/torch/nn/modules/module.py(1747): _call_impl\n/home/ik/miniconda3/envs/nemo/lib/python3.11/site-packages/torch/nn/modules/module.py(1736): _wrapped_call_impl\n/home/ik/miniconda3/envs/nemo/lib/python3.11/site-packages/torch/jit/_trace.py(1500): _get_trace_graph\n/home/ik/miniconda3/envs/nemo/lib/python3.11/site-packages/torch/onnx/utils.py(904): _trace_and_get_graph_from_model\n/home/ik/miniconda3/envs/nemo/lib/python3.11/site-packages/torch/onnx/utils.py(997): _create_jit_graph\n/home/ik/miniconda3/envs/nemo/lib/python3.11/site-packages/torch/onnx/utils.py(1113): _model_to_graph\n/home/ik/miniconda3/envs/nemo/lib/python3.11/site-packages/torch/onnx/utils.py(1564): _export\n/home/ik/miniconda3/envs/nemo/lib/python3.11/site-packages/torch/onnx/utils.py(502): export\n/home/ik/miniconda3/envs/nemo/lib/python3.11/site-packages/torch/onnx/__init__.py(375): export\n/home/ik/Documents/aakhor/asr/NeMo/nemo/core/classes/exportable.py(221): _export\n/home/ik/Documents/aakhor/asr/NeMo/nemo/core/classes/exportable.py(114): export\n/tmp/ipykernel_10033/3317768717.py(8): <module>\n/home/ik/miniconda3/envs/nemo/lib/python3.11/site-packages/IPython/core/interactiveshell.py(3577): run_code\n/home/ik/miniconda3/envs/nemo/lib/python3.11/site-packages/IPython/core/interactiveshell.py(3517): run_ast_nodes\n/home/ik/miniconda3/envs/nemo/lib/python3.11/site-packages/IPython/core/interactiveshell.py(3334): run_cell_async\n/home/ik/miniconda3/envs/nemo/lib/python3.11/site-packages/IPython/core/async_helpers.py(128): _pseudo_sync_runner\n/home/ik/miniconda3/envs/nemo/lib/python3.11/site-packages/IPython/core/interactiveshell.py(3130): _run_cell\n/home/ik/miniconda3/envs/nemo/lib/python3.11/site-packages/IPython/core/interactiveshell.py(3075): run_cell\n/home/ik/miniconda3/envs/nemo/lib/python3.11/site-packages/ipykernel/zmqshell.py(549): run_cell\n/home/ik/miniconda3/envs/nemo/lib/python3.11/site-packages/ipykernel/ipkernel.py(449): do_execute\n/home/ik/miniconda3/envs/nemo/lib/python3.11/site-packages/ipykernel/kernelbase.py(778): execute_request\n/home/ik/miniconda3/envs/nemo/lib/python3.11/site-packages/ipykernel/ipkernel.py(362): execute_request\n/home/ik/miniconda3/envs/nemo/lib/python3.11/site-packages/ipykernel/kernelbase.py(437): dispatch_shell\n/home/ik/miniconda3/envs/nemo/lib/python3.11/site-packages/ipykernel/kernelbase.py(534): process_one\n/home/ik/miniconda3/envs/nemo/lib/python3.11/site-packages/ipykernel/kernelbase.py(545): dispatch_queue\n/home/ik/miniconda3/envs/nemo/lib/python3.11/asyncio/events.py(84): _run\n/home/ik/miniconda3/envs/nemo/lib/python3.11/asyncio/base_events.py(1936): _run_once\n/home/ik/miniconda3/envs/nemo/lib/python3.11/asyncio/base_events.py(608): run_forever\n/home/ik/miniconda3/envs/nemo/lib/python3.11/site-packages/tornado/platform/asyncio.py(205): start\n/home/ik/miniconda3/envs/nemo/lib/python3.11/site-packages/ipykernel/kernelapp.py(739): start\n/home/ik/miniconda3/envs/nemo/lib/python3.11/site-packages/traitlets/config/application.py(1075): launch_instance\n/home/ik/miniconda3/envs/nemo/lib/python3.11/site-packages/ipykernel_launcher.py(18): <module>\n<frozen runpy>(88): _run_code\n<frozen runpy>(198): _run_module_as_main\n)\n\n    Inputs:\n        #0: 86 defined in (%86 : Float(*, *, *, strides=[2946, 2946, 1], requires_grad=0, device=cpu) = onnx::Pad[mode=\\\"reflect\\\"](%59, %85), scope: nemo.collections.asr.modules.audio_preprocessing.AudioToMelSpectrogramPreprocessor::/nemo.collections.asr.parts.preprocessing.features.FilterbankFeatures::featurizer # /home/ik/miniconda3/envs/nemo/lib/python3.11/site-packages/torch/nn/functional.py:5096:0\n    )  (type 'Tensor')\n        #1: 96 defined in (%96 : int[] = prim::ListConstruct(%90, %95), scope: nemo.collections.asr.modules.audio_preprocessing.AudioToMelSpectrogramPreprocessor::/nemo.collections.asr.parts.preprocessing.features.FilterbankFeatures::featurizer\n    )  (type 'List[int]')\n    Outputs:\n        #0: 97 defined in (%97 : Float(*, *, strides=[2946, 1], requires_grad=0, device=cpu) = onnx::Reshape[allowzero=0](%86, %96), scope: nemo.collections.asr.modules.audio_preprocessing.AudioToMelSpectrogramPreprocessor::/nemo.collections.asr.parts.preprocessing.features.FilterbankFeatures::featurizer # /home/ik/miniconda3/envs/nemo/lib/python3.11/site-packages/torch/functional.py:703:0\n    )  (type 'Tensor')\",\n\t\"stack\": \"---------------------------------------------------------------------------\nSymbolicValueError                        Traceback (most recent call last)\nCell In[10], line 8\n      4 preprocessor = quartznet.preprocessor.to(device)\n      6 preprocessor.eval()\n----> 8 preprocessor.export(\\\"prep.onnx\\\", onnx_opset_version=18)\n\nFile ~/Documents/aakhor/asr/NeMo/nemo/core/classes/exportable.py:114, in Exportable.export(self, output, input_example, verbose, do_constant_folding, onnx_opset_version, check_trace, dynamic_axes, check_tolerance, export_modules_as_functions, keep_initializers_as_inputs)\n    112 model = self.get_export_subnet(subnet_name)\n    113 out_name = augment_filename(output, subnet_name)\n--> 114 out, descr, out_example = model._export(\n    115     out_name,\n    116     input_example=input_example,\n    117     verbose=verbose,\n    118     do_constant_folding=do_constant_folding,\n    119     onnx_opset_version=onnx_opset_version,\n    120     check_trace=check_trace,\n    121     dynamic_axes=dynamic_axes,\n    122     check_tolerance=check_tolerance,\n    123     export_modules_as_functions=export_modules_as_functions,\n    124     keep_initializers_as_inputs=keep_initializers_as_inputs,\n    125 )\n    126 # Propagate input example (default scenario, may need to be overriden)\n    127 if input_example is not None:\n\nFile ~/Documents/aakhor/asr/NeMo/nemo/core/classes/exportable.py:221, in Exportable._export(self, output, input_example, verbose, do_constant_folding, onnx_opset_version, check_trace, dynamic_axes, check_tolerance, export_modules_as_functions, keep_initializers_as_inputs)\n    219     dynamic_axes = get_dynamic_axes(self.input_module.input_types_for_export, input_names)\n    220     dynamic_axes.update(get_dynamic_axes(self.output_module.output_types_for_export, output_names))\n--> 221 torch.onnx.export(\n    222     jitted_model,\n    223     input_example,\n    224     output,\n    225     input_names=input_names,\n    226     output_names=output_names,\n    227     verbose=verbose,\n    228     do_constant_folding=do_constant_folding,\n    229     dynamic_axes=dynamic_axes,\n    230     opset_version=onnx_opset_version,\n    231     keep_initializers_as_inputs=keep_initializers_as_inputs,\n    232     export_modules_as_functions=export_modules_as_functions,\n    233 )\n    235 if check_trace:\n    236     verify_runtime(self, output, check_trace_input, input_names, check_tolerance=check_tolerance)\n\nFile ~/miniconda3/envs/nemo/lib/python3.11/site-packages/torch/onnx/__init__.py:375, in export(model, args, f, kwargs, export_params, verbose, input_names, output_names, opset_version, dynamic_axes, keep_initializers_as_inputs, dynamo, external_data, dynamic_shapes, report, verify, profile, dump_exported_program, artifacts_dir, fallback, training, operator_export_type, do_constant_folding, custom_opsets, export_modules_as_functions, autograd_inlining, **_)\n    369 if dynamic_shapes:\n    370     raise ValueError(\n    371         \\\"The exporter only supports dynamic shapes \\\"\n    372         \\\"through parameter dynamic_axes when dynamo=False.\\\"\n    373     )\n--> 375 export(\n    376     model,\n    377     args,\n    378     f,  # type: ignore[arg-type]\n    379     kwargs=kwargs,\n    380     export_params=export_params,\n    381     verbose=verbose is True,\n    382     input_names=input_names,\n    383     output_names=output_names,\n    384     opset_version=opset_version,\n    385     dynamic_axes=dynamic_axes,\n    386     keep_initializers_as_inputs=keep_initializers_as_inputs,\n    387     training=training,\n    388     operator_export_type=operator_export_type,\n    389     do_constant_folding=do_constant_folding,\n    390     custom_opsets=custom_opsets,\n    391     export_modules_as_functions=export_modules_as_functions,\n    392     autograd_inlining=autograd_inlining,\n    393 )\n    394 return None\n\nFile ~/miniconda3/envs/nemo/lib/python3.11/site-packages/torch/onnx/utils.py:502, in export(model, args, f, kwargs, export_params, verbose, training, input_names, output_names, operator_export_type, opset_version, do_constant_folding, dynamic_axes, keep_initializers_as_inputs, custom_opsets, export_modules_as_functions, autograd_inlining)\n    499 if kwargs is not None:\n    500     args = args + (kwargs,)\n--> 502 _export(\n    503     model,\n    504     args,\n    505     f,\n    506     export_params,\n    507     verbose,\n    508     training,\n    509     input_names,\n    510     output_names,\n    511     operator_export_type=operator_export_type,\n    512     opset_version=opset_version,\n    513     do_constant_folding=do_constant_folding,\n    514     dynamic_axes=dynamic_axes,\n    515     keep_initializers_as_inputs=keep_initializers_as_inputs,\n    516     custom_opsets=custom_opsets,\n    517     export_modules_as_functions=export_modules_as_functions,\n    518     autograd_inlining=autograd_inlining,\n    519 )\n    521 return None\n\nFile ~/miniconda3/envs/nemo/lib/python3.11/site-packages/torch/onnx/utils.py:1564, in _export(model, args, f, export_params, verbose, training, input_names, output_names, operator_export_type, export_type, opset_version, do_constant_folding, dynamic_axes, keep_initializers_as_inputs, fixed_batch_size, custom_opsets, add_node_names, onnx_shape_inference, export_modules_as_functions, autograd_inlining)\n   1561     dynamic_axes = {}\n   1562 _validate_dynamic_axes(dynamic_axes, model, input_names, output_names)\n-> 1564 graph, params_dict, torch_out = _model_to_graph(\n   1565     model,\n   1566     args,\n   1567     verbose,\n   1568     input_names,\n   1569     output_names,\n   1570     operator_export_type,\n   1571     val_do_constant_folding,\n   1572     fixed_batch_size=fixed_batch_size,\n   1573     training=training,\n   1574     dynamic_axes=dynamic_axes,\n   1575 )\n   1577 # TODO: Don't allocate a in-memory string for the protobuf\n   1578 defer_weight_export = (\n   1579     export_type is not _exporter_states.ExportTypes.PROTOBUF_FILE\n   1580 )\n\nFile ~/miniconda3/envs/nemo/lib/python3.11/site-packages/torch/onnx/utils.py:1117, in _model_to_graph(model, args, verbose, input_names, output_names, operator_export_type, do_constant_folding, _disable_torch_constant_prop, fixed_batch_size, training, dynamic_axes)\n   1114 params_dict = _get_named_param_dict(graph, params)\n   1116 try:\n-> 1117     graph = _optimize_graph(\n   1118         graph,\n   1119         operator_export_type,\n   1120         _disable_torch_constant_prop=_disable_torch_constant_prop,\n   1121         fixed_batch_size=fixed_batch_size,\n   1122         params_dict=params_dict,\n   1123         dynamic_axes=dynamic_axes,\n   1124         input_names=input_names,\n   1125         module=module,\n   1126     )\n   1127 except Exception as e:\n   1128     torch.onnx.log(\\\"Torch IR graph at exception: \\\", graph)\n\nFile ~/miniconda3/envs/nemo/lib/python3.11/site-packages/torch/onnx/utils.py:639, in _optimize_graph(graph, operator_export_type, _disable_torch_constant_prop, fixed_batch_size, params_dict, dynamic_axes, input_names, module)\n    636     _C._jit_pass_onnx_set_dynamic_input_shape(graph, dynamic_axes, input_names)\n    637 _C._jit_pass_onnx_lint(graph)\n--> 639 graph = _C._jit_pass_onnx(graph, operator_export_type)\n    640 _C._jit_pass_onnx_lint(graph)\n    641 _C._jit_pass_lint(graph)\n\nFile ~/miniconda3/envs/nemo/lib/python3.11/site-packages/torch/onnx/utils.py:1836, in _run_symbolic_function(graph, block, node, inputs, env, values_in_env, new_nodes, operator_export_type)\n   1831     if symbolic_fn is not None:\n   1832         # TODO Wrap almost identical attrs assignment or comment the difference.\n   1833         attrs = {\n   1834             k: symbolic_helper._node_get(node, k) for k in node.attributeNames()\n   1835         }\n-> 1836         return symbolic_fn(graph_context, *inputs, **attrs)\n   1838 attrs = {\n   1839     k + \\\"_\\\" + node.kindOf(k)[0]: symbolic_helper._node_get(node, k)\n   1840     for k in node.attributeNames()\n   1841 }\n   1842 if namespace == \\\"onnx\\\":\n   1843     # Clone node to trigger ONNX shape inference\n\nFile ~/miniconda3/envs/nemo/lib/python3.11/site-packages/torch/onnx/symbolic_helper.py:281, in parse_args.<locals>.decorator.<locals>.wrapper(g, *args, **kwargs)\n    275 if len(kwargs) == 1:\n    276     assert \\\"_outputs\\\" in kwargs, (\n    277         f\\\"Symbolic function {fn.__name__}'s '**kwargs' can only contain \\\"\n    278         f\\\"'_outputs' key at '**kwargs'. \\\"\n    279         f\\\"{FILE_BUG_MSG}\\\"\n    280     )\n--> 281 return fn(g, *args, **kwargs)\n\nFile ~/miniconda3/envs/nemo/lib/python3.11/site-packages/torch/onnx/symbolic_opset17.py:135, in stft(g, input, n_fft, hop_length, win_length, window, normalized, onesided, return_complex)\n    133 # Checks\n    134 if return_complex:\n--> 135     raise errors.SymbolicValueError(\n    136         msg=\\\"STFT does not currently support complex types\\\", value=input\n    137     )\n    139 # Get STFT sizes\n    140 frame_step_value = hop_length if hop_length is not None else n_fft // 4\n\nSymbolicValueError: STFT does not currently support complex types  [Caused by the value '97 defined in (%97 : Float(*, *, strides=[2946, 1], requires_grad=0, device=cpu) = onnx::Reshape[allowzero=0](%86, %96), scope: nemo.collections.asr.modules.audio_preprocessing.AudioToMelSpectrogramPreprocessor::/nemo.collections.asr.parts.preprocessing.features.FilterbankFeatures::featurizer # /home/ik/miniconda3/envs/nemo/lib/python3.11/site-packages/torch/functional.py:703:0\n)' (type 'Tensor') in the TorchScript graph. The containing node has kind 'onnx::Reshape'.] \n    (node defined in /home/ik/miniconda3/envs/nemo/lib/python3.11/site-packages/torch/functional.py(703): stft\n/home/ik/Documents/aakhor/asr/NeMo/nemo/collections/asr/parts/preprocessing/features.py(308): <lambda>\n/home/ik/Documents/aakhor/asr/NeMo/nemo/collections/asr/parts/preprocessing/features.py(423): forward\n/home/ik/miniconda3/envs/nemo/lib/python3.11/site-packages/torch/utils/_contextlib.py(116): decorate_context\n/home/ik/miniconda3/envs/nemo/lib/python3.11/site-packages/torch/nn/modules/module.py(1726): _slow_forward\n/home/ik/miniconda3/envs/nemo/lib/python3.11/site-packages/torch/nn/modules/module.py(1747): _call_impl\n/home/ik/miniconda3/envs/nemo/lib/python3.11/site-packages/torch/nn/modules/module.py(1736): _wrapped_call_impl\n/home/ik/Documents/aakhor/asr/NeMo/nemo/collections/asr/modules/audio_preprocessing.py(292): get_features\n/home/ik/Documents/aakhor/asr/NeMo/nemo/collections/asr/modules/audio_preprocessing.py(91): forward\n/home/ik/miniconda3/envs/nemo/lib/python3.11/site-packages/torch/utils/_contextlib.py(116): decorate_context\n/home/ik/miniconda3/envs/nemo/lib/python3.11/site-packages/torch/nn/modules/module.py(1726): _slow_forward\n/home/ik/miniconda3/envs/nemo/lib/python3.11/site-packages/torch/nn/modules/module.py(1747): _call_impl\n/home/ik/miniconda3/envs/nemo/lib/python3.11/site-packages/torch/nn/modules/module.py(1736): _wrapped_call_impl\n/home/ik/miniconda3/envs/nemo/lib/python3.11/site-packages/torch/jit/_trace.py(130): wrapper\n/home/ik/miniconda3/envs/nemo/lib/python3.11/site-packages/torch/jit/_trace.py(139): forward\n/home/ik/miniconda3/envs/nemo/lib/python3.11/site-packages/torch/nn/modules/module.py(1747): _call_impl\n/home/ik/miniconda3/envs/nemo/lib/python3.11/site-packages/torch/nn/modules/module.py(1736): _wrapped_call_impl\n/home/ik/miniconda3/envs/nemo/lib/python3.11/site-packages/torch/jit/_trace.py(1500): _get_trace_graph\n/home/ik/miniconda3/envs/nemo/lib/python3.11/site-packages/torch/onnx/utils.py(904): _trace_and_get_graph_from_model\n/home/ik/miniconda3/envs/nemo/lib/python3.11/site-packages/torch/onnx/utils.py(997): _create_jit_graph\n/home/ik/miniconda3/envs/nemo/lib/python3.11/site-packages/torch/onnx/utils.py(1113): _model_to_graph\n/home/ik/miniconda3/envs/nemo/lib/python3.11/site-packages/torch/onnx/utils.py(1564): _export\n/home/ik/miniconda3/envs/nemo/lib/python3.11/site-packages/torch/onnx/utils.py(502): export\n/home/ik/miniconda3/envs/nemo/lib/python3.11/site-packages/torch/onnx/__init__.py(375): export\n/home/ik/Documents/aakhor/asr/NeMo/nemo/core/classes/exportable.py(221): _export\n/home/ik/Documents/aakhor/asr/NeMo/nemo/core/classes/exportable.py(114): export\n/tmp/ipykernel_10033/3317768717.py(8): <module>\n/home/ik/miniconda3/envs/nemo/lib/python3.11/site-packages/IPython/core/interactiveshell.py(3577): run_code\n/home/ik/miniconda3/envs/nemo/lib/python3.11/site-packages/IPython/core/interactiveshell.py(3517): run_ast_nodes\n/home/ik/miniconda3/envs/nemo/lib/python3.11/site-packages/IPython/core/interactiveshell.py(3334): run_cell_async\n/home/ik/miniconda3/envs/nemo/lib/python3.11/site-packages/IPython/core/async_helpers.py(128): _pseudo_sync_runner\n/home/ik/miniconda3/envs/nemo/lib/python3.11/site-packages/IPython/core/interactiveshell.py(3130): _run_cell\n/home/ik/miniconda3/envs/nemo/lib/python3.11/site-packages/IPython/core/interactiveshell.py(3075): run_cell\n/home/ik/miniconda3/envs/nemo/lib/python3.11/site-packages/ipykernel/zmqshell.py(549): run_cell\n/home/ik/miniconda3/envs/nemo/lib/python3.11/site-packages/ipykernel/ipkernel.py(449): do_execute\n/home/ik/miniconda3/envs/nemo/lib/python3.11/site-packages/ipykernel/kernelbase.py(778): execute_request\n/home/ik/miniconda3/envs/nemo/lib/python3.11/site-packages/ipykernel/ipkernel.py(362): execute_request\n/home/ik/miniconda3/envs/nemo/lib/python3.11/site-packages/ipykernel/kernelbase.py(437): dispatch_shell\n/home/ik/miniconda3/envs/nemo/lib/python3.11/site-packages/ipykernel/kernelbase.py(534): process_one\n/home/ik/miniconda3/envs/nemo/lib/python3.11/site-packages/ipykernel/kernelbase.py(545): dispatch_queue\n/home/ik/miniconda3/envs/nemo/lib/python3.11/asyncio/events.py(84): _run\n/home/ik/miniconda3/envs/nemo/lib/python3.11/asyncio/base_events.py(1936): _run_once\n/home/ik/miniconda3/envs/nemo/lib/python3.11/asyncio/base_events.py(608): run_forever\n/home/ik/miniconda3/envs/nemo/lib/python3.11/site-packages/tornado/platform/asyncio.py(205): start\n/home/ik/miniconda3/envs/nemo/lib/python3.11/site-packages/ipykernel/kernelapp.py(739): start\n/home/ik/miniconda3/envs/nemo/lib/python3.11/site-packages/traitlets/config/application.py(1075): launch_instance\n/home/ik/miniconda3/envs/nemo/lib/python3.11/site-packages/ipykernel_launcher.py(18): <module>\n<frozen runpy>(88): _run_code\n<frozen runpy>(198): _run_module_as_main\n)\n\n    Inputs:\n        #0: 86 defined in (%86 : Float(*, *, *, strides=[2946, 2946, 1], requires_grad=0, device=cpu) = onnx::Pad[mode=\\\"reflect\\\"](%59, %85), scope: nemo.collections.asr.modules.audio_preprocessing.AudioToMelSpectrogramPreprocessor::/nemo.collections.asr.parts.preprocessing.features.FilterbankFeatures::featurizer # /home/ik/miniconda3/envs/nemo/lib/python3.11/site-packages/torch/nn/functional.py:5096:0\n    )  (type 'Tensor')\n        #1: 96 defined in (%96 : int[] = prim::ListConstruct(%90, %95), scope: nemo.collections.asr.modules.audio_preprocessing.AudioToMelSpectrogramPreprocessor::/nemo.collections.asr.parts.preprocessing.features.FilterbankFeatures::featurizer\n    )  (type 'List[int]')\n    Outputs:\n        #0: 97 defined in (%97 : Float(*, *, strides=[2946, 1], requires_grad=0, device=cpu) = onnx::Reshape[allowzero=0](%86, %96), scope: nemo.collections.asr.modules.audio_preprocessing.AudioToMelSpectrogramPreprocessor::/nemo.collections.asr.parts.preprocessing.features.FilterbankFeatures::featurizer # /home/ik/miniconda3/envs/nemo/lib/python3.11/site-packages/torch/functional.py:703:0\n    )  (type 'Tensor')\"\n}\n```\n\n**Expected behavior**\n\nThe preprocessor should export to onnx successfully.\n\n**Environment overview (please complete the following information)**\n\n - Environment location: Local Ubuntu 22.04 \n - Method of NeMo install: from source using pip. Installed in editable mode.\n\n**Environment details**\n\n- OS version Ubuntu 22.04\n- PyTorch version 2.5.1+cu124\n- Python version 3.11.10\n\n**Additional context**\n\nThe full model exports fine though. But that does not contain the preprocessor.\n",
    "state": "closed",
    "created_at": "2024-12-20T06:38:13+00:00",
    "closed_at": "2025-01-27T01:57:57+00:00",
    "updated_at": "2025-01-27T01:57:57+00:00",
    "author": "kabyanil",
    "author_type": "User",
    "comments_count": 2,
    "reactions_count": 0,
    "labels": "bug,stale",
    "milestone": null,
    "closed_by": "github-actions[bot]",
    "resolution_time_hours": 907.3288888888889,
    "first_comments": [
      {
        "author": "github-actions[bot]",
        "created_at": "2025-01-20T01:57:43+00:00",
        "body": "This issue is stale because it has been open for 30 days with no activity. Remove stale label or comment or this will be closed in 7 days."
      },
      {
        "author": "github-actions[bot]",
        "created_at": "2025-01-27T01:57:56+00:00",
        "body": "This issue was closed because it has been inactive for 7 days since being marked as stale."
      }
    ],
    "url": "https://github.com/NVIDIA/NeMo/issues/11684"
  },
  {
    "repository": "NVIDIA/NeMo",
    "issue_number": 11683,
    "title": "Which version of transformer engine should I use, when I try to open ub_tp_comm_overlap?",
    "body": "I am using NeMo with version v2.0.0rc0. When I set ub_tp_comm_overlap to true with tp and sp 2, I met the following error:\n![Image](https://github.com/user-attachments/assets/8d78142c-2c64-4e8a-bdd8-71313485ef9c)\n\nThe version of transformer engine 1.6.0+c81733f. Should I update to newer te version?\n\nWhen I update transformer engine to 1.13.0+e5edd6c. There occurs another error in NeMo:\n![Image](https://github.com/user-attachments/assets/35d702a9-a599-4dab-99c6-42502e9f37c0)\n\nCUDA 11.8 should be used in NeMo. But  transformer engine in 1.13.0+e5edd6c version requires CUDA newer than 12.0.\n\nI'm stuck in these version issues.\nCould you please tell me which version of NeMo and TE and CUDA should I use to enable tp_comm_overlap feature?",
    "state": "closed",
    "created_at": "2024-12-20T05:40:25+00:00",
    "closed_at": "2025-01-27T01:57:58+00:00",
    "updated_at": "2025-01-27T01:57:59+00:00",
    "author": "sallyjunjun",
    "author_type": "User",
    "comments_count": 2,
    "reactions_count": 0,
    "labels": "stale",
    "milestone": null,
    "closed_by": "github-actions[bot]",
    "resolution_time_hours": 908.2925,
    "first_comments": [
      {
        "author": "github-actions[bot]",
        "created_at": "2025-01-20T01:57:45+00:00",
        "body": "This issue is stale because it has been open for 30 days with no activity. Remove stale label or comment or this will be closed in 7 days."
      },
      {
        "author": "github-actions[bot]",
        "created_at": "2025-01-27T01:57:58+00:00",
        "body": "This issue was closed because it has been inactive for 7 days since being marked as stale."
      }
    ],
    "url": "https://github.com/NVIDIA/NeMo/issues/11683"
  },
  {
    "repository": "NVIDIA/NeMo",
    "issue_number": 11625,
    "title": "How to disable `torch_dist` ckpt format ?",
    "body": "I noticed that `torch_dist` is the default ckpt format. How to set it to the normal `.pt` format? Thanks in advance.",
    "state": "closed",
    "created_at": "2024-12-17T13:21:30+00:00",
    "closed_at": "2025-01-24T01:56:59+00:00",
    "updated_at": "2025-01-24T01:57:00+00:00",
    "author": "zhaoyang-star",
    "author_type": "User",
    "comments_count": 2,
    "reactions_count": 0,
    "labels": "stale",
    "milestone": null,
    "closed_by": "github-actions[bot]",
    "resolution_time_hours": 900.5913888888889,
    "first_comments": [
      {
        "author": "github-actions[bot]",
        "created_at": "2025-01-17T01:55:53+00:00",
        "body": "This issue is stale because it has been open for 30 days with no activity. Remove stale label or comment or this will be closed in 7 days."
      },
      {
        "author": "github-actions[bot]",
        "created_at": "2025-01-24T01:56:59+00:00",
        "body": "This issue was closed because it has been inactive for 7 days since being marked as stale."
      }
    ],
    "url": "https://github.com/NVIDIA/NeMo/issues/11625"
  },
  {
    "repository": "NVIDIA/NeMo",
    "issue_number": 11623,
    "title": "speaker diarization inference Error: Missing 'speaker_model_cfg' Key in Configuration When Using Pretrained CKPT File",
    "body": "When running inference with the `multiscale_diar_decoder_infer.py` script after training the model, an error occurs related to a missing configuration attribute (`speaker_model_cfg`). The error traceback indicates that the configuration used for inference does not contain the required `speaker_model_cfg` key.\n\n**Steps/Code to reproduce bug**  \n1. Train the model using the following command:\n   ```bash\n   ./multiscale_diar_decoder.py --config-path=../conf/neural_diarizer --config-name=msdd_5scl_15_05_50Povl_256x3x32x2.yaml model.train_ds.manifest_filepath=/data2/xxx_data/SD/nemo/msdwild/train/msdd_data.50step.json model.validation_ds.manifest_filepath=/data2/xxx_data/SD/nemo/msdwild/test/msdd_data.50step.json model.train_ds.emb_dir=/data2/xxx_data/SD/nemo/msdwild/train model.validation_ds.emb_dir=/data2/xxx_data/SD/nemo/msdwild/test exp_manager.name=sample_train exp_manager.exp_dir=./msdd_exp\n   ```\n2. After training, use the following command to run inference:\n   ```bash\n   python multiscale_diar_decoder_infer.py --config-path='../conf/inference' --config-name='diar_infer_telephonic.yaml' \\\n   diarizer.msdd_model.model_path='/home/data/xxx/project/SD/NeMo/examples/speaker_tasks/diarization/neural_diarizer/msdd_exp/sample_train/2024-12-17_12-32-23/checkpoints/test.ckpt' \\\n   diarizer.oracle_vad=True \\\n   diarizer.manifest_filepath='/data2/xxx_data/SD/nemo/msdwild/train/msdd_data.50step.json' \\\n   diarizer.out_dir='output'\n   ```\n3. The following error occurs during inference:\n   ```\n   Traceback (most recent call last):\n   File \"/home/data/xxx/project/SD/NeMo/examples/speaker_tasks/diarization/neural_diarizer/multiscale_diar_decoder_infer.py\", line 31, in main\n   diarizer_model = NeuralDiarizer(cfg=cfg).to(cfg.device)\n   File \"/home/data/xxx/anaconda3/envs/nemo/lib/python3.10/site-packages/nemo/collections/asr/models/msdd_models.py\", line 1019, in __init__\n   self._init_msdd_model(cfg)\n   File \"/home/data/xxx/anaconda3/envs/nemo/lib/python3.10/site-packages/nemo/collections/asr/models/msdd_models.py\", line 1114, in _init_msdd_model\n   self.msdd_model = EncDecDiarLabelModel.load_from_checkpoint(\n   File \"/home/data/xxx/anaconda3/envs/nemo/lib/python3.10/site-packages/nemo/core/classes/modelPT.py\", line 506, in load_from_checkpoint\n   checkpoint = super().load_from_checkpoint(\n   File \"/home/data/xxx/anaconda3/envs/nemo/lib/python3.10/site-packages/lightning/pytorch/utilities/model_helpers.py\", line 125, in wrapper\n   return self.method(cls, *args, **kwargs)\n   File \"/home/data/xxx/anaconda3/envs/nemo/lib/python3.10/site-packages/lightning/pytorch/core/module.py\", line 1582, in load_from_checkpoint\n   loaded = _load_from_checkpoint(\n   File \"/home/data/xxx/anaconda3/envs/nemo/lib/python3.10/site-packages/lightning/pytorch/core/saving.py\", line 91, in _load_from_checkpoint\n   model = _load_state(cls, checkpoint, strict=strict, **kwargs)\n   File \"/home/data/xxx/anaconda3/envs/nemo/lib/python3.10/site-packages/lightning/pytorch/core/saving.py\", line 165, in _load_state\n   obj = instantiator(cls, _cls_kwargs) if instantiator else cls(**_cls_kwargs)\n   File \"/home/data/xxx/anaconda3/envs/nemo/lib/python3.10/site-packages/nemo/collections/asr/models/msdd_models.py\", line 145, in __init__\n   self.msdd._speaker_model = EncDexxxeakerLabelModel.from_config_dict(cfg.speaker_model_cfg)\n   omegaconf.errors.ConfigAttributeError: Key 'speaker_model_cfg' is not in struct\n   full_key: cfg.speaker_model_cfg\n   object_type=dict\n   ```\n\nI found a similar issue in #8463, but I am passing the ckpt file and the issue is not resolved.\nCould you please help?",
    "state": "closed",
    "created_at": "2024-12-17T07:10:32+00:00",
    "closed_at": "2025-01-24T01:57:00+00:00",
    "updated_at": "2025-01-24T01:57:01+00:00",
    "author": "sipercai",
    "author_type": "User",
    "comments_count": 2,
    "reactions_count": 0,
    "labels": "bug,stale",
    "milestone": null,
    "closed_by": "github-actions[bot]",
    "resolution_time_hours": 906.7744444444445,
    "first_comments": [
      {
        "author": "github-actions[bot]",
        "created_at": "2025-01-17T01:55:54+00:00",
        "body": "This issue is stale because it has been open for 30 days with no activity. Remove stale label or comment or this will be closed in 7 days."
      },
      {
        "author": "github-actions[bot]",
        "created_at": "2025-01-24T01:57:00+00:00",
        "body": "This issue was closed because it has been inactive for 7 days since being marked as stale."
      }
    ],
    "url": "https://github.com/NVIDIA/NeMo/issues/11623"
  },
  {
    "repository": "NVIDIA/NeMo",
    "issue_number": 11602,
    "title": "nemo forced aligner ''Tutorial: \"How to use NFA?\" 🚀 \" not working",
    "body": "**Describe the bug**\n\nnemo\n[A clear and concise description of what the bug is.\n\nhttps://github.com/NVIDIA/NeMo/tree/main/tools/nemo_forced_aligner\nis no more working\n](https://colab.research.google.com/github/NVIDIA/NeMo/blob/main/tutorials/tools/NeMo_Forced_Aligner_Tutorial.ipynb)\n\n**Steps/Code to reproduce bug**\n\n3. Run NFA\nIt keeps requiring installing packages \n\n\n\n\nunfortunately because of knowledge i could not resolve the problem.",
    "state": "closed",
    "created_at": "2024-12-15T12:54:57+00:00",
    "closed_at": "2025-01-22T01:58:12+00:00",
    "updated_at": "2025-01-22T01:58:13+00:00",
    "author": "rachidis",
    "author_type": "User",
    "comments_count": 2,
    "reactions_count": 0,
    "labels": "bug,stale",
    "milestone": null,
    "closed_by": "github-actions[bot]",
    "resolution_time_hours": 901.0541666666667,
    "first_comments": [
      {
        "author": "github-actions[bot]",
        "created_at": "2025-01-15T01:56:39+00:00",
        "body": "This issue is stale because it has been open for 30 days with no activity. Remove stale label or comment or this will be closed in 7 days."
      },
      {
        "author": "github-actions[bot]",
        "created_at": "2025-01-22T01:58:11+00:00",
        "body": "This issue was closed because it has been inactive for 7 days since being marked as stale."
      }
    ],
    "url": "https://github.com/NVIDIA/NeMo/issues/11602"
  },
  {
    "repository": "NVIDIA/NeMo",
    "issue_number": 11599,
    "title": "Memory consumption for Mamba2 8B finetuning (Nemo1)",
    "body": "In the documentation, it is mentioned that fine-tuning Mamba2 8B should be possible on 2 80GB A100s, which makes sense, and assuming everything to be fp32, the memory consumption is expected to be:\n- 32 GB for model params\n- 32 GB for gradients\n- 32 * 2 GB for optimizer states\nThis will sum up to 128 GB, however, in practice, it takes around ~240 GB to fine-tune Mamba8B using Nemo1 scripts.\n\nI would appreciate any information or explanation regarding this difference.\n\nThanks in advance for your assistance!",
    "state": "closed",
    "created_at": "2024-12-15T00:20:18+00:00",
    "closed_at": "2025-01-21T01:56:40+00:00",
    "updated_at": "2025-01-21T01:56:41+00:00",
    "author": "YasamanJafari",
    "author_type": "User",
    "comments_count": 2,
    "reactions_count": 0,
    "labels": "stale",
    "milestone": null,
    "closed_by": "github-actions[bot]",
    "resolution_time_hours": 889.6061111111111,
    "first_comments": [
      {
        "author": "github-actions[bot]",
        "created_at": "2025-01-14T01:55:47+00:00",
        "body": "This issue is stale because it has been open for 30 days with no activity. Remove stale label or comment or this will be closed in 7 days."
      },
      {
        "author": "github-actions[bot]",
        "created_at": "2025-01-21T01:56:40+00:00",
        "body": "This issue was closed because it has been inactive for 7 days since being marked as stale."
      }
    ],
    "url": "https://github.com/NVIDIA/NeMo/issues/11599"
  },
  {
    "repository": "NVIDIA/NeMo",
    "issue_number": 11598,
    "title": "Nemo1 to Nemo2 checkpoint conversion for Mamba2",
    "body": "Hi,\n\nEven though the Nemo2 finetuning recipes have been released for Mamba2, there is no code for converting the checkpoints from nemo1 to nemo2 for Mamba2 or from Megatron checkpoint to nemo2. Is there an estimate of when this code will be released? It would be greatly appreciated!\n\nThanks!",
    "state": "closed",
    "created_at": "2024-12-14T23:36:16+00:00",
    "closed_at": "2025-01-21T01:56:42+00:00",
    "updated_at": "2025-01-21T01:56:42+00:00",
    "author": "YasamanJafari",
    "author_type": "User",
    "comments_count": 2,
    "reactions_count": 0,
    "labels": "stale",
    "milestone": null,
    "closed_by": "github-actions[bot]",
    "resolution_time_hours": 890.3405555555555,
    "first_comments": [
      {
        "author": "github-actions[bot]",
        "created_at": "2025-01-14T01:55:49+00:00",
        "body": "This issue is stale because it has been open for 30 days with no activity. Remove stale label or comment or this will be closed in 7 days."
      },
      {
        "author": "github-actions[bot]",
        "created_at": "2025-01-21T01:56:41+00:00",
        "body": "This issue was closed because it has been inactive for 7 days since being marked as stale."
      }
    ],
    "url": "https://github.com/NVIDIA/NeMo/issues/11598"
  },
  {
    "repository": "NVIDIA/NeMo",
    "issue_number": 11536,
    "title": "Does Nemo support soft targets or doing regression?",
    "body": "**Is your feature request related to a problem? Please describe.**\n\nI'm trying to do supervised learning given labeled data of an input token sequence mapping to a probability output. Does Nemo support a simple way to do this? For example, via soft targets (predicting one of two tokens each with a certain probability that sums to 1) as opposed to 1-hot vectors, or otherwise doing regression for a continuous value?\n\n**Describe the solution you'd like**\n\nSome modification to standard SFT training for example.\n\n**Describe alternatives you've considered**\n\nHave tried looking at distillation as a proxy, where the probabilities imitate the logits produced by a teacher model. But this is quite cumbersome.\n\n**Additional context**\n",
    "state": "closed",
    "created_at": "2024-12-10T19:11:39+00:00",
    "closed_at": "2025-01-18T01:54:15+00:00",
    "updated_at": "2025-01-18T01:54:16+00:00",
    "author": "m-harmonic",
    "author_type": "User",
    "comments_count": 2,
    "reactions_count": 0,
    "labels": "stale",
    "milestone": null,
    "closed_by": "github-actions[bot]",
    "resolution_time_hours": 918.71,
    "first_comments": [
      {
        "author": "github-actions[bot]",
        "created_at": "2025-01-10T02:00:14+00:00",
        "body": "This issue is stale because it has been open for 30 days with no activity. Remove stale label or comment or this will be closed in 7 days."
      },
      {
        "author": "github-actions[bot]",
        "created_at": "2025-01-18T01:54:15+00:00",
        "body": "This issue was closed because it has been inactive for 7 days since being marked as stale."
      }
    ],
    "url": "https://github.com/NVIDIA/NeMo/issues/11536"
  },
  {
    "repository": "NVIDIA/NeMo",
    "issue_number": 11529,
    "title": "How can i get cache streaming on Fastconformer-ctc nemo?",
    "body": "Recently I'm looking for a streaming algorithm for Fastconformer-ctc but I only find the streaming for stt_en_fastconformer_hybrid_large_streaming (refer in https://github.com/NVIDIA/NeMo/blob/main/tutorials/asr/Online_ASR_Microphone_Demo_Cache_Aware_Streaming.ipynb). How I can find the cache streaming of Fastconformer-ctc ? thanks !",
    "state": "closed",
    "created_at": "2024-12-10T03:36:49+00:00",
    "closed_at": "2025-01-18T01:54:16+00:00",
    "updated_at": "2025-01-18T01:54:17+00:00",
    "author": "PhamDangNguyen",
    "author_type": "User",
    "comments_count": 2,
    "reactions_count": 0,
    "labels": "stale",
    "milestone": null,
    "closed_by": "github-actions[bot]",
    "resolution_time_hours": 934.2908333333334,
    "first_comments": [
      {
        "author": "github-actions[bot]",
        "created_at": "2025-01-10T02:00:16+00:00",
        "body": "This issue is stale because it has been open for 30 days with no activity. Remove stale label or comment or this will be closed in 7 days."
      },
      {
        "author": "github-actions[bot]",
        "created_at": "2025-01-18T01:54:16+00:00",
        "body": "This issue was closed because it has been inactive for 7 days since being marked as stale."
      }
    ],
    "url": "https://github.com/NVIDIA/NeMo/issues/11529"
  },
  {
    "repository": "NVIDIA/NeMo",
    "issue_number": 11516,
    "title": "transcribe method broken?",
    "body": "Hi.\n\nFor some reason, the `transcribe` method causes a core dump with quartznet or conformer. I'm running this in docker. My code is tweaked from one of the tutorials.\n\nMike H\n\nHere's my code:\n\n```\nimport os,glob,os,subprocess,tarfile\nimport wget,nemo,librosa,json\nfrom ruamel.yaml import YAML\nimport nemo.collections.asr as nemo_asr\nimport pytorch_lightning as pl\nfrom omegaconf import DictConfig\n\ndata_dir = '/data/an4'\nconfig_path = 'quartzconf.yaml'\nepochs = 3\n\nif not os.path.exists(data_dir):\n\tos.makedirs(data_dir)\n\n#download data\nif not os.path.exists(\n\t\tdata_dir + '/an4_sphere.tar.gz'\n\t):\n\tan4_url = 'https://dldata-public.s3.us' + \\\n\t\t'-east-2.amazonaws.com/an4_sphere.tar.gz'\n\tan4_path = wget.download(an4_url,data_dir)\nelse:\n\tan4_path = data_dir + '/an4_sphere.tar.gz'\n\n#convert to wav files\nif not os.path.exists(data_dir + '/an4/'):\n\ttar = tarfile.open(an4_path)\n\ttar.extractall(path=data_dir)\n\tsph_list = glob.glob(\n\t\tdata_dir + '/an4/**/*.sph',\n\t\trecursive=True\n\t)\n\tfor sph_path in sph_list:\n\t\twav_path = sph_path[:-4] + '.wav'\n\t\tcmd = [\"sox\",sph_path,wav_path]\n\t\tsubprocess.run(cmd)\n\n#function to create manifest file\ndef build_manifest(\n\t\ttranscripts_path,\n\t\tmanifest_path,wav_path\n\t):\n\twith open(transcripts_path,'r') as fin:\n\t\twith open(manifest_path,'w') as fout:\n\t\t\tfor line in fin:\n\t\t\t\ttranscript = line[: \\\n\t\t\t\t\tline.find('(')-1].lower()\n\t\t\t\ttranscript = transcript.replace(\n\t\t\t\t\t'<s>',''\n\t\t\t\t).replace('</s>','')\n\t\t\t\ttranscript = transcript.strip()\n\t\t\t\tfile_id = line[line.find('(')+1 : -2]\n\t\t\t\taudio_path = os.path.join(\n\t\t\t\t\tdata_dir,wav_path,\n\t\t\t\t\tfile_id[file_id.find('-')+1 : \\\n\t\t\t\t\t\tfile_id.rfind('-')],\n\t\t\t\t\tfile_id + '.wav')\n\t\t\t\tduration = librosa.core.get_duration(\n\t\t\t\t\tfilename=audio_path\n\t\t\t\t)\n\t\t\t\tmetadata = {\n\t\t\t\t\t\"audio_filepath\": audio_path,\n\t\t\t\t\t\"duration\": duration,\n\t\t\t\t\t\"text\": transcript\n\t\t\t\t}\n\t\t\t\tjson.dump(metadata,fout)\n\t\t\t\tfout.write('\\n')\n\t\t\t\t\n#make manifest files\ntrain_transcripts = data_dir + \\\n\t'/an4/etc/an4_train.transcription'\ntrain_manifest = data_dir + \\\n\t'/an4/train_manifest.json'\nif not os.path.isfile(train_manifest):\n\tbuild_manifest(\n\t\ttrain_transcripts,\n\t\ttrain_manifest,\n\t\t'an4/wav/an4_clstk'\n\t)\ntest_transcripts = data_dir + \\\n\t'/an4/etc/an4_test.transcription'\ntest_manifest = data_dir + \\\n\t'/an4/test_manifest.json'\nif not os.path.isfile(test_manifest):\n\tbuild_manifest(\n\t\ttest_transcripts,\n\t\ttest_manifest,\n\t\t'an4/wav/an4test_clstk'\n\t)\n\n#read config from yaml file\nyaml = YAML(typ='safe')\nwith open(config_path) as f:\n\tparams = yaml.load(f)\n\nprint(params)\n\n#build trainer\ntrainer = pl.Trainer(\n\tdevices=1,\n\taccelerator='gpu',\n\tmax_epochs=epochs\n)\n\n#specify training and validation data\nparams['model']['train_ds']\\\n\t['manifest_filepath'] = train_manifest\nparams['model']['validation_ds']\\\n\t['manifest_filepath'] = test_manifest\n\n#build model\nfirst_asr_model = \\\n\tnemo_asr.models.EncDecCTCModel(\n\t\tcfg=DictConfig(params['model']),\n\t\ttrainer=trainer\n)\n\n#train\ntrainer.fit(first_asr_model)\n\n#do some inference\npaths2audio_files = [\n\t\tos.path.join(\n\t\t\tdata_dir,\n\t\t\t'an4/wav/an4_clstk/mgah/cen2-mgah-b.wav'\n\t\t),\n\t\tos.path.join(\n\t\t\tdata_dir,\n\t\t\t'an4/wav/an4_clstk/fmjd/cen7-fmjd-b.wav'\n\t\t),\n\t\tos.path.join(\n\t\t\tdata_dir,\n\t\t\t'an4/wav/an4_clstk/fmjd/cen8-fmjd-b.wav'\n\t\t),\n\t\tos.path.join(\n\t\t\tdata_dir,\n\t\t\t'an4/wav/an4_clstk/fkai/cen8-fkai-b.wav'\n\t\t)\n\t]\n#print(first_asr_model.transcribe(\n#\tpaths2audio_files=paths2audio_files,\n#\tbatch_size=4\n#))\n```\n\nHere's the output:\n\n```\n{'name': 'QuartzNet15x5', 'sample_rate': 16000, 'repeat': 1, 'dropout': 0.0, 'separable': True, 'labels': [' ', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', \"'\"], 'model': {'train_ds': {'manifest_filepath': '???', 'sample_rate': 16000, 'labels': [' ', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', \"'\"], 'batch_size': 32, 'trim_silence': True, 'max_duration': 16.7, 'shuffle': True, 'num_workers': 4, 'pin_memory': True, 'is_tarred': False, 'tarred_audio_filepaths': None, 'shuffle_n': 2048, 'bucketing_strategy': 'synced_randomized', 'bucketing_batch_size': None}, 'validation_ds': {'manifest_filepath': '???', 'sample_rate': 16000, 'labels': [' ', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', \"'\"], 'batch_size': 32, 'shuffle': False, 'num_workers': 4, 'pin_memory': True}, 'preprocessor': {'_target_': 'nemo.collections.asr.modules.audio_preprocessing.AudioToMelSpectrogramPreprocessor', 'normalize': 'per_feature', 'window_size': 0.02, 'sample_rate': 16000, 'window_stride': 0.01, 'window': 'hann', 'features': 64, 'n_fft': 512, 'frame_splicing': 1, 'dither': 1e-05, 'stft_conv': False}, 'spec_augment': {'_target_': 'nemo.collections.asr.modules.SpectrogramAugmentation', 'rect_freq': 50, 'rect_masks': 5, 'rect_time': 120}, 'encoder': {'_target_': 'nemo.collections.asr.modules.ConvASREncoder', 'feat_in': 64, 'activation': 'relu', 'conv_mask': True, 'jasper': [{'filters': 128, 'repeat': 1, 'kernel': [11], 'stride': [1], 'dilation': [1], 'dropout': 0.0, 'residual': True, 'separable': True, 'se': True, 'se_context_size': -1}, {'filters': 256, 'repeat': 1, 'kernel': [13], 'stride': [1], 'dilation': [1], 'dropout': 0.0, 'residual': True, 'separable': True, 'se': True, 'se_context_size': -1}, {'filters': 256, 'repeat': 1, 'kernel': [15], 'stride': [1], 'dilation': [1], 'dropout': 0.0, 'residual': True, 'separable': True, 'se': True, 'se_context_size': -1}, {'filters': 256, 'repeat': 1, 'kernel': [17], 'stride': [1], 'dilation': [1], 'dropout': 0.0, 'residual': True, 'separable': True, 'se': True, 'se_context_size': -1}, {'filters': 256, 'repeat': 1, 'kernel': [19], 'stride': [1], 'dilation': [1], 'dropout': 0.0, 'residual': True, 'separable': True, 'se': True, 'se_context_size': -1}, {'filters': 256, 'repeat': 1, 'kernel': [21], 'stride': [1], 'dilation': [1], 'dropout': 0.0, 'residual': False, 'separable': True, 'se': True, 'se_context_size': -1}, {'filters': 1024, 'repeat': 1, 'kernel': [1], 'stride': [1], 'dilation': [1], 'dropout': 0.0, 'residual': False, 'separable': True, 'se': True, 'se_context_size': -1}]}, 'decoder': {'_target_': 'nemo.collections.asr.modules.ConvASRDecoder', 'feat_in': 1024, 'num_classes': 28, 'vocabulary': [' ', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', \"'\"]}, 'optim': {'name': 'novograd', 'lr': 0.01, 'betas': [0.8, 0.5], 'weight_decay': 0.001, 'sched': {'name': 'CosineAnnealing', 'monitor': 'val_loss', 'reduce_on_plateau': False, 'warmup_steps': None, 'warmup_ratio': None, 'min_lr': 0.0, 'last_epoch': -1}}}, 'trainer': {'devices': 1, 'max_epochs': 5, 'max_steps': -1, 'num_nodes': 1, 'accelerator': 'gpu', 'strategy': 'ddp', 'accumulate_grad_batches': 1, 'enable_checkpointing': False, 'logger': False, 'log_every_n_steps': 1, 'val_check_interval': 1.0, 'benchmark': False}, 'exp_manager': {'exp_dir': None, 'name': 'QuartzNet15x5', 'create_tensorboard_logger': True, 'create_checkpoint_callback': True}}\nGPU available: True (cuda), used: True\nTPU available: False, using: 0 TPU cores\nIPU available: False, using: 0 IPUs\nHPU available: False, using: 0 HPUs\n[NeMo I 2024-12-09 16:13:59 audio_to_text_dataset:49] Model level config does not contain `sample_rate`, please explicitly provide `sample_rate` to the dataloaders.\n[NeMo I 2024-12-09 16:13:59 audio_to_text_dataset:49] Model level config does not contain `labels`, please explicitly provide `labels` to the dataloaders.\n[NeMo I 2024-12-09 16:13:59 collections:196] Dataset loaded with 948 files totalling 0.71 hours\n[NeMo I 2024-12-09 16:13:59 collections:197] 0 files were filtered totalling 0.00 hours\n[NeMo I 2024-12-09 16:13:59 audio_to_text_dataset:49] Model level config does not contain `sample_rate`, please explicitly provide `sample_rate` to the dataloaders.\n[NeMo I 2024-12-09 16:13:59 audio_to_text_dataset:49] Model level config does not contain `labels`, please explicitly provide `labels` to the dataloaders.\n[NeMo I 2024-12-09 16:13:59 collections:196] Dataset loaded with 130 files totalling 0.10 hours\n[NeMo I 2024-12-09 16:13:59 collections:197] 0 files were filtered totalling 0.00 hours\n[NeMo I 2024-12-09 16:13:59 features:289] PADDING: 16\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n[NeMo I 2024-12-09 16:14:00 modelPT:728] Optimizer config = Novograd (\n    Parameter Group 0\n        amsgrad: False\n        betas: [0.8, 0.5]\n        eps: 1e-08\n        grad_averaging: False\n        lr: 0.01\n        weight_decay: 0.001\n    )\n[NeMo I 2024-12-09 16:14:00 lr_scheduler:910] Scheduler \"<nemo.core.optim.lr_scheduler.CosineAnnealing object at 0x7f8ff9db6da0>\" \n    will be used during training (effective maximum steps = 90) - \n    Parameters : \n    (warmup_steps: null\n    warmup_ratio: null\n    min_lr: 0.0\n    last_epoch: -1\n    max_steps: 90\n    )\n\n  | Name              | Type                              | Params\n------------------------------------------------------------------------\n0 | preprocessor      | AudioToMelSpectrogramPreprocessor | 0     \n1 | encoder           | ConvASREncoder                    | 1.2 M \n2 | decoder           | ConvASRDecoder                    | 29.7 K\n3 | loss              | CTCLoss                           | 0     \n4 | spec_augmentation | SpectrogramAugmentation           | 0     \n5 | _wer              | WER                               | 0     \n------------------------------------------------------------------------\n1.2 M     Trainable params\n0         Non-trainable params\n1.2 M     Total params\n4.836     Total estimated model params size (MB)\n[NeMo W 2024-12-09 16:14:01 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/fit_loop.py:281: PossibleUserWarning: The number of training batches (30) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n      rank_zero_warn(\n    \nEpoch 2: 100%|████████████████████████| 30/30 [00:01<00:00, 15.07it/s, v_num=15]`Trainer.fit` stopped: `max_epochs=3` reached.                                  \nEpoch 2: 100%|████████████████████████| 30/30 [00:02<00:00, 14.87it/s, v_num=15]\nTranscribing:   0%|                                       | 0/1 [00:00<?, ?it/s]Segmentation fault (core dumped)\n```\n\nHere's how the container was built:\n\n```\ndocker run -it \\\n  --gpus all \\\n  --ipc=host \\\n  --ulimit memlock=-1 \\\n  --ulimit stack=67108864 \\\n  -p 8888:8888 \\\n  --name ne \\\n  -v /data/:/mhdata \\\n  nvcr.io/nvidia/nemo:23.10\n```\n\n\n\n***\n\n**Describe the bug**\n\nA clear and concise description of what the bug is.\n\n**Steps/Code to reproduce bug**\n\nPlease list *minimal* steps or code snippet for us to be able to reproduce the bug.\n\nA  helpful guide on on how to craft a minimal bug report  http://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports. \n\n\n**Expected behavior**\n\nA clear and concise description of what you expected to happen.\n\n**Environment overview (please complete the following information)**\n\n - Environment location: [Bare-metal, Docker, Cloud(specify cloud provider - AWS, Azure, GCP, Collab)]\n - Method of NeMo install: [pip install or from source]. Please specify exact commands you used to install.\n - If method of install is [Docker], provide `docker pull` & `docker run` commands used\n\n**Environment details**\n\nIf NVIDIA docker image is used you don't need to specify these.\nOtherwise, please provide:\n- OS version\n- PyTorch version\n- Python version\n\n**Additional context**\n\nAdd any other context about the problem here.\nExample: GPU model\n",
    "state": "closed",
    "created_at": "2024-12-09T16:17:40+00:00",
    "closed_at": "2025-01-17T01:55:58+00:00",
    "updated_at": "2025-01-17T01:55:59+00:00",
    "author": "hammondm",
    "author_type": "User",
    "comments_count": 2,
    "reactions_count": 0,
    "labels": "bug,stale",
    "milestone": null,
    "closed_by": "github-actions[bot]",
    "resolution_time_hours": 921.6383333333333,
    "first_comments": [
      {
        "author": "github-actions[bot]",
        "created_at": "2025-01-09T02:13:14+00:00",
        "body": "This issue is stale because it has been open for 30 days with no activity. Remove stale label or comment or this will be closed in 7 days."
      },
      {
        "author": "github-actions[bot]",
        "created_at": "2025-01-17T01:55:58+00:00",
        "body": "This issue was closed because it has been inactive for 7 days since being marked as stale."
      }
    ],
    "url": "https://github.com/NVIDIA/NeMo/issues/11516"
  },
  {
    "repository": "NVIDIA/NeMo",
    "issue_number": 11512,
    "title": "An error occurs when exporting T5G2PModel to ONNX.",
    "body": "**Describe the bug**\n\nAn error occurs when exporting T5G2PModel to ONNX.\n\n**Steps/Code to reproduce bug**\n\n> import torch\n> from transformers import PreTrainedTokenizerBase, AutoTokenizer\n> \n> from nemo.collections.tts.g2p.models.t5 import T5G2PModel\n> model_name = \"T5G2P.nemo\"\n> model = T5G2PModel.restore_from(model_name, map_location=torch.device(\"cpu\"))\n> model.eval()\n> \n> model.export(\"test.onnx\")\n\n[Error]\nRuntimeError: 0 INTERNAL ASSERT FAILED at \"../torch/csrc/jit/ir/alias_analysis.cpp\":615, please report a bug to PyTorch. We don't have an op for aten::full but it isn't a special case.  Argument types: int[], bool, int, NoneType, Device, bool, \n\nCandidates:\n\taten::full.names(int[] size, Scalar fill_value, *, str[]? names, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor\n\taten::full(SymInt[] size, Scalar fill_value, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor\n\taten::full.names_out(int[] size, Scalar fill_value, *, str[]? names, Tensor(a!) out) -> Tensor(a!)\n\taten::full.out(SymInt[] size, Scalar fill_value, *, Tensor(a!) out) -> Tensor(a!)\n\n**Environment details**\n\nIf NVIDIA docker image is used you don't need to specify these.\nOtherwise, please provide:\n- Ubuntu 20.04\n- PyTorch version - 2.3.0+cu118\n- Python version - 3.10.12",
    "state": "closed",
    "created_at": "2024-12-09T05:01:02+00:00",
    "closed_at": "2025-01-17T01:55:59+00:00",
    "updated_at": "2025-01-17T01:56:00+00:00",
    "author": "VoiceGenerationProject",
    "author_type": "User",
    "comments_count": 2,
    "reactions_count": 0,
    "labels": "bug,stale",
    "milestone": null,
    "closed_by": "github-actions[bot]",
    "resolution_time_hours": 932.9158333333334,
    "first_comments": [
      {
        "author": "github-actions[bot]",
        "created_at": "2025-01-09T02:13:15+00:00",
        "body": "This issue is stale because it has been open for 30 days with no activity. Remove stale label or comment or this will be closed in 7 days."
      },
      {
        "author": "github-actions[bot]",
        "created_at": "2025-01-17T01:55:59+00:00",
        "body": "This issue was closed because it has been inactive for 7 days since being marked as stale."
      }
    ],
    "url": "https://github.com/NVIDIA/NeMo/issues/11512"
  },
  {
    "repository": "NVIDIA/NeMo",
    "issue_number": 11508,
    "title": "[ASR]: Compatibility error with NumPy v2.0",
    "body": "Hi\n\n**Describe the bug**\n\nCompatibility error with NumPy v2.0: AttributeError: `np.sctypes was removed in the NumPy 2.0 release. Access dtypes explicitly instead.. Did you mean: 'dtypes'?`\n\n**Steps/Code to reproduce bug**\n\nRun this code\n\n```python\nimport nemo.collections.asr as nemo_asr\n\n# 1. Download and instantiate the Persian model\nasr_model_name=\"nvidia/stt_fa_fastconformer_hybrid_large\"\nasr_model = nemo_asr.models.EncDecHybridRNNTCTCBPEModel.from_pretrained(model_name=asr_model_name)\n\naudio_file_path = \"voice_01.wav\"\nasr_model.transcribe([audio_file_path])\n```\n\n**Expected behavior**\n\nWork fine!\n\n**Environment overview (please complete the following information)**\n\n - Environment location: [Github Codespace)]\n - pip install nemo_toolkit==2.0.0\n\n**Environment details**\n\n- OS version: Linux codespaces-aa0b2f 6.5.0-1025-azure\n- PyTorch version: `2.5.1`\n- Python version: 3.12.7\n",
    "state": "closed",
    "created_at": "2024-12-07T19:14:48+00:00",
    "closed_at": "2024-12-16T17:47:18+00:00",
    "updated_at": "2024-12-19T14:10:48+00:00",
    "author": "njfamirm",
    "author_type": "User",
    "comments_count": 3,
    "reactions_count": 0,
    "labels": "bug",
    "milestone": null,
    "closed_by": "njfamirm",
    "resolution_time_hours": 214.54166666666666,
    "first_comments": [
      {
        "author": "njfamirm",
        "created_at": "2024-12-16T17:47:18+00:00",
        "body": "fixed with static version set for numpy"
      },
      {
        "author": "ymzayek",
        "created_at": "2024-12-19T08:38:39+00:00",
        "body": "@njfamirm what version did you fix numpy to? I'm having the same problem. I tried to downgrade to numpy 2.0.2 but had other compatibility issues. Is [ASR] not compatible with numpy 2 at all or just the latest release?\n\nEDIT: in the end it worked with 1.23.5"
      },
      {
        "author": "njfamirm",
        "created_at": "2024-12-19T14:10:48+00:00",
        "body": "> [@njfamirm](https://github.com/njfamirm) what version did you fix numpy to? I'm having the same problem. I tried to downgrade to numpy 2.0.2 but had other compatibility issues. Is [ASR] not compatible with numpy 2 at all or just the latest release?\n> \n> EDIT: in the end it worked with 1.23.5\n\nHi, `numpy==1.26.4`"
      }
    ],
    "url": "https://github.com/NVIDIA/NeMo/issues/11508"
  },
  {
    "repository": "NVIDIA/NeMo",
    "issue_number": 11507,
    "title": "[NLP] Punctuation and capitalisation",
    "body": "I have noticed that in the section https://github.com/NVIDIA/NeMo/tree/main/examples/nlp/token_classification, a warning has been added (2 months ago), which states the section is deprecated and scheduled for removal in release 23.11. \n\nIs this a mistake since on ngc the container is already at version 24.09? If not, could you share information on the future of punctuation and capitalisation, especially since the task is also an integral part in Riva pipelines?",
    "state": "closed",
    "created_at": "2024-12-07T08:49:50+00:00",
    "closed_at": "2025-01-15T01:56:42+00:00",
    "updated_at": "2025-01-15T01:56:43+00:00",
    "author": "itzsimpl",
    "author_type": "User",
    "comments_count": 2,
    "reactions_count": 0,
    "labels": "bug,stale",
    "milestone": null,
    "closed_by": "github-actions[bot]",
    "resolution_time_hours": 929.1144444444444,
    "first_comments": [
      {
        "author": "github-actions[bot]",
        "created_at": "2025-01-07T01:59:05+00:00",
        "body": "This issue is stale because it has been open for 30 days with no activity. Remove stale label or comment or this will be closed in 7 days."
      },
      {
        "author": "github-actions[bot]",
        "created_at": "2025-01-15T01:56:42+00:00",
        "body": "This issue was closed because it has been inactive for 7 days since being marked as stale."
      }
    ],
    "url": "https://github.com/NVIDIA/NeMo/issues/11507"
  },
  {
    "repository": "NVIDIA/NeMo",
    "issue_number": 11481,
    "title": "[NeMo2] Lightning Interaction with resuming within the same epoch",
    "body": "**Describe the bug**\n\nNeMo has some custom logic to resume the dataloader state with Megatron-LM custom Batch sampler based on consumed samples and microbatches calculated by mcore.\nBut it does not seem to account for how Lightning itself attempts to restore the data fetcher state for within the training epoch loop.\n\nThis results in a previous state from a checkpoint being accounted for in the dataloader / data fetcher iterator, and can cause StopIteration to be raised early before reaching the expected max_steps\n\n\n\n**Steps/Code to reproduce bug**\n\nFor example, if my initial training run setup is as follows\n```\ntrainer:\n  accelerator: gpu\n  devices: 1\n  num_nodes: 1\n  max_epochs: 1\n  max_steps: 1000\n  limit_val_batches: 2\n  num_sanity_val_steps: 2\n\ndata:\n  micro_batch_size: 1\n  global_batch_size: 8\n```\n\nMy data module should prepare max_steps * gbs = 1000 * 8 = 8000 samples total\nI save a checkpoint at step 799, which has consumed 800 * gbz = 800 * 8 = 6400 samples\n\nWhen I resume from checkpoint saved at step 799, it results in something like:\n```\n[NeMo I data_samplers:76] Instantiating MegatronPretrainingSampler with total_samples: 8040 and consumed_samples: 6400\n```\n\nWith 200 steps remaining before 1,000 max_steps, I should expect to have 200 * 8 = 1600 samples remaining which must get loaded by the resumed dataloader.\n\n\nHowever, pytorch lightning has the following logic in the TrainingEpochLoop\n```\n    def on_run_start(self, data_fetcher: _DataFetcher) -> None:\n    \t...\n\n        # add the previous `fetched` value to properly track `is_last_batch` with no prefetching\n        data_fetcher.fetched += self.batch_progress.current.ready\n        ...\n```\n\nAs a result, this is the state of the lightning data fetcher:\n```\n{\n    '_batch': None,\n    '_batch_idx': 0,\n    '_dataloader_idx': 0,\n    'done': False,\n    'fetched': 798,\n    'iterator': < pytorch_lightning.utilities.combined_loader.CombinedLoader object at 0x7fc9240ebeb0 > ,\n    'iterator_wrapper': < pytorch_lightning.loops.fetchers._DataFetcherWrapper object at 0x7fc92410b730 > ,\n    'length': 1640\n}\n```\n\nThe internal pytorch lightning tracked attributed `_batch_idx` and `fetched` are not synced, which results in a StopIteration being raised early when the following condition is met:\n```\nclass _DataFetcher(Iterator):\n    @override\n    def __next__(self) -> _ITERATOR_RETURN:\n        ....\n        self.fetched += 1\n        if self.length is not None:\n            self.done = self.fetched >= self.length  <---------------\n        return batch\n\nclass _DataLoaderIterDataFetcher(_DataFetcher):\n    @override\n    def __next__(self) -> Iterator[\"_DataFetcherWrapper\"]:  # type: ignore[override]\n        if self.done:   <---------------\n            raise StopIteration\n        return self.iterator_wrapper\n```\n\n\nIf trainer.max_epochs == 1, then training stops immediately before the expected 1000 steps. If unset, then it jumps to epoch 1 unexpectedly:\n```\n...\n[0]:Training epoch 0, iteration 901/999 | global_batch_size: 8 | val_loss: 9.243 | lr: 3e-06 | consumed_samples: 7216 | global_step: 901 | reduced_train_loss: 9.531 | train_step_timing in s: 0.4671\n[0]:Training epoch 0, iteration 902/999 | global_batch_size: 8 | val_loss: 9.243 | lr: 3e-06 | consumed_samples: 7224 | global_step: 902 | reduced_train_loss: 9.362 | train_step_timing in s: 0.3907\n[0]:Training epoch 0, iteration 903/999 | global_batch_size: 8 | val_loss: 9.243 | lr: 3e-06 | consumed_samples: 7232 | global_step: 903 | reduced_train_loss: 9.038 | train_step_timing in s: 0.4661\n[0]:`Trainer.fit` stopped: `max_epochs=1` reached.\n```\n\n\n**Expected behavior**\n\nInstead, we expect to see the full 1000k steps completed when we resume from a checkpoint in the middle of an epoch:\n\n```\n...\n[0]:Training epoch 0, iteration 997/999 | global_batch_size: 8 | val_loss: 9.243 | lr: 3e-06 | consumed_samples: 7984 | global_step: 997 | reduced_train_loss: 9.112 | train_step_timing in s: 0.4716\n[0]:Training epoch 0, iteration 998/999 | global_batch_size: 8 | val_loss: 9.243 | lr: 3e-06 | consumed_samples: 7992 | global_step: 998 | reduced_train_loss: 9.261 | train_step_timing in s: 0.4639\n[0]:Training epoch 0, iteration 999/999 | global_batch_size: 8 | val_loss: 9.243 | lr: 3e-06 | consumed_samples: 8000 | global_step: 999 | reduced_train_loss: 9.092 | train_step_timing in s: 0.4736\n[0]:`Trainer.fit` stopped: `max_steps=1000` reached.\n```\n\n\nA simple fix to get the expected behavior above is:\n\n```\nclass CustomTrainingLoop(_TrainingEpochLoop):\n\n    def on_run_start(self, data_fetcher: _DataFetcher) -> None:\n        super().on_run_start(data_fetcher)\n        data_fetcher.fetched = 0\n\ndef configure_custom_training_loop(trainer: pl.Trainer) -> None:\n    if type(trainer.fit_loop.epoch_loop) is not _TrainingEpochLoop:\n        print(\"Detected custom epoch loop. Skipping customizing training loop.\", UserWarning)\n        return\n\n    loop = CustomTrainingLoop(trainer, trainer.min_steps, trainer.max_steps)\n    trainer.fit_loop.epoch_loop = loop\n```\n\nAlternatively, we could also modify the logic in BaseMegatronSampler (nemo/collections/nlp/data/language_modeling/megatron/data_samplers.py) to return the total samples, but this could likely cause issues with NeMo1.\n```\n    def __len__(self):\n        num_available_samples: int = self.total_samples - self.consumed_samples\n        if self.global_batch_size is not None:\n            if self.drop_last:\n                num_global_batches = num_available_samples // self.global_batch_size\n            else:\n                num_global_batches = (num_available_samples + self.global_batch_size - 1) // self.global_batch_size\n            # return len of dataloader in terms of micro batches to avoid discrepancy between len of dataloader and\n            # num of batches fetched (as training step fetches in terms of micro batches)\n            return num_global_batches * (self.global_batch_size // self.micro_batch_times_data_parallel_size)\n        else:\n            return (num_available_samples - 1) // self.micro_batch_times_data_parallel_size + 1\n```\n\n\n**Environment**\n\n```\npytorch-lightning                  2.4.0\nnemo-toolkit                       2.1.0rc0\nmegatron-core                      0.10.0rc0\n```\n\n**Additional context**\n\nrelevant: https://github.com/Lightning-AI/pytorch-lightning/issues/19764\n\nAdd any other context about the problem here.\nExample: GPU model\n",
    "state": "closed",
    "created_at": "2024-12-05T12:20:05+00:00",
    "closed_at": "2024-12-06T02:56:10+00:00",
    "updated_at": "2024-12-06T02:56:11+00:00",
    "author": "ryxli",
    "author_type": "User",
    "comments_count": 1,
    "reactions_count": 0,
    "labels": "bug",
    "milestone": null,
    "closed_by": "ryxli",
    "resolution_time_hours": 14.60138888888889,
    "first_comments": [
      {
        "author": "ryxli",
        "created_at": "2024-12-06T02:56:10+00:00",
        "body": "root cause was due to usage of https://github.com/NVIDIA/NeMo/blob/9a268dd6cec33e0e7190cf9e1543e6473c32da2c/nemo/collections/nlp/data/language_modeling/megatron/data_samplers.py#L131\n\ninstead of the data samplers defined in https://github.com/NVIDIA/NeMo/blob/main/nemo/lightning/data.py"
      }
    ],
    "url": "https://github.com/NVIDIA/NeMo/issues/11481"
  },
  {
    "repository": "NVIDIA/NeMo",
    "issue_number": 11480,
    "title": "RuntimeError: world_size (1) is not divisible by 2",
    "body": "**Describe the bug**\n\nUnable to run the hello world script `nemo llm pretrain --factory llama3_8b` like in the documentation https://github.com/NVIDIA/NeMo/blob/main/examples/llm/pretrain/README.md\n\nI get: \n```\n[rank0]:   File \"/opt/megatron-lm/megatron/core/parallel_state.py\", line 532, in initialize_model_parallel\n[rank0]:     raise RuntimeError(f\"world_size ({world_size}) is not divisible by {total_model_size}\")\n[rank0]: RuntimeError: world_size (1) is not divisible by 2\n```\n\n**Steps/Code to reproduce bug**\n\n`srun -p batch -N 1 --container-image nvcr.io/nvidia/nemo:24.09 --pty bash`\nThe machine has 8 gpu according to nvidia-smi \n`run nemo llm pretrain --factory llama3_8b`\n\n**Expected behavior**\n\nI would expect it to run seamlessly \n\n**Environment overview (please complete the following information)**\n\n`srun -p batch -N 1 --container-image nvcr.io/nvidia/nemo:24.09 --pty bash`\nThe machine has 8 gpu according to nvidia-smi \n",
    "state": "closed",
    "created_at": "2024-12-05T11:34:45+00:00",
    "closed_at": "2025-01-12T02:04:16+00:00",
    "updated_at": "2025-01-12T02:04:16+00:00",
    "author": "chanansh",
    "author_type": "User",
    "comments_count": 2,
    "reactions_count": 0,
    "labels": "bug,stale",
    "milestone": null,
    "closed_by": "github-actions[bot]",
    "resolution_time_hours": 902.4919444444445,
    "first_comments": [
      {
        "author": "github-actions[bot]",
        "created_at": "2025-01-05T02:03:07+00:00",
        "body": "This issue is stale because it has been open for 30 days with no activity. Remove stale label or comment or this will be closed in 7 days."
      },
      {
        "author": "github-actions[bot]",
        "created_at": "2025-01-12T02:04:16+00:00",
        "body": "This issue was closed because it has been inactive for 7 days since being marked as stale."
      }
    ],
    "url": "https://github.com/NVIDIA/NeMo/issues/11480"
  },
  {
    "repository": "NVIDIA/NeMo",
    "issue_number": 11456,
    "title": "Bug when generating confidence scores with timestamps for a buffered rnnt model",
    "body": "**Describe the bug**\n\nIn trying to generate confidence scores with timestamps using an RNN transducer model (`stt_en_conformer_transducer_large`), with buffering, there is a type mismatch error on [this line](https://github.com/NVIDIA/NeMo/blob/9abd81bee41b44a3946a6e535025752eaa4314cd/nemo/collections/asr/parts/submodules/rnnt_decoding.py#L687) `for ts, te in zip(hyp.timestep, hyp.timestep[1:] + [len(hyp.frame_confidence)])`\n\n```\nSample:: 100%|██████████| 1/1 [00:00<00:00, 11618.57it/s]\n[NeMo W 2024-12-03 14:55:42 rnnt_decoding:1184] Specified segment seperators are not in supported punctuation {\"'\"}. If the seperators are not punctuation marks, ignore this warning. Otherwise, specify 'segment_gap_threshold' parameter in decoding config to form segments.\n<class 'dict'>\nBackend macosx is interactive backend. Turning interactive mode on.\nError executing job with overrides: ['model_path=null', 'pretrained_name=stt_en_conformer_transducer_large', 'audio_dir=/Users/aanchan/work/podcast_transcription_using_nemo/test', 'output_filename=/Users/aanchan/work/podcast_transcription_using_nemo/test_rnn_t_f1.json', 'total_buffer_in_secs=4.0', 'chunk_len_in_secs=1.6', 'model_stride=4', 'batch_size=32', 'merge_algo=lcs', 'lcs_alignment_dir=$PWD/lcs']\nTraceback (most recent call last):\n  File \"/Users/aanchan/work/podcast_transcription_using_nemo/env_nemo_1/lib/python3.10/site-packages/hydra/_internal/utils.py\", line 394, in _run_hydra\n    _run_app(\n  File \"/Users/aanchan/work/podcast_transcription_using_nemo/env_nemo_1/lib/python3.10/site-packages/hydra/_internal/utils.py\", line 457, in _run_app\n    run_and_report(\n  File \"/Users/aanchan/work/podcast_transcription_using_nemo/env_nemo_1/lib/python3.10/site-packages/hydra/_internal/utils.py\", line 223, in run_and_report\n    raise ex\n  File \"/Users/aanchan/work/podcast_transcription_using_nemo/env_nemo_1/lib/python3.10/site-packages/hydra/_internal/utils.py\", line 220, in run_and_report\n    return func()\n  File \"/Users/aanchan/work/podcast_transcription_using_nemo/env_nemo_1/lib/python3.10/site-packages/hydra/_internal/utils.py\", line 458, in <lambda>\n    lambda: hydra.run(\n  File \"/Users/aanchan/work/podcast_transcription_using_nemo/env_nemo_1/lib/python3.10/site-packages/hydra/_internal/hydra.py\", line 132, in run\n    _ = ret.return_value\n  File \"/Users/aanchan/work/podcast_transcription_using_nemo/env_nemo_1/lib/python3.10/site-packages/hydra/core/utils.py\", line 260, in return_value\n    raise self._return_value\n  File \"/Users/aanchan/work/podcast_transcription_using_nemo/env_nemo_1/lib/python3.10/site-packages/hydra/core/utils.py\", line 186, in run_job\n    ret.return_value = task_function(task_cfg)\n  File \"/Users/aanchan/work/podcast_transcription_using_nemo/rnnt_timestamps.py\", line 301, in main\n    hyps = get_buffered_pred_feat_rnnt(\n  File \"/Users/aanchan/work/podcast_transcription_using_nemo/env_nemo_1/lib/python3.10/site-packages/nemo/collections/asr/parts/utils/transcribe_utils.py\", line 95, in get_buffered_pred_feat_rnnt\n    hyp_list = asr.transcribe(tokens_per_chunk, delay)\n  File \"/Users/aanchan/work/podcast_transcription_using_nemo/env_nemo_1/lib/python3.10/site-packages/nemo/collections/asr/parts/utils/streaming_utils.py\", line 1309, in transcribe\n    self.infer_logits()\n  File \"/Users/aanchan/work/podcast_transcription_using_nemo/env_nemo_1/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n    return func(*args, **kwargs)\n  File \"/Users/aanchan/work/podcast_transcription_using_nemo/env_nemo_1/lib/python3.10/site-packages/nemo/collections/asr/parts/utils/streaming_utils.py\", line 1081, in infer_logits\n    self._get_batch_preds()\n  File \"/Users/aanchan/work/podcast_transcription_using_nemo/env_nemo_1/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n    return func(*args, **kwargs)\n  File \"/Users/aanchan/work/podcast_transcription_using_nemo/env_nemo_1/lib/python3.10/site-packages/nemo/collections/asr/parts/utils/streaming_utils.py\", line 1148, in _get_batch_preds\n    best_hyp, _ = self.asr_model.decoding.rnnt_decoder_predictions_tensor(\n  File \"/Users/aanchan/work/podcast_transcription_using_nemo/env_nemo_1/lib/python3.10/site-packages/nemo/collections/asr/parts/submodules/rnnt_decoding.py\", line 569, in rnnt_decoder_predictions_tensor\n    hypotheses = self.compute_confidence(hypotheses)\n  File \"/Users/aanchan/work/podcast_transcription_using_nemo/env_nemo_1/lib/python3.10/site-packages/nemo/collections/asr/parts/submodules/rnnt_decoding.py\", line 688, in compute_confidence\n    for ts, te in zip(hyp.timestep, hyp.timestep[1:] + [len(hyp.frame_confidence)]):\nTypeError: unhashable type: 'slice'\n```\n\nA clear and concise description of what the bug is.\n\nFrom the debugger it looks like hyp.timestamp is a dict, and really the zip should be happening over hyp.timestamp['timestamp'] which happens to be a PyTorch tensor. The slicing over a dict type seems incorrect e.g. hyp.timestamp[1:]\n\n![Image](https://github.com/user-attachments/assets/ac5bee79-e5fc-4d4f-9135-7d09d8c5d30f)\n\n**Steps/Code to reproduce bug**\n\nPlease list *minimal* steps or code snippet for us to be able to reproduce the bug.\n\nA  helpful guide on on how to craft a minimal bug report  http://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports. \n\n- Start with the code example [speech_to_text_buffered_infer_rnnt.py](https://github.com/NVIDIA/NeMo/blob/main/examples/asr/asr_chunked_inference/rnnt/speech_to_text_buffered_infer_rnnt.py)\n- Add a ConfidenceConfig as in the ASR with [confidence estimation tutorial ](https://github.com/NVIDIA/NeMo/blob/main/tutorials/asr/ASR_Confidence_Estimation.ipynb)\n```\nfrom nemo.collections.asr.parts.utils.asr_confidence_utils import (\n    ConfidenceConfig,\n    ConfidenceConstants,\n    ConfidenceMethodConfig,\n    ConfidenceMethodConstants,\n)\n\nconfidence_cfg = ConfidenceConfig(\n    preserve_frame_confidence=True, # Internally set to true if preserve_token_confidence == True\n    # or preserve_word_confidence == True\n    preserve_token_confidence=True, # Internally set to true if preserve_word_confidence == True\n    preserve_word_confidence=True,\n    aggregation=\"prod\", # How to aggregate frame scores to token scores and token scores to word scores\n    exclude_blank=False, # If true, only non-blank emissions contribute to confidence scores\n    tdt_include_duration=False, # If true, calculate duration confidence for the TDT models\n    method_cfg=ConfidenceMethodConfig( # Config for per-frame scores calculation (before aggregation)\n        name=\"max_prob\", # Or \"entropy\" (default), which usually works better\n        entropy_type=\"gibbs\", # Used only for name == \"entropy\". Recommended: \"tsallis\" (default) or \"renyi\"\n        alpha=0.5, # Low values (<1) increase sensitivity, high values decrease sensitivity\n        entropy_norm=\"lin\" # How to normalize (map to [0,1]) entropy. Default: \"exp\"\n    )\n)\n\n```\n\n- Change the decoding strategy and attach the confidence config to the RNNTDecoderConfig being used\n```\nasr_model.change_decoding_strategy(\n        RNNTDecodingConfig(compute_timestamps=True,\n                           preserve_alignments=True,\n                           confidence_cfg=confidence_cfg)\n    )\n\n```\n\n- Run the script with the following arguments\n\n```speech_to_text_buffered_infer_rnnt.py\nmodel_path=null\npretrained_name=stt_en_conformer_transducer_large\naudio_dir=/Users/aanchan/work/podcast_transcription_using_nemo/test\noutput_filename=/Users/aanchan/work/podcast_transcription_using_nemo/test_rnn_t_f1.json\ntotal_buffer_in_secs=4.0\nchunk_len_in_secs=1.6\nmodel_stride=4\nbatch_size=32\nmerge_algo=\"lcs\"\nlcs_alignment_dir=$PWD/lcs\n```\n\nAn example code file and input is in [this Google Drive folder](https://drive.google.com/drive/folders/1fzYkRT6ZRQcU7BvJLTrNCyB4bYWbN8uB?usp=sharing) \n\n**Expected behavior**\n\nA clear and concise description of what you expected to happen.\n\nThe expected output was a json file with time stamps written out.\n\n**Environment overview (please complete the following information)**\n\n - Environment location: [Bare-metal, Docker, Cloud(specify cloud provider - AWS, Azure, GCP, Collab)] \n The environment is a local laptop installation\n - Method of NeMo install: [pip install or from source]. Please specify exact commands you used to install.\n \n```\npip install Cython packaging\npip install --upgrade pip\nexport BRANCH=\"main\"\npip install torch torchvision torchaudio \npip install git+https://github.com/NVIDIA/NeMo.git@$BRANCH#egg=nemo_toolkit[asr]\n\n```\n\n - If method of install is [Docker], provide `docker pull` & `docker run` commands used\n\n**Environment details**\n\nIf NVIDIA docker image is used you don't need to specify these.\nOtherwise, please provide:\n- OS version : MacOS Monterey 12.5 \n- PyTorch version : 2.5.1\n- Python version 3.10.4\n\n**Additional context**\n\nAdd any other context about the problem here.\nExample: GPU model\n\nThis was run on a CPU, and not a GPU\n",
    "state": "closed",
    "created_at": "2024-12-03T09:47:54+00:00",
    "closed_at": "2025-01-10T02:00:20+00:00",
    "updated_at": "2025-01-10T02:00:20+00:00",
    "author": "aanchan",
    "author_type": "User",
    "comments_count": 2,
    "reactions_count": 0,
    "labels": "bug,stale",
    "milestone": null,
    "closed_by": "github-actions[bot]",
    "resolution_time_hours": 904.2072222222222,
    "first_comments": [
      {
        "author": "github-actions[bot]",
        "created_at": "2025-01-03T01:58:32+00:00",
        "body": "This issue is stale because it has been open for 30 days with no activity. Remove stale label or comment or this will be closed in 7 days."
      },
      {
        "author": "github-actions[bot]",
        "created_at": "2025-01-10T02:00:19+00:00",
        "body": "This issue was closed because it has been inactive for 7 days since being marked as stale."
      }
    ],
    "url": "https://github.com/NVIDIA/NeMo/issues/11456"
  },
  {
    "repository": "NVIDIA/NeMo",
    "issue_number": 11431,
    "title": "Question about attn_context_size in fastconformer_hybrid_transducer_ctc_bpe_streaming.yaml",
    "body": "I'm trying to fit the Hybrid Fastconformer for streaming, and found a strange thing, in the config that you specify with huggingface, it is indicated that self_attention_model: rel_pos. However, in this case, RelPositionMultiHeadAttention is called which does not accept attn_context_size but only max_cache_len. Is this a config error or am I missing something?",
    "state": "closed",
    "created_at": "2024-11-28T16:28:43+00:00",
    "closed_at": "2025-01-06T02:01:08+00:00",
    "updated_at": "2025-01-06T02:01:09+00:00",
    "author": "jootanehorror",
    "author_type": "User",
    "comments_count": 3,
    "reactions_count": 0,
    "labels": "stale",
    "milestone": null,
    "closed_by": "github-actions[bot]",
    "resolution_time_hours": 921.5402777777778,
    "first_comments": [
      {
        "author": "jootanehorror",
        "created_at": "2024-11-29T11:22:24+00:00",
        "body": "@titu1994 mb u can help me?\n"
      },
      {
        "author": "github-actions[bot]",
        "created_at": "2024-12-30T02:00:05+00:00",
        "body": "This issue is stale because it has been open for 30 days with no activity. Remove stale label or comment or this will be closed in 7 days."
      },
      {
        "author": "github-actions[bot]",
        "created_at": "2025-01-06T02:01:08+00:00",
        "body": "This issue was closed because it has been inactive for 7 days since being marked as stale."
      }
    ],
    "url": "https://github.com/NVIDIA/NeMo/issues/11431"
  },
  {
    "repository": "NVIDIA/NeMo",
    "issue_number": 11430,
    "title": "TitaNet-LID Identifying Spoken Language",
    "body": "**Is your feature request related to a problem? Please describe.**\n\nHi,\n\nI was reading the paper \"A Compact End-to-End Model with Local and Global Context for Spoken Language Identification\".\n\nIn the paper, it is mentioned that \"We open-source the model through NVIDIA NeMo\". However, after searching the repository, I could only find AmberNet.\n\nCould you please advise where I can find the setup or training scripts for TitaNet-LID? Is there a pre-trained model available for experimentation?\n\nThank you for your help!\n",
    "state": "closed",
    "created_at": "2024-11-28T14:42:32+00:00",
    "closed_at": "2025-01-06T02:01:10+00:00",
    "updated_at": "2025-01-06T02:01:11+00:00",
    "author": "apoori95",
    "author_type": "User",
    "comments_count": 2,
    "reactions_count": 0,
    "labels": "stale",
    "milestone": null,
    "closed_by": "github-actions[bot]",
    "resolution_time_hours": 923.3105555555555,
    "first_comments": [
      {
        "author": "github-actions[bot]",
        "created_at": "2024-12-30T02:00:06+00:00",
        "body": "This issue is stale because it has been open for 30 days with no activity. Remove stale label or comment or this will be closed in 7 days."
      },
      {
        "author": "github-actions[bot]",
        "created_at": "2025-01-06T02:01:09+00:00",
        "body": "This issue was closed because it has been inactive for 7 days since being marked as stale."
      }
    ],
    "url": "https://github.com/NVIDIA/NeMo/issues/11430"
  },
  {
    "repository": "NVIDIA/NeMo",
    "issue_number": 11414,
    "title": "Add support for instruct models in nemo 2",
    "body": "**Is your feature request related to a problem? Please describe.**\n\nI mostly work with instruct models, and most of the recipes are focused on base models. This is particularly prominent in the sequence packing, which doesn't account for instruct templates in the sequence.\n\n**Describe the solution you'd like**\n\nExamples of performing PEFT, with and without sequence packing on llama 3.1 8b instruct would be helpful.\n\n**Describe alternatives you've considered**\n\nRight now we've patched nemo 1 to work with this.\n\n",
    "state": "closed",
    "created_at": "2024-11-26T21:47:16+00:00",
    "closed_at": "2025-01-03T01:58:35+00:00",
    "updated_at": "2025-01-03T01:58:35+00:00",
    "author": "gabwow",
    "author_type": "User",
    "comments_count": 2,
    "reactions_count": 0,
    "labels": "stale",
    "milestone": null,
    "closed_by": "github-actions[bot]",
    "resolution_time_hours": 892.1886111111111,
    "first_comments": [
      {
        "author": "github-actions[bot]",
        "created_at": "2024-12-27T01:58:19+00:00",
        "body": "This issue is stale because it has been open for 30 days with no activity. Remove stale label or comment or this will be closed in 7 days."
      },
      {
        "author": "github-actions[bot]",
        "created_at": "2025-01-03T01:58:35+00:00",
        "body": "This issue was closed because it has been inactive for 7 days since being marked as stale."
      }
    ],
    "url": "https://github.com/NVIDIA/NeMo/issues/11414"
  },
  {
    "repository": "NVIDIA/NeMo",
    "issue_number": 11393,
    "title": "asr + diarization config setting problem",
    "body": "**Describe the bug**\n\nI am following the tutorial of asr+diariation under examples/speaker_tasks/diarization.\nI want to use diar_msdd_telephonic model so I have to add more parameters like this:\n\n```\npython  clustering_diarizer/offline_diar_with_asr_infer.py \\\n    diarizer.manifest_filepath='./manifest.json' \\\n    diarizer.out_dir='demo_asr_output' \\\n    diarizer.asr.model_path='stt_en_conformer_ctc_large' \\\n    diarizer.speaker_embeddings.parameters.save_embeddings=False \\\n    diarizer.asr.parameters.asr_based_vad=True \\\n    diarizer.speaker_embeddings.model_path='diar_msdd_telephonic' \\\n    parameters.window_length_in_sec=[1.5,1.0,0.5] \\\n    parameters.shift_length_in_sec=[0.75,0.5,0.25] \\\n    parameters.multiscale_weights=[0.33,0.33,0.33] \n\n```\n\nBut this setting will get error message caught, somthing like the follows:\n\n>  raise ConfigCompositionException(\nhydra.errors.ConfigCompositionException: Could not override 'parameters.window_length_in_sec'.\nTo append to your config use +parameters.window_length_in_sec=[1.5,1.0,0.5]\n\nI do not know if I understood it correctly, so I used \"+parameters.window_length_in_sec=[1.5,1.0,0.5] \\ \" instead.\nThe script can be executed successfully and proper results can be returned. But I am not sure if I have\nset the config correctly. I did not find anywhere in the verbose logging message my config had taken effect.\nHere  is logging output.\n\n>    speaker_embeddings:\n        model_path: diar_msdd_telephonic\n        parameters:\n          window_length_in_sec:\n          - 3.0\n          - 2.5\n          - 2.0\n          - 1.5\n          - 1.0\n          - 0.5\n          shift_length_in_sec:\n          - 1.5\n          - 1.25\n          - 1.0\n          - 0.75\n          - 0.5\n          - 0.25\n          multiscale_weights:\n          - 1\n          - 1\n          - 1\n          - 1\n          - 1\n          - 1\n          save_embeddings: false\n\n\n",
    "state": "closed",
    "created_at": "2024-11-25T09:00:25+00:00",
    "closed_at": "2024-11-25T09:45:37+00:00",
    "updated_at": "2024-11-25T09:45:37+00:00",
    "author": "ywangwxd",
    "author_type": "User",
    "comments_count": 1,
    "reactions_count": 0,
    "labels": "bug",
    "milestone": null,
    "closed_by": "ywangwxd",
    "resolution_time_hours": 0.7533333333333333,
    "first_comments": [
      {
        "author": "ywangwxd",
        "created_at": "2024-11-25T09:45:34+00:00",
        "body": "I know the reason, it should be \n\n>   #diarizer.speaker_embeddings.parameters.window_length_in_sec=[1.5,1.0,0.5] \\\n    #diarizer.speaker_embeddings.parameters.shift_length_in_sec=[0.75,0.5,0.25] \\\n    #diarizer.speaker_embeddings.parameters.multiscale_weights=[0.33,0.33,0.33] "
      }
    ],
    "url": "https://github.com/NVIDIA/NeMo/issues/11393"
  },
  {
    "repository": "NVIDIA/NeMo",
    "issue_number": 11386,
    "title": "Fine-tuning ASR Lightning Error",
    "body": "**Describe the bug**\n\nI'm getting this error when trying to fine-tune the Parakeet model using the `/examples/asr/speech_to_text_finetune.py` file.\n\n```python\nException has occurred: TypeError\n`model` must be a `LightningModule` or `torch._dynamo.OptimizedModule`, got `EncDecCTCModelBPE`\n  File \"/Users/gerrycervantes/code/NeMo/examples/asr/speech_to_text_finetune.py\", line 221, in main\n    trainer.fit(asr_model)\n  File \"/Users/gerrycervantes/code/NeMo/examples/asr/speech_to_text_finetune.py\", line 225, in <module>\n    main()  # noqa pylint: disable=no-value-for-parameter\n    ^^^^^^\nTypeError: `model` must be a `LightningModule` or `torch._dynamo.OptimizedModule`, got `EncDecCTCModelBPE`\n```\n\n**Steps/Code to reproduce bug**\n\n**Command**\nThis is the command I use for fine-tuning\n```bash\npython examples/asr/speech_to_text_finetune.py --config-path=conf/asr_finetune --config-name=asr_custom\n```\n\n**Data**\nSo I've downloaded the Librispeech Datasets and processed it using `/scripts/dataset_processing/get_librispeech_data.py`\n\n**Model**\nI've also downloaded the models from HuggingFace. https://huggingface.co/nvidia/parakeet-ctc-0.6b\n\n**Config**\nI've done a few changes to the configuration file so that it fits my environment.  I'm on a Mac computer, so I've tried running with CPU and MPS, but get the error with both ways.\n\nThis is the corresponding config YAML file, resembling the example config file.\n\n```YAML\nname: \"ASR_Custom_Finetuning\"\n\n# use `init_from_nemo_model` or `init_from_pretrained_model` to initialize the model\n# We do not currently support `init_from_ptl_ckpt` to create a single script for all types of models.\ninit_from_nemo_model: models/parakeet-ctc-1.1b.nemo # path to nemo model\n# init_from_pretrained_model: nvidia/parakeet-ctc-0.6b # path to nemo model\n\nmodel:\n  sample_rate: 16000\n\n  train_ds:\n    manifest_filepath: data/TRAIN_CLEAN_100.json\n    sample_rate: ${model.sample_rate}\n    batch_size: 16 # you may increase batch_size if your memory allows\n    shuffle: true\n    num_workers: 8\n    pin_memory: true\n    max_duration: 20\n    min_duration: 0.1\n    # tarred datasets\n    is_tarred: false\n    tarred_audio_filepaths: null\n    shuffle_n: 2048\n    # bucketing params\n    bucketing_strategy: \"fully_randomized\"\n    bucketing_batch_size: null\n\n  validation_ds:\n    manifest_filepath: data/dev_clean.json\n    sample_rate: ${model.sample_rate}\n    batch_size: 16\n    shuffle: false\n    use_start_end_token: false\n    num_workers: 8\n    pin_memory: true\n\n  test_ds:\n    manifest_filepath: null\n    sample_rate: ${model.sample_rate}\n    batch_size: 16\n    shuffle: false\n    use_start_end_token: false\n    num_workers: 8\n    pin_memory: true\n  \n  char_labels: # use for char based models\n    update_labels: false\n    labels: null # example list config: \\[' ', 'a', 'b', 'c'\\]\n\n  tokenizer: # use for spe/bpe based tokenizer models\n    update_tokenizer: false\n    dir: null  # path to directory which contains either tokenizer.model (bpe) or vocab.txt (for wpe)\n    type: bpe  # Can be either bpe (SentencePiece tokenizer) or wpe (WordPiece tokenizer)\n\n  spec_augment:\n    _target_: nemo.collections.asr.modules.SpectrogramAugmentation\n    freq_masks: 2 # set to zero to disable it\n    time_masks: 10 # set to zero to disable it\n    freq_width: 27\n    time_width: 0.05\n\n  optim:\n    name: adamw\n    lr: 1e-4\n    # optimizer arguments\n    betas: [0.9, 0.98]\n    weight_decay: 1e-3\n\n    # scheduler setup\n    sched:\n      name: CosineAnnealing\n      # scheduler config override\n      warmup_steps: 5000\n      warmup_ratio: null\n      min_lr: 5e-6\n\ntrainer:\n  devices: 4 # number of GPUs, -1 would use all available GPUs\n  num_nodes: 1\n  max_epochs: 50\n  max_steps: -1 # computed at runtime if not set\n  val_check_interval: 1.0 # Set to 0.25 to check 4 times per epoch, or an int for number of iterations\n  accelerator: cpu\n  strategy:\n    _target_: lightning.pytorch.strategies.SingleDeviceStrategy\n    # _target_: lightning.pytorch.strategies.DDPStrategy\n    gradient_as_bucket_view: true\n  accumulate_grad_batches: 1\n  gradient_clip_val: 0.0\n  precision: 32 # 16, 32, or bf16\n  log_every_n_steps: 10  # Interval of logging.\n  enable_progress_bar: True\n  num_sanity_val_steps: 0 # number of steps to perform validation steps for sanity check the validation process before starting the training, setting to 0 disables it\n  check_val_every_n_epoch: 1 # number of evaluations on validation every n epochs\n  sync_batchnorm: true\n  enable_checkpointing: False  # Provided by exp_manager\n  logger: false  # Provided by exp_manager\n  benchmark: false # needs to be false for models with variable-length speech input as it slows down training\n\n\nexp_manager:\n  exp_dir: null\n  name: ${name}\n  create_tensorboard_logger: true\n  create_checkpoint_callback: true\n  checkpoint_callback_params:\n    # in case of multiple validation sets, first one is used\n    monitor: \"val_wer\"\n    mode: \"min\"\n    save_top_k: 5\n    always_save_nemo: True # saves the checkpoints as nemo files along with PTL checkpoints\n  resume_if_exists: false\n  resume_ignore_no_checkpoint: false\n\n  create_wandb_logger: false\n  wandb_logger_kwargs:\n    name: null\n    project: null\n```\n\n**Environment overview (please complete the following information)**\n\n - Environment location: Bare-metal\n - Method of NeMo install: From source (git commit 34f740822a11802c2f0cea1c8039163b2b8bff92)\n ```bash\nconda init --all\n\nconda create --name nemo python==3.11.1\nconda activate nemo\nconda install pytorch::pytorch torchvision torchaudio -c pytorch\n\nbrew install mecab\nconda install -c conda-forge pynini\npip install cython packaging\n\ncd ~/code\ngit clone https://github.com/NVIDIA/NeMo.git\ncd NeMo\npip install 'nemo_toolkit[asr]'\n```\n\n**Environment details**\n- macOS: Sonoma 14.6.1\n- PyTorch: 2.5.1\n- Python version: 3.11.8\n- lightning                 2.4.0              pyhd8ed1ab_0    conda-forge\n- lightning-utilities       0.11.9             pyhff2d567_0    conda-forge",
    "state": "closed",
    "created_at": "2024-11-23T00:54:48+00:00",
    "closed_at": "2024-11-27T00:07:59+00:00",
    "updated_at": "2024-11-27T00:08:01+00:00",
    "author": "gcervantes8",
    "author_type": "User",
    "comments_count": 1,
    "reactions_count": 0,
    "labels": "bug",
    "milestone": null,
    "closed_by": "gcervantes8",
    "resolution_time_hours": 95.21972222222222,
    "first_comments": [
      {
        "author": "gcervantes8",
        "created_at": "2024-11-27T00:07:59+00:00",
        "body": "I was able to solve this by pulling the Nemo code from the 2.0.0 tag, and using that.  I believe a mismatch in source code and the nemo library caused this.  Probably because of the recent commit that changed from 'pytorch_lightning' -> 'lightning.pytorch'."
      }
    ],
    "url": "https://github.com/NVIDIA/NeMo/issues/11386"
  },
  {
    "repository": "NVIDIA/NeMo",
    "issue_number": 11374,
    "title": "NeMo docker image 24.09 shows weird FileNotFoundError for shards FileNotFoundError: [Errno 2] No such file or directory: '/tmp/tmpxoyazmhf/model_weights/model.decoder.layers.self_attention.core_attention._extra_state/shard_0_32.pt'",
    "body": "Hello everyone and Nvidia folks,\n\n\nWe are trying to finetune llama3_1_8b_instruct. (70b as well after 8b)  \n\nDocker pull Nvidia NeMo docker image 24.09 and run inside a container.\n\nUsing this notebook from Brev (now part of Nvidia) https://github.com/brevdev/notebooks/blob/main/llama31_law.ipynb\n\nThe error below happened when running the 1st cell at step 2 in the notebook for finetuning \n\n![Image](https://github.com/user-attachments/assets/14a4b19b-82f7-4d0d-a6d6-09b1c412cb35)\n\nWe are running this on our local machine with 4* A10 GPUs. \n\nTried both 1 GPU and 4 GPUs but got the same FileNotFoundError for shard\n\nFileNotFoundError: [Errno 2] No such file or directory: '/tmp/tmpxoyazmhf/model_weights/model.decoder.layers.self_attention.core_attention._extra_state/shard_0_32.pt'\n\n\nAnyone knows why this happened and how to solve? \n",
    "state": "closed",
    "created_at": "2024-11-22T02:15:20+00:00",
    "closed_at": "2024-12-30T02:00:09+00:00",
    "updated_at": "2024-12-30T17:09:40+00:00",
    "author": "groccy",
    "author_type": "User",
    "comments_count": 3,
    "reactions_count": 0,
    "labels": "bug,stale",
    "milestone": null,
    "closed_by": "github-actions[bot]",
    "resolution_time_hours": 911.7469444444445,
    "first_comments": [
      {
        "author": "github-actions[bot]",
        "created_at": "2024-12-23T01:59:37+00:00",
        "body": "This issue is stale because it has been open for 30 days with no activity. Remove stale label or comment or this will be closed in 7 days."
      },
      {
        "author": "github-actions[bot]",
        "created_at": "2024-12-30T02:00:08+00:00",
        "body": "This issue was closed because it has been inactive for 7 days since being marked as stale."
      },
      {
        "author": "Shahad-Mohammed",
        "created_at": "2024-12-30T17:09:38+00:00",
        "body": "Did you solve this? \n"
      }
    ],
    "url": "https://github.com/NVIDIA/NeMo/issues/11374"
  },
  {
    "repository": "NVIDIA/NeMo",
    "issue_number": 11361,
    "title": "Shape mismatch between fastcont_hybrid_bpe_large.yaml config and 110M TDT-CTC Model, when using custom 1024 size tokenizer",
    "body": "Hello!\n\nI am trying to fine-tune a Parakeet 110M TDT-CTC model, but with a different tokenizer with 1024 tokens. I get the following error when i do `asr_model.maybe_init_from_pretrained_checkpoint(cfg)`, from which I understand that the huggingface model's TDT decoder has 1030 tokens? From the model card I understood that it has 1024, the same as my decoder's size:\n\n```\n    raise RuntimeError(\nRuntimeError: Error(s) in loading state_dict for EncDecHybridRNNTCTCBPEModel:\n        size mismatch for joint.joint_net.2.weight: copying a param with shape torch.Size([1030, 640]) from checkpoint, the shape in current model is torch.Size([1025, 640]).\n        size mismatch for joint.joint_net.2.bias: copying a param with shape torch.Size([1030]) from checkpoint, the shape in current model is torch.Size([1025]).\n```\n\nThis is my config:\n\n```\n# It contains the default values for training a FastConformer-Hybrid-Transducer-CTC ASR model, large size (~115M) with sub-word encoding.\n# The model would have two decoders: RNNT (Transducer) and CTC\n\n# You may find more detail:\n# FastConformer here: https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/stable/asr/models.html#fast-conformer\n# Hybrid ASR: https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/stable/asr/models.html#hybrid-transducer-ctc\n# FastConformer-CTC's architecture config: NeMo/examples/asr/conf/fastconformer/fast-conformer_ctc_bpe.yaml\n# FastConformer-Transducer's architecture config, along with the optimal batch size and precision: NeMo/examples/asr/conf/fastconformer/fast-conformer_transducer_bpe.yaml\n\nname: \"FastConformer-Hybrid-Transducer-CTC-BPE\"\ninit_from_nemo_model: \"models/parakeet_tdt-ctc_110m.nemo\"\ninit_strict: false\n\nmodel:\n  sample_rate: 16000\n  compute_eval_loss: true # eval samples can be very long and exhaust memory. Disable computation of transducer loss during validation/testing with this flag.\n  log_prediction: true # enables logging sample predictions in the output during training\n  skip_nan_grad: false\n\n  model_defaults:\n    enc_hidden: ${model.encoder.d_model}\n    pred_hidden: 640\n    joint_hidden: 640\n\n  train_ds:\n    manifest_filepath: \"...\"\n    sample_rate: ${model.sample_rate}\n    batch_size: 80 # you may increase batch_size if your memory allows\n    use_semi_sorted_batching: true\n    randomization_factor: 0.1\n    drop_last: false\n    shuffle: true\n    num_workers: 8\n    pin_memory: true\n    max_duration: 20 # you may need to update it for your dataset\n    min_duration: 0.2\n    # tarred datasets\n    is_tarred: false\n    tarred_audio_filepaths: null\n    shuffle_n: 2048\n    # bucketing params\n    bucketing_strategy: \"synced_randomized\"\n    bucketing_batch_size: null\n\n  validation_ds:\n    manifest_filepath: ...\n    sample_rate: ${model.sample_rate}\n    batch_size: 8\n    shuffle: false\n    use_start_end_token: false\n    num_workers: 8\n    pin_memory: true\n\n  test_ds:\n    manifest_filepath: ..\nsample_rate: ${model.sample_rate}\n    batch_size: 8\n    shuffle: false\n    use_start_end_token: false\n    num_workers: 8\n    pin_memory: true\n\n  # You may find more detail on how to train a tokenizer at: /scripts/tokenizers/process_asr_text_tokenizer.py\n  # We recommend to use vocab size of 1024 with SPE Unigram for most languages\n  tokenizer:\n    dir:  \"tokenizer/\" # path to directory which contains either tokenizer.model (bpe) or vocab.txt (for wpe)\n    type: bpe  # Can be either bpe (SentencePiece tokenizer) or wpe (WordPiece tokenizer)\n\n  preprocessor:\n    _target_: nemo.collections.asr.modules.AudioToMelSpectrogramPreprocessor\n    sample_rate: ${model.sample_rate}\n    normalize: \"per_feature\"\n    window_size: 0.025\n    window_stride: 0.01\n    window: \"hann\"\n    features: 80\n    n_fft: 512\n    frame_splicing: 1\n    dither: 0.00001\n    pad_to: 0\n\n  spec_augment:\n    _target_: nemo.collections.asr.modules.SpectrogramAugmentation\n    freq_masks: 5 # set to zero to disable it\n    time_masks: 10 # set to zero to disable it\n    freq_width: 27\n    time_width: 0.05\n\n  encoder:\n    _target_: nemo.collections.asr.modules.ConformerEncoder\n    feat_in: ${model.preprocessor.features}\n    feat_out: -1 # you may set it if you need different output size other than the default d_model\n    n_layers: 17\n    d_model: 512\n\n    # Sub-sampling parameters\n    subsampling: dw_striding # vggnet, striding, stacking or stacking_norm, dw_striding\n    subsampling_factor: 8 # must be power of 2 for striding and vggnet\n    subsampling_conv_channels: 256 # set to -1 to make it equal to the d_model\n\n    # Feed forward module's params\n    ff_expansion_factor: 4\n\n    # Multi-headed Attention Module's params\n    self_attention_model: rel_pos # rel_pos or abs_pos\n    n_heads: 8 # may need to be lower for smaller d_models\n    # [left, right] specifies the number of steps to be seen from left and right of each step in self-attention\n    att_context_size: [-1, -1] # -1 means unlimited context\n    att_context_style: regular # regular or chunked_limited\n    xscaling: true # scales up the input embeddings by sqrt(d_model)\n    pos_emb_max_len: 5000\n\n    # Convolution module's params\n    conv_kernel_size: 9\n    conv_norm_type: 'batch_norm' # batch_norm or layer_norm or groupnormN (N specifies the number of groups)\n\n    ### regularization\n    dropout: 0.1 # The dropout used in most of the Conformer Modules\n    dropout_pre_encoder: 0.1 # The dropout used before the encoder\n    dropout_emb: 0.0 # The dropout used for embeddings\n    dropout_att: 0.1 # The dropout for multi-headed attention modules\n\n    # set to non-zero to enable stochastic depth\n    stochastic_depth_drop_prob: 0.0\n    stochastic_depth_mode: linear  # linear or uniform\n    stochastic_depth_start_layer: 1\n\n  decoder:\n    _target_: nemo.collections.asr.modules.RNNTDecoder\n    normalization_mode: null # Currently only null is supported for export.\n    random_state_sampling: false # Random state sampling: https://arxiv.org/pdf/1910.11455.pdf\n    blank_as_pad: true # This flag must be set in order to support exporting of RNNT models + efficient inference.\n\n    prednet:\n      pred_hidden: ${model.model_defaults.pred_hidden}\n      pred_rnn_layers: 1\n      t_max: null\n      dropout: 0.2\n\n  joint:\n    _target_: nemo.collections.asr.modules.RNNTJoint\n    log_softmax: null  # 'null' would set it automatically according to CPU/GPU device\n    preserve_memory: false  # dramatically slows down training, but might preserve some memory\n\n    # Fuses the computation of prediction net + joint net + loss + WER calculation\n    # to be run on sub-batches of size `fused_batch_size`.\n    # When this flag is set to true, consider the `batch_size` of *_ds to be just `encoder` batch size.\n    # `fused_batch_size` is the actual batch size of the prediction net, joint net and transducer loss.\n    # Using small values here will preserve a lot of memory during training, but will make training slower as well.\n    # An optimal ratio of fused_batch_size : *_ds.batch_size is 1:1.\n    # However, to preserve memory, this ratio can be 1:8 or even 1:16.\n    # Extreme case of 1:B (i.e. fused_batch_size=1) should be avoided as training speed would be very slow.\n    fuse_loss_wer: true\n    fused_batch_size: 4\n\n    jointnet:\n      joint_hidden: ${model.model_defaults.joint_hidden}\n      activation: \"relu\"\n      dropout: 0.2\n\n  decoding:\n    strategy: \"greedy_batch\" # can be greedy, greedy_batch, beam, tsd, alsd.\n\n    # greedy strategy config\n    greedy:\n      max_symbols: 10\n\n    # beam strategy config\n    beam:\n      beam_size: 2\n      return_best_hypothesis: False\n      score_norm: true\n      tsd_max_sym_exp: 50  # for Time Synchronous Decoding\n      alsd_max_target_len: 2.0  # for Alignment-Length Synchronous Decoding\n\n  # The section which would contain the decoder and decoding configs of the auxiliary CTC decoder\n  aux_ctc:\n    ctc_loss_weight: 0.3 # the weight used to combine the CTC loss with the RNNT loss\n    use_cer: false\n    ctc_reduction: 'mean_batch'\n    decoder:\n      _target_: nemo.collections.asr.modules.ConvASRDecoder\n      feat_in: null\n      num_classes: -1\n      vocabulary: []\n    decoding:\n      strategy: \"greedy\"\n\n  # config for InterCTC loss: https://arxiv.org/abs/2102.03216\n  # specify loss weights and which layers to use for InterCTC\n  # e.g., to reproduce the paper results, set loss_weights: [0.3]\n  # and apply_at_layers: [8] (assuming 18 layers). Note that final\n  # layer loss coefficient is automatically adjusted (to 0.7 in above example)\n  interctc:\n    loss_weights: []\n    apply_at_layers: []\n\n  loss:\n    loss_name: \"default\"\n    warprnnt_numba_kwargs:\n      # FastEmit regularization: https://arxiv.org/abs/2010.11148\n      # You may enable FastEmit to reduce the latency of the model for streaming\n      # It also helps to improve the accuracy of the model in streaming mode\n      fastemit_lambda: 0.0  # Recommended values to be in range [1e-4, 1e-2], 0.001 is a good start.\n      clamp: -1.0  # if > 0, applies gradient clamping in range [-clamp, clamp] for the joint tensor only.\n\n  optim:\n    name: adamw\n    lr: 6e-4\n    # optimizer arguments\n    betas: [0.9, 0.98]\n    # weight decay of 0.0 with lr of 2.0 also works fine\n    weight_decay: 1e-3\n\n    # scheduler setup\n    sched:\n      name: CosineAnnealing\n      # scheduler config override\n      warmup_steps: 25000\n      warmup_ratio: null\n      min_lr: 1e-6\n\ntrainer:\n  devices: -1 # number of GPUs, -1 would use all available GPUs\n  num_nodes: 1\n  max_epochs: 100\n  max_steps: -1 # computed at runtime if not set\n  val_check_interval: 1.0 # Set to 0.25 to check 4 times per epoch, or an int for number of iterations\n  accelerator: auto\n  strategy: ddp\n  accumulate_grad_batches: 4\n  gradient_clip_val: 1.0\n  precision: bf16-mixed # 16, 32, or bf16\n  log_every_n_steps: 100  # Interval of logging.\n  enable_progress_bar: True\n  num_sanity_val_steps: 0 # number of steps to perform validation steps for sanity check the validation process before starting the training, setting to 0 disables it\n  check_val_every_n_epoch: 1 # number of evaluations on validation every n epochs\n  sync_batchnorm: true\n  enable_checkpointing: False  # Provided by exp_manager\n  logger: false  # Provided by exp_manager\n  benchmark: false # needs to be false for models with variable-length speech input as it slows down training\n\n\nexp_manager:\n  exp_dir: null\n  name: \"parakeet_tdt_ctc_110m\"\n  create_tensorboard_logger: true\n  create_checkpoint_callback: true\n  checkpoint_callback_params:\n    # in case of multiple validation sets, first one is used\n    monitor: \"val_wer\"\n    mode: \"min\"\n    save_top_k: 100\n    always_save_nemo: True # saves the checkpoints as nemo files instead of PTL checkpoints.\n  resume_if_exists: true\n  resume_ignore_no_checkpoint: true\n\n  create_wandb_logger: false\n  wandb_logger_kwargs:\n    name: null\n    project: null\n(nemo) gpirlogeanu@vm-gabi-fr:~/git_repos/NeMo/examples/asr/train_dir$ vim config/tdt.yaml\n(nemo) gpirlogeanu@vm-gabi-fr:~/git_repos/NeMo/examples/asr/train_dir$ cat config/tdt.yaml\n# It contains the default values for training a FastConformer-Hybrid-Transducer-CTC ASR model, large size (~115M) with sub-word encoding.\n# The model would have two decoders: RNNT (Transducer) and CTC\n\n# You may find more detail:\n# FastConformer here: https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/stable/asr/models.html#fast-conformer\n# Hybrid ASR: https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/stable/asr/models.html#hybrid-transducer-ctc\n# FastConformer-CTC's architecture config: NeMo/examples/asr/conf/fastconformer/fast-conformer_ctc_bpe.yaml\n# FastConformer-Transducer's architecture config, along with the optimal batch size and precision: NeMo/examples/asr/conf/fastconformer/fast-conformer_transducer_bpe.yaml\n\nname: \"FastConformer-Hybrid-Transducer-CTC-BPE\"\ninit_from_nemo_model: \"models/parakeet_tdt-ctc_110m.nemo\"\nmodel:\n  sample_rate: 16000\n  compute_eval_loss: false # eval samples can be very long and exhaust memory. Disable computation of transducer loss during validation/testing with this flag.\n  log_prediction: true # enables logging sample predictions in the output during training\n  skip_nan_grad: false\n\n  model_defaults:\n    enc_hidden: ${model.encoder.d_model}\n    pred_hidden: 640\n    joint_hidden: 640\n\n  train_ds:\n    manifest_filepath: \"manifests/rodigits.json\" #???\n    sample_rate: ${model.sample_rate}\n    batch_size: 16 # you may increase batch_size if your memory allows\n    shuffle: true\n    num_workers: 8\n    pin_memory: true\n    max_duration: 20 # you may need to update it for your dataset\n    min_duration: 0.1\n    # tarred datasets\n    is_tarred: false\n    tarred_audio_filepaths: null\n    shuffle_n: 2048\n    # bucketing params\n    bucketing_strategy: \"synced_randomized\"\n    bucketing_batch_size: null\n\n  validation_ds:\n    manifest_filepath: \"manifests/rodigits.json\" #???\n    sample_rate: ${model.sample_rate}\n    batch_size: 16\n    shuffle: false\n    use_start_end_token: false\n    num_workers: 8\n    pin_memory: true\n\n  test_ds:\n    manifest_filepath: null\n    sample_rate: ${model.sample_rate}\n    batch_size: 16\n    shuffle: false\n    use_start_end_token: false\n    num_workers: 8\n    pin_memory: true\n\n  # You may find more detail on how to train a tokenizer at: /scripts/tokenizers/process_asr_text_tokenizer.py\n  # We recommend to use vocab size of 1024 with SPE Unigram for most languages\n  tokenizer:\n    dir:  \"tokenizers/5_normal_BAS+news_unigram/tokenizer_spe_unigram_v1024_max_5\"  # path to directory which contains either tokenizer.model (bpe) or vocab.txt (for wpe)\n    type: bpe  # Can be either bpe (SentencePiece tokenizer) or wpe (WordPiece tokenizer)\n\n  preprocessor:\n    _target_: nemo.collections.asr.modules.AudioToMelSpectrogramPreprocessor\n    sample_rate: ${model.sample_rate}\n    normalize: \"per_feature\"\n    window_size: 0.025\n    window_stride: 0.01\n    window: \"hann\"\n    features: 80\n    n_fft: 512\n    frame_splicing: 1\n    dither: 0.00001\n    pad_to: 0\n\n  spec_augment:\n    _target_: nemo.collections.asr.modules.SpectrogramAugmentation\n    freq_masks: 2 # set to zero to disable it\n    time_masks: 10 # set to zero to disable it\n    freq_width: 27\n    time_width: 0.05\n\n  encoder:\n    _target_: nemo.collections.asr.modules.ConformerEncoder\n    feat_in: ${model.preprocessor.features}\n    feat_out: -1 # you may set it if you need different output size other than the default d_model\n    n_layers: 17\n    d_model: 512\n    use_bias: True # whether to apply bias in the feedforward, MHA and convolution modules\n\n    # Sub-sampling parameters\n    subsampling: dw_striding # vggnet, striding, stacking or stacking_norm, dw_striding\n    subsampling_factor: 8 # must be power of 2 for striding and vggnet\n    subsampling_conv_channels: 256 # set to -1 to make it equal to the d_model\n\n    # Feed forward module's params\n    ff_expansion_factor: 4\n\n    # Multi-headed Attention Module's params\n    self_attention_model: rel_pos # rel_pos or abs_pos\n    n_heads: 8 # may need to be lower for smaller d_models\n    # [left, right] specifies the number of steps to be seen from left and right of each step in self-attention\n    att_context_size: [-1, -1] # -1 means unlimited context\n    att_context_style: regular # regular or chunked_limited\n    xscaling: true # scales up the input embeddings by sqrt(d_model)\n    pos_emb_max_len: 5000\n\n    # Convolution module's params\n    conv_kernel_size: 9\n    conv_norm_type: 'batch_norm' # batch_norm or layer_norm or groupnormN (N specifies the number of groups)\n\n    ### regularization\n    dropout: 0.1 # The dropout used in most of the Conformer Modules\n    dropout_pre_encoder: 0.1 # The dropout used before the encoder\n    dropout_emb: 0.0 # The dropout used for embeddings\n    dropout_att: 0.1 # The dropout for multi-headed attention modules\n\n    # set to non-zero to enable stochastic depth\n    stochastic_depth_drop_prob: 0.0\n    stochastic_depth_mode: linear  # linear or uniform\n    stochastic_depth_start_layer: 1\n\n  decoder:\n    _target_: nemo.collections.asr.modules.RNNTDecoder\n    normalization_mode: null # Currently only null is supported for export.\n    random_state_sampling: false # Random state sampling: https://arxiv.org/pdf/1910.11455.pdf\n    blank_as_pad: true # This flag must be set in order to support exporting of RNNT models + efficient inference.\n\n    prednet:\n      pred_hidden: ${model.model_defaults.pred_hidden}\n      pred_rnn_layers: 1\n      t_max: null\n      dropout: 0.2\n\n  joint:\n    _target_: nemo.collections.asr.modules.RNNTJoint\n    log_softmax: null  # 'null' would set it automatically according to CPU/GPU device\n    preserve_memory: false  # dramatically slows down training, but might preserve some memory\n\n    # Fuses the computation of prediction net + joint net + loss + WER calculation\n    # to be run on sub-batches of size `fused_batch_size`.\n    # When this flag is set to true, consider the `batch_size` of *_ds to be just `encoder` batch size.\n    # `fused_batch_size` is the actual batch size of the prediction net, joint net and transducer loss.\n    # Using small values here will preserve a lot of memory during training, but will make training slower as well.\n    # An optimal ratio of fused_batch_size : *_ds.batch_size is 1:1.\n    # However, to preserve memory, this ratio can be 1:8 or even 1:16.\n    # Extreme case of 1:B (i.e. fused_batch_size=1) should be avoided as training speed would be very slow.\n    fuse_loss_wer: true\n    fused_batch_size: 4\n\n    jointnet:\n      joint_hidden: ${model.model_defaults.joint_hidden}\n      activation: \"relu\"\n      dropout: 0.2\n\n  decoding:\n    strategy: \"greedy_batch\" # can be greedy, greedy_batch, beam, tsd, alsd.\n\n    # greedy strategy config\n    greedy:\n      max_symbols: 10\n\n    # beam strategy config\n    beam:\n      beam_size: 2\n      return_best_hypothesis: False\n      score_norm: true\n      tsd_max_sym_exp: 50  # for Time Synchronous Decoding\n      alsd_max_target_len: 2.0  # for Alignment-Length Synchronous Decoding\n\n  # The section which would contain the decoder and decoding configs of the auxiliary CTC decoder\n  aux_ctc:\n    ctc_loss_weight: 0.3 # the weight used to combine the CTC loss with the RNNT loss\n    use_cer: false\n    ctc_reduction: 'mean_batch'\n    decoder:\n      _target_: nemo.collections.asr.modules.ConvASRDecoder\n      feat_in: null\n      num_classes: -1\n      vocabulary: []\n    decoding:\n      strategy: \"greedy\"\n\n  # config for InterCTC loss: https://arxiv.org/abs/2102.03216\n  # specify loss weights and which layers to use for InterCTC\n  # e.g., to reproduce the paper results, set loss_weights: [0.3]\n  # and apply_at_layers: [8] (assuming 18 layers). Note that final\n  # layer loss coefficient is automatically adjusted (to 0.7 in above example)\n  interctc:\n    loss_weights: []\n    apply_at_layers: []\n\n  loss:\n    loss_name: \"default\"\n    warprnnt_numba_kwargs:\n      # FastEmit regularization: https://arxiv.org/abs/2010.11148\n      # You may enable FastEmit to reduce the latency of the model for streaming\n      # It also helps to improve the accuracy of the model in streaming mode\n      fastemit_lambda: 0.0  # Recommended values to be in range [1e-4, 1e-2], 0.001 is a good start.\n      clamp: -1.0  # if > 0, applies gradient clamping in range [-clamp, clamp] for the joint tensor only.\n\n  optim:\n    name: adamw\n    lr: 5.0\n    # optimizer arguments\n    betas: [0.9, 0.98]\n    weight_decay: 1e-3\n\n    # scheduler setup\n    sched:\n      name: NoamAnnealing\n      d_model: ${model.encoder.d_model}\n      # scheduler config override\n      warmup_steps: 10000\n      warmup_ratio: null\n      min_lr: 1e-6\n\ntrainer:\n  devices: -1 # number of GPUs, -1 would use all available GPUs\n  num_nodes: 1\n  max_epochs: 1000\n  max_steps: -1 # computed at runtime if not set\n  val_check_interval: 1.0 # Set to 0.25 to check 4 times per epoch, or an int for number of iterations\n  accelerator: auto\n  strategy: ddp\n    #_target_: lightning.pytorch.strategies.DDPStrategy\n    #gradient_as_bucket_view: true\n  accumulate_grad_batches: 1\n  gradient_clip_val: 1.0\n  precision: 32 # 16, 32, or bf16\n  log_every_n_steps: 10  # Interval of logging.\n  enable_progress_bar: True\n  num_sanity_val_steps: 0 # number of steps to perform validation steps for sanity check the validation process before starting the training, setting to 0 disables it\n  check_val_every_n_epoch: 1 # number of evaluations on validation every n epochs\n  sync_batchnorm: true\n  enable_checkpointing: False  # Provided by exp_manager\n  logger: false  # Provided by exp_manager\n  benchmark: false # needs to be false for models with variable-length speech input as it slows down training\n\n\nexp_manager:\n  exp_dir: null\n  name: ${name}\n  create_tensorboard_logger: true\n  create_checkpoint_callback: true\n  checkpoint_callback_params:\n    # in case of multiple validation sets, first one is used\n    monitor: \"val_wer\"\n    mode: \"min\"\n    save_top_k: 5\n    always_save_nemo: True # saves the checkpoints as nemo files instead of PTL checkpoints\n  resume_from_checkpoint: null # The path to a checkpoint file to continue the training, restores the whole state including the epoch, step, LR schedulers, apex, etc.\n  resume_if_exists: false\n  resume_ignore_no_checkpoint: false\n\n  create_wandb_logger: false\n  wandb_logger_kwargs:\n    name: null\n    project: null\n```\n\nAny help would be greatly appreciated!!",
    "state": "closed",
    "created_at": "2024-11-21T16:50:36+00:00",
    "closed_at": "2024-11-22T10:42:38+00:00",
    "updated_at": "2024-11-22T10:42:39+00:00",
    "author": "gabitza-tech",
    "author_type": "User",
    "comments_count": 3,
    "reactions_count": 0,
    "labels": "bug",
    "milestone": null,
    "closed_by": "gabitza-tech",
    "resolution_time_hours": 17.86722222222222,
    "first_comments": [
      {
        "author": "jeremy110",
        "created_at": "2024-11-22T03:12:48+00:00",
        "body": "You can use the following program to modify your vocabulary. Additionally, the reason it is 1030 is that it originally consists of 1024 + 5 (duration tokens) + 1 (padding).\n\n```python\nasr_model=ASRModel.from_pretrained('nvidia/parakeet-tdt_ctc-110m')\nasr_model.change_vocabulary(\n    new_tokenizer_dir=\"your_tokenizer\", \n    new_tokenizer_type=\"bpe\",\n    )\n```"
      },
      {
        "author": "gabitza-tech",
        "created_at": "2024-11-22T06:14:46+00:00",
        "body": "> You can use the following program to modify your vocabulary. Additionally, the reason it is 1030 is that it originally consists of 1024 + 5 (duration tokens) + 1 (padding).\n> \n> asr_model=ASRModel.from_pretrained('nvidia/parakeet-tdt_ctc-110m')\n> asr_model.change_vocabulary(\n>     new_tokenizer_dir=\"your_tokenizer\", \n>     new_tokenizer_type=\"bpe\",\n>     )\n\nThank you for your response! Will this affect performance, as my tokenizer doesn't have duration tokens? ( or does it add them automatically?) Also, does this mean it will train the decoder from scratch? ( as I change it with a different size one)"
      },
      {
        "author": "jeremy110",
        "created_at": "2024-11-22T07:29:37+00:00",
        "body": "1. The program will add it automatically.\n2. Yes, when you want to train a new language or if the token size is different, you need to retrain the decoder from scratch."
      }
    ],
    "url": "https://github.com/NVIDIA/NeMo/issues/11361"
  },
  {
    "repository": "NVIDIA/NeMo",
    "issue_number": 11360,
    "title": "Drastic difference between .nemo and HF checkpoint",
    "body": "**Describe the bug**\n\nI have trained a llama-like model with nemo using the below model config:\n```\nmodel:\n  mcore_gpt: True\n  micro_batch_size: 1\n  global_batch_size: 512\n  tensor_model_parallel_size: 1\n  pipeline_model_parallel_size: 1\n  virtual_pipeline_model_parallel_size: null\n  context_parallel_size: 1\n  encoder_seq_length: 8192\n  max_position_embeddings: ${.encoder_seq_length}\n  num_layers: 28\n  hidden_size: 2048\n  ffn_hidden_size: 11008\n  num_attention_heads: 16\n  init_method_std: 0.02\n  use_scaled_init_method: True\n  hidden_dropout: 0.0\n  attention_dropout: 0.0\n  ffn_dropout: 0.0\n  kv_channels: null\n  apply_query_key_layer_scaling: True\n  normalization: 'rmsnorm'\n  layernorm_epsilon: 1e-6\n  do_layer_norm_weight_decay: False\n  make_vocab_size_divisible_by: 128\n  pre_process: True\n  post_process: True\n  persist_layer_norm: True\n  bias: False\n  activation: 'fast-swiglu'\n  headscale: False\n  transformer_block_type: 'pre_ln'\n  openai_gelu: False\n  normalize_attention_scores: True\n  position_embedding_type: 'rope'\n  rotary_percentage: 1.0\n  attention_type: 'multihead'\n  share_embeddings_and_output_weights: False\n  overlap_p2p_comm: False\n  batch_p2p_comm: True\n  num_query_groups: 8\n  rotary_base: 10000.0\n```\nThe model works well when I run inference using the nemo checkpoint ([script](https://github.com/NVIDIA/NeMo/blob/main/scripts/deploy/nlp/query.py)). But the converted checkpoint ([script](https://github.com/NVIDIA/NeMo/blob/main/scripts/checkpoint_converters/convert_llama_nemo_to_hf.py)) drastically drops in performance. Any ideas why this might be happening? My only hunch is that `apply_query_key_layer_scaling=True` in nemo, which might not be the case in HF.\n\n**Environment details**\nhttps://docs.nvidia.com/nemo-framework/user-guide/latest/softwarecomponentversions.html#nemo-framework-24-05\n",
    "state": "closed",
    "created_at": "2024-11-21T13:08:59+00:00",
    "closed_at": "2025-01-01T02:03:36+00:00",
    "updated_at": "2025-01-01T02:03:36+00:00",
    "author": "rahul-sarvam",
    "author_type": "User",
    "comments_count": 3,
    "reactions_count": 0,
    "labels": "bug,stale",
    "milestone": null,
    "closed_by": "github-actions[bot]",
    "resolution_time_hours": 972.9102777777778,
    "first_comments": [
      {
        "author": "rahul-sarvam",
        "created_at": "2024-11-25T00:01:41+00:00",
        "body": "I have compared a bunch of things between the 2 models and looks like there is a large difference between the logits of the 2 models.\n\n```\nnemo_model = MegatronGPTModel.restore_from(\n    nemo_path,\n    trainer=dummy_trainer,\n    override_config_path=model_config,\n    map_location=map_location\n)\n\n# Load HuggingFace model\nhf_model = AutoModelForCausalLM.from_pretrained(\n    hf_path,\n    local_files_only=True,\n    torch_dtype=torch.bfloat16 # nemo_model.dtype\n)\n\n# Load tokenizer\ntokenizer = LlamaTokenizer.from_pretrained(tokenizer_path, legacy=False)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.pad_token_id = tokenizer.eos_token_id\n\n# Move models to device\nnemo_model = nemo_model.to(device)\nhf_model = hf_model.to(device)\n\n# Set both models to eval mode\nnemo_model.eval()\nhf_model.eval()\n\n# Create random input ids\ninput_ids = torch.randint(\n    100, 1000,\n    (test_batch_size, test_seq_length),\n    device=device\n)\nattention_mask = torch.ones_like(input_ids)\n\nwith torch.no_grad():\n    # NeMo forward pass\n    nemo_output = nemo_model(\n        tokens=input_ids,\n        text_position_ids=torch.arange(test_seq_length, device=device),\n        attention_mask=attention_mask,\n        labels=None\n    )\n    \n    # HF forward pass\n    hf_output = hf_model(\n        input_ids=input_ids,\n        attention_mask=attention_mask,\n        output_hidden_states=True,\n        return_dict=True\n    ).logits\n\n# Compare logits\nlogits_match = torch.allclose(\n    nemo_output,\n    hf_output,\n    rtol=rtol,\n    atol=atol\n)\n\nmetrics['logits_max_diff'] = float(\n    torch.max(torch.abs(nemo_output - hf_output)).cpu()\n)\n```\n\nOutput:\n```\nConversion test results:\nLogits match: False (max diff: 4.91e+00)\n  Parameters match: True (max diff: 0.00e+00)\n  Generation match: 0.0\n    Sample generation comparison:\n      Input text: '<s>[INST] Hello [/INST]\\n'\n      NeMo output: \"<s>[INST] Hello [/INST]\\n Hello. It's nice to meet you. Is there something I can help you with or\"\n      HF output: '<s> [INST] Hello [/INST]\\n Hello. ನಿಮ್ಮನ್ನ ಭೇಟಿ ಮಾಡಿ ಸಂತೋಷ ಆಯ್ತು. ನಿಮಗೆ ಏನ'\nNumber of parameters match: 1.0 (Nemo: 2525087744, HF: 2525087744)\n❌ Conversion test failed!\n```\n\nI am not able to pinpoint why this is happening. Any pointers will be helpful."
      },
      {
        "author": "github-actions[bot]",
        "created_at": "2024-12-25T01:57:41+00:00",
        "body": "This issue is stale because it has been open for 30 days with no activity. Remove stale label or comment or this will be closed in 7 days."
      },
      {
        "author": "github-actions[bot]",
        "created_at": "2025-01-01T02:03:34+00:00",
        "body": "This issue was closed because it has been inactive for 7 days since being marked as stale."
      }
    ],
    "url": "https://github.com/NVIDIA/NeMo/issues/11360"
  },
  {
    "repository": "NVIDIA/NeMo",
    "issue_number": 11359,
    "title": "Doesn't have the shards in expected directory.",
    "body": "**Describe the bug**\n\nI was trying to run the following lines:\n\n\n```\n%%bash\n\n# Set paths to the model, train, validation and test sets.\nMODEL=\"./llama-3_1-8b-instruct-nemo_v1.0/llama3_1_8b_instruct.nemo\"\n\nTRAIN_DS=\"[./curated-data/law-qa-train_preprocessed.jsonl]\"\nVALID_DS=\"[./curated-data/law-qa-val_preprocessed.jsonl]\"\nTEST_DS=\"[./curated-data/law-qa-test_preprocessed.jsonl]\"\nTEST_NAMES=\"[law]\"\n\nSCHEME=\"lora\"\nTP_SIZE=1\nPP_SIZE=1\n\nrm -rf results\nOUTPUT_DIR=\"./results/Meta-llama3.1-8B-Instruct-titlegen\"\n\ntorchrun --nproc_per_node=1 \\\n/opt/NeMo/examples/nlp/language_modeling/tuning/megatron_gpt_finetuning.py \\\n    exp_manager.exp_dir=${OUTPUT_DIR} \\\n    exp_manager.explicit_log_dir=${OUTPUT_DIR} \\\n    trainer.devices=1 \\\n    trainer.num_nodes=1 \\\n    trainer.precision=bf16-mixed \\\n    trainer.val_check_interval=0.2 \\\n    trainer.max_steps=50 \\\n    model.megatron_amp_O2=True \\\n    ++model.mcore_gpt=True \\\n    model.tensor_model_parallel_size=${TP_SIZE} \\\n    model.pipeline_model_parallel_size=${PP_SIZE} \\\n    model.micro_batch_size=1 \\\n    model.global_batch_size=64 \\\n    model.restore_from_path=${MODEL} \\\n    model.data.train_ds.file_names=${TRAIN_DS} \\\n    model.data.train_ds.concat_sampling_probabilities=[1.0] \\\n    model.data.validation_ds.file_names=${VALID_DS} \\\n    model.peft.peft_scheme=${SCHEME}\n```\n\nbut keep getting the following error:\n\n`'FileNotFoundError: [Errno 2] No such file or directory: '/tmp/tmpelprgyyt/model_weights/model.decoder.layers.self_attention.core_attention._extra_state/shard_0_32.pt''`\n\nJust can't seem to have the weights file under the directory of `\"/tmp/tmpelprgyyt/model_weights/model.decoder.layers.self_attention.core_attention._extra_state\"`\n\n**Expected behavior**\n\nTo successfully load the shards to the model.\n\n**Environment overview (please complete the following information)**\n\n - Environment location: Primehub docker website interface(Jupyter lab)\n - Method of NeMo install: pull from nvcr.io/nvidia/nemo:24.09\n\n**Environment details**\n\nI'm using NeMo image 24.09 version on 4 A10 GPU cores.\n\n\n",
    "state": "closed",
    "created_at": "2024-11-21T12:47:29+00:00",
    "closed_at": "2024-12-29T02:04:08+00:00",
    "updated_at": "2024-12-29T02:04:09+00:00",
    "author": "wittysam",
    "author_type": "User",
    "comments_count": 2,
    "reactions_count": 0,
    "labels": "bug,stale",
    "milestone": null,
    "closed_by": "github-actions[bot]",
    "resolution_time_hours": 901.2775,
    "first_comments": [
      {
        "author": "github-actions[bot]",
        "created_at": "2024-12-22T02:03:01+00:00",
        "body": "This issue is stale because it has been open for 30 days with no activity. Remove stale label or comment or this will be closed in 7 days."
      },
      {
        "author": "github-actions[bot]",
        "created_at": "2024-12-29T02:04:07+00:00",
        "body": "This issue was closed because it has been inactive for 7 days since being marked as stale."
      }
    ],
    "url": "https://github.com/NVIDIA/NeMo/issues/11359"
  },
  {
    "repository": "NVIDIA/NeMo",
    "issue_number": 11357,
    "title": "Sequence packing with ChatDataset",
    "body": "Hello, may I know how to use gpt_sft_chat_dataset with sequence packing? I couldnt find documentation related to it. I only found to convert from input-output format. ",
    "state": "closed",
    "created_at": "2024-11-21T01:56:47+00:00",
    "closed_at": "2025-01-06T02:01:12+00:00",
    "updated_at": "2025-01-06T02:01:13+00:00",
    "author": "LiweiPE",
    "author_type": "User",
    "comments_count": 3,
    "reactions_count": 0,
    "labels": "stale",
    "milestone": null,
    "closed_by": "github-actions[bot]",
    "resolution_time_hours": 1104.0736111111112,
    "first_comments": [
      {
        "author": "qijiaxing",
        "created_at": "2024-11-28T03:38:12+00:00",
        "body": "Sequence packing steps can found [here](https://docs.nvidia.com/nemo-framework/user-guide/24.07/llms/starcoder2/dataprep.html?highlight=packed#step-4-prepare-packed-dataset)\n\nTo make it work for chat dataset, you probably need to add the chat format option, like this:\n```\npython /workspace/sequence_packing/tokenize_dataset.py \\\n  model.data.train_ds.file_names=[/path/to/training.jsonl] \\\n  model.data.train_ds.max_seq_length=4096 \\\n  +model.data.chat=True \\\n  +model.data.chat_prompt_tokens.system_turn_start='<extra_id_0>' \\\n      +model.data.chat_prompt_tokens.turn_start='<extra_id_1>' \\\n      +model.data.chat_prompt_tokens.label_start='<extra_id_2>' \\\n      +model.data.chat_prompt_tokens.end_of_turn=\"\\x0A\" \\\n      +model.data.chat_prompt_tokens.end_of_name=\"\\x0A\" \\\n  model.restore_from_path=/path/to/starcoder2.nemo \\ # any starcoder2 .nemo models works here\n  +output_path=/path/to/my_dataset.npy\n```"
      },
      {
        "author": "github-actions[bot]",
        "created_at": "2024-12-29T02:04:08+00:00",
        "body": "This issue is stale because it has been open for 30 days with no activity. Remove stale label or comment or this will be closed in 7 days."
      },
      {
        "author": "github-actions[bot]",
        "created_at": "2025-01-06T02:01:11+00:00",
        "body": "This issue was closed because it has been inactive for 7 days since being marked as stale."
      }
    ],
    "url": "https://github.com/NVIDIA/NeMo/issues/11357"
  },
  {
    "repository": "NVIDIA/NeMo",
    "issue_number": 11350,
    "title": "AttributeError: module 'threading' has no attribute '_Condition",
    "body": "**Describe the bug**\n\nHuggingFace datasets imported by nemo.collections.llm.gpt.data in /opt/NeMo/nemo/collections/llm/gpt/data/__init__.py:15\n`from nemo.collections.llm.gpt.data.dolly import DollyDataModule`\n\n/opt/NeMo/nemo/collections/llm/gpt/data/dolly.py:20\n`from datasets import load_dataset`\n\nusr/local/lib/python3.10/dist-packages/datasets/__init__.py:17\nfrom .arrow_dataset import Dataset\n\n...\n\nTypo in Python 3.10? should be threading.Condition instead of threading._Condition ? cf [huggingface/datasets:5613](https://github.com/huggingface/datasets/issues/5613) [apache-beam:24458](https://github.com/apache/beam/issues/24458)\nTypo in source code ? should be subclassing threading.Condition instead of threading._Condition ?\n\n~/.local/lib/python3.10/site-packages/multiprocess/dummy/__init__.py:87\nclass Condition(threading._Condition):\n     88     # XXX\n     89     if sys.version_info < (3, 0):\n     90         notify_all = threading._Condition.notify_all.__func__\n\nAttributeError: module 'threading' has no attribute '_Condition'\n\n**Steps/Code to reproduce bug**\n\n`from nemo.collections import llm`\n\n> ---------------------------------------------------------------------------\n> AttributeError                            Traceback (most recent call last)\n> Cell In[4], line 1\n> ----> 1 from nemo.collections import llm\n> \n> File /opt/NeMo/nemo/collections/llm/__init__.py:22\n>      20 from nemo.collections.llm import peft\n>      21 from nemo.collections.llm.api import export_ckpt, finetune, generate, import_ckpt, pretrain, train, validate\n> ---> 22 from nemo.collections.llm.gpt.data import (\n>      23     DollyDataModule,\n>      24     FineTuningDataModule,\n>      25     MockDataModule,\n>      26     PreTrainingDataModule,\n>      27     SquadDataModule,\n>      28 )\n>      29 from nemo.collections.llm.gpt.data.api import dolly, mock, squad\n>      30 from nemo.collections.llm.gpt.model import (\n>      31     Baichuan2Config,\n>      32     Baichuan2Config7B,\n>    (...)\n>     103     gpt_forward_step,\n>     104 )\n> \n> File /opt/NeMo/nemo/collections/llm/gpt/data/__init__.py:15\n>       1 # Copyright (c) 2024, NVIDIA CORPORATION.  All rights reserved.\n>       2 #\n>       3 # Licensed under the Apache License, Version 2.0 (the \"License\");\n>    (...)\n>      12 # See the License for the specific language governing permissions and\n>      13 # limitations under the License.\n> ---> 15 from nemo.collections.llm.gpt.data.dolly import DollyDataModule\n>      16 from nemo.collections.llm.gpt.data.fine_tuning import FineTuningDataModule\n>      17 from nemo.collections.llm.gpt.data.mock import MockDataModule\n> \n> File /opt/NeMo/nemo/collections/llm/gpt/data/dolly.py:20\n>      17 from typing import TYPE_CHECKING, List, Optional\n>      19 import numpy as np\n> ---> 20 from datasets import load_dataset\n>      22 from nemo.collections.llm.gpt.data.core import get_dataset_root\n>      23 from nemo.collections.llm.gpt.data.fine_tuning import FineTuningDataModule\n> \n> File /usr/local/lib/python3.10/dist-packages/datasets/__init__.py:17\n>       1 # Copyright 2020 The HuggingFace Datasets Authors and the TensorFlow Datasets Authors.\n>       2 #\n>       3 # Licensed under the Apache License, Version 2.0 (the \"License\");\n>    (...)\n>      12 # See the License for the specific language governing permissions and\n>      13 # limitations under the License.\n>      15 __version__ = \"3.1.0\"\n> ---> 17 from .arrow_dataset import Dataset\n>      18 from .arrow_reader import ReadInstruction\n>      19 from .builder import ArrowBasedBuilder, BuilderConfig, DatasetBuilder, GeneratorBasedBuilder\n> \n> File /usr/local/lib/python3.10/dist-packages/datasets/arrow_dataset.py:77\n>      75 from . import config\n>      76 from .arrow_reader import ArrowReader\n> ---> 77 from .arrow_writer import ArrowWriter, OptimizedTypedSequence\n>      78 from .data_files import sanitize_patterns\n>      79 from .download.streaming_download_manager import xgetsize\n> \n> File /usr/local/lib/python3.10/dist-packages/datasets/arrow_writer.py:27\n>      24 from fsspec.core import url_to_fs\n>      26 from . import config\n> ---> 27 from .features import Audio, Features, Image, Value, Video\n>      28 from .features.features import (\n>      29     FeatureType,\n>      30     _ArrayXDExtensionType,\n>    (...)\n>      37     to_pyarrow_listarray,\n>      38 )\n>      39 from .filesystems import is_remote_filesystem\n> \n> File /usr/local/lib/python3.10/dist-packages/datasets/features/__init__.py:17\n>       1 __all__ = [\n>       2     \"Audio\",\n>       3     \"Array2D\",\n>    (...)\n>      15     \"Video\",\n>      16 ]\n> ---> 17 from .audio import Audio\n>      18 from .features import Array2D, Array3D, Array4D, Array5D, ClassLabel, Features, LargeList, Sequence, Value\n>      19 from .image import Image\n> \n> File /usr/local/lib/python3.10/dist-packages/datasets/features/audio.py:13\n>      11 from ..table import array_cast\n>      12 from ..utils.file_utils import xopen, xsplitext\n> ---> 13 from ..utils.py_utils import no_op_if_value_is_null, string_to_dict\n>      16 if TYPE_CHECKING:\n>      17     from .features import FeatureType\n> \n> File /usr/local/lib/python3.10/dist-packages/datasets/utils/py_utils.py:37\n>      34 from urllib.parse import urlparse\n>      36 import multiprocess\n> ---> 37 import multiprocess.pool\n>      38 import numpy as np\n>      39 from tqdm.auto import tqdm\n> \n> File ~/.local/lib/python3.10/site-packages/multiprocess/pool.py:609\n>     603             self._cond.release()\n>     605 #\n>     606 #\n>     607 #\n> --> 609 class ThreadPool(Pool):\n>     611     from .dummy import Process\n>     613     def __init__(self, processes=None, initializer=None, initargs=()):\n> \n> File ~/.local/lib/python3.10/site-packages/multiprocess/pool.py:611, in ThreadPool()\n>     609 class ThreadPool(Pool):\n> --> 611     from .dummy import Process\n>     613     def __init__(self, processes=None, initializer=None, initargs=()):\n>     614         Pool.__init__(self, processes, initializer, initargs)\n> \n> File ~/.local/lib/python3.10/site-packages/multiprocess/dummy/__init__.py:87\n>      81             return None\n>      83 #\n>      84 #\n>      85 #\n> ---> 87 class Condition(threading._Condition):\n>      88     # XXX\n>      89     if sys.version_info < (3, 0):\n>      90         notify_all = threading._Condition.notify_all.__func__\n> \n> AttributeError: module 'threading' has no attribute '_Condition'\n\n**Expected behavior**\n\nImport should occur without triggering an error\n\n**Environment overview (please complete the following information)**\n\n - Environment location: [Docker]\n - Method of NeMo install: Docker, docker pull nvcr.io/nvidia/nemo:24.09 docker run docker run --gpus all -it --rm --shm-size=16g --ulimit memlock=-1 --ulimit stack=67108864 nvcr.io/nvidia/nemo:24.09\n\n**Additional context**\n\nAdd any other context about the problem here.\n\nTypo in Python 3.10? should be subclassing threading.Condition instead of threading._Condition ?",
    "state": "closed",
    "created_at": "2024-11-20T20:58:34+00:00",
    "closed_at": "2024-12-29T02:04:10+00:00",
    "updated_at": "2024-12-29T02:04:11+00:00",
    "author": "BenoitDalFerro",
    "author_type": "User",
    "comments_count": 3,
    "reactions_count": 0,
    "labels": "bug,stale",
    "milestone": null,
    "closed_by": "github-actions[bot]",
    "resolution_time_hours": 917.0933333333334,
    "first_comments": [
      {
        "author": "BenoitDalFerro",
        "created_at": "2024-11-20T22:16:41+00:00",
        "body": "Ok docker quietened following error log from pip, this is an apache-beam issue, closing issue.\n\n> ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n> cudf 24.4.0 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 16.1.0 which is incompatible.\n> cugraph 24.4.0 requires dask-cuda==24.4.*, but you have dask-cuda 24.8.2 which is incompatible.\n> cugraph 24.4.0 requires rapids-dask-dependency==24.4.*, but you have rapids-dask-dependency 24.8.0 which is incompatible.\n> cugraph-service-server 24.4.0 requires dask-cuda==24.4.*, but you have dask-cuda 24.8.2 which is incompatible.\n> cugraph-service-server 24.4.0 requires rapids-dask-dependency==24.4.*, but you have rapids-dask-dependency 24.8.0 which is incompatible.\n> cuml 24.4.0 requires dask-cuda==24.4.*, but you have dask-cuda 24.8.2 which is incompatible.\n> cuml 24.4.0 requires rapids-dask-dependency==24.4.*, but you have rapids-dask-dependency 24.8.0 which is incompatible.\n> cuml 24.4.0 requires treelite==4.1.2, but you have treelite 4.3.0 which is incompatible.\n> dask-cudf 24.4.0 requires rapids-dask-dependency==24.4.*, but you have rapids-dask-dependency 24.8.0 which is incompatible.\n> multiprocess 0.70.16 requires dill>=0.3.8, but you have dill 0.3.1.1 which is incompatible.\n> tensorflow-metadata 1.16.1 requires protobuf<4.21,>=3.20.3; python_version < \"3.11\", but you have protobuf 4.25.5 which is incompatible.\n> tensorrt-llm 0.12.0 requires nvidia-modelopt~=0.15.0, but you have nvidia-modelopt 0.0.0 which is incompatible.\n> tensorrt-llm 0.12.0 requires pynvml>=11.5.0, but you have pynvml 11.4.1 which is incompatible.\n> tensorrt-llm 0.12.0 requires transformers<=4.42.4,>=4.38.2, but you have transformers 4.46.2 which is incompatible."
      },
      {
        "author": "github-actions[bot]",
        "created_at": "2024-12-21T01:57:31+00:00",
        "body": "This issue is stale because it has been open for 30 days with no activity. Remove stale label or comment or this will be closed in 7 days."
      },
      {
        "author": "github-actions[bot]",
        "created_at": "2024-12-29T02:04:10+00:00",
        "body": "This issue was closed because it has been inactive for 7 days since being marked as stale."
      }
    ],
    "url": "https://github.com/NVIDIA/NeMo/issues/11350"
  },
  {
    "repository": "NVIDIA/NeMo",
    "issue_number": 11320,
    "title": "Inference code request",
    "body": "Hi there,\n\nI gone through the repo, and i ran the inference for speaker diarization on toy sample using system VAD, its giving the output. But for the recorded audio file its giving errors. can you provide the code for custom samples inference for system VAD.",
    "state": "closed",
    "created_at": "2024-11-19T09:22:04+00:00",
    "closed_at": "2024-12-28T01:57:02+00:00",
    "updated_at": "2024-12-28T01:57:02+00:00",
    "author": "MK-1307",
    "author_type": "User",
    "comments_count": 2,
    "reactions_count": 0,
    "labels": "stale",
    "milestone": null,
    "closed_by": "github-actions[bot]",
    "resolution_time_hours": 928.5827777777778,
    "first_comments": [
      {
        "author": "github-actions[bot]",
        "created_at": "2024-12-20T01:58:48+00:00",
        "body": "This issue is stale because it has been open for 30 days with no activity. Remove stale label or comment or this will be closed in 7 days."
      },
      {
        "author": "github-actions[bot]",
        "created_at": "2024-12-28T01:57:01+00:00",
        "body": "This issue was closed because it has been inactive for 7 days since being marked as stale."
      }
    ],
    "url": "https://github.com/NVIDIA/NeMo/issues/11320"
  },
  {
    "repository": "NVIDIA/NeMo",
    "issue_number": 11314,
    "title": "Merge multiple \"distributed LoRA checkpoints\"",
    "body": "**Is your feature request related to a problem? Please describe.**\n\nTensorRT-LLM only accepts a single rank .nemo LoRA checkpoint (in the case of Llama 3.1 8b). Therefore, the only way to use my fine-tuned model with TensorRT-LLM backend is to merge my distributed LoRA checkpoints into the base model using the scripts/nlp_language_modeling/merge_lora_weights/merge.py script. However, that results in a lot of big models, if I want to do that for multiple downstream tasks/fine-tuned models.\n\nMore specifically, my checkpoints after training with TP=PP=2 look like (the contents of the megatron_gpt_peft_lora_tuning.nemo LoRA checkpoint file):\n```\n./                                                                                                                                                                           \n./model_config.yaml                                                                                                                                                          \n./tp_rank_00_pp_rank_000/\n./tp_rank_00_pp_rank_000/model_weights.ckpt \n./tp_rank_00_pp_rank_001/\n./tp_rank_00_pp_rank_001/model_weights.ckpt \n./tp_rank_01_pp_rank_000/\n./tp_rank_01_pp_rank_000/model_weights.ckpt \n./tp_rank_01_pp_rank_001/\n./tp_rank_01_pp_rank_001/model_weights.ckpt\n```\n\n**Describe the solution you'd like**\n\nIt would be nice if we could merge the distributed LoRA weights to a .nemo LoRA checkpoint file that only contains weights for a single rank. That way, the LoRA would be compatible with TensorRT-LLM even if training on multiple GPUs.\n\nThanks in advance!\n\nBest regards,\nJohn\n",
    "state": "closed",
    "created_at": "2024-11-18T16:05:37+00:00",
    "closed_at": "2024-12-28T01:57:03+00:00",
    "updated_at": "2024-12-28T01:57:03+00:00",
    "author": "jolyons123",
    "author_type": "User",
    "comments_count": 2,
    "reactions_count": 0,
    "labels": "stale",
    "milestone": null,
    "closed_by": "github-actions[bot]",
    "resolution_time_hours": 945.8572222222223,
    "first_comments": [
      {
        "author": "github-actions[bot]",
        "created_at": "2024-12-20T01:58:50+00:00",
        "body": "This issue is stale because it has been open for 30 days with no activity. Remove stale label or comment or this will be closed in 7 days."
      },
      {
        "author": "github-actions[bot]",
        "created_at": "2024-12-28T01:57:02+00:00",
        "body": "This issue was closed because it has been inactive for 7 days since being marked as stale."
      }
    ],
    "url": "https://github.com/NVIDIA/NeMo/issues/11314"
  },
  {
    "repository": "NVIDIA/NeMo",
    "issue_number": 11307,
    "title": "Japanese model name for Forced Aligner",
    "body": "Somebody tell me the name of the model to be used with the Forced Aligner for Japanese Language, the english one works but, I'm looking for a japanese one and latest with more accuracy, Thanks.",
    "state": "closed",
    "created_at": "2024-11-16T17:05:42+00:00",
    "closed_at": "2024-12-25T01:57:46+00:00",
    "updated_at": "2024-12-25T01:57:47+00:00",
    "author": "andriken",
    "author_type": "User",
    "comments_count": 2,
    "reactions_count": 0,
    "labels": "stale",
    "milestone": null,
    "closed_by": "github-actions[bot]",
    "resolution_time_hours": 920.8677777777777,
    "first_comments": [
      {
        "author": "github-actions[bot]",
        "created_at": "2024-12-17T02:04:44+00:00",
        "body": "This issue is stale because it has been open for 30 days with no activity. Remove stale label or comment or this will be closed in 7 days."
      },
      {
        "author": "github-actions[bot]",
        "created_at": "2024-12-25T01:57:46+00:00",
        "body": "This issue was closed because it has been inactive for 7 days since being marked as stale."
      }
    ],
    "url": "https://github.com/NVIDIA/NeMo/issues/11307"
  },
  {
    "repository": "NVIDIA/NeMo",
    "issue_number": 11303,
    "title": "OOM with RAM with Lhotse",
    "body": "**Describe the bug**\n\nWhen training a model consuming more memory, I noticed that my training would stop after a constant number of epochs. Upon further investigation, I found that during training / validation, the memory usage of the CPU memory (RAM) continues to rise and never get released, which results in OOM after a number of epochs. This happens while I was using the Lhotse dataloader, while I was able to train a small fast-conformer CTC model for hundreds of epochs, the same version of NeMo only is able to run the training for an XL fast-conformer CTC model for ~20 epochs. ~110 epochs if I use 1/4 of the works for the dataloader. So somehow the dataloader is not releasing memory for the data loaded.\n\n**Steps/Code to reproduce bug**\n\n- shows up in XL fast-conformer CTC and medium fast-conformer CTC RNNT hybrid\n- standard configs for these models in terms of hyperparameters\n- other lhotse related configs:\n\n```\ntrainer:\n  devices: -1\n  num_nodes: 1\n  max_epochs: 150\n  max_steps: 150000\n  val_check_interval: 1000\n  accelerator: auto\n  strategy: ddp\n  accumulate_grad_batches: 1\n  gradient_clip_val: 1.0\n  precision: bf16-mixed\n  log_every_n_steps: 200\n  enable_progress_bar: true\n  num_sanity_val_steps: 1\n  check_val_every_n_epoch: 1\n  sync_batchnorm: true\n  enable_checkpointing: false\n  logger: false\n  benchmark: false\n  use_distributed_sampler: false\n  limit_train_batches: 1000\n train_ds:\n  manifest_filepath: null\n  sample_rate: 16000\n  batch_size: null\n  shuffle: true\n  num_workers: 8\n  pin_memory: true\n  max_duration: 45\n  min_duration: 1\n  is_tarred: false\n  tarred_audio_filepaths: null\n  shuffle_n: 2048\n  bucketing_strategy: synced_randomized\n  bucketing_batch_size: null\n  shar_path:\n  - xxxxx\n  use_lhotse: true\n  bucket_duration_bins:\n  - xxx\n    batch_duration: 600\n    quadratic_duration: 30\n    num_buckets: 30\n    bucket_buffer_size: 10000\n    shuffle_buffer_size: 10000\n    num_cuts_for_bins_estimate: 10000\n    use_bucketing: true\n```\n\n**Expected behavior**\n\nTraining should continue until specified stop.\n\n**Environment overview (please complete the following information)**\n\n - GCP\n - Method of NeMo install: poetry: nemo-toolkit = {version = \"2.0.0rc1\", extras = [\"asr\"]}\n\n**Environment details**\n\n- nemo-toolkit 2.0.0rc1 / 2.0.0\n- PyTorch-lightning 2.4.0\n- PyTorch 2.2.2+cu121\n- Python 3.11.8\n\n**Additional context**\n\nGPU: A100 40G.\n\n@nithinraok ~~has told me to try using limit_validation_batches, use smaller duration audios, use fully_randomized which I haven't fully tested. I'll report back when these are tested but the issue persists so far~~\n\nEdit: tried these suggestions, still OOM.\n",
    "state": "closed",
    "created_at": "2024-11-15T23:08:08+00:00",
    "closed_at": "2025-01-06T02:01:13+00:00",
    "updated_at": "2025-01-06T02:01:14+00:00",
    "author": "riqiang-dp",
    "author_type": "User",
    "comments_count": 5,
    "reactions_count": 0,
    "labels": "bug,stale",
    "milestone": null,
    "closed_by": "github-actions[bot]",
    "resolution_time_hours": 1226.8847222222223,
    "first_comments": [
      {
        "author": "nithinraok",
        "created_at": "2024-11-15T23:22:38+00:00",
        "body": "@pzelasko fyi"
      },
      {
        "author": "riqiang-dp",
        "created_at": "2024-11-29T01:02:14+00:00",
        "body": "Is this related to https://github.com/lhotse-speech/lhotse/issues/518? @pzelasko "
      },
      {
        "author": "riqiang-dp",
        "created_at": "2024-11-30T00:03:21+00:00",
        "body": "I changed it to 1 worker and it is much more stable"
      },
      {
        "author": "github-actions[bot]",
        "created_at": "2024-12-30T02:00:11+00:00",
        "body": "This issue is stale because it has been open for 30 days with no activity. Remove stale label or comment or this will be closed in 7 days."
      },
      {
        "author": "github-actions[bot]",
        "created_at": "2025-01-06T02:01:13+00:00",
        "body": "This issue was closed because it has been inactive for 7 days since being marked as stale."
      }
    ],
    "url": "https://github.com/NVIDIA/NeMo/issues/11303"
  },
  {
    "repository": "NVIDIA/NeMo",
    "issue_number": 11297,
    "title": "Missing BOS tokens for HF tokenizer",
    "body": "**Describe the bug**\n\nIn the current implementation of HF AutoTokenizer, the tokenizer does not behave in the same way as the HF version. I noticed the problem with the BOS token (which is extremely important for the Gemma 2 model). Specifically, the problem occurs with `text_to_ids` function, https://github.com/NVIDIA/NeMo/blob/ed244d943948b105982f5cb17b51991e8acebfb7/nemo/collections/common/tokenizers/huggingface/auto_tokenizer.py#L222-L225 which is relevant for `preprocess_data_for_megatron.py`. The problem is that this function first transforms text to tokens and then tokens to IDs and consequently does not append the BOS token at the beginning as the HF encode method does (I assume the problem applies to other special tokens as well).\n\n**Steps/Code to reproduce bug**\n\n```\nfrom nemo.collections.common.tokenizers.huggingface.auto_tokenizer import AutoTokenizer as ATNemo\nfrom transformers import AutoTokenizer as ATHF\n\n\ntokenizer_path = \"google/gemma-2-9b\"\nhf_tokenizer = ATHF.from_pretrained(tokenizer_path)\nnemo_tokenizer = ATNemo(tokenizer_path)\n\ntext = \"Text to tokenize\"\n\n# Common HF tokenization\nids = hf_tokenizer.encode(text)\nprint(\"HF tokenization:\", ids)\n\n# NeMo tokenization\nids = nemo_tokenizer.text_to_ids(text)\nprint(\"NeMo tokenization:\", ids)\n\n# HF tokenization using NeMo steps\ntokens = hf_tokenizer.tokenize(text)\nids = hf_tokenizer.convert_tokens_to_ids(tokens)\nprint(\"HF tokenization using NeMo steps:\", ids)\n```\n\nThe outputs for provided code snippet are:\n```\nHF tokenization: [2, 1637, 577, 223491]\nNeMo tokenization: [1637, 577, 223491]\nHF tokenization using NeMo steps: [1637, 577, 223491]\n```\n\nNotice how HF tokenization have token ID 2 at the beginning, which corresponds to the BOS token.\n\n**Expected behavior**\n\nI expect NeMo HF wrapper to behave the same way as the HF tokenizer. The solution for this case would be to change the `text_to_ids` function to:\n```\n def text_to_ids(self, text):\n     ids = self.tokenizer.encode(text) \n     return ids \n```\n\nIs there any reason why it isn't implemented this way?\n\n**Environment overview (please complete the following information)**\n\nI was using official NeMo container (24.09). However, I see that code for HF wrapper is still the same on main branch, so the issue should still apply.\n",
    "state": "closed",
    "created_at": "2024-11-15T14:37:18+00:00",
    "closed_at": "2024-12-24T01:58:47+00:00",
    "updated_at": "2025-01-21T19:21:25+00:00",
    "author": "domenVres",
    "author_type": "User",
    "comments_count": 2,
    "reactions_count": 0,
    "labels": "bug",
    "milestone": null,
    "closed_by": "github-actions[bot]",
    "resolution_time_hours": 923.3580555555556,
    "first_comments": [
      {
        "author": "github-actions[bot]",
        "created_at": "2024-12-16T02:07:06+00:00",
        "body": "This issue is stale because it has been open for 30 days with no activity. Remove stale label or comment or this will be closed in 7 days."
      },
      {
        "author": "github-actions[bot]",
        "created_at": "2024-12-24T01:58:46+00:00",
        "body": "This issue was closed because it has been inactive for 7 days since being marked as stale."
      }
    ],
    "url": "https://github.com/NVIDIA/NeMo/issues/11297"
  },
  {
    "repository": "NVIDIA/NeMo",
    "issue_number": 11272,
    "title": "using skip_nan_grad with gradient accumulation for ASR",
    "body": "Hi, I notice that while training the ASR fastconformer model on my data, some after a few epochs there can be a batch that cause NaN error. So, I tried setting skip_nan_grad = True, which allow the train_loss curve to look normal (without spiking to NaN), but I notice in the error log that after the message \"detected inf or nan values in gradients! Setting gradients to zero.\", the all prediction in the following validation rounds are \"??\" and the validation wer increased to 1. I set accumulate_grad_batches to 4 for the training so I wonder whether this might be the culprit. Does setting skip_nan_grad = True can still work with gradient accumulation. I'd really appreciate any comment you can give on the issue!",
    "state": "closed",
    "created_at": "2024-11-13T09:27:00+00:00",
    "closed_at": "2024-12-22T02:03:06+00:00",
    "updated_at": "2024-12-22T02:03:06+00:00",
    "author": "qhoangdl",
    "author_type": "User",
    "comments_count": 2,
    "reactions_count": 0,
    "labels": "stale",
    "milestone": null,
    "closed_by": "github-actions[bot]",
    "resolution_time_hours": 928.6016666666667,
    "first_comments": [
      {
        "author": "github-actions[bot]",
        "created_at": "2024-12-14T02:02:33+00:00",
        "body": "This issue is stale because it has been open for 30 days with no activity. Remove stale label or comment or this will be closed in 7 days."
      },
      {
        "author": "github-actions[bot]",
        "created_at": "2024-12-22T02:03:05+00:00",
        "body": "This issue was closed because it has been inactive for 7 days since being marked as stale."
      }
    ],
    "url": "https://github.com/NVIDIA/NeMo/issues/11272"
  },
  {
    "repository": "NVIDIA/NeMo",
    "issue_number": 11256,
    "title": "Fail to convert Llama3 Nemo 2.0 checkpoint to HF",
    "body": "**Describe the bug**\n\nI use Nemo 2.0 to train my model and get Nemo 2.0 checkpoint like this with .distcp files:\nmodel_name\n├── context\n│   ├── model_config.yaml\n│   ├── io.json\n│   └── tokenizer\n├── weights\n│   ├── distributed checkpointing directories/files in torch_dist format\n│   ├── metadata.json\n│   └── common.pt\n\n\nbut filed to use NeMo/scripts/checkpoint_converters/convert_llama_nemo_to_hf.py to export it to a HF file\n\n**Expected behavior**\n\nget HF files by Nemo2.0 checkpoint\n\n",
    "state": "closed",
    "created_at": "2024-11-12T05:42:28+00:00",
    "closed_at": "2024-12-23T01:59:41+00:00",
    "updated_at": "2024-12-23T01:59:42+00:00",
    "author": "EthanLI24",
    "author_type": "User",
    "comments_count": 5,
    "reactions_count": 0,
    "labels": "bug,stale",
    "milestone": null,
    "closed_by": "github-actions[bot]",
    "resolution_time_hours": 980.2869444444444,
    "first_comments": [
      {
        "author": "EthanLI24",
        "created_at": "2024-11-12T09:01:20+00:00",
        "body": "Is there a way we can now save non .distcp files or huggingface .bin files directly after training?\nIf not, how to convert nemo2.0 checkpoint to the community common format?"
      },
      {
        "author": "hemildesai",
        "created_at": "2024-11-14T01:32:18+00:00",
        "body": "Hi, the scripts/checkpoint_converters/convert_llama_nemo_to_hf.py script is only for NeMo 1.0 checkpoints. To export 2.0 checkpoints, you can use a custom script like\n\n```python\nfrom pathlib import Path\n\nfrom nemo.collections.llm import export_ckpt\n\nif __name__ == \"__main__\":\n    export_ckpt(\n        path=Path(\"/workspace/input_ckpt\"),\n        target=\"hf\",\n        output_path=Path(\"/workspace/output_ckpt.hf\"),\n    )\n```"
      },
      {
        "author": "EthanLI24",
        "created_at": "2024-11-14T06:30:36+00:00",
        "body": "> Hi, the scripts/checkpoint_converters/convert_llama_nemo_to_hf.py script is only for NeMo 1.0 checkpoints. To export 2.0 checkpoints, you can use a custom script like\n> \n> from pathlib import Path\n> \n> from nemo.collections.llm import export_ckpt\n> \n> if __name__ == \"__main__\":\n>     export_ckpt(\n>         path=Path(\"/workspace/input_ckpt\"),\n>         target=\"hf\",\n>         output_path=Path(\"/workspace/output_ckpt.hf\"),\n>     )\n\n\n\nHere are my fold arch:\n```shell\nnemo2_llama3\n├── context\n│ ├── model.yaml\n│ ├── io.json\n│ └── nemo_tokenizer\n├── weights\n│ ├── __0_0.distcp\n│ ├── __0_1.distcp\n│ ├── metadata.json\n│ └── common.pt\n``` \n\nand I follow your instruction to use\n```python\nif __name__ == \"__main__\":\n    export_ckpt(\n        path=Path(\"./nemo2_llama3\"),\n        target=\"hf\",\n        output_path=Path(\"./nemo2exporthf\"),\n    )\n\n``` \nbut i get failed again, could you help me with this?\n"
      },
      {
        "author": "github-actions[bot]",
        "created_at": "2024-12-15T02:09:04+00:00",
        "body": "This issue is stale because it has been open for 30 days with no activity. Remove stale label or comment or this will be closed in 7 days."
      },
      {
        "author": "github-actions[bot]",
        "created_at": "2024-12-23T01:59:41+00:00",
        "body": "This issue was closed because it has been inactive for 7 days since being marked as stale."
      }
    ],
    "url": "https://github.com/NVIDIA/NeMo/issues/11256"
  },
  {
    "repository": "NVIDIA/NeMo",
    "issue_number": 11250,
    "title": "Initialize a Parakeet Cache-Aware Streaming model's encoder from an offline model.",
    "body": "Hello,\n\nThank you for all your amazing work!\n\nAt the moment, nemo only support the fastconformer 120M streaming model, however I would like to also try a bigger model. I was wondering if it is possible to modify the architecture to match de 600M parakeet model and initialize the encoder weights from the offline parakeet model? Otherwise, training a bare 600M streaming model without any SSL pretraining would probably be worse and slower to train than the 120M model.\n\nBest regards,\nGabi",
    "state": "closed",
    "created_at": "2024-11-11T09:21:57+00:00",
    "closed_at": "2024-12-08T11:59:18+00:00",
    "updated_at": "2024-12-08T11:59:19+00:00",
    "author": "gabitza-tech",
    "author_type": "User",
    "comments_count": 2,
    "reactions_count": 0,
    "labels": "",
    "milestone": null,
    "closed_by": "gabitza-tech",
    "resolution_time_hours": 650.6225,
    "first_comments": [
      {
        "author": "sandergs92",
        "created_at": "2024-12-04T14:18:03+00:00",
        "body": "See the following answer: [https://github.com/NVIDIA/NeMo/issues/9615#issuecomment-2264136385](https://github.com/NVIDIA/NeMo/issues/9615#issuecomment-2264136385)"
      },
      {
        "author": "gabitza-tech",
        "created_at": "2024-12-08T11:59:18+00:00",
        "body": "Thanks! "
      }
    ],
    "url": "https://github.com/NVIDIA/NeMo/issues/11250"
  },
  {
    "repository": "NVIDIA/NeMo",
    "issue_number": 11248,
    "title": "How to convert downloaded local models into Nemo files？",
    "body": null,
    "state": "closed",
    "created_at": "2024-11-11T08:10:40+00:00",
    "closed_at": "2024-12-20T01:58:54+00:00",
    "updated_at": "2024-12-20T01:58:55+00:00",
    "author": "llll111111",
    "author_type": "User",
    "comments_count": 2,
    "reactions_count": 0,
    "labels": "stale",
    "milestone": null,
    "closed_by": "github-actions[bot]",
    "resolution_time_hours": 929.8038888888889,
    "first_comments": [
      {
        "author": "github-actions[bot]",
        "created_at": "2024-12-12T02:05:10+00:00",
        "body": "This issue is stale because it has been open for 30 days with no activity. Remove stale label or comment or this will be closed in 7 days."
      },
      {
        "author": "github-actions[bot]",
        "created_at": "2024-12-20T01:58:54+00:00",
        "body": "This issue was closed because it has been inactive for 7 days since being marked as stale."
      }
    ],
    "url": "https://github.com/NVIDIA/NeMo/issues/11248"
  },
  {
    "repository": "NVIDIA/NeMo",
    "issue_number": 11231,
    "title": "Need env.yaml or requirements.txt",
    "body": "On ubuntu 22.04 \ntried all varients of python 3.10<\nbut \n\n```\nconda create --name nemo python==3.10.12\nconda activate nemo\n\napt-get update && apt-get install -y libsndfile1 ffmpeg\npip install Cython packaging\npip install nemo_toolkit['all']\n```\n\ninstallation works but multiple errors during code exectution\nmostly \n\n`\nImportError: cannot import name 'ModelFilter' from 'huggingface_hub' (/home/jugal/anaconda3/envs/nemo/lib/python3.10/site-packages/huggingface_hub/__init__.py)\n`\n\n\ncan someone please export and share their python env along with python version\n\n\nThank you in advance ",
    "state": "closed",
    "created_at": "2024-11-08T15:03:27+00:00",
    "closed_at": "2024-12-16T02:07:09+00:00",
    "updated_at": "2024-12-16T02:07:10+00:00",
    "author": "jugal-sheth",
    "author_type": "User",
    "comments_count": 3,
    "reactions_count": 0,
    "labels": "bug,stale",
    "milestone": null,
    "closed_by": "github-actions[bot]",
    "resolution_time_hours": 899.0616666666666,
    "first_comments": [
      {
        "author": "jugal-sheth",
        "created_at": "2024-11-08T15:08:36+00:00",
        "body": "as per https://github.com/NVIDIA/NeMo/issues/9793\ndegrading huggingface-hub impacted other packages\n"
      },
      {
        "author": "github-actions[bot]",
        "created_at": "2024-12-09T02:07:01+00:00",
        "body": "This issue is stale because it has been open for 30 days with no activity. Remove stale label or comment or this will be closed in 7 days."
      },
      {
        "author": "github-actions[bot]",
        "created_at": "2024-12-16T02:07:08+00:00",
        "body": "This issue was closed because it has been inactive for 7 days since being marked as stale."
      }
    ],
    "url": "https://github.com/NVIDIA/NeMo/issues/11231"
  },
  {
    "repository": "NVIDIA/NeMo",
    "issue_number": 11224,
    "title": "[Question] Is there a way to get the results of each layer of a model?",
    "body": "Hi,\n\nIn PyTorch, we can use `register_forward_hook` to capture the output of specific layers. I’m wondering if there’s a similar feature in NeMo or a way to convert a NeMo model to PyTorch to achieve this. Any guidance would be appreciated!",
    "state": "closed",
    "created_at": "2024-11-08T03:42:43+00:00",
    "closed_at": "2024-12-16T02:07:10+00:00",
    "updated_at": "2024-12-16T02:07:11+00:00",
    "author": "GO0108",
    "author_type": "User",
    "comments_count": 2,
    "reactions_count": 0,
    "labels": "stale",
    "milestone": null,
    "closed_by": "github-actions[bot]",
    "resolution_time_hours": 910.4075,
    "first_comments": [
      {
        "author": "github-actions[bot]",
        "created_at": "2024-12-09T02:07:02+00:00",
        "body": "This issue is stale because it has been open for 30 days with no activity. Remove stale label or comment or this will be closed in 7 days."
      },
      {
        "author": "github-actions[bot]",
        "created_at": "2024-12-16T02:07:10+00:00",
        "body": "This issue was closed because it has been inactive for 7 days since being marked as stale."
      }
    ],
    "url": "https://github.com/NVIDIA/NeMo/issues/11224"
  },
  {
    "repository": "NVIDIA/NeMo",
    "issue_number": 11218,
    "title": "Nemo run pretrain raised Exception: No dot product attention support for the provided inputs",
    "body": "**Describe the bug**\n\nWhen trying to run `nemo` with `gemma_2b`, `Exception: No dot product attention support for the provided inputs!` was returned.\n```\n[rank0]: Traceback (most recent call last):\n[rank0]:   File \"/usr/local/bin/nemo\", line 8, in <module>\n[rank0]:     sys.exit(app())\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/typer/main.py\", line 326, in __call__\n[rank0]:     raise e\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/typer/main.py\", line 309, in __call__\n[rank0]:     return get_command(self)(*args, **kwargs)\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/click/core.py\", line 1157, in __call__\n[rank0]:     return self.main(*args, **kwargs)\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/typer/core.py\", line 723, in main\n[rank0]:     return _main(\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/typer/core.py\", line 193, in _main\n[rank0]:     rv = self.invoke(ctx)\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/click/core.py\", line 1688, in invoke\n[rank0]:     return _process_result(sub_ctx.command.invoke(sub_ctx))\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/click/core.py\", line 1688, in invoke\n[rank0]:     return _process_result(sub_ctx.command.invoke(sub_ctx))\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/click/core.py\", line 1434, in invoke\n[rank0]:     return ctx.invoke(self.callback, **ctx.params)\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/click/core.py\", line 783, in invoke\n[rank0]:     return __callback(*args, **kwargs)\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/typer/main.py\", line 692, in wrapper\n[rank0]:     return callback(**use_params)\n[rank0]:   File \"/opt/NeMo-Run/src/nemo_run/cli/api.py\", line 793, in command\n[rank0]:     self.cli_execute(fn, ctx.args, type)\n[rank0]:   File \"/opt/NeMo-Run/src/nemo_run/cli/api.py\", line 845, in cli_execute\n[rank0]:     self._execute_task(fn, filtered_args)\n[rank0]:   File \"/opt/NeMo-Run/src/nemo_run/cli/api.py\", line 895, in _execute_task\n[rank0]:     run_task()\n[rank0]:   File \"/opt/NeMo-Run/src/nemo_run/cli/api.py\", line 874, in run_task\n[rank0]:     run.run(\n[rank0]:   File \"/opt/NeMo-Run/src/nemo_run/run/api.py\", line 65, in run\n[rank0]:     direct_run_fn(fn_or_script, dryrun=dryrun)\n[rank0]:   File \"/opt/NeMo-Run/src/nemo_run/run/task.py\", line 77, in direct_run_fn\n[rank0]:     built_fn()\n[rank0]:   File \"/opt/NeMo/nemo/collections/llm/api.py\", line 134, in pretrain\n[rank0]:     return train(\n[rank0]:   File \"/opt/NeMo/nemo/collections/llm/api.py\", line 92, in train\n[rank0]:     trainer.fit(model, data)\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\", line 538, in fit\n[rank0]:     call._call_and_handle_interrupt(\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/call.py\", line 46, in _call_and_handle_interrupt\n[rank0]:     return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/strategies/launchers/subprocess_script.py\", line 105, in launch\n[rank0]:     return function(*args, **kwargs)\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\", line 574, in _fit_impl\n[rank0]:     self._run(model, ckpt_path=ckpt_path)\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\", line 981, in _run\n[rank0]:     results = self._run_stage()\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\", line 1023, in _run_stage\n[rank0]:     self._run_sanity_check()\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\", line 1052, in _run_sanity_check\n[rank0]:     val_loop.run()\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/utilities.py\", line 178, in _decorator\n[rank0]:     return loop_run(self, *args, **kwargs)\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/evaluation_loop.py\", line 135, in run\n[rank0]:     self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/evaluation_loop.py\", line 396, in _evaluation_step\n[rank0]:     output = call._call_strategy_hook(trainer, hook_name, *step_args)\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/call.py\", line 319, in _call_strategy_hook\n[rank0]:     output = fn(*args, **kwargs)\n[rank0]:   File \"/opt/NeMo/nemo/lightning/pytorch/strategies/megatron_strategy.py\", line 571, in validation_step\n[rank0]:     out = self.model.validation_step(dataloader_iter, *args, **kwargs)\n[rank0]:   File \"/opt/NeMo/nemo/lightning/megatron_parallel.py\", line 342, in validation_step\n[rank0]:     return self._step(\n[rank0]:   File \"/opt/NeMo/nemo/lightning/megatron_parallel.py\", line 429, in _step\n[rank0]:     return self.forward(\n[rank0]:   File \"/opt/NeMo/nemo/lightning/megatron_parallel.py\", line 279, in forward\n[rank0]:     microbatch_outputs = step()\n[rank0]:   File \"/opt/NeMo/nemo/lightning/megatron_parallel.py\", line 1126, in __call__\n[rank0]:     return self.forward_backward_func(\n[rank0]:   File \"/opt/megatron-lm/megatron/core/pipeline_parallel/schedules.py\", line 450, in forward_backward_no_pipelining\n[rank0]:     output_tensor, num_tokens = forward_step(\n[rank0]:   File \"/opt/megatron-lm/megatron/core/pipeline_parallel/schedules.py\", line 273, in forward_step\n[rank0]:     output_tensor, loss_func = forward_step_func(data_iterator, model)\n[rank0]:   File \"/opt/NeMo/nemo/lightning/megatron_parallel.py\", line 492, in wrapped_forward_step_func\n[rank0]:     output_tensor = _forward_step(model, batch)\n[rank0]:   File \"/opt/NeMo/nemo/lightning/megatron_parallel.py\", line 742, in wrapped\n[rank0]:     return attr(*args)\n[rank0]:   File \"/opt/NeMo/nemo/collections/llm/gpt/model/base.py\", line 311, in validation_step\n[rank0]:     return self.forward_step(batch)\n[rank0]:   File \"/opt/NeMo/nemo/collections/llm/gpt/model/base.py\", line 302, in forward_step\n[rank0]:     return self.config.forward_step_fn(self, batch)\n[rank0]:   File \"/opt/NeMo/nemo/collections/llm/gpt/model/base.py\", line 101, in gpt_forward_step\n[rank0]:     return model(**forward_args)\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1552, in _wrapped_call_impl\n[rank0]:     return self._call_impl(*args, **kwargs)\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1561, in _call_impl\n[rank0]:     return forward_call(*args, **kwargs)\n[rank0]:   File \"/opt/NeMo/nemo/collections/llm/gpt/model/base.py\", line 286, in forward\n[rank0]:     output_tensor = self.module(\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1552, in _wrapped_call_impl\n[rank0]:     return self._call_impl(*args, **kwargs)\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1561, in _call_impl\n[rank0]:     return forward_call(*args, **kwargs)\n[rank0]:   File \"/opt/megatron-lm/megatron/core/distributed/distributed_data_parallel.py\", line 305, in forward\n[rank0]:     return self.module(*inputs, **kwargs)\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1552, in _wrapped_call_impl\n[rank0]:     return self._call_impl(*args, **kwargs)\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1602, in _call_impl\n[rank0]:     result = forward_call(*args, **kwargs)\n[rank0]:   File \"/opt/megatron-lm/megatron/core/transformer/module.py\", line 178, in forward\n[rank0]:     outputs = self.module(*inputs, **kwargs)\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1552, in _wrapped_call_impl\n[rank0]:     return self._call_impl(*args, **kwargs)\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1602, in _call_impl\n[rank0]:     result = forward_call(*args, **kwargs)\n[rank0]:   File \"/opt/megatron-lm/megatron/core/models/gpt/gpt_model.py\", line 226, in forward\n[rank0]:     hidden_states = self.decoder(\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1552, in _wrapped_call_impl\n[rank0]:     return self._call_impl(*args, **kwargs)\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1602, in _call_impl\n[rank0]:     result = forward_call(*args, **kwargs)\n[rank0]:   File \"/opt/megatron-lm/megatron/core/transformer/transformer_block.py\", line 493, in forward\n[rank0]:     hidden_states, context = layer(\n[rank0]:   File \"/opt/megatron-lm/megatron/core/transformer/transformer_layer.py\", line 390, in __call__\n[rank0]:     return super(MegatronModule, self).__call__(*args, **kwargs)\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1552, in _wrapped_call_impl\n[rank0]:     return self._call_impl(*args, **kwargs)\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1602, in _call_impl\n[rank0]:     result = forward_call(*args, **kwargs)\n[rank0]:   File \"/opt/megatron-lm/megatron/core/transformer/transformer_layer.py\", line 294, in forward\n[rank0]:     attention_output_with_bias = self.self_attention(\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1552, in _wrapped_call_impl\n[rank0]:     return self._call_impl(*args, **kwargs)\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1602, in _call_impl\n[rank0]:     result = forward_call(*args, **kwargs)\n[rank0]:   File \"/opt/megatron-lm/megatron/core/transformer/attention.py\", line 314, in forward\n[rank0]:     core_attn_out = self.core_attention(\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1552, in _wrapped_call_impl\n[rank0]:     return self._call_impl(*args, **kwargs)\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1602, in _call_impl\n[rank0]:     result = forward_call(*args, **kwargs)\n[rank0]:   File \"/opt/megatron-lm/megatron/core/extensions/transformer_engine.py\", line 699, in forward\n[rank0]:     core_attn_out = super().forward(\n[rank0]:   File \"/opt/TransformerEngine/transformer_engine/pytorch/attention.py\", line 6984, in forward\n[rank0]:     raise Exception(\"No dot product attention support for the provided inputs!\")\n[rank0]: Exception: No dot product attention support for the provided inputs!\n```\n\n**Steps/Code to reproduce bug**\n\n```\nnemo llm pretrain -f gemma_2b trainer.max_steps=10\n```\n\n\n**Expected behavior**\n\nException should not pop up or abort.\n\n**Environment overview (please complete the following information)**\n```\nsudo docker run --gpus all --ipc=host --ulimit memlock=-1 --ulimit stack=67108864 -it nvcr.io/nvidia/nemo:dev\n```\non GCP VM\n**Environment details**\n\nIf NVIDIA docker image is used you don't need to specify these.\nOtherwise, please provide:\n- OS version: Ubuntu Ubuntu 22.04.4 LTS\n- PyTorch version: 2.4.0a0+3bcc3cddb5.nv24.07\n- Python version: 3.10.12\n\n**Additional context**\n GPU model: NVIDIA L4 * 8\n",
    "state": "closed",
    "created_at": "2024-11-07T22:29:30+00:00",
    "closed_at": "2024-12-16T02:07:12+00:00",
    "updated_at": "2024-12-16T02:07:12+00:00",
    "author": "ycchenzheng",
    "author_type": "User",
    "comments_count": 2,
    "reactions_count": 0,
    "labels": "bug,stale",
    "milestone": null,
    "closed_by": "github-actions[bot]",
    "resolution_time_hours": 915.6283333333333,
    "first_comments": [
      {
        "author": "github-actions[bot]",
        "created_at": "2024-12-08T02:09:11+00:00",
        "body": "This issue is stale because it has been open for 30 days with no activity. Remove stale label or comment or this will be closed in 7 days."
      },
      {
        "author": "github-actions[bot]",
        "created_at": "2024-12-16T02:07:11+00:00",
        "body": "This issue was closed because it has been inactive for 7 days since being marked as stale."
      }
    ],
    "url": "https://github.com/NVIDIA/NeMo/issues/11218"
  },
  {
    "repository": "NVIDIA/NeMo",
    "issue_number": 11207,
    "title": "Availability of Meeting MSDD models",
    "body": "Is there any update regarding MSDD for meeting domain publishing?",
    "state": "closed",
    "created_at": "2024-11-07T15:20:47+00:00",
    "closed_at": "2024-12-16T02:07:14+00:00",
    "updated_at": "2024-12-16T02:07:14+00:00",
    "author": "uro-sh",
    "author_type": "User",
    "comments_count": 2,
    "reactions_count": 0,
    "labels": "stale",
    "milestone": null,
    "closed_by": "github-actions[bot]",
    "resolution_time_hours": 922.7741666666667,
    "first_comments": [
      {
        "author": "github-actions[bot]",
        "created_at": "2024-12-08T02:09:13+00:00",
        "body": "This issue is stale because it has been open for 30 days with no activity. Remove stale label or comment or this will be closed in 7 days."
      },
      {
        "author": "github-actions[bot]",
        "created_at": "2024-12-16T02:07:14+00:00",
        "body": "This issue was closed because it has been inactive for 7 days since being marked as stale."
      }
    ],
    "url": "https://github.com/NVIDIA/NeMo/issues/11207"
  },
  {
    "repository": "NVIDIA/NeMo",
    "issue_number": 11204,
    "title": "How can I get stt_en_fastconformer_ctc_small pretrain model??",
    "body": "I fill \"stt_en_fastconformer_ctc_small\" in init_from_pretrained_model but i get an error \"not found\" the \"stt_en_fastconformer_ctc_small\"\n```yaml\nname: \"FastConformer-CTC-BPE\"\n# name: \"model STT N test\"\n\ninit_from_pretrained_model: \"stt_en_fastconformer_ctc_small\" \n\nmodel:\n  sample_rate: 16000\n  log_prediction: true # enables logging sample predictions in the output during training\n  ctc_reduction: 'mean_volume'\n  skip_nan_grad: false\n  model_defaults:\n    pred_hidden: 320\n    joint_hidden: 320\n\n  train_ds:\n    manifest_filepath: /home/team_voice/STT_pdnguyen/finetune-fast-conformer_14m/metadata_train/fubon_add_all_data_train_10_10_2024.json\n    sample_rate: ${model.sample_rate}\n    batch_size: 1 # you may increase batch_size if your memory allows\n    shuffle: true\n    num_workers: 32\n    pin_memory: true\n    max_duration: 20 # it is set for LibriSpeech, you may need to update it for your dataset\n    min_duration: 0.3\n    # tarred datasets\n    is_tarred: false\n    tarred_audio_filepaths: null\n    shuffle_n: 2048\n    # bucketing params\n    bucketing_strategy: \"fully_randomized\"\n    bucketing_batch_size: null\n```",
    "state": "closed",
    "created_at": "2024-11-07T10:29:30+00:00",
    "closed_at": "2024-12-20T01:58:56+00:00",
    "updated_at": "2024-12-20T01:58:56+00:00",
    "author": "PhamDangNguyen",
    "author_type": "User",
    "comments_count": 3,
    "reactions_count": 0,
    "labels": "stale",
    "milestone": null,
    "closed_by": "github-actions[bot]",
    "resolution_time_hours": 1023.4905555555556,
    "first_comments": [
      {
        "author": "MedAymenF",
        "created_at": "2024-11-11T15:02:42+00:00",
        "body": "As far as I can tell, this model doesn't exist. The only \"stt_*_*conformer_ctc_small\" models I've found were Conformer models, not FastConformer. Do you have a link to this model?"
      },
      {
        "author": "github-actions[bot]",
        "created_at": "2024-12-12T02:05:13+00:00",
        "body": "This issue is stale because it has been open for 30 days with no activity. Remove stale label or comment or this will be closed in 7 days."
      },
      {
        "author": "github-actions[bot]",
        "created_at": "2024-12-20T01:58:55+00:00",
        "body": "This issue was closed because it has been inactive for 7 days since being marked as stale."
      }
    ],
    "url": "https://github.com/NVIDIA/NeMo/issues/11204"
  },
  {
    "repository": "NVIDIA/NeMo",
    "issue_number": 11200,
    "title": "NameError: name' flash_attn_with_kvcache 'is not defined",
    "body": "**Describe the bug**\n\nWhen I use flash attn=2.0.4, running Nemo will result in an error `NameError: name' flash_attn_with_kvcache 'is not defined`\n\nAfter checking the [code,](https://github.com/NVIDIA/NeMo/blob/main/nemo/collections/nlp/modules/common/megatron/attention.py) I found that when\n```python\ntry:\n  # Flash Attention 1.X\n  from flash_attn.bert_padding import pad_input, unpad_input\n  from flash_attn.flash_attn_interface import flash_attn_unpadded_func\n\n  HAVE_FLASH_ATTENTION = True\n  flash_attn_func = None\n```\nAfter success, below\n\n```python\nexcept (ImportError, ModuleNotFoundError):\n  try:\n    from flash_attn import flash_attn_with_kvcache\n  except\n    flash_attn_with_kvcache = None\n```\nI won't execute it\nSo below\n```python\nif (\n  flash_attn_with_kvcache is not None\n  and self.use_flash_attention\n  and rotary_pos_emb is not None\n  and inference_max_sequence_len\n  and not set_inference_key_value_memory\n  ）\n```\nThe above error will be reported, I think this is a bug, right?\n\n",
    "state": "closed",
    "created_at": "2024-11-07T03:24:44+00:00",
    "closed_at": "2024-12-22T02:03:09+00:00",
    "updated_at": "2024-12-22T02:03:10+00:00",
    "author": "1074224619",
    "author_type": "User",
    "comments_count": 3,
    "reactions_count": 0,
    "labels": "bug,stale",
    "milestone": null,
    "closed_by": "github-actions[bot]",
    "resolution_time_hours": 1078.6402777777778,
    "first_comments": [
      {
        "author": "csn1011",
        "created_at": "2024-11-13T18:50:37+00:00",
        "body": "what is the output of `pip freeze`?\n\n I'm running `flash-attn==2.0.4` and `nemo_toolkit==1.23.0` and \n\n```\n>>> from nemo.collections.nlp.modules.common.megatron.attention import CoreAttention\n```\n\nruns without issue"
      },
      {
        "author": "github-actions[bot]",
        "created_at": "2024-12-14T02:02:35+00:00",
        "body": "This issue is stale because it has been open for 30 days with no activity. Remove stale label or comment or this will be closed in 7 days."
      },
      {
        "author": "github-actions[bot]",
        "created_at": "2024-12-22T02:03:09+00:00",
        "body": "This issue was closed because it has been inactive for 7 days since being marked as stale."
      }
    ],
    "url": "https://github.com/NVIDIA/NeMo/issues/11200"
  },
  {
    "repository": "NVIDIA/NeMo",
    "issue_number": 11182,
    "title": "Would it be possible to use pyctcdecode with ASR cache-aware streaming?",
    "body": "Hello, \n\nI am interested if it would be possible to use the pyctcdecoder with hybrid CTC cache-aware streaming, and what would be the difficulties to implement it? I have seen that pyctcdecode support streaming with a stateful LM, but I am not sure how this would be affected in the case of cache-aware streaming.\n\nCould you help me with some insights and direct me towards some resources in order to implement a feature like this?\n\nAny insights are greatly appreciated! \n\nBest regards,\nGabi",
    "state": "closed",
    "created_at": "2024-11-06T12:10:36+00:00",
    "closed_at": "2024-12-15T02:09:09+00:00",
    "updated_at": "2024-12-15T02:09:09+00:00",
    "author": "gabitza-tech",
    "author_type": "User",
    "comments_count": 2,
    "reactions_count": 0,
    "labels": "stale",
    "milestone": null,
    "closed_by": "github-actions[bot]",
    "resolution_time_hours": 925.9758333333333,
    "first_comments": [
      {
        "author": "github-actions[bot]",
        "created_at": "2024-12-07T02:03:51+00:00",
        "body": "This issue is stale because it has been open for 30 days with no activity. Remove stale label or comment or this will be closed in 7 days."
      },
      {
        "author": "github-actions[bot]",
        "created_at": "2024-12-15T02:09:09+00:00",
        "body": "This issue was closed because it has been inactive for 7 days since being marked as stale."
      }
    ],
    "url": "https://github.com/NVIDIA/NeMo/issues/11182"
  },
  {
    "repository": "NVIDIA/NeMo",
    "issue_number": 11178,
    "title": "tutorials/tts/Inference_ModelSelect.ipynb is throwing error when installing dependancies",
    "body": "**Describe the bug**\n\nFirst cell is throwing error when executed in Colab or any Jupyter Notebook\n\n**Steps/Code to reproduce bug**\n\nExecute cell\n\nA  helpful guide on on how to craft a minimal bug report  http://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports. \n\n**Expected behavior**\n\nInstall all dependancies without any errors\n\n**Environment overview (please complete the following information)**\n\n - Environment location: Collab, Paperspace Notebooks, tried on both\n\n**Additional context**\n\n[Image](https://github.com/user-attachments/assets/dfb6837f-3fc8-41f9-a4aa-8cc461cdbcd5)\n\n",
    "state": "closed",
    "created_at": "2024-11-06T09:02:20+00:00",
    "closed_at": "2024-12-16T02:07:16+00:00",
    "updated_at": "2024-12-16T02:07:17+00:00",
    "author": "bnarasimha",
    "author_type": "User",
    "comments_count": 2,
    "reactions_count": 0,
    "labels": "bug,stale",
    "milestone": null,
    "closed_by": "github-actions[bot]",
    "resolution_time_hours": 953.0822222222222,
    "first_comments": [
      {
        "author": "github-actions[bot]",
        "created_at": "2024-12-08T02:09:14+00:00",
        "body": "This issue is stale because it has been open for 30 days with no activity. Remove stale label or comment or this will be closed in 7 days."
      },
      {
        "author": "github-actions[bot]",
        "created_at": "2024-12-16T02:07:16+00:00",
        "body": "This issue was closed because it has been inactive for 7 days since being marked as stale."
      }
    ],
    "url": "https://github.com/NVIDIA/NeMo/issues/11178"
  },
  {
    "repository": "NVIDIA/NeMo",
    "issue_number": 11163,
    "title": "How to finetune the NEST model with CTC Loss for ASR task?",
    "body": "We have trained SSL model based on the NEST, how to finetune the previous model based on the CTC Loss function?\nThe pretraining scripts  are as follows: [NEST](https://github.com/NVIDIA/NeMo/blob/main/examples/asr/speech_pretraining/masked_token_pred_pretrain.py)",
    "state": "closed",
    "created_at": "2024-11-05T12:33:39+00:00",
    "closed_at": "2025-01-03T01:58:41+00:00",
    "updated_at": "2025-01-03T01:58:42+00:00",
    "author": "zw76859420",
    "author_type": "User",
    "comments_count": 4,
    "reactions_count": 0,
    "labels": "stale",
    "milestone": null,
    "closed_by": "github-actions[bot]",
    "resolution_time_hours": 1405.4172222222223,
    "first_comments": [
      {
        "author": "tomschelsen",
        "created_at": "2024-11-24T20:46:04+00:00",
        "body": "Have you checked paragraph \"Fine-tuning with pre-trained encoder\" of https://github.com/NVIDIA/NeMo/blob/stable/tutorials/asr/Self_Supervised_Pre_Training.ipynb ? "
      },
      {
        "author": "zw76859420",
        "created_at": "2024-11-26T09:28:39+00:00",
        "body": "Thanks for your kind reply, and i will review the above link."
      },
      {
        "author": "github-actions[bot]",
        "created_at": "2024-12-27T01:58:25+00:00",
        "body": "This issue is stale because it has been open for 30 days with no activity. Remove stale label or comment or this will be closed in 7 days."
      },
      {
        "author": "github-actions[bot]",
        "created_at": "2025-01-03T01:58:41+00:00",
        "body": "This issue was closed because it has been inactive for 7 days since being marked as stale."
      }
    ],
    "url": "https://github.com/NVIDIA/NeMo/issues/11163"
  },
  {
    "repository": "NVIDIA/NeMo",
    "issue_number": 11161,
    "title": "How to export fastconformer based asr model to support torchscript model?",
    "body": "We looked up the code in nemo (in example/asr/ folder), and we can't find any codes to export the asr model (based on fastconformer) to support thr pytorch torchscript format. \nPlease give us some suggestions to solve the above problems.",
    "state": "closed",
    "created_at": "2024-11-05T12:17:31+00:00",
    "closed_at": "2024-12-13T02:06:04+00:00",
    "updated_at": "2024-12-13T02:06:05+00:00",
    "author": "zw76859420",
    "author_type": "User",
    "comments_count": 2,
    "reactions_count": 0,
    "labels": "stale",
    "milestone": null,
    "closed_by": "github-actions[bot]",
    "resolution_time_hours": 901.8091666666667,
    "first_comments": [
      {
        "author": "github-actions[bot]",
        "created_at": "2024-12-06T02:04:41+00:00",
        "body": "This issue is stale because it has been open for 30 days with no activity. Remove stale label or comment or this will be closed in 7 days."
      },
      {
        "author": "github-actions[bot]",
        "created_at": "2024-12-13T02:06:04+00:00",
        "body": "This issue was closed because it has been inactive for 7 days since being marked as stale."
      }
    ],
    "url": "https://github.com/NVIDIA/NeMo/issues/11161"
  },
  {
    "repository": "NVIDIA/NeMo",
    "issue_number": 11151,
    "title": "How to use wandb sweep for hyperparameter search when finetuning with llama2",
    "body": "**Is your feature request related to a problem? Please describe.**\n\nA clear and concise description of what the problem is. Ex. I'm always frustrated when [...]\n\n**Describe the solution you'd like**\n\nA clear and concise description of what you want to happen.\nProvide a code snippet on how new APIs/changes would be used by others.\n\n```\nI would like to do hyperparameter search with wandb sweep. Right now I can't find an option to add sweep config in yaml file. How can I enable wandb sweep and fine-tune llama2 model?\n\nhttps://wandb.ai/aarora/Nvidia%20NeMO/reports/Train-Optimize-Analyze-Visualize-and-Deploy-Models-for-Automatic-Speech-Recognition-with-NVIDIA-s-NeMo--VmlldzoxNzI0ODEw\n```\n\n**Describe alternatives you've considered**\n\nA clear and concise description of any alternative solutions or features you've considered.\n\n**Additional context**\n\nAdd any other context or screenshots about the feature request here.\n",
    "state": "closed",
    "created_at": "2024-11-04T22:33:09+00:00",
    "closed_at": "2024-11-07T01:34:43+00:00",
    "updated_at": "2024-11-07T01:34:43+00:00",
    "author": "PurvangL",
    "author_type": "User",
    "comments_count": 1,
    "reactions_count": 0,
    "labels": "",
    "milestone": null,
    "closed_by": "PurvangL",
    "resolution_time_hours": 51.02611111111111,
    "first_comments": [
      {
        "author": "PurvangL",
        "created_at": "2024-11-05T19:21:57+00:00",
        "body": "@okuchaiev following up regarding issue. please let me know if any other information needed."
      }
    ],
    "url": "https://github.com/NVIDIA/NeMo/issues/11151"
  },
  {
    "repository": "NVIDIA/NeMo",
    "issue_number": 11149,
    "title": "Learning Rate Sudden Dropped to min_lr after Warm-up Steps",
    "body": "**Describe the bug**\n\nHi, I am observing that the learning rate suddenly dropped to `model.optim.sched.min_lr` after `model.optim.sched.warmup_steps`. I am using  `CosineAnnealing`, where I am expecting the learning rate will gradually drop to min_lr after warmup steps instead of suddenly dropping. \n![Image](https://github.com/user-attachments/assets/3fdee1ba-2fc2-40fa-b244-43a16f0bb43b)\n\n**Steps/Code to reproduce bug**\n\n```\n  optim:\n    name: distributed_fused_adam\n    lr: 5e-6\n    weight_decay: 0.01 \n    betas: \n    - 0.9\n    - 0.98\n    sched:\n\n      name: CosineAnnealing\n      warmup_steps: 250\n      constant_steps: 2500\n\n      min_lr: 1e-7\n```\n\n\n**Expected behavior**\n\nI am expecting the learning rate will gradually drop to min_lr after warmup steps instead of suddenly dropping. If I am doing it the wrong way, what should be the correct way of making this possible? \n\n\n**Environment overview (please complete the following information)**\n\n- PyTorch version 2.3\n- Python version 3.10",
    "state": "closed",
    "created_at": "2024-11-04T18:11:58+00:00",
    "closed_at": "2024-12-18T02:02:11+00:00",
    "updated_at": "2024-12-18T02:02:11+00:00",
    "author": "zixianwang2022",
    "author_type": "User",
    "comments_count": 3,
    "reactions_count": 0,
    "labels": "bug,stale",
    "milestone": null,
    "closed_by": "github-actions[bot]",
    "resolution_time_hours": 1039.8369444444445,
    "first_comments": [
      {
        "author": "csn1011",
        "created_at": "2024-11-09T23:40:03+00:00",
        "body": "Looking here[https://github.com/NVIDIA/NeMo/blob/main/nemo/core/optim/lr_scheduler.py#L353](url) you may need to set `decay_steps` (If I'm looking in the correct place). It looks like during the `warmup_steps` the learning rate linearly ramps to `max_lr`, then decays to `min_lr` during `decay_steps`. Curious if that works for you! "
      },
      {
        "author": "github-actions[bot]",
        "created_at": "2024-12-10T02:06:16+00:00",
        "body": "This issue is stale because it has been open for 30 days with no activity. Remove stale label or comment or this will be closed in 7 days."
      },
      {
        "author": "github-actions[bot]",
        "created_at": "2024-12-18T02:02:10+00:00",
        "body": "This issue was closed because it has been inactive for 7 days since being marked as stale."
      }
    ],
    "url": "https://github.com/NVIDIA/NeMo/issues/11149"
  },
  {
    "repository": "NVIDIA/NeMo",
    "issue_number": 11142,
    "title": "convert_qwen2_hf_to_nemo error",
    "body": "environment:  nvcr.io/nvidia/nemo:24.07 image (I also tried installing it from scratch according to the readme. They all make the same mistake)\n\n```python\npython ./scripts/checkpoint_converters/convert_qwen2_hf_to_nemo.py --input_name_or_path ../Qwen2.5-7B/ --output_path ../Qwen2.5-7B.nemo\n```\n\nTraceback (most recent call last):\n  File \"/workspace/NeMo/nemo/collections/nlp/models/language_modeling/megatron/bert/bert_spec.py\", line 17, in <module>\n    from megatron.core.extensions.transformer_engine import (\nModuleNotFoundError: No module named 'megatron.core.extensions'\n\nDuring handling of the above exception, another exception occurred:\nNameError: name 'ApexGuardDefaults' is not defined",
    "state": "closed",
    "created_at": "2024-11-04T08:58:36+00:00",
    "closed_at": "2024-12-13T02:06:06+00:00",
    "updated_at": "2024-12-13T02:06:06+00:00",
    "author": "huangqingyi-code",
    "author_type": "User",
    "comments_count": 2,
    "reactions_count": 0,
    "labels": "stale",
    "milestone": null,
    "closed_by": "github-actions[bot]",
    "resolution_time_hours": 929.125,
    "first_comments": [
      {
        "author": "github-actions[bot]",
        "created_at": "2024-12-05T02:05:33+00:00",
        "body": "This issue is stale because it has been open for 30 days with no activity. Remove stale label or comment or this will be closed in 7 days."
      },
      {
        "author": "github-actions[bot]",
        "created_at": "2024-12-13T02:06:05+00:00",
        "body": "This issue was closed because it has been inactive for 7 days since being marked as stale."
      }
    ],
    "url": "https://github.com/NVIDIA/NeMo/issues/11142"
  },
  {
    "repository": "NVIDIA/NeMo",
    "issue_number": 11139,
    "title": "How to Visualize Backward Computational Graph",
    "body": "Hi,\n\nIs there a way I can visualize the computational graph during backward propagation in Megatron?",
    "state": "closed",
    "created_at": "2024-11-02T18:39:52+00:00",
    "closed_at": "2024-12-10T02:06:18+00:00",
    "updated_at": "2024-12-10T02:06:18+00:00",
    "author": "zixianwang2022",
    "author_type": "User",
    "comments_count": 2,
    "reactions_count": 0,
    "labels": "stale",
    "milestone": null,
    "closed_by": "github-actions[bot]",
    "resolution_time_hours": 895.4405555555555,
    "first_comments": [
      {
        "author": "github-actions[bot]",
        "created_at": "2024-12-03T02:05:18+00:00",
        "body": "This issue is stale because it has been open for 30 days with no activity. Remove stale label or comment or this will be closed in 7 days."
      },
      {
        "author": "github-actions[bot]",
        "created_at": "2024-12-10T02:06:18+00:00",
        "body": "This issue was closed because it has been inactive for 7 days since being marked as stale."
      }
    ],
    "url": "https://github.com/NVIDIA/NeMo/issues/11139"
  },
  {
    "repository": "NVIDIA/NeMo",
    "issue_number": 11105,
    "title": "Slurm interactive mode, transcribe_speech_parallel.py gets stuck on consecutive runs",
    "body": "With container `nvcr.io/nvidia/nemo:24.07` (Pyxis/Enroot), ran in Slurm  interactive mode with 1 GPU if I execute the command\n```bash\npython3 /opt/NeMo/examples/asr/transcribe_speech_parallel.py \\\n...\n```\nthe script will get stuck on the second of multiple consecutive runs. The point where it does so is \n```bash\n> HERE\nUsing bfloat16 Automatic Mixed Precision (AMP)\nGPU available: True (cuda), used: True\n...\n```\n\nHowever if I execute the command\n```bash\ntorchrun --standalone --nnodes=1 --nproc-per-node=1 /opt/NeMo/examples/asr/transcribe_speech_parallel.py \\\n...\n``` \nI can run multiple consecutive times without any issues.\n\nI have tried with standard debug parameters like \n```\nTORCH_CPP_LOG_LEVEL=INFO \nTORCH_DISTRIBUTED_DEBUG=INFO \nNCCL_DEBUG=INFO \nNCCL_DEBUG_SUBSYS=ALL\n```\nbut nothing peculiar pops out.",
    "state": "closed",
    "created_at": "2024-10-30T21:30:06+00:00",
    "closed_at": "2024-12-07T02:03:54+00:00",
    "updated_at": "2024-12-07T02:03:55+00:00",
    "author": "itzsimpl",
    "author_type": "User",
    "comments_count": 2,
    "reactions_count": 0,
    "labels": "bug,stale",
    "milestone": null,
    "closed_by": "github-actions[bot]",
    "resolution_time_hours": 892.5633333333334,
    "first_comments": [
      {
        "author": "github-actions[bot]",
        "created_at": "2024-11-30T02:01:16+00:00",
        "body": "This issue is stale because it has been open for 30 days with no activity. Remove stale label or comment or this will be closed in 7 days."
      },
      {
        "author": "github-actions[bot]",
        "created_at": "2024-12-07T02:03:53+00:00",
        "body": "This issue was closed because it has been inactive for 7 days since being marked as stale."
      }
    ],
    "url": "https://github.com/NVIDIA/NeMo/issues/11105"
  },
  {
    "repository": "NVIDIA/NeMo",
    "issue_number": 11074,
    "title": "Code adoptability for other pretrained ASR",
    "body": "can we adopt this confidence estimation method for other pretrained ASR model like whisper or any LLM based ASR ? As these model have more tendency to produce hallucination as compared to model used in this code ?\n\nif yes so what things i need to take care ?",
    "state": "closed",
    "created_at": "2024-10-29T09:34:40+00:00",
    "closed_at": "2024-12-06T02:04:47+00:00",
    "updated_at": "2024-12-06T02:04:48+00:00",
    "author": "Amg9794",
    "author_type": "User",
    "comments_count": 2,
    "reactions_count": 0,
    "labels": "stale",
    "milestone": null,
    "closed_by": "github-actions[bot]",
    "resolution_time_hours": 904.5019444444445,
    "first_comments": [
      {
        "author": "github-actions[bot]",
        "created_at": "2024-11-29T02:03:45+00:00",
        "body": "This issue is stale because it has been open for 30 days with no activity. Remove stale label or comment or this will be closed in 7 days."
      },
      {
        "author": "github-actions[bot]",
        "created_at": "2024-12-06T02:04:47+00:00",
        "body": "This issue was closed because it has been inactive for 7 days since being marked as stale."
      }
    ],
    "url": "https://github.com/NVIDIA/NeMo/issues/11074"
  },
  {
    "repository": "NVIDIA/NeMo",
    "issue_number": 11064,
    "title": "activate subscription: Not able to find the Promotional Code, Serial Number, or Token to run the inference script?",
    "body": "Please activate your subscription:\nSerial Numbers:\nSerial numbers can be added in bulk, please comma separate them or line break all serial numbers\nSubmit serial numbers in bulk only if they are of the same hardware-software bundle type (e.g. RTX6000 or H100)\nTokens:\nTokens obtained from CSP-hosted virtual machine images must be submitted separately\nPromotional Code, Serial Number, or Token\nEnter promotional code, serial number, or token\n\n\n\nCode is here:\n``` # Choose a container name for bookkeeping\nexport CONTAINER_NAME=llama3-8b-instruct\n \n# Define the vendor name for the LLM\nexport VENDOR_NAME=meta\n \n# Choose a LLM NIM Image from NGC\nexport IMG_NAME=\"nvcr.io/nim/{VENDOR_NAME}/${CONTAINER_NAME}:1.0.0\"\n \n \n# Choose a path on your system to cache the downloaded models\nexport LOCAL_NIM_CACHE=\"~/.cache/nim\"\nmkdir -p \"$LOCAL_NIM_CACHE\"\n \n# Start the LLM NIM\ndocker run -it --rm --name=$CONTAINER_NAME \\\n  --runtime=nvidia \\\n  --gpus all \\\n  -e NGC_API_KEY \\\n  -v \"$LOCAL_NIM_CACHE:/opt/nim/.cache\" \\\n  -u $(id -u) \\\n  -p 8000:8000 \\\n  $IMG_NAME\n \n``` \n\n",
    "state": "closed",
    "created_at": "2024-10-28T19:22:37+00:00",
    "closed_at": "2024-12-05T02:05:36+00:00",
    "updated_at": "2024-12-05T02:05:36+00:00",
    "author": "allaabdella2",
    "author_type": "User",
    "comments_count": 2,
    "reactions_count": 0,
    "labels": "stale",
    "milestone": null,
    "closed_by": "github-actions[bot]",
    "resolution_time_hours": 894.7163888888889,
    "first_comments": [
      {
        "author": "github-actions[bot]",
        "created_at": "2024-11-28T02:03:55+00:00",
        "body": "This issue is stale because it has been open for 30 days with no activity. Remove stale label or comment or this will be closed in 7 days."
      },
      {
        "author": "github-actions[bot]",
        "created_at": "2024-12-05T02:05:36+00:00",
        "body": "This issue was closed because it has been inactive for 7 days since being marked as stale."
      }
    ],
    "url": "https://github.com/NVIDIA/NeMo/issues/11064"
  },
  {
    "repository": "NVIDIA/NeMo",
    "issue_number": 11025,
    "title": "RuntimeError: Function 'AcosBackward0' returned nan values in its 0th output.",
    "body": "I have this error:\n**RuntimeError: Function 'AcosBackward0' returned nan values in its 0th output.**\n\nI'm doing finetuning for verification. \nI've tried with different datasets of different size but I always get the same error.\n\nThe only time I got this to work was by reducing the learning rate to 0.001, but it resulted in a significant drop in performance.\n\nThis is my config file:\n\nname: &name \"TitaNet-Finetune\"\nsample_rate: &sample_rate 16000\n\ninit_from_pretrained_model:\n  speaker_tasks:\n    name: 'titanet_large'\n    include: [\"preprocessor\",\"encoder\"]\n    exclude: [\"decoder.final\"] # Add specific layer names here to exlude or just [\"decoder\"] if to exclude all of decoder pretrained weights\n\nmodel:\n  train_ds:\n    manifest_filepath: ???\n    sample_rate: 16000\n    labels: null\n    batch_size: 16\n    shuffle: True\n    is_tarred: False\n    tarred_audio_filepaths: null\n    tarred_shard_strategy: \"scatter\"\n    augmentor:\n      speed:\n        prob: 0.3\n        sr: *sample_rate\n        resample_type: 'kaiser_fast'\n        min_speed_rate: 0.95\n        max_speed_rate: 1.05\n\n  validation_ds:\n    manifest_filepath: ???\n    sample_rate: 16000\n    labels: null\n    batch_size: 32\n    shuffle: False\n\n  model_defaults:\n    filters: 1024\n    repeat: 3\n    dropout: 0.1\n    separable: true\n    se: true\n    se_context_size: -1\n    kernel_size_factor: 1.0\n    \n  preprocessor:\n    _target_: nemo.collections.asr.modules.AudioToMelSpectrogramPreprocessor\n    normalize: \"per_feature\"\n    window_size: 0.025\n    sample_rate: *sample_rate\n    window_stride: 0.01\n    window: \"hann\"\n    features: &n_mels 80\n    n_fft: 512\n    frame_splicing: 1\n    dither: 0.00001\n\n  encoder:\n    _target_: nemo.collections.asr.modules.ConvASREncoder\n    feat_in: *n_mels\n    activation: relu\n    conv_mask: true\n\n    jasper:\n      -   filters: ${model.model_defaults.filters}\n          repeat: 1\n          kernel: [3]\n          stride: [1]\n          dilation: [1]\n          dropout: 0.0\n          residual: false\n          separable: ${model.model_defaults.separable}\n          se: ${model.model_defaults.se}\n          se_context_size: ${model.model_defaults.se_context_size}\n\n      -   filters: ${model.model_defaults.filters}\n          repeat:  ${model.model_defaults.repeat}\n          kernel: [7]\n          stride: [1]\n          dilation: [1]\n          dropout: ${model.model_defaults.dropout}\n          residual: true\n          separable: ${model.model_defaults.separable}\n          se: ${model.model_defaults.se}\n          se_context_size: ${model.model_defaults.se_context_size}\n\n      -   filters: ${model.model_defaults.filters}\n          repeat: ${model.model_defaults.repeat}\n          kernel: [11]\n          stride: [1]\n          dilation: [1]\n          dropout: ${model.model_defaults.dropout}\n          residual: true\n          separable: ${model.model_defaults.separable}\n          se: ${model.model_defaults.se}\n          se_context_size: ${model.model_defaults.se_context_size}\n\n      -   filters: ${model.model_defaults.filters}\n          repeat: ${model.model_defaults.repeat}\n          kernel: [15]\n          stride: [1]\n          dilation: [1]\n          dropout: ${model.model_defaults.dropout}\n          residual: true\n          separable: ${model.model_defaults.separable}\n          se: ${model.model_defaults.se}\n          se_context_size: ${model.model_defaults.se_context_size}\n\n      -   filters: &enc_feat_out 3072\n          repeat: 1\n          kernel: [1]\n          stride: [1]\n          dilation: [1]\n          dropout: 0.0\n          residual: false\n          separable: ${model.model_defaults.separable}\n          se: ${model.model_defaults.se}\n          se_context_size: ${model.model_defaults.se_context_size}\n\n  decoder:\n    _target_: nemo.collections.asr.modules.SpeakerDecoder\n    feat_in: *enc_feat_out\n    num_classes: ???\n    pool_mode: 'attention'\n    emb_sizes: 192\n    angular: True\n\n  loss:\n    _target_: nemo.collections.asr.losses.angularloss.AngularSoftmaxLoss # you could also use cross-entrophy loss\n    scale: 30\n    margin: 0.2\n\n  optim_param_groups:\n    encoder:\n      lr: .001\n\n  optim:\n    name: adamw\n    lr: .0001 #(original titanet-large was trained with 0.08 lr)\n    weight_decay: 0.0002\n\n    # scheduler setup\n    sched:\n      name: CosineAnnealing\n      warmup_ratio: 0.1\n      min_lr: 0.0\n\ntrainer:\n  devices: 1 # number of gpus (original titanet-large was trained on 4 nodes with 8 gpus each)\n  max_epochs: 100\n  max_steps: -1 # computed at runtime if not set\n  num_nodes: 1\n  accelerator: gpu\n  strategy: ddp\n  deterministic: True\n  enable_checkpointing: False\n  logger: False\n  log_every_n_steps: 1  # Interval of logging.\n  val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations\n  gradient_clip_val: 1.0\n  detect_anomaly: True\n\nexp_manager:\n  exp_dir: null\n  name: *name\n  create_tensorboard_logger: True\n  create_checkpoint_callback: True\n  create_early_stopping_callback: True\n  early_stopping_callback_params:\n    monitor: \"val_loss\"  # The metric that early stopping should consider.\n    mode: \"min\"  # inform early stopping whether to look for increase or decrease in monitor.\n    min_delta: 0.001  # smallest change to consider as improvement.\n    patience: 20  # how many (continuous) validation cycles to wait with no improvement and stopping training.\n    verbose: True\n    strict: True\n    check_finite: True\n    log_rank_zero_only: False\n",
    "state": "closed",
    "created_at": "2024-10-24T13:42:37+00:00",
    "closed_at": "2024-12-01T02:12:10+00:00",
    "updated_at": "2024-12-01T02:12:11+00:00",
    "author": "gor2000",
    "author_type": "User",
    "comments_count": 2,
    "reactions_count": 0,
    "labels": "bug,stale",
    "milestone": null,
    "closed_by": "github-actions[bot]",
    "resolution_time_hours": 900.4925,
    "first_comments": [
      {
        "author": "github-actions[bot]",
        "created_at": "2024-11-24T02:05:55+00:00",
        "body": "This issue is stale because it has been open for 30 days with no activity. Remove stale label or comment or this will be closed in 7 days."
      },
      {
        "author": "github-actions[bot]",
        "created_at": "2024-12-01T02:12:10+00:00",
        "body": "This issue was closed because it has been inactive for 7 days since being marked as stale."
      }
    ],
    "url": "https://github.com/NVIDIA/NeMo/issues/11025"
  },
  {
    "repository": "NVIDIA/NeMo",
    "issue_number": 11019,
    "title": "Deploy ASR STT Streaming model",
    "body": "Hi\n\nAny suggestions on how to deploy and scale the STT En FastConformer Hybrid Transducer-CTC Large Streaming Multi model. We have finetuned this model and need to handle 100 users (to start with) using this model.\n\nWe will receive the chunks of audio data through a websocket connection from our application. How do we tackle multiple users? Do we create multiple instances of the model? Do we connect multiple users to same model? We are using the following tutorial for doing the inference through the model. (https://github.com/NVIDIA/NeMo/blob/main/tutorials/asr/Online_ASR_Microphone_Demo_Cache_Aware_Streaming.ipynb)\nThis notebook has a transcribe chunk function which takes an audio chunk and processes it. Can we do batch transcription here for multiple users?\n\nThank You",
    "state": "closed",
    "created_at": "2024-10-24T08:59:21+00:00",
    "closed_at": "2025-01-03T01:58:43+00:00",
    "updated_at": "2025-01-03T01:58:43+00:00",
    "author": "rkchamp25",
    "author_type": "User",
    "comments_count": 3,
    "reactions_count": 0,
    "labels": "stale",
    "milestone": null,
    "closed_by": "github-actions[bot]",
    "resolution_time_hours": 1696.9894444444444,
    "first_comments": [
      {
        "author": "jayakommuru",
        "created_at": "2024-11-26T16:53:55+00:00",
        "body": "@vsl9 can we deploy ASR Nemo checkpoints on Triton ? "
      },
      {
        "author": "github-actions[bot]",
        "created_at": "2024-12-27T01:58:26+00:00",
        "body": "This issue is stale because it has been open for 30 days with no activity. Remove stale label or comment or this will be closed in 7 days."
      },
      {
        "author": "github-actions[bot]",
        "created_at": "2025-01-03T01:58:42+00:00",
        "body": "This issue was closed because it has been inactive for 7 days since being marked as stale."
      }
    ],
    "url": "https://github.com/NVIDIA/NeMo/issues/11019"
  },
  {
    "repository": "NVIDIA/NeMo",
    "issue_number": 11004,
    "title": "canary-1b is not exportable",
    "body": "**Describe the bug**\n\nThe Canary model is not exportable to onnx. It seems like it was not configured to be exportable.\n\n**Steps/Code to reproduce bug**\n```\nimport nemo.collections.asr as nemo_asr\n\ndef main():\n    # Load Canary model\n    model_name = \"nvidia/canary-1b\"\n    print(f\"Loading model: {model_name}\")\n    model = nemo_asr.models.ASRModel.from_pretrained(model_name=model_name)\n    \n    # Prepare for export\n    model.eval()\n    \n    # Try to export - this will raise the error\n    output_path = \"canary.onnx\"\n    print(\"Attempting ONNX export...\")\n    model.export(\n        output_path,\n        onnx_opset_version=17,\n        verbose=True\n    )\n\nif __name__ == \"__main__\":\n    main()\n```\n\nThis gives the error:\n```\nAttributeError: 'EncDecMultiTaskModel' object has no attribute 'output_names'\nTraceback:\nFile \"venv/lib/python3.11/site-packages/streamlit/runtime/scriptrunner/exec_code.py\", line 88, in exec_func_with_error_handling\n    result = func()\n             ^^^^^^\nFile \"venv/lib/python3.11/site-packages/streamlit/runtime/scriptrunner/script_runner.py\", line 579, in code_to_exec\n    exec(code, module.__dict__)\nFile \"app.py\", line 22, in <module>\n    main()\nFile \"app.py\", line 15, in main\n    model.export(\nFile \"nemo/core/classes/exportable.py\", line 117, in export\n    out, descr, out_example = model._export(\n                              ^^^^^^^^^^^^^^\nFile \"nemo/core/classes/exportable.py\", line 196, in _export\n    output_names = self.output_names\n                   ^^^^^^^^^^^^^^^^^\nFile \"venv/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1931, in __getattr__\n    raise AttributeError(\n```\n\n\n**Expected behavior**\n\nI would expect to be able to export this model.\n\n**Environment overview (please complete the following information)**\n\n - Environment location: Mac\n - Method of NeMo install: pip install",
    "state": "closed",
    "created_at": "2024-10-23T14:56:16+00:00",
    "closed_at": "2025-01-01T02:03:42+00:00",
    "updated_at": "2025-01-01T02:03:43+00:00",
    "author": "pdufour",
    "author_type": "User",
    "comments_count": 4,
    "reactions_count": 0,
    "labels": "bug,stale",
    "milestone": null,
    "closed_by": "github-actions[bot]",
    "resolution_time_hours": 1667.1238888888888,
    "first_comments": [
      {
        "author": "github-actions[bot]",
        "created_at": "2024-11-23T01:59:38+00:00",
        "body": "This issue is stale because it has been open for 30 days with no activity. Remove stale label or comment or this will be closed in 7 days."
      },
      {
        "author": "pdufour",
        "created_at": "2024-11-23T02:14:11+00:00",
        "body": "Hi just commenting since GH marked this as stale."
      },
      {
        "author": "github-actions[bot]",
        "created_at": "2024-12-25T01:57:49+00:00",
        "body": "This issue is stale because it has been open for 30 days with no activity. Remove stale label or comment or this will be closed in 7 days."
      },
      {
        "author": "github-actions[bot]",
        "created_at": "2025-01-01T02:03:41+00:00",
        "body": "This issue was closed because it has been inactive for 7 days since being marked as stale."
      }
    ],
    "url": "https://github.com/NVIDIA/NeMo/issues/11004"
  },
  {
    "repository": "NVIDIA/NeMo",
    "issue_number": 10999,
    "title": "Unable to export MSDD model to pt or ONNX",
    "body": "**Describe the bug**\n\n`EncDecDiarLabelModel` inherits `ExportableEncDecModel` which inherits `Exportable`. That means it should be exported to pt or ONNX file. When I ran the following code to export it, \n```\nmodel = EncDecDiarLabelModel(cfg=modelConfig)\n msdd_model.export(output=\"msdd_model.pt\", input_example=input_example)\n```\nI ran into errors:\n```\nAttributeError: 'EncDecDiarLabelModel' object has no attribute 'input_names'\n```\nThe `input_names` attribute is defined in`Exportable`at [here](https://github.com/NVIDIA/NeMo/blob/1ba8bb1c623c2b7e938549002f5b77ab02fbe4bc/nemo/core/classes/exportable.py#L320). Is this an issue related to Python MRO (method resolution order)? \n\n**Steps/Code to reproduce bug**\n\nI added some lines after [here](https://github.com/NVIDIA/NeMo/blob/1ba8bb1c623c2b7e938549002f5b77ab02fbe4bc/tests/collections/asr/test_diar_label_models.py#L155).\n```\n            input_example = (input_signal, input_signal_length, emb_vectors, targets)\n            msdd_model.msdd._speaker_model.export(output=\"speaker_model.onnx\")\n            msdd_model.export(output=\"msdd_model.pt\", input_example=input_example)\n```\n\n**Expected behavior**\n\nBoth`speaker_model.py`and`msdd_model.pt` are generated.\n\n**Environment overview (please complete the following information)**\n\n - Environment location: Bare-metal\n - Method of NeMo install: check out the latest NeMo (commit 9a66eeeac2aa62541476bb4b66ae375b70d2db83) and run `pip install -e \".[asr]\"` under the root folder.\n\n**Environment details**\n\nIf NVIDIA docker image is used you don't need to specify these.\nOtherwise, please provide:\n- OS version: Ubuntu 24.04\n- PyTorch version: 2.4.1+cu124\n- Python version: 3.11.10\n\n**Additional context**\n\nAdd any other context about the problem here.\nGPU model: Nvidia L4 \n",
    "state": "closed",
    "created_at": "2024-10-22T21:32:02+00:00",
    "closed_at": "2024-12-01T02:12:12+00:00",
    "updated_at": "2024-12-01T02:12:12+00:00",
    "author": "jingzhaoo",
    "author_type": "User",
    "comments_count": 3,
    "reactions_count": 0,
    "labels": "bug,stale",
    "milestone": null,
    "closed_by": "github-actions[bot]",
    "resolution_time_hours": 940.6694444444445,
    "first_comments": [
      {
        "author": "tango4j",
        "created_at": "2024-10-24T15:44:28+00:00",
        "body": "Hi. MSDD is not an end-to-end model that performs speaker diarization from audio to label and it does not support ONNX export.\nWe are less than a month ahead of releasing end-to-end speaker diarizer, so please try using the new model once it gets released. "
      },
      {
        "author": "github-actions[bot]",
        "created_at": "2024-11-24T02:05:57+00:00",
        "body": "This issue is stale because it has been open for 30 days with no activity. Remove stale label or comment or this will be closed in 7 days."
      },
      {
        "author": "github-actions[bot]",
        "created_at": "2024-12-01T02:12:11+00:00",
        "body": "This issue was closed because it has been inactive for 7 days since being marked as stale."
      }
    ],
    "url": "https://github.com/NVIDIA/NeMo/issues/10999"
  },
  {
    "repository": "NVIDIA/NeMo",
    "issue_number": 10997,
    "title": "srun issue with nemorun",
    "body": "I am trying to run a simple pretraining job with nemorun: `nemorun llm pretrain --factory llama3_8b`\n\nHowever, I see the following error before the training starts:\n```\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM's PMI support and therefore cannot\nexecute. There are several options for building PMI support under\nSLURM, depending upon the SLURM version you are using:\n\n  version 16.05 or later: you can use SLURM's PMIx support. This\n  requires that you configure and build SLURM --with-pmix.\n\n  Versions earlier than 16.05: you must use either SLURM's PMI-1 or\n  PMI-2 support. SLURM builds PMI-1 by default, or you can manually\n  install PMI-2. You must then build Open MPI using --with-pmi pointing\n  to the SLURM PMI library location.\n\nPlease configure as appropriate and try again.\n```\n\nI understand that this is a problem with the way openmpi is configured on my cluster but I can't change it.\nAn alternate solution that I have been told is to replace the `srun` command with something like `mpirun -np`. I want to try this but I am unable to locate where the `srun` command is being run in the first place? I dug into the [slurm.py file in NeMo-run](https://github.com/NVIDIA/NeMo-Run/blob/main/src/nemo_run/core/execution/slurm.py), but changing the commands there did not yield any difference.\n\nAre there any other potential alternatives to this?",
    "state": "closed",
    "created_at": "2024-10-22T18:51:53+00:00",
    "closed_at": "2024-12-05T02:05:38+00:00",
    "updated_at": "2024-12-05T02:05:38+00:00",
    "author": "RachitBansal",
    "author_type": "User",
    "comments_count": 5,
    "reactions_count": 0,
    "labels": "stale",
    "milestone": null,
    "closed_by": "github-actions[bot]",
    "resolution_time_hours": 1039.2291666666667,
    "first_comments": [
      {
        "author": "hemildesai",
        "created_at": "2024-10-23T22:31:38+00:00",
        "body": "Can you try adding `executor.srun_args = [\"--mpi=pmix\"]` and see if that works?"
      },
      {
        "author": "RachitBansal",
        "created_at": "2024-10-24T00:31:47+00:00",
        "body": "@hemildesai \nHow do I set that argument? I don't see it in the listed arguments with the command `nemorun llm pretrain --help`:\n\n![Image](https://github.com/user-attachments/assets/d8af7cdf-4c57-4411-889e-859f52e5e581)\n"
      },
      {
        "author": "RachitBansal",
        "created_at": "2024-10-28T17:39:19+00:00",
        "body": "In the current version of the repo, the `srun_args` ([here](https://github.com/NVIDIA/NeMo/blob/f27a982ed192f8039d5a30b51208847c9c1b0fc3/nemo/collections/common/parts/run_utils.py#L496)) seem to have the `--mpi=pmix` command by default? However, I still see the same error as before."
      },
      {
        "author": "github-actions[bot]",
        "created_at": "2024-11-28T02:03:57+00:00",
        "body": "This issue is stale because it has been open for 30 days with no activity. Remove stale label or comment or this will be closed in 7 days."
      },
      {
        "author": "github-actions[bot]",
        "created_at": "2024-12-05T02:05:37+00:00",
        "body": "This issue was closed because it has been inactive for 7 days since being marked as stale."
      }
    ],
    "url": "https://github.com/NVIDIA/NeMo/issues/10997"
  },
  {
    "repository": "NVIDIA/NeMo",
    "issue_number": 10988,
    "title": "NeuralDiarizer with the telephonic config mix speakers at the very beginning of shorter audio files (less than 2 minutes duration)",
    "body": "**Describe the bug**\n\nI am using NeuralDiarizer with the default diar_infer_telephonic.yaml settings (nemo version 1.21.0). I am using it to diarize the real-life phone call recordings.\nI have experienced the same issue for almost any shorter audio file (less than 2 minutes duration) I have diarized: the first couple of utterances, pronounced by two different speakers, are merged into the same one, and labeled such that it was spoken by a single speaker. \nAfter the initial glitch, diarizer continues to work with the very precise predictions, so this issue is really only about the very first couple of sentences.\n\nAny recommendation how to improve its precision for that particular problem? \n\n**Steps/Code to reproduce bug**\n\nI am using NeuralDiarizer with the default diar_infer_telephonic.yaml settings file, with this addition:\n```python\nmeta = {\n        \"audio_filepath\": os.path.join(output_dir, \"mono_file.wav\"),\n        \"offset\": 0,\n        \"duration\": None,\n        \"label\": \"infer\",\n        \"text\": \"-\",\n        \"rttm_filepath\": None,\n        \"uem_filepath\": None,\n    }\n    with open(os.path.join(data_dir, \"input_manifest.json\"), \"w\") as fp:\n        json.dump(meta, fp)\n        fp.write(\"\\n\")\n\n    pretrained_vad = \"vad_multilingual_marblenet\"\n    pretrained_speaker_model = \"titanet_large\"\n    config.num_workers = 0\n    config.diarizer.manifest_filepath = os.path.join(data_dir, \"input_manifest.json\")\n    config.diarizer.out_dir = (\n        output_dir  # Directory to store intermediate files and prediction outputs\n    )\n\n    config.diarizer.speaker_embeddings.model_path = pretrained_speaker_model\n    config.diarizer.oracle_vad = (\n        False  # compute VAD provided with model_path to vad config\n    )\n    config.diarizer.clustering.parameters.oracle_num_speakers = False\n\n    config.diarizer.clustering.parameters.enhanced_count_thres = 80\n    config.diarizer.clustering.parameters.max_speaker_num = 2\n\n    # Here, we use our in-house pretrained NeMo VAD model\n    config.diarizer.vad.model_path = pretrained_vad\n    config.diarizer.vad.parameters.onset = 0.8\n    config.diarizer.vad.parameters.offset = 0.6\n    config.diarizer.vad.parameters.pad_offset = -0.05\n    config.diarizer.msdd_model.model_path = (\n        \"diar_msdd_telephonic\"  # Telephonic speaker diarization model\n    )\n``` \n",
    "state": "closed",
    "created_at": "2024-10-22T12:16:22+00:00",
    "closed_at": "2024-11-30T02:01:21+00:00",
    "updated_at": "2024-11-30T02:01:22+00:00",
    "author": "uro-sh",
    "author_type": "User",
    "comments_count": 2,
    "reactions_count": 0,
    "labels": "bug,stale",
    "milestone": null,
    "closed_by": "github-actions[bot]",
    "resolution_time_hours": 925.7497222222222,
    "first_comments": [
      {
        "author": "github-actions[bot]",
        "created_at": "2024-11-23T01:59:39+00:00",
        "body": "This issue is stale because it has been open for 30 days with no activity. Remove stale label or comment or this will be closed in 7 days."
      },
      {
        "author": "github-actions[bot]",
        "created_at": "2024-11-30T02:01:21+00:00",
        "body": "This issue was closed because it has been inactive for 7 days since being marked as stale."
      }
    ],
    "url": "https://github.com/NVIDIA/NeMo/issues/10988"
  },
  {
    "repository": "NVIDIA/NeMo",
    "issue_number": 10976,
    "title": "Megatron Multilingual En Any 500M",
    "body": "Hi!\n\nI'm using the [Megatron Multilingual En Any 500M model](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/nemo/models/megatronnmt_en_any_500m) with [this example script](https://github.com/NVIDIA/NeMo/blob/main/examples/nlp/machine_translation/nmt_transformer_infer_megatron.py).\n\nI got the script to work in a [Nemo Docker container](https://github.com/NVIDIA/NeMo/blob/main/examples/nlp/machine_translation/nmt_transformer_infer_megatron.py) in the sense that the script runs without error. However, the output file only contains empty lines indicating that the translation doesn't work properly.\nThe model seems to be loaded fine and there is no hint in the logs.\n\nAny idea what is going wrong? Is this model still supported?\n\nThanks!\n\n\n",
    "state": "closed",
    "created_at": "2024-10-21T17:31:23+00:00",
    "closed_at": "2024-10-29T10:40:29+00:00",
    "updated_at": "2024-10-29T10:40:31+00:00",
    "author": "rogerwelo",
    "author_type": "User",
    "comments_count": 2,
    "reactions_count": 0,
    "labels": "",
    "milestone": null,
    "closed_by": "rogerwelo",
    "resolution_time_hours": 185.15166666666667,
    "first_comments": [
      {
        "author": "shuoyangd",
        "created_at": "2024-10-23T18:29:46+00:00",
        "body": "Hi @rogerwelo . Can you specify which Nemo container you are using?\n\nI've been using this one (`nvcr.io/nvidia/nemo:23.08`) for inference with this model. If you are using a different container, can you try it out?\n\nShuoyang"
      },
      {
        "author": "rogerwelo",
        "created_at": "2024-10-29T10:40:30+00:00",
        "body": "I ended up using Riva and followed the steps described [here](https://docs.nvidia.com/deeplearning/riva/user-guide/docs/quick-start-guide.html#local-deployment-using-quick-start-scripts). This worked fine."
      }
    ],
    "url": "https://github.com/NVIDIA/NeMo/issues/10976"
  },
  {
    "repository": "NVIDIA/NeMo",
    "issue_number": 10966,
    "title": "Converting Mamba to tp4: RuntimeError: The size of tensor a (18560) must match the size of tensor b (4640) at non-singleton dimension 0",
    "body": "**Describe the bug**\n\nI am trying to convert the default `mamba.nemo` file (I converted [form huggingface](https://huggingface.co/nvidia/mamba2-8b-3t-4k/tree/main) .pt to .nemo) to have `tensor_parallel=4`. I have been following the [documentation here](https://docs.nvidia.com/nemo-framework/user-guide/latest/llms/mamba/checkpointconversion.html), but I got the following error. \n\n\n```\n[NeMo I 2024-10-21 06:50:54 megatron_init:314] Rank 0 has data parallel group : [0]                                                                                                                 \n[NeMo I 2024-10-21 06:50:54 megatron_init:320] Rank 0 has combined group of data parallel and context parallel : [0]                                                                                \n[NeMo I 2024-10-21 06:50:54 megatron_init:325] All data parallel group ranks with context parallel combined: [[0], [1], [2], [3]]                                                                   \n[NeMo I 2024-10-21 06:50:54 megatron_init:328] Ranks 0 has data parallel rank: 0                                                                                                                    \n[NeMo I 2024-10-21 06:50:54 megatron_init:336] Rank 0 has context parallel group: [0]                                                                                                               \n[NeMo I 2024-10-21 06:50:54 megatron_init:339] All context parallel group ranks: [[0], [1], [2], [3]]                                                                                               \n[NeMo I 2024-10-21 06:50:54 megatron_init:340] Ranks 0 has context parallel rank: 0                                                                                                                 \n[NeMo I 2024-10-21 06:50:54 megatron_init:347] Rank 0 has model parallel group: [0, 1, 2, 3]                                                                                                        \n[NeMo I 2024-10-21 06:50:54 megatron_init:348] All model parallel group ranks: [[0, 1, 2, 3]]                                                                                                       \n[NeMo I 2024-10-21 06:50:54 megatron_init:357] Rank 0 has tensor model parallel group: [0, 1, 2, 3]                                                                                                 \n[NeMo I 2024-10-21 06:50:54 megatron_init:361] All tensor model parallel group ranks: [[0, 1, 2, 3]]                                                                                                \n[NeMo I 2024-10-21 06:50:54 megatron_init:362] Rank 0 has tensor model parallel rank: 0                                                                                                             \n[NeMo I 2024-10-21 06:50:54 megatron_init:382] Rank 0 has pipeline model parallel group: [0]                                                                                                        \n[NeMo I 2024-10-21 06:50:54 megatron_init:394] Rank 0 has embedding group: [0]                                                                                                                      \n[NeMo I 2024-10-21 06:50:54 megatron_init:400] All pipeline model parallel group ranks: [[0], [1], [2], [3]]                                                                                        \n[NeMo I 2024-10-21 06:50:54 megatron_init:401] Rank 0 has pipeline model parallel rank 0                                                                                                            \n[NeMo I 2024-10-21 06:50:54 megatron_init:402] All embedding group ranks: [[0], [1], [2], [3]]                                                                                                      \n[NeMo I 2024-10-21 06:50:54 megatron_init:403] Rank 0 has embedding rank: 0                                                                                                                         \n[NeMo W 2024-10-21 06:50:54 megatron_base_model:1189] The model: MegatronMambaModel() does not have field.name: context_parallel_size in its cfg. Add this key to cfg or config_mapping to make to m\nake it configurable.                                                                                                                                                                                \n[NeMo W 2024-10-21 06:50:54 megatron_base_model:1189] The model: MegatronMambaModel() does not have field.name: moe_extended_tp in its cfg. Add this key to cfg or config_mapping to make to make it\n configurable.                                                                                                                                                                                      \n[NeMo W 2024-10-21 06:50:54 megatron_base_model:1189] The model: MegatronMambaModel() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make \nto make it configurable.                                                                                                                                                                            \n[NeMo W 2024-10-21 06:50:54 megatron_base_model:1189] The model: MegatronMambaModel() does not have field.name: deterministic_mode in its cfg. Add this key to cfg or config_mapping to make to make\n it configurable.                                                                                                                                                                                   \n[NeMo W 2024-10-21 06:50:54 megatron_base_model:1189] The model: MegatronMambaModel() does not have field.name: use_te_rng_tracker in its cfg. Add this key to cfg or config_mapping to make to make\n it configurable.                                                                                                                                                                                   \n[NeMo W 2024-10-21 06:50:54 megatron_base_model:1189] The model: MegatronMambaModel() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make\n it configurable.                                                                                                                                                                                   \n[NeMo W 2024-10-21 06:50:54 megatron_base_model:1189] The model: MegatronMambaModel() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make\n it configurable.                                                                                                                                                                                   \n[NeMo W 2024-10-21 06:50:54 megatron_base_model:1189] The model: MegatronMambaModel() does not have field.name: tp_comm_overlap_ag in its cfg. Add this key to cfg or config_mapping to make to make\n it configurable.                                                                                                                                                                                   \n[NeMo W 2024-10-21 06:50:54 megatron_base_model:1189] The model: MegatronMambaModel() does not have field.name: tp_comm_overlap_rs in its cfg. Add this key to cfg or config_mapping to make to make\n it configurable.                                                                                                                                                                                   \n[NeMo W 2024-10-21 06:50:54 megatron_base_model:1189] The model: MegatronMambaModel() does not have field.name: tp_comm_overlap_rs_dgrad in its cfg. Add this key to cfg or config_mapping to make t\no make it configurable.                                                                                                                                                                             \n[NeMo W 2024-10-21 06:50:54 megatron_base_model:1189] The model: MegatronMambaModel() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make i\nt configurable.                                                                                                                                                                                     \n[NeMo W 2024-10-21 06:50:54 megatron_base_model:1189] The model: MegatronMambaModel() does not have field.name: tp_comm_atomic_ag in its cfg. Add this key to cfg or config_mapping to make to make \nit configurable.                                                                                                                                                                                    \n[NeMo W 2024-10-21 06:50:54 megatron_base_model:1189] The model: MegatronMambaModel() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make i\nt configurable.                                                                                                                                                                                     \n[NeMo W 2024-10-21 06:50:54 megatron_base_model:1189] The model: MegatronMambaModel() does not have field.name: tp_comm_atomic_rs in its cfg. Add this key to cfg or config_mapping to make to make \nit configurable.                                                                                                                                                                                    \n[NeMo W 2024-10-21 06:50:54 megatron_base_model:1189] The model: MegatronMambaModel() does not have field.name: cross_entropy_loss_fusion in its cfg. Add this key to cfg or config_mapping to make \nto make it configurable.                                                                                                                                                                            \n[NeMo W 2024-10-21 06:50:54 megatron_base_model:1189] The model: MegatronMambaModel() does not have field.name: tp_comm_overlap_disable_qkv in its cfg. Add this key to cfg or config_mapping to mak\ne to make it configurable.                                                                                                                                                                          \n[NeMo W 2024-10-21 06:50:54 megatron_base_model:1189] The model: MegatronMambaModel() does not have field.name: tp_comm_overlap_disable_fc1 in its cfg. Add this key to cfg or config_mapping to mak\ne to make it configurable.                                                                                                                                                                          \n[NeMo W 2024-10-21 06:50:54 megatron_base_model:1189] The model: MegatronMambaModel() does not have field.name: tp_comm_bootstrap_backend in its cfg. Add this key to cfg or config_mapping to make \nto make it configurable.                                                                                                                                                                            \n[NeMo W 2024-10-21 06:50:54 megatron_base_model:1189] The model: MegatronMambaModel() does not have field.name: overlap_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make i\nt configurable.                                                                                                                                                                                     \n[NeMo W 2024-10-21 06:50:54 megatron_base_model:1189] The model: MegatronMambaModel() does not have field.name: batch_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it \nconfigurable.\n[NeMo W 2024-10-21 06:50:54 megatron_base_model:1189] The model: MegatronMambaModel() does not have field.name: defer_embedding_wgrad_compute in its cfg. Add this key to cfg or config_mapping to m\nake to make it configurable.\n[NeMo W 2024-10-21 06:50:54 megatron_base_model:1189] The model: MegatronMambaModel() does not have field.name: wgrad_deferral_limit in its cfg. Add this key to cfg or config_mapping to make to ma\nke it configurable.\n[NeMo W 2024-10-21 06:50:54 megatron_base_model:1189] The model: MegatronMambaModel() does not have field.name: pipeline_model_parallel_split_rank in its cfg. Add this key to cfg or config_mapping\n to make to make it configurable.\n[NeMo W 2024-10-21 06:50:54 megatron_base_model:1189] The model: MegatronMambaModel() does not have field.name: cpu_offloading in its cfg. Add this key to cfg or config_mapping to make to make it \nconfigurable.\n[NeMo W 2024-10-21 06:50:54 megatron_base_model:1189] The model: MegatronMambaModel() does not have field.name: cpu_offloading_num_layers in its cfg. Add this key to cfg or config_mapping to make \nto make it configurable.\n[NeMo W 2024-10-21 06:50:54 megatron_base_model:1189] The model: MegatronMambaModel() does not have field.name: _cpu_offloading_context in its cfg. Add this key to cfg or config_mapping to make to\n make it configurable.\n[NeMo W 2024-10-21 06:50:54 megatron_base_model:1189] The model: MegatronMambaModel() does not have field.name: cpu_offloading_activations in its cfg. Add this key to cfg or config_mapping to make\n to make it configurable.\n[NeMo W 2024-10-21 06:50:54 megatron_base_model:1189] The model: MegatronMambaModel() does not have field.name: cpu_offloading_weights in its cfg. Add this key to cfg or config_mapping to make to \nmake it configurable.\n[NeMo W 2024-10-21 06:50:54 megatron_base_model:1189] The model: MegatronMambaModel() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to ma\nke it configurable.\n[NeMo I 2024-10-21 06:50:54 tokenizer_utils:217] tokenizer_model: \n[NeMo I 2024-10-21 06:50:54 tokenizer_utils:218] /tmp/tmpozie5_vl/a649c6bb46eb404fac64c5ee8f43c407_mt_nlg_plus_multilingual_ja_zh_the_stack_frac_015_256k.model\n[NeMo I 2024-10-21 06:50:55 megatron_base_model:604] Padded vocab_size: 256000, original vocab_size: 256000, dummy tokens: 0.\n[NeMo W 2024-10-21 06:50:55 megatron_base_model:1189] The model: MegatronMambaModel() does not have field.name: context_parallel_size in its cfg. Add this key to cfg or config_mapping to make to m\nake it configurable.\n[NeMo W 2024-10-21 06:50:55 megatron_base_model:1189] The model: MegatronMambaModel() does not have field.name: moe_extended_tp in its cfg. Add this key to cfg or config_mapping to make to make it\n configurable.\n[NeMo W 2024-10-21 06:50:55 megatron_base_model:1189] The model: MegatronMambaModel() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make \nto make it configurable.\n[NeMo W 2024-10-21 06:50:55 megatron_base_model:1189] The model: MegatronMambaModel() does not have field.name: deterministic_mode in its cfg. Add this key to cfg or config_mapping to make to make\n it configurable.\n[NeMo W 2024-10-21 06:50:55 megatron_base_model:1189] The model: MegatronMambaModel() does not have field.name: use_te_rng_tracker in its cfg. Add this key to cfg or config_mapping to make to make\n it configurable.\n[NeMo W 2024-10-21 06:50:55 megatron_base_model:1189] The model: MegatronMambaModel() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make\n it configurable.\n[NeMo W 2024-10-21 06:50:55 megatron_base_model:1189] The model: MegatronMambaModel() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make\n it configurable.\n[NeMo W 2024-10-21 06:50:55 megatron_base_model:1189] The model: MegatronMambaModel() does not have field.name: tp_comm_overlap_ag in its cfg. Add this key to cfg or config_mapping to make to make\n it configurable.\n[NeMo W 2024-10-21 06:50:55 megatron_base_model:1189] The model: MegatronMambaModel() does not have field.name: tp_comm_overlap_rs in its cfg. Add this key to cfg or config_mapping to make to make\n it configurable.\n[NeMo W 2024-10-21 06:50:55 megatron_base_model:1189] The model: MegatronMambaModel() does not have field.name: tp_comm_overlap_rs_dgrad in its cfg. Add this key to cfg or config_mapping to make t\no make it configurable.\n[NeMo W 2024-10-21 06:50:55 megatron_base_model:1189] The model: MegatronMambaModel() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make i\nt configurable.\n[NeMo W 2024-10-21 06:50:55 megatron_base_model:1189] The model: MegatronMambaModel() does not have field.name: tp_comm_atomic_ag in its cfg. Add this key to cfg or config_mapping to make to make \nit configurable.\n[NeMo W 2024-10-21 06:50:55 megatron_base_model:1189] The model: MegatronMambaModel() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make i\nt configurable.\n[NeMo W 2024-10-21 06:50:55 megatron_base_model:1189] The model: MegatronMambaModel() does not have field.name: tp_comm_atomic_rs in its cfg. Add this key to cfg or config_mapping to make to make \nit configurable.\n[NeMo W 2024-10-21 06:50:55 megatron_base_model:1189] The model: MegatronMambaModel() does not have field.name: cross_entropy_loss_fusion in its cfg. Add this key to cfg or config_mapping to make \nto make it configurable.\n[NeMo W 2024-10-21 06:50:55 megatron_base_model:1189] The model: MegatronMambaModel() does not have field.name: defer_embedding_wgrad_compute in its cfg. Add this key to cfg or config_mapping to m\nake to make it configurable.\n[NeMo W 2024-10-21 06:50:55 megatron_base_model:1189] The model: MegatronMambaModel() does not have field.name: wgrad_deferral_limit in its cfg. Add this key to cfg or config_mapping to make to ma\nke it configurable.\n[NeMo W 2024-10-21 06:50:55 megatron_base_model:1189] The model: MegatronMambaModel() does not have field.name: pipeline_model_parallel_split_rank in its cfg. Add this key to cfg or config_mapping\n to make to make it configurable.\n[NeMo W 2024-10-21 06:50:55 megatron_base_model:1189] The model: MegatronMambaModel() does not have field.name: cpu_offloading in its cfg. Add this key to cfg or config_mapping to make to make it \nconfigurable.\n[NeMo W 2024-10-21 06:50:55 megatron_base_model:1189] The model: MegatronMambaModel() does not have field.name: cpu_offloading_num_layers in its cfg. Add this key to cfg or config_mapping to make \nto make it configurable.\n[NeMo W 2024-10-21 06:50:55 megatron_base_model:1189] The model: MegatronMambaModel() does not have field.name: _cpu_offloading_context in its cfg. Add this key to cfg or config_mapping to make to\n make it configurable.\n[NeMo W 2024-10-21 06:50:55 megatron_base_model:1189] The model: MegatronMambaModel() does not have field.name: cpu_offloading_activations in its cfg. Add this key to cfg or config_mapping to make\n to make it configurable.\n[NeMo W 2024-10-21 06:50:55 megatron_base_model:1189] The model: MegatronMambaModel() does not have field.name: cpu_offloading_weights in its cfg. Add this key to cfg or config_mapping to make to \nmake it configurable.\n[NeMo W 2024-10-21 06:50:55 megatron_base_model:1189] The model: MegatronMambaModel() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to ma\nke it configurable.\n[NeMo W 2024-10-21 06:50:55 megatron_base_model:516] apply_query_key_layer_scaling is only enabled when using FP16, setting it to False and setting NVTE_APPLY_QK_LAYER_SCALING=0\n[NeMo W 2024-10-21 06:50:55 megatron_base_model:577] The model: MegatronMambaModel() does not have field.name: first_pipeline_num_layers in its cfg. Add this key to cfg or config_mapping to make t\no make it configurable.\n[NeMo W 2024-10-21 06:50:55 megatron_base_model:577] The model: MegatronMambaModel() does not have field.name: last_pipeline_num_layers in its cfg. Add this key to cfg or config_mapping to make to\n make it configurable.\n[NeMo W 2024-10-21 06:50:55 megatron_base_model:577] The model: MegatronMambaModel() does not have field.name: activation_func_fp8_input_store in its cfg. Add this key to cfg or config_mapping to \nmake to make it configurable.\n[NeMo W 2024-10-21 06:50:55 megatron_base_model:577] The model: MegatronMambaModel() does not have field.name: num_moe_experts in its cfg. Add this key to cfg or config_mapping to make to make it \nconfigurable.\n[NeMo W 2024-10-21 06:50:55 megatron_base_model:577] The model: MegatronMambaModel() does not have field.name: window_size in its cfg. Add this key to cfg or config_mapping to make to make it conf\nigurable.\n[NeMo W 2024-10-21 06:50:55 megatron_base_model:577] The model: MegatronMambaModel() does not have field.name: qk_layernorm in its cfg. Add this key to cfg or config_mapping to make to make it con\nfigurable.\n[NeMo W 2024-10-21 06:50:55 megatron_base_model:577] The model: MegatronMambaModel() does not have field.name: test_mode in its cfg. Add this key to cfg or config_mapping to make to make it config\nurable.\n[NeMo W 2024-10-21 06:50:55 megatron_base_model:577] The model: MegatronMambaModel() does not have field.name: calculate_per_token_loss in its cfg. Add this key to cfg or config_mapping to make to\n make it configurable.\n[NeMo W 2024-10-21 06:50:55 megatron_base_model:577] The model: MegatronMambaModel() does not have field.name: multi_latent_attention in its cfg. Add this key to cfg or config_mapping to make to m\nake it configurable.\n[NeMo W 2024-10-21 06:50:55 megatron_base_model:577] The model: MegatronMambaModel() does not have field.name: memory_efficient_layer_norm in its cfg. Add this key to cfg or config_mapping to make\n to make it configurable.\n[NeMo W 2024-10-21 06:50:55 megatron_base_model:577] The model: MegatronMambaModel() does not have field.name: fp8_margin in its cfg. Add this key to cfg or config_mapping to make to make it confi\ngurable.\n[NeMo W 2024-10-21 06:50:55 megatron_base_model:577] The model: MegatronMambaModel() does not have field.name: fp8_interval in its cfg. Add this key to cfg or config_mapping to make to make it con\nfigurable.\n[NeMo W 2024-10-21 06:50:55 megatron_base_model:577] The model: MegatronMambaModel() does not have field.name: fp8_amax_history_len in its cfg. Add this key to cfg or config_mapping to make to mak\ne it configurable.\n[NeMo W 2024-10-21 06:50:55 megatron_base_model:577] The model: MegatronMambaModel() does not have field.name: fp8_amax_compute_algo in its cfg. Add this key to cfg or config_mapping to make to ma\nke it configurable.\n[NeMo W 2024-10-21 06:50:55 megatron_base_model:577] The model: MegatronMambaModel() does not have field.name: fp8_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it config\nurable.\n[NeMo W 2024-10-21 06:50:55 megatron_base_model:577] The model: MegatronMambaModel() does not have field.name: fp8_dot_product_attention in its cfg. Add this key to cfg or config_mapping to make t\no make it configurable.\n[NeMo W 2024-10-21 06:50:55 megatron_base_model:577] The model: MegatronMambaModel() does not have field.name: fp8_multi_head_attention in its cfg. Add this key to cfg or config_mapping to make to\n make it configurable.\n[NeMo W 2024-10-21 06:50:55 megatron_base_model:577] The model: MegatronMambaModel() does not have field.name: moe_shared_expert_intermediate_size in its cfg. Add this key to cfg or config_mapping\n to make to make it configurable.\n[NeMo W 2024-10-21 06:50:55 megatron_base_model:577] The model: MegatronMambaModel() does not have field.name: moe_shared_expert_overlap in its cfg. Add this key to cfg or config_mapping to make t\no make it configurable.\n[NeMo W 2024-10-21 06:50:55 megatron_base_model:577] The model: MegatronMambaModel() does not have field.name: moe_router_load_balancing_type in its cfg. Add this key to cfg or config_mapping to m\nake to make it configurable.\n[NeMo W 2024-10-21 06:50:55 megatron_base_model:577] The model: MegatronMambaModel() does not have field.name: moe_router_topk in its cfg. Add this key to cfg or config_mapping to make to make it \nconfigurable.\n[NeMo W 2024-10-21 06:50:55 megatron_base_model:577] The model: MegatronMambaModel() does not have field.name: moe_router_pre_softmax in its cfg. Add this key to cfg or config_mapping to make to m\nake it configurable.\n[NeMo W 2024-10-21 06:50:55 megatron_base_model:577] The model: MegatronMambaModel() does not have field.name: moe_grouped_gemm in its cfg. Add this key to cfg or config_mapping to make to make it\n configurable.\n[NeMo W 2024-10-21 06:50:55 megatron_base_model:577] The model: MegatronMambaModel() does not have field.name: moe_aux_loss_coeff in its cfg. Add this key to cfg or config_mapping to make to make \nit configurable.\n[NeMo W 2024-10-21 06:50:55 megatron_base_model:577] The model: MegatronMambaModel() does not have field.name: moe_z_loss_coeff in its cfg. Add this key to cfg or config_mapping to make to make it\n configurable.\n[NeMo W 2024-10-21 06:50:55 megatron_base_model:577] The model: MegatronMambaModel() does not have field.name: moe_input_jitter_eps in its cfg. Add this key to cfg or config_mapping to make to mak\ne it configurable.\n[NeMo W 2024-10-21 06:50:55 megatron_base_model:577] The model: MegatronMambaModel() does not have field.name: moe_token_dropping in its cfg. Add this key to cfg or config_mapping to make to make \nit configurable.\n[NeMo W 2024-10-21 06:50:55 megatron_base_model:577] The model: MegatronMambaModel() does not have field.name: moe_token_dispatcher_type in its cfg. Add this key to cfg or config_mapping to make t\no make it configurable.\n[NeMo W 2024-10-21 06:50:55 megatron_base_model:577] The model: MegatronMambaModel() does not have field.name: moe_per_layer_logging in its cfg. Add this key to cfg or config_mapping to make to ma\nke it configurable.\n[NeMo W 2024-10-21 06:50:55 megatron_base_model:577] The model: MegatronMambaModel() does not have field.name: moe_expert_capacity_factor in its cfg. Add this key to cfg or config_mapping to make \nto make it configurable.\n[NeMo W 2024-10-21 06:50:55 megatron_base_model:577] The model: MegatronMambaModel() does not have field.name: moe_pad_expert_input_to_capacity in its cfg. Add this key to cfg or config_mapping to\n make to make it configurable.\n[NeMo W 2024-10-21 06:50:55 megatron_base_model:577] The model: MegatronMambaModel() does not have field.name: moe_token_drop_policy in its cfg. Add this key to cfg or config_mapping to make to ma\nke it configurable.\n[NeMo W 2024-10-21 06:50:55 megatron_base_model:577] The model: MegatronMambaModel() does not have field.name: moe_layer_recompute in its cfg. Add this key to cfg or config_mapping to make to make\n it configurable.\n[NeMo W 2024-10-21 06:50:55 megatron_base_model:577] The model: MegatronMambaModel() does not have field.name: clone_scatter_output_in_embedding in its cfg. Add this key to cfg or config_mapping t\no make to make it configurable.\n[NeMo W 2024-10-21 06:50:55 megatron_base_model:577] The model: MegatronMambaModel() does not have field.name: disable_parameter_transpose_cache in its cfg. Add this key to cfg or config_mapping t\no make to make it configurable.\n[NeMo W 2024-10-21 06:50:55 megatron_base_model:577] The model: MegatronMambaModel() does not have field.name: enable_cuda_graph in its cfg. Add this key to cfg or config_mapping to make to make i\nt configurable.\n[NeMo W 2024-10-21 06:50:55 megatron_base_model:577] The model: MegatronMambaModel() does not have field.name: external_cuda_graph in its cfg. Add this key to cfg or config_mapping to make to make\n it configurable.\n[NeMo W 2024-10-21 06:50:55 megatron_base_model:577] The model: MegatronMambaModel() does not have field.name: config_logger_dir in its cfg. Add this key to cfg or config_mapping to make to make i\nt configurable.\n[rank0]: Traceback (most recent call last):\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/megatron/core/transformer/spec_utils.py\", line 97, in build_module\n[rank0]:     return module(\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/megatron/core/extensions/transformer_engine.py\", line 333, in __init__\n[rank0]:     _ = _initialize_affine_weight_cpu(\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/megatron/core/tensor_parallel/layers.py\", line 150, in _initialize_affine_weight_cpu\n[rank0]:     weight.data.copy_(cpu_weight)\n[rank0]: RuntimeError: The size of tensor a (18560) must match the size of tensor b (4640) at non-singleton dimension 0\n\n[rank0]: During handling of the above exception, another exception occurred:\n\n[rank0]: Traceback (most recent call last):\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/megatron/core/transformer/spec_utils.py\", line 97, in build_module\n[rank0]:     return module(\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/megatron/core/ssm/mamba_mixer.py\", line 168, in __init__\n[rank0]:     self.in_proj = build_module(\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/megatron/core/transformer/spec_utils.py\", line 104, in build_module\n[rank0]:     raise type(e)(f\"{str(e)} when instantiating {module.__name__}\").with_traceback(\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/megatron/core/transformer/spec_utils.py\", line 97, in build_module\n[rank0]:     return module(\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/megatron/core/extensions/transformer_engine.py\", line 333, in __init__\n[rank0]:     _ = _initialize_affine_weight_cpu(\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/megatron/core/tensor_parallel/layers.py\", line 150, in _initialize_affine_weight_cpu\n[rank0]:     weight.data.copy_(cpu_weight)\n[rank0]: RuntimeError: The size of tensor a (18560) must match the size of tensor b (4640) at non-singleton dimension 0 when instantiating TELayerNormColumnParallelLinear\n\n[rank0]: During handling of the above exception, another exception occurred:\n\n[rank0]: Traceback (most recent call last):\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/megatron/core/transformer/spec_utils.py\", line 97, in build_module\n[rank0]:     return module(\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/megatron/core/ssm/mamba_layer.py\", line 45, in __init__\n[rank0]:     self.mixer = build_module(\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/megatron/core/transformer/spec_utils.py\", line 104, in build_module\n[rank0]:     raise type(e)(f\"{str(e)} when instantiating {module.__name__}\").with_traceback(\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/megatron/core/transformer/spec_utils.py\", line 97, in build_module\n[rank0]:     return module(\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/megatron/core/ssm/mamba_mixer.py\", line 168, in __init__\n[rank0]:     self.in_proj = build_module(\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/megatron/core/transformer/spec_utils.py\", line 104, in build_module\n[rank0]:     raise type(e)(f\"{str(e)} when instantiating {module.__name__}\").with_traceback(\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/megatron/core/transformer/spec_utils.py\", line 97, in build_module\n[rank0]:     return module(\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/megatron/core/extensions/transformer_engine.py\", line 333, in __init__\n[rank0]:     _ = _initialize_affine_weight_cpu(\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/megatron/core/tensor_parallel/layers.py\", line 150, in _initialize_affine_weight_cpu\n[rank0]:     weight.data.copy_(cpu_weight)\n[rank0]: RuntimeError: The size of tensor a (18560) must match the size of tensor b (4640) at non-singleton dimension 0 when instantiating TELayerNormColumnParallelLinear when instantiating MambaM\nixer\n\n[rank0]: During handling of the above exception, another exception occurred:\n\n[rank0]: Traceback (most recent call last):\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/megatron/core/transformer/spec_utils.py\", line 97, in build_module\n[rank0]:     return module(\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/megatron/core/ssm/mamba_block.py\", line 155, in __init__\n[rank0]:     layer = build_module(\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/megatron/core/transformer/spec_utils.py\", line 104, in build_module\n[rank0]:     raise type(e)(f\"{str(e)} when instantiating {module.__name__}\").with_traceback(\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/megatron/core/transformer/spec_utils.py\", line 97, in build_module\n[rank0]:     return module(\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/megatron/core/ssm/mamba_layer.py\", line 45, in __init__\n[rank0]:     self.mixer = build_module(\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/megatron/core/transformer/spec_utils.py\", line 104, in build_module\n[rank0]:     raise type(e)(f\"{str(e)} when instantiating {module.__name__}\").with_traceback(\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/megatron/core/transformer/spec_utils.py\", line 97, in build_module\n[rank0]:     return module(\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/megatron/core/ssm/mamba_mixer.py\", line 168, in __init__\n[rank0]:     self.in_proj = build_module(\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/megatron/core/transformer/spec_utils.py\", line 104, in build_module\n[rank0]:     raise type(e)(f\"{str(e)} when instantiating {module.__name__}\").with_traceback(\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/megatron/core/transformer/spec_utils.py\", line 97, in build_module\n[rank0]:     return module(\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/megatron/core/extensions/transformer_engine.py\", line 333, in __init__\n[rank0]:     _ = _initialize_affine_weight_cpu(\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/megatron/core/tensor_parallel/layers.py\", line 150, in _initialize_affine_weight_cpu\n[rank0]:     weight.data.copy_(cpu_weight)\n[rank0]: RuntimeError: The size of tensor a (18560) must match the size of tensor b (4640) at non-singleton dimension 0 when instantiating TELayerNormColumnParallelLinear when instantiating MambaM\nixer when instantiating MambaLayer\n\n[rank0]: During handling of the above exception, another exception occurred:\n\n```\n\n**Steps/Code to reproduce bug**\n\nPlease list *minimal* steps or code snippet for us to be able to reproduce the bug.\nI am using the `main branch`. \nload the model huggingface: https://huggingface.co/nvidia/mamba2-8b-3t-4k/tree/main\n\n```\nCUDA_VISIBLE_DEVICES=\"0\" python /opt/NeMo/scripts/checkpoint_converters/convert_mamba2_pyt_to_nemo.py \\\n                                --input_name_or_path <path to the source pytorch model> \\\n                                --output_path <path to target .nemo model> \\\n                                --mamba_ssm_ngroups 8 \\\n                                --precision bf16 \\\n                                --tokenizer_model_dir=<path to tokenizer.model> # Remove this line (or set it to None) for 130m, 370m, 780m, 1.3b, and 2.7b models.\n```\n\n```\npython /opt/NeMo/examples/nlp/language_modeling/mamba_change_num_partition.py \\\n       --model_file=<path to source .nemo model> \\\n       --target_file=<path to target .nemo model> \\\n       --tensor_model_parallel_size=1 \\\n       --target_tensor_model_parallel_size=4 \\\n       --precision=bf16 \\\n       --tokenizer_path=<path to tokenizer.model>\n``` \n\nA  helpful guide on on how to craft a minimal bug report  http://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports. \n\n\n**Expected behavior**\n\nIt should work without any errors by following the documentation. \n\n**Environment overview (please complete the following information)**\n\n - Environment location: `docker.`\n - Method of NeMo install: [pip install or from source]. Please specify exact commands you used to install.\n running the docker command \n - If method of install is [Docker], provide `docker pull` & `docker run` commands used\n - https://github.com/NVIDIA/NeMo/blob/main/Dockerfile.ci\n ```\ndocker build -f Dockerfile.ci -t nemo:latest . \n```\n\nAdd any other context about the problem here.\nExample: 8-SXM H100\n",
    "state": "closed",
    "created_at": "2024-10-21T07:18:59+00:00",
    "closed_at": "2024-11-30T02:01:23+00:00",
    "updated_at": "2024-11-30T02:01:24+00:00",
    "author": "zixianwang2022",
    "author_type": "User",
    "comments_count": 2,
    "reactions_count": 0,
    "labels": "bug,stale",
    "milestone": null,
    "closed_by": "github-actions[bot]",
    "resolution_time_hours": 954.7066666666667,
    "first_comments": [
      {
        "author": "github-actions[bot]",
        "created_at": "2024-11-23T01:59:41+00:00",
        "body": "This issue is stale because it has been open for 30 days with no activity. Remove stale label or comment or this will be closed in 7 days."
      },
      {
        "author": "github-actions[bot]",
        "created_at": "2024-11-30T02:01:22+00:00",
        "body": "This issue was closed because it has been inactive for 7 days since being marked as stale."
      }
    ],
    "url": "https://github.com/NVIDIA/NeMo/issues/10966"
  },
  {
    "repository": "NVIDIA/NeMo",
    "issue_number": 10940,
    "title": "NeMO dependency issues on HuggingFace Hub (for ASR models)",
    "body": "**Describe the bug**\n\nNeMo is not loading a ASR model I wanted to run because of internal dependency issues on HuggingFace Hub.\n\n**Steps/Code to reproduce bug**\n\nColab Link: [colab link](https://colab.research.google.com/drive/1JqMW-JkyHJ61UOFO9eQhoHkQx6I3cH05?usp=sharing)\n\n```python\n!pip install nemo_toolkit['all']\n\nimport nemo.collections.asr as nemo_asr\nasr_model = nemo_asr.models.ASRModel.from_pretrained(model_name=\"nvidia/parakeet-tdt_ctc-110m\")\n```\n\n**Expected behavior**\n\nIt should load up the model. \n\n**Environment overview (please complete the following information)**\n\nCheck colab notebook\n\n**Environment details**\n\nCheck colab notebook \n\n**Additional context**\n\nCheck colab notebook",
    "state": "closed",
    "created_at": "2024-10-18T09:24:21+00:00",
    "closed_at": "2024-11-23T08:03:53+00:00",
    "updated_at": "2024-11-23T08:03:53+00:00",
    "author": "bhavnicksm",
    "author_type": "User",
    "comments_count": 3,
    "reactions_count": 0,
    "labels": "bug,stale",
    "milestone": null,
    "closed_by": "bhavnicksm",
    "resolution_time_hours": 862.6588888888889,
    "first_comments": [
      {
        "author": "MahmoudAshraf97",
        "created_at": "2024-10-19T18:46:02+00:00",
        "body": "this was solved in `r2.0.0rc1`"
      },
      {
        "author": "nithinraok",
        "created_at": "2024-10-22T18:41:18+00:00",
        "body": "please install `!pip install huggingface-hub==0.23.3` "
      },
      {
        "author": "github-actions[bot]",
        "created_at": "2024-11-23T01:59:42+00:00",
        "body": "This issue is stale because it has been open for 30 days with no activity. Remove stale label or comment or this will be closed in 7 days."
      }
    ],
    "url": "https://github.com/NVIDIA/NeMo/issues/10940"
  },
  {
    "repository": "NVIDIA/NeMo",
    "issue_number": 10939,
    "title": "NeMo2.0 nemorun llm export ValueError: PyTorch DDP is not enabled for mcore optimizer",
    "body": "**Describe the bug**\n\nA clear and concise description of what the bug is.\n\n**Steps/Code to reproduce bug**\n\n```\nnemorun llm import llama3_8b hf://meta-llama/Meta-Llama-3-8B -y\nnemorun llm export ~/.cache/nemo/models/meta-llama/Meta-Llama-3-8B/context hf exp/PreTrain/export_llama3_8b -y\n```\n\nerror\n```\nDry run for task nemo.collections.llm.api:export_ckpt\nResolved Arguments\n┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ Argument Name        ┃ Resolved Value                                               ┃\n┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ load_connector       │ <function load_connector_from_trainer_ckpt at                │\n│                      │ 0x7feaf6adfbe0>                                              │\n│ output_path          │ PosixPath('exp/PreTrain/export_llama3_8b')                   │\n│ overwrite            │ False                                                        │\n│ path                 │ PosixPath('/root/.cache/nemo/models/meta-llama/Meta-Llama-3… │\n│ target               │ 'hf'                                                         │\n└──────────────────────┴──────────────────────────────────────────────────────────────┘\nLaunching None...\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n        - Avoid using `tokenizers` before the fork if possible\n        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nGPU available: True (cuda), used: False\nTPU available: False, using: 0 TPU cores\nHPU available: False, using: 0 HPUs\n[NeMo W 2024-10-18 09:16:11 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n    \nGPU available: True (cuda), used: False\nTPU available: False, using: 0 TPU cores\nHPU available: False, using: 0 HPUs\nGPU available: True (cuda), used: False\nTPU available: False, using: 0 TPU cores\nHPU available: False, using: 0 HPUs\nGPU available: True (cuda), used: False\nTPU available: False, using: 0 TPU cores\nHPU available: False, using: 0 HPUs\nGPU available: True (cuda), used: False\nTPU available: False, using: 0 TPU cores\nHPU available: False, using: 0 HPUs\n[ERROR    | root               ]: An error occurred: PyTorch DDP is not enabled for mcore optimizer\nTraceback (most recent call last):\n  File \"/usr/local/bin/nemorun\", line 8, in <module>\n    sys.exit(app())\n  File \"/usr/local/lib/python3.10/dist-packages/typer/main.py\", line 326, in __call__\n    raise e\n  File \"/usr/local/lib/python3.10/dist-packages/typer/main.py\", line 309, in __call__\n    return get_command(self)(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/click/core.py\", line 1157, in __call__\n    return self.main(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/typer/core.py\", line 723, in main\n    return _main(\n  File \"/usr/local/lib/python3.10/dist-packages/typer/core.py\", line 193, in _main\n    rv = self.invoke(ctx)\n  File \"/usr/local/lib/python3.10/dist-packages/click/core.py\", line 1688, in invoke\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\n  File \"/usr/local/lib/python3.10/dist-packages/click/core.py\", line 1688, in invoke\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\n  File \"/usr/local/lib/python3.10/dist-packages/click/core.py\", line 1434, in invoke\n    return ctx.invoke(self.callback, **ctx.params)\n  File \"/usr/local/lib/python3.10/dist-packages/click/core.py\", line 783, in invoke\n    return __callback(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/typer/main.py\", line 692, in wrapper\n    return callback(**use_params)\n  File \"/home/lifeiteng/code/NeMo-Run/src/nemo_run/cli/api.py\", line 793, in command\n    self.cli_execute(fn, ctx.args, type)\n  File \"/home/lifeiteng/code/NeMo-Run/src/nemo_run/cli/api.py\", line 845, in cli_execute\n    self._execute_task(fn, filtered_args)\n  File \"/home/lifeiteng/code/NeMo-Run/src/nemo_run/cli/api.py\", line 895, in _execute_task\n    run_task()\n  File \"/home/lifeiteng/code/NeMo-Run/src/nemo_run/cli/api.py\", line 874, in run_task\n    run.run(\n  File \"/home/lifeiteng/code/NeMo-Run/src/nemo_run/run/api.py\", line 65, in run\n    direct_run_fn(fn_or_script, dryrun=dryrun)\n  File \"/home/lifeiteng/code/NeMo-Run/src/nemo_run/run/task.py\", line 77, in direct_run_fn\n    built_fn()\n  File \"/home/lifeiteng/code/NeMo/nemo/collections/llm/api.py\", line 432, in export_ckpt\n    return io.export_ckpt(path, target, output_path, overwrite, load_connector)\n  File \"/home/lifeiteng/code/NeMo/nemo/lightning/io/api.py\", line 197, in export_ckpt\n    return exporter(overwrite=overwrite, output_path=_output_path)\n  File \"/home/lifeiteng/code/NeMo/nemo/lightning/io/connector.py\", line 85, in __call__\n    to_return = self.apply(_output_path)\n  File \"/home/lifeiteng/code/NeMo/nemo/collections/llm/gpt/model/llama.py\", line 301, in apply\n    source, _ = self.nemo_load(str(self))\n  File \"/home/lifeiteng/code/NeMo/nemo/lightning/io/connector.py\", line 216, in nemo_load\n    _trainer.strategy.connect(model)\n  File \"/home/lifeiteng/code/NeMo/nemo/lightning/pytorch/strategies/megatron_strategy.py\", line 286, in connect\n    raise ValueError(\"PyTorch DDP is not enabled for mcore optimizer\")\nValueError: PyTorch DDP is not enabled for mcore optimizer\n```\n\n**Expected behavior**\n\nThe export of the Meta-Llama-3-8B model should complete successfully without errors, resulting in a checkpoint file stored in the specified path.\n",
    "state": "closed",
    "created_at": "2024-10-18T09:20:42+00:00",
    "closed_at": "2024-10-30T12:55:13+00:00",
    "updated_at": "2024-10-30T12:55:13+00:00",
    "author": "lifeiteng",
    "author_type": "User",
    "comments_count": 2,
    "reactions_count": 0,
    "labels": "bug",
    "milestone": null,
    "closed_by": "akoumpa",
    "resolution_time_hours": 291.5752777777778,
    "first_comments": [
      {
        "author": "akoumpa",
        "created_at": "2024-10-23T07:02:33+00:00",
        "body": "Thanks for reporting this bug, will look into it asap & push a fix."
      },
      {
        "author": "akoumpa",
        "created_at": "2024-10-29T11:16:22+00:00",
        "body": "Hi this has been fixed in https://github.com/NVIDIA/NeMo/pull/11081 please give it a try, will be merged soon. Thanks."
      }
    ],
    "url": "https://github.com/NVIDIA/NeMo/issues/10939"
  },
  {
    "repository": "NVIDIA/NeMo",
    "issue_number": 10938,
    "title": "The SDXL Infer  output image is full of noise",
    "body": "**Describe the bug**\n\nI follow the [tutorial](https://github.com/NVIDIA/NeMo/blob/main/tutorials/multimodal/SDXL%20Quantization.ipynb) convert a model download from https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/resolve/main/unet/diffusion_pytorch_model.safetensors.\nand I only convert from pytorch model to nemo model.\nand infer the nemo model.\nbut the output image is full of noise.\nIs there a bug or am I doing something wrong？\n\n**Steps/Code to reproduce bug**\n\nI just follow the [tutorial](https://github.com/NVIDIA/NeMo/blob/main/tutorials/multimodal/SDXL%20Quantization.ipynb) .\nNo quantization model is needed, the nemo model is converted, and the sd_xl_infer.py script is directly used for inference\nThe same result can be deduced from quantified model\n\n1. download model:\n```shell\nmkdir -p /sdxl_ckpts/stable-diffusion-xl-base-1.0/unet && wget -P /sdxl_ckpts/stable-diffusion-xl-base-1.0/unet https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/resolve/main/unet/diffusion_pytorch_model.safetensors\n``` \n```shell\nmkdir -p /sdxl_ckpts/stable-diffusion-xl-base-1.0/vae && wget -P /sdxl_ckpts/stable-diffusion-xl-base-1.0/vae https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/resolve/main/vae/diffusion_pytorch_model.safetensors\n``` \n2. convert safetensors to nemo model\n```shell\npython3 /opt/NeMo/examples/multimodal/text_to_image/convert_hf_ckpt_to_nemo.py \\\n    --model_type sdxl \\\n    --ckpt_path /sdxl_ckpts/stable-diffusion-xl-base-1.0/unet/diffusion_pytorch_model.safetensors \\\n    --hparams_file /opt/NeMo/examples/multimodal/text_to_image/stable_diffusion/conf/sd_xl_base_train.yaml \\\n    --nemo_file_path $WORKDIR/sdxl_base.nemo\n``` \n\n3. infer nemo model\n```shell\npython3 /opt/NeMo/examples/multimodal/text_to_image/stable_diffusion/sd_xl_infer.py model.restore_from_path=/sdxl_base.nemo out_path=/sdxl_infer_out\n``` \n\n\n**Expected behavior**\n\nExpect to produce a normal image instead of all noise\n\n**Environment details**\n\n- OS version ubuntu 22.04\n- PyTorch version 2.3.0\n- Python version  3.10.12\n\n**Additional context**\nThe output image:\n![Image](https://github.com/user-attachments/assets/df9f8932-e68e-4285-9076-bc919e050b62)\n\n\n",
    "state": "closed",
    "created_at": "2024-10-18T08:56:38+00:00",
    "closed_at": "2024-12-13T02:06:08+00:00",
    "updated_at": "2024-12-13T02:06:08+00:00",
    "author": "blacklong28",
    "author_type": "User",
    "comments_count": 8,
    "reactions_count": 0,
    "labels": "bug,stale",
    "milestone": null,
    "closed_by": "github-actions[bot]",
    "resolution_time_hours": 1337.1583333333333,
    "first_comments": [
      {
        "author": "blacklong28",
        "created_at": "2024-10-21T08:03:32+00:00",
        "body": "@Victor49152 Hi, Can you help me? I just used the config provided in the NeMo/examples/multimodal/text_to_image/stable_diffusion/conf  and follow the tutorials.\nI also use the nemo  docker to run sd_xl_infer.py，and is still noise output image. "
      },
      {
        "author": "Victor49152",
        "created_at": "2024-10-21T16:42:24+00:00",
        "body": "Thanks for your post. Could you please check the log executing 'convert_hf_ckpt_to_nemo.py', I think you might see some unexpected keys and missing keys warning. \n\nSome layer names might be changed in NeMo so the conversion script is not mapping the keys properly. Please let me know if that is the case, I will try to update the conversion script. Thanks."
      },
      {
        "author": "blacklong28",
        "created_at": "2024-10-22T02:33:45+00:00",
        "body": "Thanks for your reply. \nThis is the log executing 'convert_hf_ckpt_to_nemo.py': [convert_nemo_test.log](https://github.com/user-attachments/files/17469050/convert_nemo_test.log)\nI also saw some Missing and Unexpected keys in the SDXL Quantization.ipynb you provided. I thought they were normal, so I didn't pay much attention to them. Please help me to see if they are normal. Thank you.\n\n\n"
      },
      {
        "author": "Victor49152",
        "created_at": "2024-10-22T17:08:42+00:00",
        "body": "This conversion script is obsolete. Can you try https://github.com/NVIDIA/NeMo/blob/main/scripts/checkpoint_converters/convert_stablediffusion_hf_to_nemo.py and https://github.com/NVIDIA/NeMo/blob/409f1d847ff53a66e56763da3a83e2980e9afe53/examples/multimodal/text_to_image/stable_diffusion/conf/sd_xl_infer_v2.yaml as the new inference config. \n\nLet me know if these work for you. I will update the notebook later."
      },
      {
        "author": "blacklong28",
        "created_at": "2024-10-23T03:36:20+00:00",
        "body": "I use this script(https://github.com/NVIDIA/NeMo/blob/main/scripts/checkpoint_converters/convert_stablediffusion_hf_to_nemo.py)to convert a safetensors model to nemo.ckpt\nI notice that the saved model uses torch.save to save a ckpt model, is not a .nemo model.\nand use this config.\n```python\n        model_cfg.unet_config.from_pretrained = \"/opt/NeMo/nemo_out/sdxl_base_new_test1023A_nemo.ckpt\"\n        model_cfg.unet_config.from_NeMo = True\n        model_cfg.first_stage_config.from_pretrained = \"/opt/NeMo/nemo_out/sdxl_vae_new_test1023A_nemo.ckpt\"\n        model_cfg.first_stage_config.from_NeMo = True\n``` \n```shell\npython3 /opt/NeMo/examples/multimodal/text_to_image/stable_diffusion/sd_xl_infer.py model.restore_from_path=/opt/NeMo/nemo_out/sdxl_base_new_test1023A_nemo.ckpt  out_path=/opt/NeMo/infer_out\n``` \nI got the error:\n```shell\nroot@d392d2e1fa20:~# python3 /opt/NeMo/examples/multimodal/text_to_image/stable_diffusion/sd_xl_infer.py model.restore_from_path=/opt/NeMo/nemo_out/sdxl_base_new_test1023A_nemo.ckpt  out_path=/opt/NeMo/infer_out\n[NeMo I 2024-10-23 03:19:48 utils:285] FSDP is False, using DDP strategy.\nGPU available: True (cuda), used: True\nTPU available: False, using: 0 TPU cores\nIPU available: False, using: 0 IPUs\nHPU available: False, using: 0 HPUs\n[NeMo W 2024-10-23 03:19:48 utils:333] Loading from .ckpt checkpoint for inference is experimental! It doesn't support models with model parallelism!\nError executing job with overrides: ['model.restore_from_path=/opt/NeMo/nemo_out/sdxl_base_new_test1023A_nemo.ckpt', 'out_path=/opt/NeMo/infer_out']\nTraceback (most recent call last):\n  File \"/opt/NeMo/examples/multimodal/text_to_image/stable_diffusion/sd_xl_infer.py\", line 37, in main\n    trainer, megatron_diffusion_model = setup_trainer_and_model_for_inference(\n  File \"/opt/NeMo/nemo/collections/multimodal/parts/utils.py\", line 337, in setup_trainer_and_model_for_inference\n    model = model_provider.load_from_checkpoint(\n  File \"/opt/NeMo/nemo/collections/nlp/models/nlp_model.py\", line 385, in load_from_checkpoint\n    model = ptl_load_state(cls, checkpoint, strict=strict, cfg=cfg, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/core/saving.py\", line 158, in _load_state\n    obj = cls(**_cls_kwargs)\n  File \"/opt/NeMo/nemo/collections/multimodal/models/text_to_image/stable_diffusion/diffusion_engine.py\", line 367, in __init__\n    super().__init__(cfg, trainer=trainer)\n  File \"/opt/NeMo/nemo/collections/nlp/parts/mixins/nlp_adapter_mixins.py\", line 88, in __init__\n    super().__init__(*args, **kwargs)\n  File \"/opt/NeMo/nemo/collections/nlp/models/language_modeling/megatron_base_model.py\", line 118, in __init__\n    with open_dict(cfg):\n  File \"/usr/lib/python3.10/contextlib.py\", line 135, in __enter__\n    return next(self.gen)\nAttributeError: 'dict' object has no attribute '_get_node_flag'\n\nSet the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.\n``` \nAre there any other parameters or code I need to change here ?\n\nIf the.nemo suffix model is used as the file name for saving the model, an error will be reported when loading sdxl_infer.\n```\nroot@d392d2e1fa20:~# python3 /opt/NeMo/scripts/checkpoint_converters/convert_stablediffusion_hf_to_nemo.py --input_name_or_path /sdxl_ckpts/stable-diffusion-xl-base-1.0/unet/ --output_path /opt/NeMo/nemo_out/sdxl_base_new_test1023A.nemo --model unet --debug\n[NeMo I 2024-10-23 02:22:15 convert_stablediffusion_hf_to_nemo:413] loading checkpoint /sdxl_ckpts/stable-diffusion-xl-base-1.0/unet/\n[NeMo I 2024-10-23 02:22:15 convert_stablediffusion_hf_to_nemo:418] converting unet...\n[NeMo I 2024-10-23 02:22:15 convert_stablediffusion_hf_to_nemo:268] Add embedding found...\n[NeMo I 2024-10-23 02:22:15 convert_stablediffusion_hf_to_nemo:273] Time embedding found...\n[NeMo I 2024-10-23 02:23:16 convert_stablediffusion_hf_to_nemo:447] Saved nemo file to /opt/NeMo/nemo_out/sdxl_base_new_test1023A.nemo\nroot@d392d2e1fa20:~# python3 /opt/NeMo/examples/multimodal/text_to_image/stable_diffusion/sd_xl_infer.py model.restore_from_path=/opt/NeMo/nemo_out/sdxl_base_new_test1023A.nemo  out_path=/opt/NeMo/infer_out\n[NeMo I 2024-10-23 02:24:52 utils:285] FSDP is False, using DDP strategy.\nGPU available: True (cuda), used: True\nTPU available: False, using: 0 TPU cores\nIPU available: False, using: 0 IPUs\nHPU available: False, using: 0 HPUs\nError executing job with overrides: ['model.restore_from_path=/opt/NeMo/nemo_out/sdxl_base_new_test1023A.nemo', 'out_path=/opt/NeMo/infer_out']\nTraceback (most recent call last):\n  File \"/usr/lib/python3.10/tarfile.py\", line 1870, in gzopen\n    t = cls.taropen(name, mode, fileobj, **kwargs)\n  File \"/usr/lib/python3.10/tarfile.py\", line 1847, in taropen\n    return cls(name, mode, fileobj, **kwargs)\n  File \"/usr/lib/python3.10/tarfile.py\", line 1707, in __init__\n    self.firstmember = self.next()\n  File \"/usr/lib/python3.10/tarfile.py\", line 2622, in next\n    raise e\n  File \"/usr/lib/python3.10/tarfile.py\", line 2595, in next\n    tarinfo = self.tarinfo.fromtarfile(self)\n  File \"/usr/lib/python3.10/tarfile.py\", line 1285, in fromtarfile\n    buf = tarfile.fileobj.read(BLOCKSIZE)\n  File \"/usr/lib/python3.10/gzip.py\", line 301, in read\n    return self._buffer.read(size)\n  File \"/usr/lib/python3.10/_compression.py\", line 68, in readinto\n    data = self.read(len(byte_view))\n  File \"/usr/lib/python3.10/gzip.py\", line 488, in read\n    if not self._read_gzip_header():\n  File \"/usr/lib/python3.10/gzip.py\", line 436, in _read_gzip_header\n    raise BadGzipFile('Not a gzipped file (%r)' % magic)\ngzip.BadGzipFile: Not a gzipped file (b'PK')\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/opt/NeMo/examples/multimodal/text_to_image/stable_diffusion/sd_xl_infer.py\", line 37, in main\n    trainer, megatron_diffusion_model = setup_trainer_and_model_for_inference(\n  File \"/opt/NeMo/nemo/collections/multimodal/parts/utils.py\", line 314, in setup_trainer_and_model_for_inference\n    model_cfg = model_provider.restore_from(\n  File \"/opt/NeMo/nemo/collections/nlp/models/nlp_model.py\", line 478, in restore_from\n    return super().restore_from(\n  File \"/opt/NeMo/nemo/core/classes/modelPT.py\", line 468, in restore_from\n    instance = cls._save_restore_connector.restore_from(\n  File \"/opt/NeMo/nemo/collections/nlp/parts/nlp_overrides.py\", line 1298, in restore_from\n    loaded_params = super().load_config_and_state_dict(\n  File \"/opt/NeMo/nemo/core/connectors/save_restore_connector.py\", line 148, in load_config_and_state_dict\n    members = self._filtered_tar_info(restore_path, filter_fn=filter_fn)\n  File \"/opt/NeMo/nemo/core/connectors/save_restore_connector.py\", line 622, in _filtered_tar_info\n    with SaveRestoreConnector._tar_open(tar_path) as tar:\n  File \"/usr/lib/python3.10/contextlib.py\", line 135, in __enter__\n    return next(self.gen)\n  File \"/opt/NeMo/nemo/core/connectors/save_restore_connector.py\", line 661, in _tar_open\n    tar = tarfile.open(path2file, tar_header)\n  File \"/usr/lib/python3.10/tarfile.py\", line 1817, in open\n    return func(name, filemode, fileobj, **kwargs)\n  File \"/usr/lib/python3.10/tarfile.py\", line 1874, in gzopen\n    raise ReadError(\"not a gzip file\") from e\ntarfile.ReadError: not a gzip file\n\nSet the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.\n``` \n"
      }
    ],
    "url": "https://github.com/NVIDIA/NeMo/issues/10938"
  },
  {
    "repository": "NVIDIA/NeMo",
    "issue_number": 10905,
    "title": "global batch size at different sequence length",
    "body": "Are we supposed to use the same global batch size when increasing sequence_lengh?\nShouldn't the total number of tokens per global batch size remain the same?",
    "state": "closed",
    "created_at": "2024-10-16T16:31:11+00:00",
    "closed_at": "2024-11-24T02:06:00+00:00",
    "updated_at": "2024-11-24T02:06:00+00:00",
    "author": "erhoo82",
    "author_type": "User",
    "comments_count": 2,
    "reactions_count": 0,
    "labels": "stale",
    "milestone": null,
    "closed_by": "github-actions[bot]",
    "resolution_time_hours": 921.5802777777777,
    "first_comments": [
      {
        "author": "github-actions[bot]",
        "created_at": "2024-11-16T02:00:47+00:00",
        "body": "This issue is stale because it has been open for 30 days with no activity. Remove stale label or comment or this will be closed in 7 days."
      },
      {
        "author": "github-actions[bot]",
        "created_at": "2024-11-24T02:05:59+00:00",
        "body": "This issue was closed because it has been inactive for 7 days since being marked as stale."
      }
    ],
    "url": "https://github.com/NVIDIA/NeMo/issues/10905"
  },
  {
    "repository": "NVIDIA/NeMo",
    "issue_number": 10900,
    "title": "[Question] Pipeline Parallel for Mamba Megatron",
    "body": "Hi, \n\nI am wondering if the scripts implemented in Mamba Megatron pipeline parallel will be possibly used inside Nemo? Specifically Mamba Megatron supports converting Mamba both to Tensor Parallel & Pipeline Parallel [at here](https://github.com/NVIDIA/Megatron-LM/blob/main/tools/checkpoint/hybrid_conversion.py). The pipeline parallel code has been using Megatron's [without interleaved implementation](https://github.com/NVIDIA/Megatron-LM/blob/main/megatron/core/pipeline_parallel/schedules.py#L1274). Will we be able to utilize any of that Megatron implementation for SFT on Mamba in Nemo? \n\nThank you! ",
    "state": "closed",
    "created_at": "2024-10-16T06:20:58+00:00",
    "closed_at": "2024-10-23T18:25:42+00:00",
    "updated_at": "2024-10-23T18:25:42+00:00",
    "author": "zixianwang2022",
    "author_type": "User",
    "comments_count": 1,
    "reactions_count": 0,
    "labels": "",
    "milestone": null,
    "closed_by": null,
    "resolution_time_hours": 180.07888888888888,
    "first_comments": [
      {
        "author": "akoumpa",
        "created_at": "2024-10-23T07:01:55+00:00",
        "body": "@JRD971000 "
      }
    ],
    "url": "https://github.com/NVIDIA/NeMo/issues/10900"
  },
  {
    "repository": "NVIDIA/NeMo",
    "issue_number": 10899,
    "title": "Link Not Found at Mamba Tutorial",
    "body": "Hi, when I am looking at the [Mamba tutorial here ](https://github.com/NVIDIA/NeMo/tree/main/tutorials/llm/mamba#run-fine-tuning), the link for \n```\nFollow the steps from [here](https://nemo-framework-tme.gitlab-master-pages.nvidia.com/documentation/user-guide/latest/llms/gemma/dataprep.html) to obtain and preprocess the fine-tuning dataset.\n``` \ncan't be opened. ",
    "state": "closed",
    "created_at": "2024-10-16T06:13:33+00:00",
    "closed_at": "2024-11-24T02:06:01+00:00",
    "updated_at": "2024-11-24T02:06:02+00:00",
    "author": "zixianwang2022",
    "author_type": "User",
    "comments_count": 2,
    "reactions_count": 0,
    "labels": "stale",
    "milestone": null,
    "closed_by": "github-actions[bot]",
    "resolution_time_hours": 931.8744444444444,
    "first_comments": [
      {
        "author": "github-actions[bot]",
        "created_at": "2024-11-16T02:00:48+00:00",
        "body": "This issue is stale because it has been open for 30 days with no activity. Remove stale label or comment or this will be closed in 7 days."
      },
      {
        "author": "github-actions[bot]",
        "created_at": "2024-11-24T02:06:01+00:00",
        "body": "This issue was closed because it has been inactive for 7 days since being marked as stale."
      }
    ],
    "url": "https://github.com/NVIDIA/NeMo/issues/10899"
  },
  {
    "repository": "NVIDIA/NeMo",
    "issue_number": 10888,
    "title": "Modules fail for Dreambooth example",
    "body": "I'm trying to run the dreambooth tutorial, and when executing the dreambooth.py it raises an error related to the libraries, for example:\n\n[NeMo W 2024-10-08 09:12:02 megatron_lm_encoder_decoder_model:78] Megatron num_microbatches_calculator not found, using Apex version.\nTraceback (most recent call last):\n  File \"/opt/NeMo/clip/convert_external_clip_to_nemo.py\", line 53, in <module>\n    from nemo.collections.multimodal.models.vision_language_foundation.clip.megatron_clip_models import MegatronCLIPModel\n  File \"/usr/local/lib/python3.10/dist-packages/nemo/collections/multimodal/models/vision_language_foundation/clip/megatron_clip_models.py\", line 311, in <module>\n    class SiglipMHAPoolingHead(TransformerLayer):\nNameError: name 'TransformerLayer' is not defined\n\n\nI don't think you specify which NeMo image is compatible with this example. Two months ago it was working with the image nvcr.io/nvidia/nemo:24.02, but now it gives the error I put above. \nPlease could you tell me which image works with this tutorial? or which versions of the libraries are needed?\n\n\nThank you so much. \n",
    "state": "closed",
    "created_at": "2024-10-15T07:04:50+00:00",
    "closed_at": "2024-11-24T02:06:02+00:00",
    "updated_at": "2024-11-24T02:06:03+00:00",
    "author": "paulaserna16",
    "author_type": "User",
    "comments_count": 3,
    "reactions_count": 0,
    "labels": "bug,stale",
    "milestone": null,
    "closed_by": "github-actions[bot]",
    "resolution_time_hours": 955.02,
    "first_comments": [
      {
        "author": "paulaserna16",
        "created_at": "2024-10-17T11:16:24+00:00",
        "body": "Update on the procedure I'm following:\n- I use the Nemo framework 24.07 image\n- Launch a container with the image (witha  couple of volumes): docker run --runtime=nvidia --gpus all -it --rm \\\n    -v /mnt/shared_demos/dreambooth/nemo:/opt/NeMo \\\n    -v /mnt/shared_models/huggingface/cache/hub/models--runwayml--stable-diffusion-v1-5/snapshots/1d0c4ebf6ff58a5caecab40fa1406526bca4b5b9:/opt/models \\\n    --shm-size=8g -p 8888:8888 \\\n    --ulimit memlock=-1 --ulimit stack=67108864 \\\n    nvcr.io/nvidia/nemo:24.05\n- I install the main branch from the nemo repository in order to get the nemo toolkit installed: python -m pip install git+https://github.com/NVIDIA/NeMo.git@main#egg=nemo_toolkit[all]\n- The one above installs the megatron library in /opt/megatron-lm. However, this is version 0.8; and if I go to the path where it's located, the contents of /megatron/core are not updated, for example, there's no folder called extensions. \n\nThus, it installs something that cannot be used for the dreambooth example. \n\nThere's also a section (Megatron-LM) where it is specified to run the following in roder to use it:\n\n- git clone https://github.com/NVIDIA/Megatron-LM.git && \\\n- cd Megatron-LM && \\\n- git checkout $mcore_commit && \\\n- pip install .\n\nThis has the versions updated but installs a version of megatron incompatible with dreambooth files (0.10) and image, I believe. \n\nCould you help me with this issue please? is it related to the branch I'm installing? to the image?\n"
      },
      {
        "author": "github-actions[bot]",
        "created_at": "2024-11-17T02:05:12+00:00",
        "body": "This issue is stale because it has been open for 30 days with no activity. Remove stale label or comment or this will be closed in 7 days."
      },
      {
        "author": "github-actions[bot]",
        "created_at": "2024-11-24T02:06:02+00:00",
        "body": "This issue was closed because it has been inactive for 7 days since being marked as stale."
      }
    ],
    "url": "https://github.com/NVIDIA/NeMo/issues/10888"
  },
  {
    "repository": "NVIDIA/NeMo",
    "issue_number": 10884,
    "title": "Converting trained llama 2 checkpoint to hf gives \"invalid key\" error",
    "body": "**Describe the bug**\n\nWhile converting a trained nemo checkpoint using `NeMo/scripts/checkpoint_converters/convert_llama_nemo_to_hf.py`, I get the following error\n```\n[rank0]: Traceback (most recent call last):\n[rank0]:   File \"/opt/megatron-lm/megatron/core/dist_checkpointing/strategies/tensorstore.py\", line 123, in open_ts_array\n[rank0]:     arr = ts.open(ts.Spec(spec), open=True).result()\n[rank0]: ValueError: Error opening \"zarr\" driver: Error reading local file \"../exp/nemo/exp_inhouse_7b/TP8_PP1/checkpoints/peep/model_weights/model.embedding.word_embeddings.weight/.zarray\": Invalid key: \"../exp/nemo/exp_inhouse_7b/TP8_PP1/checkpoints/peep/model_weights/model.embedding.word_embeddings.weight/.zarray\" [source locations='tensorstore/kvstore/file/file_key_value_store.cc:659\\ntensorstore/kvstore/kvstore.cc:268\\ntensorstore/driver/driver.cc:114'] [tensorstore_spec='{\\\"context\\\":{\\\"cache_pool\\\":{},\\\"data_copy_concurrency\\\":{},\\\"file_io_concurrency\\\":{},\\\"file_io_sync\\\":true},\\\"driver\\\":\\\"zarr\\\",\\\"kvstore\\\":{\\\"driver\\\":\\\"file\\\",\\\"path\\\":\\\"../exp/nemo/exp_inhouse_7b/TP8_PP1/checkpoints/peep/model_weights/model.embedding.word_embeddings.weight/\\\"}}']\n\n[rank0]: The above exception was the direct cause of the following exception:\n\n[rank0]: Traceback (most recent call last):\n[rank0]:   File \"/workspace/repo/scripts/convert_llama_nemo_to_hf.py\", line 222, in <module>\n[rank0]:     convert(\n[rank0]:   File \"/workspace/repo/scripts/convert_llama_nemo_to_hf.py\", line 85, in convert\n[rank0]:     model = MegatronSparseFTModel.restore_from(\n[rank0]:   File \"/opt/NeMo/nemo/collections/nlp/models/nlp_model.py\", line 478, in restore_from\n[rank0]:     return super().restore_from(\n[rank0]:   File \"/opt/NeMo/nemo/core/classes/modelPT.py\", line 468, in restore_from\n[rank0]:     instance = cls._save_restore_connector.restore_from(\n[rank0]:   File \"/opt/NeMo/nemo/collections/nlp/parts/nlp_overrides.py\", line 1339, in restore_from\n[rank0]:     checkpoint = checkpoint_io.load_checkpoint(\n[rank0]:   File \"/usr/lib/python3.10/contextlib.py\", line 79, in inner\n[rank0]:     return func(*args, **kwds)\n[rank0]:   File \"/opt/NeMo/nemo/utils/callbacks/dist_ckpt_io.py\", line 334, in load_checkpoint\n[rank0]:     return dist_checkpointing.load(\n[rank0]:   File \"/opt/megatron-lm/megatron/core/dist_checkpointing/serialization.py\", line 157, in load\n[rank0]:     loaded_state_dict = sharded_strategy.load(sharded_state_dict, checkpoint_dir)\n[rank0]:   File \"/opt/megatron-lm/megatron/core/dist_checkpointing/strategies/tensorstore.py\", line 43, in load\n[rank0]:     dict_list_map_inplace(load_fn, sharded_state_dict)\n[rank0]:   File \"/opt/megatron-lm/megatron/core/dist_checkpointing/dict_utils.py\", line 180, in dict_list_map_inplace\n[rank0]:     x[k] = dict_list_map_inplace(f, v)\n[rank0]:   File \"/opt/megatron-lm/megatron/core/dist_checkpointing/dict_utils.py\", line 180, in dict_list_map_inplace\n[rank0]:     x[k] = dict_list_map_inplace(f, v)\n[rank0]:   File \"/opt/megatron-lm/megatron/core/dist_checkpointing/dict_utils.py\", line 184, in dict_list_map_inplace\n[rank0]:     return f(x)\n[rank0]:   File \"/opt/megatron-lm/megatron/core/dist_checkpointing/strategies/tensorstore.py\", line 80, in _load_from_array\n[rank0]:     x = _load_regular_chunk(sharded_tensor, checkpoint_dir)\n[rank0]:   File \"/opt/megatron-lm/megatron/core/dist_checkpointing/strategies/tensorstore.py\", line 91, in _load_regular_chunk\n[rank0]:     arr = open_ts_array(checkpoint_dir / sharded_tensor.key)\n[rank0]:   File \"/opt/megatron-lm/megatron/core/dist_checkpointing/strategies/tensorstore.py\", line 125, in open_ts_array\n[rank0]:     raise CheckpointingException(f'Array {arr_path} could not be loaded. Error: {e}') from e\n[rank0]: megatron.core.dist_checkpointing.core.CheckpointingException: Array ../exp/nemo/exp_inhouse_7b/TP8_PP1/checkpoints/peep/model_weights/model.embedding.word_embeddings.weight could not be loaded. Error: Error opening \"zarr\" driver: Error reading local file \"../exp/nemo/exp_inhouse_7b/TP8_PP1/checkpoints/peep/model_weights/model.embedding.word_embeddings.weight/.zarray\": Invalid key: \"../exp/nemo/exp_inhouse_7b/TP8_PP1/checkpoints/peep/model_weights/model.embedding.word_embeddings.weight/.zarray\" [source locations='tensorstore/kvstore/file/file_key_value_store.cc:659\\ntensorstore/kvstore/kvstore.cc:268\\ntensorstore/driver/driver.cc:114'] [tensorstore_spec='{\\\"context\\\":{\\\"cache_pool\\\":{},\\\"data_copy_concurrency\\\":{},\\\"file_io_concurrency\\\":{},\\\"file_io_sync\\\":true},\\\"driver\\\":\\\"zarr\\\",\\\"kvstore\\\":{\\\"driver\\\":\\\"file\\\",\\\"path\\\":\\\"../exp/nemo/exp_inhouse_7b/TP8_PP1/checkpoints/peep/model_weights/model.embedding.word_embeddings.weight/\\\"}}']\n```\n\n**Steps/Code to reproduce bug**\n1. Train a megatron llama model as usual. Here I used TP size=8, PP size=1 on 4 nodes.\n2. Call `python NeMo/scripts/checkpoint_converters/convert_llama_nemo_to_hf.py --input_name_or_path $PATH_TO_SAVED_NEMO --output_path $PATH_TO_CONVERTED_HF.bin`\n\n\n**Environment details**\nImage used: nvcr.io/nvidia/nemo:24.05.llama3.1\n\n",
    "state": "closed",
    "created_at": "2024-10-14T19:13:19+00:00",
    "closed_at": "2024-11-21T02:01:29+00:00",
    "updated_at": "2024-11-21T02:01:30+00:00",
    "author": "jiaji-huang",
    "author_type": "User",
    "comments_count": 2,
    "reactions_count": 0,
    "labels": "bug,stale",
    "milestone": null,
    "closed_by": "github-actions[bot]",
    "resolution_time_hours": 894.8027777777778,
    "first_comments": [
      {
        "author": "github-actions[bot]",
        "created_at": "2024-11-14T01:58:18+00:00",
        "body": "This issue is stale because it has been open for 30 days with no activity. Remove stale label or comment or this will be closed in 7 days."
      },
      {
        "author": "github-actions[bot]",
        "created_at": "2024-11-21T02:01:28+00:00",
        "body": "This issue was closed because it has been inactive for 7 days since being marked as stale."
      }
    ],
    "url": "https://github.com/NVIDIA/NeMo/issues/10884"
  },
  {
    "repository": "NVIDIA/NeMo",
    "issue_number": 10882,
    "title": "Add Hydrarunner to oomptimizer",
    "body": "Oomptimizer requires a standalone config for evaluation. This doesn't take advantage of hydrarunner as used in other ASR workflows. Would make life easier to allow passing hydrarunner changes.\n",
    "state": "closed",
    "created_at": "2024-10-14T17:54:25+00:00",
    "closed_at": "2024-11-21T02:01:30+00:00",
    "updated_at": "2024-11-21T02:01:31+00:00",
    "author": "bonham79",
    "author_type": "User",
    "comments_count": 2,
    "reactions_count": 0,
    "labels": "stale",
    "milestone": null,
    "closed_by": "github-actions[bot]",
    "resolution_time_hours": 896.1180555555555,
    "first_comments": [
      {
        "author": "github-actions[bot]",
        "created_at": "2024-11-14T01:58:19+00:00",
        "body": "This issue is stale because it has been open for 30 days with no activity. Remove stale label or comment or this will be closed in 7 days."
      },
      {
        "author": "github-actions[bot]",
        "created_at": "2024-11-21T02:01:30+00:00",
        "body": "This issue was closed because it has been inactive for 7 days since being marked as stale."
      }
    ],
    "url": "https://github.com/NVIDIA/NeMo/issues/10882"
  },
  {
    "repository": "NVIDIA/NeMo",
    "issue_number": 10876,
    "title": "SFT stage use context parallel with flash attention error",
    "body": "**Describe the bug**\nwhen I use context parallel > 2 in long-context training \nI have the error of flash attention problems \n\nI use the docker of nemo\n\n\n**bug**\n\n    hidden_states = self.decoder(\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/opt/megatron-lm/megatron/core/transformer/transformer_block.py\", line 383, in forward\n    hidden_states, context = layer(\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/opt/megatron-lm/megatron/core/transformer/transformer_layer.py\", line 178, in forward\n    attention_output_with_bias = self.self_attention(\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/opt/megatron-lm/megatron/core/transformer/attention.py\", line 315, in forward\n    core_attn_out = self.core_attention(\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/opt/megatron-lm/megatron/core/transformer/custom_layers/transformer_engine.py\", line 514, in forward\n    core_attn_out = super().forward(\n  File \"/usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/attention.py\", line 3599, in forward\n    qkv_layout, query_layer, key_layer, value_layer = _get_qkv_layout(\n  File \"/usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/attention.py\", line 1887, in _get_qkv_layout\n    q, k, v = [x.contiguous() for x in [q, k, v]]\n  File \"/usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/attention.py\", line 1887, in <listcomp>\n    q, k, v = [x.contiguous() for x in [q, k, v]]\nRuntimeError: CUDA error: an illegal memory access was encountered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/root/code/NeMO_megatron/NeMo/examples/nlp/language_modeling/tuning/megatron_gpt_finetuning.py\", line 79, in main\n    trainer.fit(model)\n  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\", line 544, in fit\n    call._call_and_handle_interrupt(\n  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/call.py\", line 68, in _call_and_handle_interrupt\n    trainer._teardown()\n  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\", line 1010, in _teardown\n    self.strategy.teardown()\n  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/strategies/ddp.py\", line 419, in teardown\n    super().teardown()\n  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/strategies/parallel.py\", line 133, in teardown\n    super().teardown()\n  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/strategies/strategy.py\", line 537, in teardown\n    self.lightning_module.cpu()\n  File \"/usr/local/lib/python3.10/dist-packages/lightning_fabric/utilities/device_dtype_mixin.py\", line 82, in cpu\n    return super().cpu()\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 960, in cpu\n    return self._apply(lambda t: t.cpu())\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 802, in _apply\n    module._apply(fn)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 802, in _apply\n    module._apply(fn)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 802, in _apply\n    module._apply(fn)\n  [Previous line repeated 1 more time]\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 825, in _apply\n    param_applied = fn(param)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 960, in <lambda>\n    return self._apply(lambda t: t.cpu())\nRuntimeError: CUDA error: an illegal memory access was encountered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n\n****",
    "state": "closed",
    "created_at": "2024-10-14T09:33:02+00:00",
    "closed_at": "2024-11-21T02:01:32+00:00",
    "updated_at": "2024-11-21T02:01:32+00:00",
    "author": "ARQlalala",
    "author_type": "User",
    "comments_count": 2,
    "reactions_count": 0,
    "labels": "bug,stale",
    "milestone": null,
    "closed_by": "github-actions[bot]",
    "resolution_time_hours": 904.475,
    "first_comments": [
      {
        "author": "github-actions[bot]",
        "created_at": "2024-11-14T01:58:20+00:00",
        "body": "This issue is stale because it has been open for 30 days with no activity. Remove stale label or comment or this will be closed in 7 days."
      },
      {
        "author": "github-actions[bot]",
        "created_at": "2024-11-21T02:01:31+00:00",
        "body": "This issue was closed because it has been inactive for 7 days since being marked as stale."
      }
    ],
    "url": "https://github.com/NVIDIA/NeMo/issues/10876"
  },
  {
    "repository": "NVIDIA/NeMo",
    "issue_number": 10870,
    "title": "Allow OOMtimizer tokenizer point towards just parent directory",
    "body": "**Is your feature request related to a problem? Please describe.**\n\noomptimizer requires directly passing the `tokenizer.model` file. This is inconsistent with tokenizer behavior for other ASR modules (we just pass the directory). Probably better to change just to keep consistent.\n\n\n",
    "state": "closed",
    "created_at": "2024-10-13T17:41:07+00:00",
    "closed_at": "2024-12-23T01:59:45+00:00",
    "updated_at": "2024-12-23T01:59:45+00:00",
    "author": "tbartley94",
    "author_type": "User",
    "comments_count": 3,
    "reactions_count": 0,
    "labels": "stale",
    "milestone": null,
    "closed_by": "github-actions[bot]",
    "resolution_time_hours": 1688.3105555555555,
    "first_comments": [
      {
        "author": "github-actions[bot]",
        "created_at": "2024-11-13T01:58:08+00:00",
        "body": "This issue is stale because it has been open for 30 days with no activity. Remove stale label or comment or this will be closed in 7 days."
      },
      {
        "author": "github-actions[bot]",
        "created_at": "2024-12-15T02:09:11+00:00",
        "body": "This issue is stale because it has been open for 30 days with no activity. Remove stale label or comment or this will be closed in 7 days."
      },
      {
        "author": "github-actions[bot]",
        "created_at": "2024-12-23T01:59:44+00:00",
        "body": "This issue was closed because it has been inactive for 7 days since being marked as stale."
      }
    ],
    "url": "https://github.com/NVIDIA/NeMo/issues/10870"
  },
  {
    "repository": "NVIDIA/NeMo",
    "issue_number": 10831,
    "title": "[Question] Converting a Megatron-LM ckpt to Nemo",
    "body": "Following the [GPT Pretraining](https://github.com/NVIDIA/Megatron-LM/tree/main?tab=readme-ov-file#gpt-pretraining) section in the  `Megatron-LM` repo,  we are able to successfully train a model using  `Megatron-LM` \n\n\nI saw pointers on how to convert from HF to `nemo`.  For example, this conversion script  [ convert_llama_hf_to_nemo.py](https://github.com/NVIDIA/NeMo/blob/stable/scripts/checkpoint_converters/convert_llama_hf_to_nemo.py)\n\n\nHowever I did not see any examples of converting a ckpt saved using  the  `Megatron-LM` to `nemo` format .  Are there any examples for this?\n\n\np.s.  I am thinking of doing this conversion to `nemo`, so I can use tools like `Nemo-Aligner` for post-training\n",
    "state": "closed",
    "created_at": "2024-10-10T03:25:33+00:00",
    "closed_at": "2024-11-17T02:05:15+00:00",
    "updated_at": "2024-11-17T02:05:15+00:00",
    "author": "abgoswam",
    "author_type": "User",
    "comments_count": 4,
    "reactions_count": 0,
    "labels": "stale",
    "milestone": null,
    "closed_by": "github-actions[bot]",
    "resolution_time_hours": 910.6616666666666,
    "first_comments": [
      {
        "author": "aimarz",
        "created_at": "2024-10-10T13:30:15+00:00",
        "body": "There is a script for that: [`<NeMo_ROOT_FOLDER>/examples/nlp/language_modeling/megatron_lm_ckpt_to_nemo.py`](https://github.com/NVIDIA/NeMo/blob/b4f308600a8a35eecf33f5cfdecc9f5f648f2863/examples/nlp/language_modeling/megatron_lm_ckpt_to_nemo.py#L4)\n\nLook up this guide: https://docs.nvidia.com/nemo-framework/user-guide/latest/nemotoolkit/checkpoints/convert_mlm.html \n\nAlso this issue I opened might be relevant to you: https://github.com/NVIDIA/NeMo/issues/10480"
      },
      {
        "author": "abgoswam",
        "created_at": "2024-10-10T18:17:38+00:00",
        "body": "thanks @aimarz . let me try it out on the Megatron-LM ckpts  we pre-trained "
      },
      {
        "author": "github-actions[bot]",
        "created_at": "2024-11-10T02:00:39+00:00",
        "body": "This issue is stale because it has been open for 30 days with no activity. Remove stale label or comment or this will be closed in 7 days."
      },
      {
        "author": "github-actions[bot]",
        "created_at": "2024-11-17T02:05:14+00:00",
        "body": "This issue was closed because it has been inactive for 7 days since being marked as stale."
      }
    ],
    "url": "https://github.com/NVIDIA/NeMo/issues/10831"
  },
  {
    "repository": "NVIDIA/NeMo",
    "issue_number": 10830,
    "title": "When do add the code for Target Speaker Extraction, thank!",
    "body": "Recently, I have been conducting applied research on Target Speaker Extraction, but I have encountered many difficulties. I came across your paper titled 'Generative Speech Foundation Model Pretraining for High-Quality Speech Extraction and Restoration.' If you have made the source code open source, I believe it would be of great assistance to me. Therefore, I am eagerly looking forward to it. Thank you!",
    "state": "closed",
    "created_at": "2024-10-10T02:56:08+00:00",
    "closed_at": "2024-12-13T02:06:09+00:00",
    "updated_at": "2024-12-13T02:06:09+00:00",
    "author": "haha010508",
    "author_type": "User",
    "comments_count": 4,
    "reactions_count": 0,
    "labels": "stale",
    "milestone": null,
    "closed_by": "github-actions[bot]",
    "resolution_time_hours": 1535.1669444444444,
    "first_comments": [
      {
        "author": "Petriorlz",
        "created_at": "2024-11-05T05:45:21+00:00",
        "body": "Is there one now?"
      },
      {
        "author": "Petriorlz",
        "created_at": "2024-11-05T05:46:01+00:00",
        "body": "I still haven't found it"
      },
      {
        "author": "github-actions[bot]",
        "created_at": "2024-12-06T02:04:51+00:00",
        "body": "This issue is stale because it has been open for 30 days with no activity. Remove stale label or comment or this will be closed in 7 days."
      },
      {
        "author": "github-actions[bot]",
        "created_at": "2024-12-13T02:06:08+00:00",
        "body": "This issue was closed because it has been inactive for 7 days since being marked as stale."
      }
    ],
    "url": "https://github.com/NVIDIA/NeMo/issues/10830"
  },
  {
    "repository": "NVIDIA/NeMo",
    "issue_number": 10825,
    "title": "Conversion script of phi3 from HF to Nemo",
    "body": "**Is your feature request related to a problem? Please describe.**\nContionual pretraining of Phi-3-mini-128k-instruct using nemo.\n\nHi, we are looking to run continual pretraining based on Phi-3-mini-128k-instruct in Nemo. Currently, there's no conversion script available under convert_mistral_7b_hf_to_nemo.py. Could you please add the support?\n\n\nA clear and concise description of what you want to happen.\nPhi-3-mini-128k-instruct conversion script from HF to Nemo.\n\n**Describe alternatives you've considered**\nI'm implementing a custom script based on https://github.com/NVIDIA/NeMo/blob/main/scripts/checkpoint_converters/convert_llama_hf_to_nemo_save_dict.py with some additional processing on the qkv_proj tensor from Phi3.\n\nAdd any other context or screenshots about the feature request here.\n",
    "state": "closed",
    "created_at": "2024-10-09T20:48:14+00:00",
    "closed_at": "2024-11-16T02:00:52+00:00",
    "updated_at": "2024-12-25T06:20:56+00:00",
    "author": "zhouxie000",
    "author_type": "User",
    "comments_count": 3,
    "reactions_count": 0,
    "labels": "stale",
    "milestone": null,
    "closed_by": "github-actions[bot]",
    "resolution_time_hours": 893.2105555555555,
    "first_comments": [
      {
        "author": "github-actions[bot]",
        "created_at": "2024-11-09T01:55:43+00:00",
        "body": "This issue is stale because it has been open for 30 days with no activity. Remove stale label or comment or this will be closed in 7 days."
      },
      {
        "author": "github-actions[bot]",
        "created_at": "2024-11-16T02:00:52+00:00",
        "body": "This issue was closed because it has been inactive for 7 days since being marked as stale."
      },
      {
        "author": "LopezGG",
        "created_at": "2024-12-25T06:20:54+00:00",
        "body": "+1 to this. I am looking for a conversion script or pointers on how to get started with this"
      }
    ],
    "url": "https://github.com/NVIDIA/NeMo/issues/10825"
  },
  {
    "repository": "NVIDIA/NeMo",
    "issue_number": 10805,
    "title": "[NeVa Pretraining] Vision Encoder Created on All GPUs During Pipeline Parallelism",
    "body": "Dear all,\n\nI'm encountering an issue with the vision encoder when using pipeline parallelism. \nThe vision encoder appears to be instantiated on all GPUs rather than allocated to GPU 0, which is the first stage of the pipeline.\n\nI tracked the vision encode in `NeMo/nemo/collections/multimodal/models/multimodal_llm/neva/neva_model.py` like this.\n\n```\ndef create_vision_encoder_and_processor(self, mm_cfg):\n    # Initialize vision encoder and freeze it\n    if mm_cfg.vision_encoder.get(\"from_hf\", False):\n        from transformers import AutoConfig\n\n        config = AutoConfig.from_pretrained(mm_cfg.vision_encoder.from_pretrained)\n        if config.architectures[0] == \"CLIPVisionModel\" or config.architectures[0] == \"CLIPModel\":\n            vision_encoder = CLIPVisionModel.from_pretrained(\n                mm_cfg.vision_encoder.from_pretrained,\n                torch_dtype=torch.bfloat16,\n            ).cuda()\n            vision_encoder = vision_encoder.to(torch.bfloat16)\n            print(f\"vision_encoder is on device: {next(vision_encoder.parameters()).device}\")\n```\nThe log shows that the vision encoder is on every device.\nSince I used pipeline parallelism, I think the vision encoder must reside in the first stage of the pipeline.\nIf I misunderstood or my tracking is wrong, let me know.\n\nThank you.\n\n### Experimental Setup\n\nI used the `nvcr.io/nvidia/nemo:24.07` docker image on testing.\n\nThis is the following script to run with NeMo.\n```\n! torchrun --nproc_per_node=4 \\\n    /opt/NeMo/examples/multimodal/multimodal_llm/neva/neva_pretrain.py \\\n    trainer.precision=bf16 \\\n    trainer.num_nodes=1 \\\n    trainer.devices=4 \\\n    trainer.val_check_interval=50 \\\n    trainer.limit_val_batches=5 \\\n    trainer.log_every_n_steps=1 \\\n    trainer.max_steps=1 \\\n    model.megatron_amp_O2=True \\\n    model.micro_batch_size=1 \\\n    model.global_batch_size=16 \\\n    model.tensor_model_parallel_size=1 \\\n    model.pipeline_model_parallel_size=4 \\\n    model.mcore_gpt=True \\\n    model.transformer_engine=True \\\n    model.data.data_path=/data/neva/datasets/LLaVA-Pretrain-LCS-558k/blip_laion_cc_sbu_558k.json \\\n    model.data.image_folder=/data/neva/datasets/LLaVA-Pretrain-LCS-558k/images \\\n    model.tokenizer.library=sentencepiece \\\n    model.tokenizer.model=/data/neva/tokenizers/tokenizer_neva.model \\\n    model.encoder_seq_length=4096 \\\n    model.num_layers=32 \\\n    model.hidden_size=4096 \\\n    model.ffn_hidden_size=11008 \\\n    model.num_attention_heads=32 \\\n    model.normalization=rmsnorm \\\n    model.do_layer_norm_weight_decay=False \\\n    model.apply_query_key_layer_scaling=True \\\n    model.bias=False \\\n    model.activation=fast-swiglu \\\n    model.headscale=False \\\n    model.position_embedding_type=rope \\\n    model.rotary_percentage=1.0 \\\n    model.num_query_groups=null \\\n    model.data.num_workers=0 \\\n    model.mm_cfg.llm.from_pretrained=/data/neva/checkpoints/checkpoints/llama-2-7b-chat.nemo \\\n    model.mm_cfg.llm.model_type=v1 \\\n    model.data.conv_template=v1 \\\n    model.mm_cfg.vision_encoder.from_pretrained='openai/clip-vit-large-patch14' \\\n    model.mm_cfg.vision_encoder.from_hf=True \\\n    model.optim.name=\"fused_adam\" \\\n    exp_manager.create_checkpoint_callback=True \\\n    exp_manager.checkpoint_callback_params.save_nemo_on_train_end=False \\\n    exp_manager.create_wandb_logger=False\n```",
    "state": "closed",
    "created_at": "2024-10-08T17:23:55+00:00",
    "closed_at": "2024-11-15T02:02:35+00:00",
    "updated_at": "2024-11-15T02:02:36+00:00",
    "author": "Esthesia",
    "author_type": "User",
    "comments_count": 2,
    "reactions_count": 0,
    "labels": "stale",
    "milestone": null,
    "closed_by": "github-actions[bot]",
    "resolution_time_hours": 896.6444444444444,
    "first_comments": [
      {
        "author": "github-actions[bot]",
        "created_at": "2024-11-08T01:57:48+00:00",
        "body": "This issue is stale because it has been open for 30 days with no activity. Remove stale label or comment or this will be closed in 7 days."
      },
      {
        "author": "github-actions[bot]",
        "created_at": "2024-11-15T02:02:35+00:00",
        "body": "This issue was closed because it has been inactive for 7 days since being marked as stale."
      }
    ],
    "url": "https://github.com/NVIDIA/NeMo/issues/10805"
  },
  {
    "repository": "NVIDIA/NeMo",
    "issue_number": 10798,
    "title": "When training ASR models, it saves .nemo 2 times in a row",
    "body": "**Describe the bug**\n\nWhen training ASR models, it calls \"on_save_checkpoint\" 2 times in a row. It is unnecessary and can take some time. It is visible when you use \"always_save_nemo\": True and \"save_last\": True because you will see the logs below:\n\n```log\n[NeMo I 2024-10-08 14:35:07 nemo_model_checkpoint:247] New .nemo model saved to:nemo_experiments/TEST/checkpoints/TEST.nemo\n[NeMo I 2024-10-08 14:35:18 nemo_model_checkpoint:321] nemo_experiments/TEST/checkpoints/TEST.nemo already exists, moving existing checkpoint to nemo_experiments/TEST/checkpoints/TEST-v1.nemo\n[NeMo I 2024-10-08 14:35:37 nemo_model_checkpoint:247] New .nemo model saved to: nemo_experiments/TEST/checkpoints/TEST.nemo\n[NeMo I 2024-10-08 14:35:37 nemo_model_checkpoint:249] Removing old .nemo backup nemo_experiments/TEST/checkpoints/TEST-v1.nemo\n```\n\n**Steps/Code to reproduce bug**\n\nI think you can use [this](https://github.com/NVIDIA/NeMo/blob/ff97c70fc9d7a0050c947b9008372bcf2e6f3b48/examples/asr/asr_ctc/speech_to_text_ctc.py) to try since it is what I'm based on.\nI use the following options in the exp_manager:\n\n```yaml\nexp_manager:\n  exp_dir: null\n  name: ${name}\n  create_tensorboard_logger: true\n  create_checkpoint_callback: true\n  create_early_stopping_callback: False\n  early_stopping_callback_params:\n    check_finite: False\n    patience: 5\n    mode: \"min\"\n    monitor: \"val_loss\"\n  checkpoint_callback_params:\n    every_n_epochs: null\n    every_n_train_steps: 8\n    monitor: train_loss\n    save_top_k: -1\n    always_save_nemo: true\n    save_last: true\n  resume_if_exists: true\n  resume_ignore_no_checkpoint: true\n```\n\n**Expected behavior**\n\nIt should not save the .nemo 2 times in a row. Or it should name with \"-last.nemo\" too.\n\n**Environment details**\n\nI locally installed NeMo with pip and I have a Nvidia GPU\n",
    "state": "closed",
    "created_at": "2024-10-08T12:55:57+00:00",
    "closed_at": "2024-12-05T02:05:41+00:00",
    "updated_at": "2024-12-05T02:05:42+00:00",
    "author": "AudranBert",
    "author_type": "User",
    "comments_count": 3,
    "reactions_count": 0,
    "labels": "bug,stale",
    "milestone": null,
    "closed_by": "github-actions[bot]",
    "resolution_time_hours": 1381.1622222222222,
    "first_comments": [
      {
        "author": "MedAymenF",
        "created_at": "2024-10-28T21:58:30+00:00",
        "body": "I remember having exactly the same problem while training a CTC model."
      },
      {
        "author": "github-actions[bot]",
        "created_at": "2024-11-28T02:03:59+00:00",
        "body": "This issue is stale because it has been open for 30 days with no activity. Remove stale label or comment or this will be closed in 7 days."
      },
      {
        "author": "github-actions[bot]",
        "created_at": "2024-12-05T02:05:41+00:00",
        "body": "This issue was closed because it has been inactive for 7 days since being marked as stale."
      }
    ],
    "url": "https://github.com/NVIDIA/NeMo/issues/10798"
  },
  {
    "repository": "NVIDIA/NeMo",
    "issue_number": 10797,
    "title": "Resuming from a checkpoint that ended before the epoch ended and your dataloader is not resumable",
    "body": "**Describe the bug**\n\nI don't know if it is a real issue but I have the following warning while resuming a training where the checkpoint is not saved at the end of the epoch. I don't know what it means by \"unreliable results\" since the training seems to start at the batch it was when the checkpoint was made.\n\n```\n[NeMo W 2024-10-08 11:49:10 nemo_logging:349] /home/.../.local/lib/python3.10/site-packages/pytorch_lightning/loops/training_epoch_loop.py:161: \n\nYou're resuming from a checkpoint that ended before the epoch ended and your dataloader is not resumable. This can cause unreliable results if further training is done. Consider using an end-of-epoch checkpoint or make your dataloader resumable by implementing the `state_dict` / `load_state_dict` interface.\n```\n\n**Steps/Code to reproduce bug**\n\nI think you can use [this](https://github.com/NVIDIA/NeMo/blob/ff97c70fc9d7a0050c947b9008372bcf2e6f3b48/examples/asr/asr_ctc/speech_to_text_ctc.py) to try since it is what I'm based on.\nI used a tarred dataset (it does it with a non tarred dataset too) with the following options in the exp_manager:\n```yaml\nexp_manager:\n  exp_dir: null\n  name: ${name}\n  create_tensorboard_logger: true\n  create_checkpoint_callback: true\n  create_early_stopping_callback: False\n  early_stopping_callback_params:\n    check_finite: False\n    patience: 5\n    mode: \"min\"\n    monitor: \"val_loss\"\n  checkpoint_callback_params:\n    every_n_epochs: null\n    every_n_train_steps: 8\n    monitor: train_loss\n    save_top_k: -1\n    always_save_nemo: false\n    save_last: true\n    filename: testing_filename-{epoch:02d}-{step:06d}-{train_loss:.4f}\n  resume_if_exists: true\n  resume_ignore_no_checkpoint: true\n```\n\n\n**Expected behavior**\n\nShould not throw a warning\n\n**Environment details**\n\nI locally installed NeMo with pip and I have a Nvidia GPU\n",
    "state": "closed",
    "created_at": "2024-10-08T10:09:48+00:00",
    "closed_at": "2024-11-17T02:05:16+00:00",
    "updated_at": "2024-11-17T02:05:17+00:00",
    "author": "AudranBert",
    "author_type": "User",
    "comments_count": 2,
    "reactions_count": 0,
    "labels": "bug,stale",
    "milestone": null,
    "closed_by": "github-actions[bot]",
    "resolution_time_hours": 951.9244444444445,
    "first_comments": [
      {
        "author": "github-actions[bot]",
        "created_at": "2024-11-10T02:00:43+00:00",
        "body": "This issue is stale because it has been open for 30 days with no activity. Remove stale label or comment or this will be closed in 7 days."
      },
      {
        "author": "github-actions[bot]",
        "created_at": "2024-11-17T02:05:16+00:00",
        "body": "This issue was closed because it has been inactive for 7 days since being marked as stale."
      }
    ],
    "url": "https://github.com/NVIDIA/NeMo/issues/10797"
  },
  {
    "repository": "NVIDIA/NeMo",
    "issue_number": 10782,
    "title": "Unable to merge lora weights: \"world_size (1) is not divisible by 4\"",
    "body": "**Describe the bug**\n\nWhen running `merge_lora_weights/merge.py` with TP and PP set to 1 on a fine-tuned minitron checkpoint, I run into the following error:\n\n```sh\nraise RuntimeError(f\"world_size ({world_size}) is not divisible by {total_model_size}\")\n[rank0]: RuntimeError: world_size (1) is not divisible by 4\n```\nThe world size should be 1 because the node I'm using only has a single A100 GPU, however, it is unclear why it's trying to split by 4.\n\nLink to `parallel_state.py` where the error is raised: https://github.com/NVIDIA/Megatron-LM/blob/73e7b58e79df9da521ff31d74053579b7a060c7e/megatron/core/parallel_state.py#L531 \n\n**Full Traceback**\n\n```python-traceback\n(base) [ema8@node0414 syn_data_PEFT_exp]$ sh merge_lora_weights.sh \n15:4: not a valid test operator:  \n15:4: not a valid test operator: 12.5\n21:4: not a valid test operator: (\n21:4: not a valid test operator: 550.54.15\nrm: cannot remove '/usr/local/cuda/compat/lib': Read-only file system\n[NeMo W 2024-10-07 10:34:46 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:289: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n      def forward(ctx, input, weight, bias, allreduce_dgrad):\n    \n[NeMo W 2024-10-07 10:34:46 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:300: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n      def backward(ctx, grad_output):\n    \n[NeMo W 2024-10-07 10:34:46 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:392: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n      def forward(\n    \n[NeMo W 2024-10-07 10:34:46 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:432: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n      def backward(ctx, grad_output):\n    \n[NeMo W 2024-10-07 10:34:47 nemo_logging:349] /opt/megatron-lm/megatron/core/dist_checkpointing/strategies/torch.py:17: DeprecationWarning: `torch.distributed._sharded_tensor` will be deprecated, use `torch.distributed._shard.sharded_tensor` instead\n      from torch.distributed._sharded_tensor import ShardedTensor as TorchShardedTensor\n    \n[NeMo W 2024-10-07 10:34:48 nemo_logging:349] /opt/megatron-lm/megatron/core/transformer/attention.py:29: DeprecationWarning: The 'megatron.core.transformer.custom_layers.transformer_engine' \n        module is deprecated and will be removed in 0.10.0. Please use \n        'megatron.core.extensions.transformer_engine' instead.\n      from megatron.core.transformer.custom_layers.transformer_engine import SplitAlongDim\n    \n[NeMo W 2024-10-07 10:34:49 nemo_logging:349] /opt/NeMo/nemo/collections/nlp/models/language_modeling/megatron_base_prompt_learning_model.py:181: DeprecationWarning: invalid escape sequence '\\{'\n      \"prompt_template_fields\": re.findall(\"\\{(.*?)\\}\", task.prompt_template),\n    \n[NeMo W 2024-10-07 10:34:49 nemo_logging:349] /opt/NeMo/nemo/collections/nlp/models/language_modeling/megatron_base_model.py:389: DeprecationWarning: invalid escape sequence '\\.'\n      return re.fullmatch(\"[0-9][0-9]\\.[0-9][0-9].*\", nvidia_torch_version)  # \"YY.MM.*\"\n    \n[NeMo W 2024-10-07 10:34:49 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/modelopt/torch/quantization/tensor_quant.py:168: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n      quantize_op_abstract = torch.library.impl_abstract(\"tensorrt::quantize_op\")(\n    \n[NeMo W 2024-10-07 10:34:50 nemo_logging:349] /opt/NeMo/nemo/collections/nlp/modules/common/megatron/vocab_parallel_cross_entropy.py:88: DeprecationWarning: invalid escape sequence '\\s'\n      \"\"\"\n    \n[NeMo W 2024-10-07 10:34:51 nemo_logging:349] /opt/NeMo/nemo/collections/asr/parts/utils/wfst_utils.py:1328: DeprecationWarning: invalid escape sequence '\\d'\n      width, height = re.findall('\\d+', line)\n    \n[NeMo W 2024-10-07 10:34:51 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pyannote/core/notebook.py:134: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap()`` or ``pyplot.get_cmap()`` instead.\n      cm = get_cmap(\"Set1\")\n    \n[NeMo W 2024-10-07 10:34:52 nemo_logging:349] /opt/NeMo/nemo/collections/asr/modules/rnnt.py:1550: DeprecationWarning: invalid escape sequence '\\*'\n      \"\"\"\n    \n[NeMo W 2024-10-07 10:34:52 nemo_logging:349] /opt/NeMo/nemo/collections/common/data/lhotse/nemo_adapters.py:198: DeprecationWarning: invalid escape sequence '\\d'\n      \"\"\"\n    \n[NeMo W 2024-10-07 10:34:52 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/nvidia/dali/_autograph/pyct/gast_util.py:79: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n      if get_gast_version() < LooseVersion(\"0.5\"):\n    \n[NeMo W 2024-10-07 10:34:52 nemo_logging:349] /home/ema8/.local/lib/python3.10/site-packages/setuptools/_distutils/version.py:337: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n      other = LooseVersion(other)\n    \n[NeMo W 2024-10-07 10:34:53 nemo_logging:349] /opt/NeMo/nemo/collections/asr/parts/utils/vad_utils.py:1082: DeprecationWarning: invalid escape sequence '\\s'\n      data = pd.read_csv(path2ground_truth_label, sep=\"\\s+\", delimiter=None, header=None)\n    \n[NeMo W 2024-10-07 10:34:53 nemo_logging:349] /opt/NeMo/nemo/collections/asr/parts/utils/asr_batching.py:39: DeprecationWarning: invalid escape sequence '\\m'\n      \"\"\"\n    \n[NeMo W 2024-10-07 10:34:53 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n    See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n      ret = run_job(\n    \n[NeMo W 2024-10-07 10:34:53 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/lightning_fabric/connector.py:571: `precision=16` is supported for historical reasons but its usage is discouraged. Please set your precision to 16-mixed instead!\n    \n[node0414.palmetto.clemson.edu:3567856] PMIX ERROR: ERROR in file gds_ds12_lock_pthread.c at line 168\nUsing 16bit Automatic Mixed Precision (AMP)\n[NeMo W 2024-10-07 10:34:54 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/plugins/precision/amp.py:53: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n    \nGPU available: True (cuda), used: True\nTPU available: False, using: 0 TPU cores\nHPU available: False, using: 0 HPUs\n[NeMo W 2024-10-07 10:35:28 megatron_base_model:1189] The model: MegatronGPTModel() does not have field.name: context_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n[NeMo W 2024-10-07 10:35:28 megatron_base_model:1189] The model: MegatronGPTModel() does not have field.name: expert_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n[NeMo W 2024-10-07 10:35:28 megatron_base_model:1189] The model: MegatronGPTModel() does not have field.name: moe_extended_tp in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n[NeMo W 2024-10-07 10:35:28 megatron_base_model:1189] The model: MegatronGPTModel() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n[NeMo W 2024-10-07 10:35:28 megatron_base_model:1189] The model: MegatronGPTModel() does not have field.name: deterministic_mode in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n[NeMo W 2024-10-07 10:35:28 megatron_base_model:1189] The model: MegatronGPTModel() does not have field.name: use_te_rng_tracker in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n[NeMo W 2024-10-07 10:35:28 megatron_base_model:1189] The model: MegatronGPTModel() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n[NeMo W 2024-10-07 10:35:28 megatron_base_model:1189] The model: MegatronGPTModel() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n[NeMo W 2024-10-07 10:35:28 megatron_base_model:1189] The model: MegatronGPTModel() does not have field.name: tp_comm_overlap_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n[NeMo W 2024-10-07 10:35:28 megatron_base_model:1189] The model: MegatronGPTModel() does not have field.name: tp_comm_overlap_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n[NeMo W 2024-10-07 10:35:28 megatron_base_model:1189] The model: MegatronGPTModel() does not have field.name: tp_comm_overlap_rs_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n[NeMo W 2024-10-07 10:35:28 megatron_base_model:1189] The model: MegatronGPTModel() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n[NeMo W 2024-10-07 10:35:28 megatron_base_model:1189] The model: MegatronGPTModel() does not have field.name: tp_comm_atomic_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n[NeMo W 2024-10-07 10:35:28 megatron_base_model:1189] The model: MegatronGPTModel() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n[NeMo W 2024-10-07 10:35:28 megatron_base_model:1189] The model: MegatronGPTModel() does not have field.name: tp_comm_atomic_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n[NeMo W 2024-10-07 10:35:28 megatron_base_model:1189] The model: MegatronGPTModel() does not have field.name: cross_entropy_loss_fusion in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n[NeMo W 2024-10-07 10:35:28 megatron_base_model:1189] The model: MegatronGPTModel() does not have field.name: tp_comm_overlap_disable_qkv in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n[NeMo W 2024-10-07 10:35:28 megatron_base_model:1189] The model: MegatronGPTModel() does not have field.name: tp_comm_overlap_disable_fc1 in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n[NeMo W 2024-10-07 10:35:28 megatron_base_model:1189] The model: MegatronGPTModel() does not have field.name: overlap_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n[NeMo W 2024-10-07 10:35:28 megatron_base_model:1189] The model: MegatronGPTModel() does not have field.name: batch_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n[NeMo W 2024-10-07 10:35:28 megatron_base_model:1189] The model: MegatronGPTModel() does not have field.name: defer_embedding_wgrad_compute in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n[NeMo W 2024-10-07 10:35:28 megatron_base_model:1189] The model: MegatronGPTModel() does not have field.name: wgrad_deferral_limit in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n[NeMo W 2024-10-07 10:35:28 megatron_base_model:1189] The model: MegatronGPTModel() does not have field.name: pipeline_model_parallel_split_rank in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n[NeMo W 2024-10-07 10:35:28 megatron_base_model:1189] The model: MegatronGPTModel() does not have field.name: cpu_offloading in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n[NeMo W 2024-10-07 10:35:28 megatron_base_model:1189] The model: MegatronGPTModel() does not have field.name: cpu_offloading_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n[NeMo W 2024-10-07 10:35:28 megatron_base_model:1189] The model: MegatronGPTModel() does not have field.name: _cpu_offloading_context in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n[NeMo W 2024-10-07 10:35:28 megatron_base_model:1189] The model: MegatronGPTModel() does not have field.name: cpu_offloading_activations in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n[NeMo W 2024-10-07 10:35:28 megatron_base_model:1189] The model: MegatronGPTModel() does not have field.name: cpu_offloading_weights in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n[NeMo W 2024-10-07 10:35:28 megatron_base_model:1189] The model: MegatronGPTModel() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n[NeMo I 2024-10-07 10:35:28 megatron_init:314] Rank 0 has data parallel group : [0]\n[NeMo I 2024-10-07 10:35:28 megatron_init:320] Rank 0 has combined group of data parallel and context parallel : [0]\n[NeMo I 2024-10-07 10:35:28 megatron_init:325] All data parallel group ranks with context parallel combined: [[0]]\n[NeMo I 2024-10-07 10:35:28 megatron_init:328] Ranks 0 has data parallel rank: 0\n[NeMo I 2024-10-07 10:35:28 megatron_init:336] Rank 0 has context parallel group: [0]\n[NeMo I 2024-10-07 10:35:28 megatron_init:339] All context parallel group ranks: [[0]]\n[NeMo I 2024-10-07 10:35:28 megatron_init:340] Ranks 0 has context parallel rank: 0\n[NeMo I 2024-10-07 10:35:28 megatron_init:347] Rank 0 has model parallel group: [0]\n[NeMo I 2024-10-07 10:35:28 megatron_init:348] All model parallel group ranks: [[0]]\n[NeMo I 2024-10-07 10:35:28 megatron_init:357] Rank 0 has tensor model parallel group: [0]\n[NeMo I 2024-10-07 10:35:28 megatron_init:361] All tensor model parallel group ranks: [[0]]\n[NeMo I 2024-10-07 10:35:28 megatron_init:362] Rank 0 has tensor model parallel rank: 0\n[NeMo I 2024-10-07 10:35:28 megatron_init:382] Rank 0 has pipeline model parallel group: [0]\n[NeMo I 2024-10-07 10:35:28 megatron_init:394] Rank 0 has embedding group: [0]\n[NeMo I 2024-10-07 10:35:28 megatron_init:400] All pipeline model parallel group ranks: [[0]]\n[NeMo I 2024-10-07 10:35:28 megatron_init:401] Rank 0 has pipeline model parallel rank 0\n[NeMo I 2024-10-07 10:35:28 megatron_init:402] All embedding group ranks: [[0]]\n[NeMo I 2024-10-07 10:35:28 megatron_init:403] Rank 0 has embedding rank: 0\nsetting number of microbatches to constant 288\n[NeMo W 2024-10-07 10:35:28 megatron_base_model:1189] The model: MegatronGPTModel() does not have field.name: context_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n[NeMo W 2024-10-07 10:35:28 megatron_base_model:1189] The model: MegatronGPTModel() does not have field.name: expert_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n[NeMo W 2024-10-07 10:35:28 megatron_base_model:1189] The model: MegatronGPTModel() does not have field.name: moe_extended_tp in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n[NeMo W 2024-10-07 10:35:28 megatron_base_model:1189] The model: MegatronGPTModel() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n[NeMo W 2024-10-07 10:35:28 megatron_base_model:1189] The model: MegatronGPTModel() does not have field.name: deterministic_mode in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n[NeMo W 2024-10-07 10:35:28 megatron_base_model:1189] The model: MegatronGPTModel() does not have field.name: use_te_rng_tracker in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n[NeMo W 2024-10-07 10:35:28 megatron_base_model:1189] The model: MegatronGPTModel() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n[NeMo W 2024-10-07 10:35:28 megatron_base_model:1189] The model: MegatronGPTModel() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n[NeMo W 2024-10-07 10:35:28 megatron_base_model:1189] The model: MegatronGPTModel() does not have field.name: tp_comm_overlap_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n[NeMo W 2024-10-07 10:35:28 megatron_base_model:1189] The model: MegatronGPTModel() does not have field.name: tp_comm_overlap_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n[NeMo W 2024-10-07 10:35:28 megatron_base_model:1189] The model: MegatronGPTModel() does not have field.name: tp_comm_overlap_rs_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n[NeMo W 2024-10-07 10:35:28 megatron_base_model:1189] The model: MegatronGPTModel() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n[NeMo W 2024-10-07 10:35:28 megatron_base_model:1189] The model: MegatronGPTModel() does not have field.name: tp_comm_atomic_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n[NeMo W 2024-10-07 10:35:28 megatron_base_model:1189] The model: MegatronGPTModel() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n[NeMo W 2024-10-07 10:35:28 megatron_base_model:1189] The model: MegatronGPTModel() does not have field.name: tp_comm_atomic_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n[NeMo W 2024-10-07 10:35:28 megatron_base_model:1189] The model: MegatronGPTModel() does not have field.name: cross_entropy_loss_fusion in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n[NeMo W 2024-10-07 10:35:28 megatron_base_model:1189] The model: MegatronGPTModel() does not have field.name: tp_comm_overlap_disable_qkv in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n[NeMo W 2024-10-07 10:35:28 megatron_base_model:1189] The model: MegatronGPTModel() does not have field.name: tp_comm_overlap_disable_fc1 in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n[NeMo W 2024-10-07 10:35:28 megatron_base_model:1189] The model: MegatronGPTModel() does not have field.name: overlap_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n[NeMo W 2024-10-07 10:35:28 megatron_base_model:1189] The model: MegatronGPTModel() does not have field.name: batch_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n[NeMo W 2024-10-07 10:35:28 megatron_base_model:1189] The model: MegatronGPTModel() does not have field.name: defer_embedding_wgrad_compute in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n[NeMo W 2024-10-07 10:35:28 megatron_base_model:1189] The model: MegatronGPTModel() does not have field.name: wgrad_deferral_limit in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n[NeMo W 2024-10-07 10:35:28 megatron_base_model:1189] The model: MegatronGPTModel() does not have field.name: pipeline_model_parallel_split_rank in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n[NeMo W 2024-10-07 10:35:28 megatron_base_model:1189] The model: MegatronGPTModel() does not have field.name: cpu_offloading in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n[NeMo W 2024-10-07 10:35:28 megatron_base_model:1189] The model: MegatronGPTModel() does not have field.name: cpu_offloading_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n[NeMo W 2024-10-07 10:35:28 megatron_base_model:1189] The model: MegatronGPTModel() does not have field.name: _cpu_offloading_context in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n[NeMo W 2024-10-07 10:35:28 megatron_base_model:1189] The model: MegatronGPTModel() does not have field.name: cpu_offloading_activations in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n[NeMo W 2024-10-07 10:35:28 megatron_base_model:1189] The model: MegatronGPTModel() does not have field.name: cpu_offloading_weights in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n[NeMo W 2024-10-07 10:35:28 megatron_base_model:1189] The model: MegatronGPTModel() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n[NeMo I 2024-10-07 10:35:28 tokenizer_utils:196] Getting SentencePiece with model: /local_scratch/slurm.777326/tmpkg5ll76b/b1bc02bf987043f3884c39152f183238_nemotron_2_256k.model\n[NeMo I 2024-10-07 10:35:28 megatron_base_model:604] Padded vocab_size: 256000, original vocab_size: 256000, dummy tokens: 0.\n[NeMo W 2024-10-07 10:35:28 megatron_base_model:1189] The model: MegatronGPTModel() does not have field.name: context_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n[NeMo W 2024-10-07 10:35:28 megatron_base_model:1189] The model: MegatronGPTModel() does not have field.name: expert_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n[NeMo W 2024-10-07 10:35:28 megatron_base_model:1189] The model: MegatronGPTModel() does not have field.name: moe_extended_tp in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n[NeMo W 2024-10-07 10:35:28 megatron_base_model:1189] The model: MegatronGPTModel() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n[NeMo W 2024-10-07 10:35:28 megatron_base_model:1189] The model: MegatronGPTModel() does not have field.name: deterministic_mode in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n[NeMo W 2024-10-07 10:35:28 megatron_base_model:1189] The model: MegatronGPTModel() does not have field.name: use_te_rng_tracker in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n[NeMo W 2024-10-07 10:35:28 megatron_base_model:1189] The model: MegatronGPTModel() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n[NeMo W 2024-10-07 10:35:28 megatron_base_model:1189] The model: MegatronGPTModel() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n[NeMo W 2024-10-07 10:35:28 megatron_base_model:1189] The model: MegatronGPTModel() does not have field.name: tp_comm_overlap_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n[NeMo W 2024-10-07 10:35:28 megatron_base_model:1189] The model: MegatronGPTModel() does not have field.name: tp_comm_overlap_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n[NeMo W 2024-10-07 10:35:28 megatron_base_model:1189] The model: MegatronGPTModel() does not have field.name: tp_comm_overlap_rs_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n[NeMo W 2024-10-07 10:35:28 megatron_base_model:1189] The model: MegatronGPTModel() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n[NeMo W 2024-10-07 10:35:28 megatron_base_model:1189] The model: MegatronGPTModel() does not have field.name: tp_comm_atomic_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n[NeMo W 2024-10-07 10:35:28 megatron_base_model:1189] The model: MegatronGPTModel() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n[NeMo W 2024-10-07 10:35:28 megatron_base_model:1189] The model: MegatronGPTModel() does not have field.name: tp_comm_atomic_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n[NeMo W 2024-10-07 10:35:28 megatron_base_model:1189] The model: MegatronGPTModel() does not have field.name: cross_entropy_loss_fusion in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n[NeMo W 2024-10-07 10:35:28 megatron_base_model:1189] The model: MegatronGPTModel() does not have field.name: tp_comm_overlap_disable_qkv in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n[NeMo W 2024-10-07 10:35:28 megatron_base_model:1189] The model: MegatronGPTModel() does not have field.name: tp_comm_overlap_disable_fc1 in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n[NeMo W 2024-10-07 10:35:28 megatron_base_model:1189] The model: MegatronGPTModel() does not have field.name: overlap_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n[NeMo W 2024-10-07 10:35:28 megatron_base_model:1189] The model: MegatronGPTModel() does not have field.name: batch_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n[NeMo W 2024-10-07 10:35:28 megatron_base_model:1189] The model: MegatronGPTModel() does not have field.name: defer_embedding_wgrad_compute in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n[NeMo W 2024-10-07 10:35:28 megatron_base_model:1189] The model: MegatronGPTModel() does not have field.name: wgrad_deferral_limit in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n[NeMo W 2024-10-07 10:35:28 megatron_base_model:1189] The model: MegatronGPTModel() does not have field.name: pipeline_model_parallel_split_rank in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n[NeMo W 2024-10-07 10:35:28 megatron_base_model:1189] The model: MegatronGPTModel() does not have field.name: cpu_offloading in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n[NeMo W 2024-10-07 10:35:28 megatron_base_model:1189] The model: MegatronGPTModel() does not have field.name: cpu_offloading_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n[NeMo W 2024-10-07 10:35:28 megatron_base_model:1189] The model: MegatronGPTModel() does not have field.name: _cpu_offloading_context in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n[NeMo W 2024-10-07 10:35:28 megatron_base_model:1189] The model: MegatronGPTModel() does not have field.name: cpu_offloading_activations in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n[NeMo W 2024-10-07 10:35:28 megatron_base_model:1189] The model: MegatronGPTModel() does not have field.name: cpu_offloading_weights in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n[NeMo W 2024-10-07 10:35:28 megatron_base_model:1189] The model: MegatronGPTModel() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n[NeMo W 2024-10-07 10:35:28 megatron_base_model:577] The model: MegatronGPTModel() does not have field.name: first_pipeline_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n[NeMo W 2024-10-07 10:35:28 megatron_base_model:577] The model: MegatronGPTModel() does not have field.name: last_pipeline_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n[NeMo W 2024-10-07 10:35:28 megatron_base_model:577] The model: MegatronGPTModel() does not have field.name: activation_func_fp8_input_store in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n[NeMo W 2024-10-07 10:35:28 megatron_base_model:577] The model: MegatronGPTModel() does not have field.name: num_moe_experts in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n[NeMo W 2024-10-07 10:35:28 megatron_base_model:577] The model: MegatronGPTModel() does not have field.name: window_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n[NeMo W 2024-10-07 10:35:28 megatron_base_model:577] The model: MegatronGPTModel() does not have field.name: qk_layernorm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n[NeMo W 2024-10-07 10:35:28 megatron_base_model:577] The model: MegatronGPTModel() does not have field.name: test_mode in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n[NeMo W 2024-10-07 10:35:28 megatron_base_model:577] The model: MegatronGPTModel() does not have field.name: calculate_per_token_loss in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n[NeMo W 2024-10-07 10:35:28 megatron_base_model:577] The model: MegatronGPTModel() does not have field.name: multi_latent_attention in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n[NeMo W 2024-10-07 10:35:28 megatron_base_model:577] The model: MegatronGPTModel() does not have field.name: memory_efficient_layer_norm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n[NeMo W 2024-10-07 10:35:28 megatron_base_model:577] The model: MegatronGPTModel() does not have field.name: fp8_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n[NeMo W 2024-10-07 10:35:28 megatron_base_model:577] The model: MegatronGPTModel() does not have field.name: fp8_dot_product_attention in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n[NeMo W 2024-10-07 10:35:28 megatron_base_model:577] The model: MegatronGPTModel() does not have field.name: fp8_multi_head_attention in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n[NeMo W 2024-10-07 10:35:28 megatron_base_model:577] The model: MegatronGPTModel() does not have field.name: moe_shared_expert_intermediate_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n[NeMo W 2024-10-07 10:35:28 megatron_base_model:577] The model: MegatronGPTModel() does not have field.name: moe_shared_expert_overlap in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n[NeMo W 2024-10-07 10:35:28 megatron_base_model:577] The model: MegatronGPTModel() does not have field.name: moe_router_load_balancing_type in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n[NeMo W 2024-10-07 10:35:28 megatron_base_model:577] The model: MegatronGPTModel() does not have field.name: moe_router_topk in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n[NeMo W 2024-10-07 10:35:28 megatron_base_model:577] The model: MegatronGPTModel() does not have field.name: moe_router_pre_softmax in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n[NeMo W 2024-10-07 10:35:28 megatron_base_model:577] The model: MegatronGPTModel() does not have field.name: moe_grouped_gemm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n[NeMo W 2024-10-07 10:35:28 megatron_base_model:577] The model: MegatronGPTModel() does not have field.name: moe_aux_loss_coeff in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n[NeMo W 2024-10-07 10:35:28 megatron_base_model:577] The model: MegatronGPTModel() does not have field.name: moe_z_loss_coeff in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n[NeMo W 2024-10-07 10:35:28 megatron_base_model:577] The model: MegatronGPTModel() does not have field.name: moe_input_jitter_eps in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n[NeMo W 2024-10-07 10:35:28 megatron_base_model:577] The model: MegatronGPTModel() does not have field.name: moe_token_dropping in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n[NeMo W 2024-10-07 10:35:28 megatron_base_model:577] The model: MegatronGPTModel() does not have field.name: moe_token_dispatcher_type in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n[NeMo W 2024-10-07 10:35:28 megatron_base_model:577] The model: MegatronGPTModel() does not have field.name: moe_per_layer_logging in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n[NeMo W 2024-10-07 10:35:28 megatron_base_model:577] The model: MegatronGPTModel() does not have field.name: moe_expert_capacity_factor in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n[NeMo W 2024-10-07 10:35:28 megatron_base_model:577] The model: MegatronGPTModel() does not have field.name: moe_pad_expert_input_to_capacity in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n[NeMo W 2024-10-07 10:35:28 megatron_base_model:577] The model: MegatronGPTModel() does not have field.name: moe_token_drop_policy in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n[NeMo W 2024-10-07 10:35:28 megatron_base_model:577] The model: MegatronGPTModel() does not have field.name: moe_layer_recompute in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n[NeMo W 2024-10-07 10:35:28 megatron_base_model:577] The model: MegatronGPTModel() does not have field.name: clone_scatter_output_in_embedding in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n[NeMo W 2024-10-07 10:35:28 megatron_base_model:577] The model: MegatronGPTModel() does not have field.name: disable_parameter_transpose_cache in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n[NeMo W 2024-10-07 10:35:28 megatron_base_model:577] The model: MegatronGPTModel() does not have field.name: enable_cuda_graph in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n[NeMo W 2024-10-07 10:35:28 megatron_base_model:577] The model: MegatronGPTModel() does not have field.name: external_cuda_graph in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n[NeMo W 2024-10-07 10:35:28 megatron_base_model:577] The model: MegatronGPTModel() does not have field.name: config_logger_dir in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n[NeMo W 2024-10-07 10:35:28 megatron_gpt_model:372] megatron_amp_O2 is enabled but transformer-engine is not.\nInitializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n----------------------------------------------------------------------------------------------------\ndistributed_backend=nccl\nAll distributed processes registered. Starting with 1 processes\n----------------------------------------------------------------------------------------------------\n\nError executing job with overrides: ['gpt_model_file=/scratch/ema8/Minitron-4B-Base/nemo/minitron-4b-base.nemo', 'lora_model_path=/scratch/ema8/PEFT/results/minitron-4b-base/peft_1000_minitron-4b-base/checkpoints/megatron_gpt_peft_lora_tuning.nemo', 'merged_model_path=/scratch/ema8/PEFT/results/minitron-4b-base/peft_1000_minitron-4b-base_merged.nemo', 'tensor_model_parallel_size=1', 'pipeline_model_parallel_size=1', 'trainer.num_nodes=1', 'trainer.devices=1', 'trainer.accelerator=gpu']\n[rank0]: Traceback (most recent call last):\n[rank0]:   File \"/opt/NeMo/scripts/nlp_language_modeling/merge_lora_weights/merge.py\", line 307, in <module>\n[rank0]:     main()  # noqa pylint: disable=no-value-for-parameter\n[rank0]:   File \"/opt/NeMo/nemo/core/config/hydra_runner.py\", line 129, in wrapper\n[rank0]:     _run_hydra(\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/hydra/_internal/utils.py\", line 394, in _run_hydra\n[rank0]:     _run_app(\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/hydra/_internal/utils.py\", line 457, in _run_app\n[rank0]:     run_and_report(\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/hydra/_internal/utils.py\", line 223, in run_and_report\n[rank0]:     raise ex\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/hydra/_internal/utils.py\", line 220, in run_and_report\n[rank0]:     return func()\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/hydra/_internal/utils.py\", line 458, in <lambda>\n[rank0]:     lambda: hydra.run(\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/hydra/_internal/hydra.py\", line 132, in run\n[rank0]:     _ = ret.return_value\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/hydra/core/utils.py\", line 260, in return_value\n[rank0]:     raise self._return_value\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/hydra/core/utils.py\", line 186, in run_job\n[rank0]:     ret.return_value = task_function(task_cfg)\n[rank0]:   File \"/opt/NeMo/scripts/nlp_language_modeling/merge_lora_weights/merge.py\", line 241, in main\n[rank0]:     model = MegatronGPTModel.restore_from(\n[rank0]:   File \"/opt/NeMo/nemo/collections/nlp/models/nlp_model.py\", line 478, in restore_from\n[rank0]:     return super().restore_from(\n[rank0]:   File \"/opt/NeMo/nemo/core/classes/modelPT.py\", line 468, in restore_from\n[rank0]:     instance = cls._save_restore_connector.restore_from(\n[rank0]:   File \"/opt/NeMo/nemo/collections/nlp/parts/nlp_overrides.py\", line 1322, in restore_from\n[rank0]:     trainer.strategy.setup_environment()\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/strategies/ddp.py\", line 154, in setup_environment\n[rank0]:     self.setup_distributed()\n[rank0]:   File \"/opt/NeMo/nemo/collections/nlp/parts/nlp_overrides.py\", line 251, in setup_distributed\n[rank0]:     init_model_parallel(\n[rank0]:   File \"/opt/NeMo/nemo/collections/nlp/parts/nlp_overrides.py\", line 155, in init_model_parallel\n[rank0]:     parallel_state.initialize_model_parallel(\n[rank0]:   File \"/opt/megatron-lm/megatron/core/parallel_state.py\", line 532, in initialize_model_parallel\n[rank0]:     raise RuntimeError(f\"world_size ({world_size}) is not divisible by {total_model_size}\")\n[rank0]: RuntimeError: world_size (1) is not divisible by 4\n```\n\n**Steps/Code to reproduce bug**\n\nBelow is the shell script I'm running to merge the lora weights. \n\n```sh\nPATH_TO_MERGED_MODEL=\"/scratch/ema8/PEFT/results/minitron-4b-base/peft_1000_minitron-4b-base_merged.nemo\"\n\nMODEL=\"/scratch/ema8/Minitron-4B-Base/nemo/minitron-4b-base.nemo\"\n\nPATH_TO_TRAINED_MODEL=\"/scratch/ema8/PEFT/results/minitron-4b-base/peft_1000_minitron-4b-base/checkpoints/megatron_gpt_peft_lora_tuning.nemo\"\n\nexport HYDRA_FULL_ERROR=1\n\nsrun singularity exec --nv /home/ema8/f24-nvidia/nemo_eval.sif python /opt/NeMo/scripts/nlp_language_modeling/merge_lora_weights/merge.py \\\n    gpt_model_file=${MODEL} \\\n    lora_model_path=${PATH_TO_TRAINED_MODEL} \\\n    merged_model_path=${PATH_TO_MERGED_MODEL} \\\n    tensor_model_parallel_size=1 \\\n    pipeline_model_parallel_size=1 \\\n    trainer.num_nodes=1 \\\n    trainer.devices=1 \\\n    trainer.accelerator=gpu \\\n\n```\n\nHere is the script to fine-tune the model:\n```sh\n#!/bin/bash\n\n# Script taken from: https://docs.nvidia.com/nemo-framework/user-guide/latest/playbooks/nemoframeworkpeft.html#nemo-framework-peft-playbook\n\n# This is the nemo model we are finetuning\n# Change this to match the model you want to finetune\nMODEL=\"/scratch/ema8/Minitron-4B-Base/nemo/minitron-4b-base.nemo\"\n\n# These are the training datasets (in our case we only have one)\nTRAIN_DS=\"[/home/ema8/f24-nvidia/syn_data_PEFT_exp/high_school_cs_dataset/train-1000.jsonl]\"\n\n# These are the validation datasets (in our case we only have one)\nVALID_DS=\"[/home/ema8/f24-nvidia/syn_data_PEFT_exp/high_school_cs_dataset/val.jsonl]\"\n\n# These are the test datasets (in our case we only have one)\nTEST_DS=\"[/home/ema8/f24-nvidia/syn_data_PEFT_exp/high_school_cs_dataset/test.jsonl]\"\n\n# These are the names of the test datasets\nTEST_NAMES=\"[high_school_cs_dataset]\"\n\n# This is the PEFT scheme that we will be using. Set to \"ptuning\" for P-Tuning instead of LoRA\nPEFT_SCHEME=\"lora\"\n\n# This is the concat sampling probability. This depends on the number of files being passed in the train set\n# and the sampling probability for each file. In our case, we have one training file. Note sum of concat sampling\n# probabilities should be 1.0. For example, with two entries in TRAIN_DS, CONCAT_SAMPLING_PROBS might be\n# \"[0.3,0.7]\". For three entries, CONCAT_SAMPLING_PROBS might be \"[0.3,0.1,0.6]\"\n# NOTE: Your entry must contain a value greater than 0.0 for each file\nCONCAT_SAMPLING_PROBS=\"[1.0]\"\n\n# This is the tensor parallel size (splitting tensors among GPUs horizontally)\n# See above matrix for proper value for the given model size\nTP_SIZE=1\n\n# This is the pipeline parallel size (splitting layers among GPUs vertically)\n# See above matrix for proper value for the given model size\nPP_SIZE=1\n\n# The number of nodes to run this on\n# See above matrix for proper value for the given model size\nNODE_COUNT=1\n\n# The number of total GPUs used\nGPU_COUNT=1\n\n# Where to store the finetuned model and training artifacts\nOUTPUT_DIR=\"/scratch/ema8/PEFT/results/minitron-4b-base/peft_1000_200steps_minitron-4b-base\"\n\n# Run the PEFT command by appropriately setting the values for the parameters such as the number of steps,\n# model checkpoint path, batch sizes etc. For a full reference of parameter\n# settings refer to the config at https://github.com/NVIDIA/NeMo/blob/main/examples/nlp/language_modeling/tuning/conf/megatron_gpt_finetuning_config.yaml\npython /opt/NeMo/examples/nlp/language_modeling/tuning/megatron_gpt_finetuning.py \\\n    trainer.log_every_n_steps=1 \\\n    trainer.precision=bf16 \\\n    trainer.devices=${GPU_COUNT} \\\n    trainer.num_nodes=1 \\\n    trainer.val_check_interval=5 \\\n    trainer.max_steps=200 \\\n    model.restore_from_path=${MODEL} \\\n    model.peft.peft_scheme=${PEFT_SCHEME} \\\n    model.peft.lora_tuning.target_modules=[attention_qkv] \\\n    +model.tp_comm_overlap_disable_qkv=True \\\n    model.micro_batch_size=1 \\\n    model.global_batch_size=128 \\\n    model.tensor_model_parallel_size=${TP_SIZE} \\\n    model.pipeline_model_parallel_size=${PP_SIZE} \\\n    model.megatron_amp_O2=True \\\n    model.activations_checkpoint_granularity=selective \\\n    model.activations_checkpoint_num_layers=null \\\n    model.activations_checkpoint_method=uniform \\\n    model.optim.name=fused_adam \\\n    model.optim.lr=1e-4 \\\n    model.answer_only_loss=True \\\n    model.data.train_ds.file_names=${TRAIN_DS} \\\n    model.data.validation_ds.file_names=${VALID_DS} \\\n    model.data.test_ds.file_names=${TEST_DS} \\\n    model.data.train_ds.concat_sampling_probabilities=${CONCAT_SAMPLING_PROBS} \\\n    model.data.train_ds.max_seq_length=4096 \\\n    model.data.validation_ds.max_seq_length=4096 \\\n    model.data.train_ds.micro_batch_size=1 \\\n    model.data.train_ds.global_batch_size=128 \\\n    model.data.validation_ds.micro_batch_size=1 \\\n    model.data.validation_ds.global_batch_size=128 \\\n    model.data.train_ds.num_workers=0 \\\n    model.data.validation_ds.num_workers=0 \\\n    model.data.test_ds.num_workers=0 \\\n    model.data.validation_ds.metric.name=loss \\\n    model.data.test_ds.metric.name=loss \\\n    exp_manager.create_wandb_logger=False \\\n    exp_manager.checkpoint_callback_params.mode=min \\\n    exp_manager.explicit_log_dir=${OUTPUT_DIR} \\\n    exp_manager.resume_if_exists=True \\\n    exp_manager.resume_ignore_no_checkpoint=True \\\n    exp_manager.create_checkpoint_callback=True \\\n    exp_manager.checkpoint_callback_params.monitor=validation_loss \\\n    ++exp_manager.checkpoint_callback_params.save_best_model=False \\\n    exp_manager.checkpoint_callback_params.save_nemo_on_train_end=True \\\n    model.save_nemo_on_validation_end=False\n```\n\n**Environment overview**\n\n - Environment location: apptainer\n - Method of NeMo install:\n\nBuilt my NeMo container based on the dev tag, and then added the lm-evaluation-harness.\n\n*nemo_eval.def*\n```def\nBootstrap: docker\nFrom: nvcr.io/nvidia/nemo:dev\n\n%post\n    git clone --depth 1 https://github.com/EleutherAI/lm-evaluation-harness\n    cd lm-evaluation-harness\n    pip install -e .\n    cd ..\n\n```\n\n The commands to build the container:\n`apptainer build nemo_eval.sif nemo_eval.def`\n\n**Additional context**\n\n- Running on the [Palmetto Cluster](https://docs.rcd.clemson.edu/palmetto/])\n- Using a node with a single A100 GPU\n",
    "state": "closed",
    "created_at": "2024-10-07T15:04:35+00:00",
    "closed_at": "2024-11-17T02:05:17+00:00",
    "updated_at": "2024-11-17T02:05:18+00:00",
    "author": "Elan456",
    "author_type": "User",
    "comments_count": 3,
    "reactions_count": 0,
    "labels": "bug,stale",
    "milestone": null,
    "closed_by": "github-actions[bot]",
    "resolution_time_hours": 971.0116666666667,
    "first_comments": [
      {
        "author": "Elan456",
        "created_at": "2024-10-10T16:55:56+00:00",
        "body": "## Fix\n\nIn the `model_config.yaml` of the .nemo checkpoint downloaded from [Hugging Face](https://huggingface.co/nvidia/Minitron-4B-Base), the `tensor_model_parallel_size` is set to 4:\n\n```\ntensor_model_parallel_size: 4\n```\n\nIf you untar the `.nemo` checkpoint, change the value of `tensor_model_parallel_size` to 1 and then retar the .nemo checkpoint, it will allow the `merge_lora_weights/merge.py` script to work with a single GPU. \n\n### Steps\n1. Untar the nemo checkpoint\n```sh\ntar -xf minitron-4b-base.nemo\n```\n\n2. Move the original checkpoint to a safe location to avoid needing to redownload if something goes wrong and to get it out of the way for running tar later\n`mv minitron-4b-base.nemo ../`\n\n3. Modify `tensor_model_parallel_size` \n```sh\nvim model_config.yaml\n```\n\n```diff\nmcore_gpt: true\nmicro_batch_size: 4\nglobal_batch_size: 1152\n-- tensor_model_parallel_size: 4\n++ tensor_model_parallel_size: 1\npipeline_model_parallel_size: 1\nvirtual_pipeline_model_parallel_size: null\nencoder_seq_length: 4096\nmax_position_embeddings: 4096\nnum_layers: 32\nhidden_size: 3072\n...\n```\n\n4. Rebuild the nemo checkpoint (tar everything in this directory)\n```sh\ntar -cvf minitron-4b-base.nemo *\n```\n\n"
      },
      {
        "author": "github-actions[bot]",
        "created_at": "2024-11-10T02:00:46+00:00",
        "body": "This issue is stale because it has been open for 30 days with no activity. Remove stale label or comment or this will be closed in 7 days."
      },
      {
        "author": "github-actions[bot]",
        "created_at": "2024-11-17T02:05:17+00:00",
        "body": "This issue was closed because it has been inactive for 7 days since being marked as stale."
      }
    ],
    "url": "https://github.com/NVIDIA/NeMo/issues/10782"
  },
  {
    "repository": "NVIDIA/NeMo",
    "issue_number": 10772,
    "title": "`IPython` should be included in the requirements",
    "body": "**Describe the bug**\n\n`IPython` is not included in the requirements but it is used in an non-optional manner\n\n**Steps/Code to reproduce bug**\n\n`from nemo.collections.asr.parts.utils.vad_utils import load_tensor_from_file`\nOn a new environment that comes without `IPython`\n\n**Expected behavior**\n\n`IPython` should be included in the requirements or imported inside the functions that need it (`plot` and `plot_sample_from_rttm`)\n\nThis was already discussed in https://github.com/NVIDIA/NeMo/pull/9890#discussion_r1704593976",
    "state": "closed",
    "created_at": "2024-10-05T09:40:09+00:00",
    "closed_at": "2024-11-18T09:07:40+00:00",
    "updated_at": "2024-11-18T09:07:42+00:00",
    "author": "MahmoudAshraf97",
    "author_type": "User",
    "comments_count": 4,
    "reactions_count": 0,
    "labels": "bug",
    "milestone": null,
    "closed_by": "MahmoudAshraf97",
    "resolution_time_hours": 1055.4586111111112,
    "first_comments": [
      {
        "author": "github-actions[bot]",
        "created_at": "2024-11-05T01:57:38+00:00",
        "body": "This issue is stale because it has been open for 30 days with no activity. Remove stale label or comment or this will be closed in 7 days."
      },
      {
        "author": "MahmoudAshraf97",
        "created_at": "2024-11-05T10:20:41+00:00",
        "body": "."
      },
      {
        "author": "svandiekendialpad",
        "created_at": "2024-11-14T18:07:12+00:00",
        "body": "👍 ➕ "
      },
      {
        "author": "MahmoudAshraf97",
        "created_at": "2024-11-18T09:07:41+00:00",
        "body": "Fixed in #11191 , release 2.0.0"
      }
    ],
    "url": "https://github.com/NVIDIA/NeMo/issues/10772"
  },
  {
    "repository": "NVIDIA/NeMo",
    "issue_number": 10745,
    "title": "Loading 70B model from .nemo checkpoint takes very long time",
    "body": "I am launching a continuous pretraining job from a 70B model checkpoint. The checkpoint is saved as .nemo file. I use\n```\nmegatronGPTModel.restore_from(restore_path=\"/path/to/my/70B/.nemo\", override_config_path, trainer)\n```\nto restore the checkpoint. It hangs while unpacking the .nemo file.\n\nThe container I am using is `nvcr.io/nvidia/nemo:24.05.llama3.1`",
    "state": "closed",
    "created_at": "2024-10-03T17:55:24+00:00",
    "closed_at": "2024-11-11T01:58:43+00:00",
    "updated_at": "2024-11-11T01:58:43+00:00",
    "author": "jiaji-huang",
    "author_type": "User",
    "comments_count": 2,
    "reactions_count": 0,
    "labels": "stale",
    "milestone": null,
    "closed_by": "github-actions[bot]",
    "resolution_time_hours": 920.0552777777777,
    "first_comments": [
      {
        "author": "github-actions[bot]",
        "created_at": "2024-11-03T02:02:49+00:00",
        "body": "This issue is stale because it has been open for 30 days with no activity. Remove stale label or comment or this will be closed in 7 days."
      },
      {
        "author": "github-actions[bot]",
        "created_at": "2024-11-11T01:58:43+00:00",
        "body": "This issue was closed because it has been inactive for 7 days since being marked as stale."
      }
    ],
    "url": "https://github.com/NVIDIA/NeMo/issues/10745"
  },
  {
    "repository": "NVIDIA/NeMo",
    "issue_number": 10738,
    "title": "How to implement weight decay towards the pre-trained model?",
    "body": "Hello, let me one question.\n\nIf using NeMo for supervised fune-tuning, how do I implement penalizing the distance between starting and current weights? This was shown to be effective in https://arxiv.org/abs/1706.03610",
    "state": "closed",
    "created_at": "2024-10-03T11:15:41+00:00",
    "closed_at": "2024-11-11T01:58:44+00:00",
    "updated_at": "2024-11-11T01:58:45+00:00",
    "author": "sedol1339",
    "author_type": "User",
    "comments_count": 2,
    "reactions_count": 0,
    "labels": "stale",
    "milestone": null,
    "closed_by": "github-actions[bot]",
    "resolution_time_hours": 926.7175,
    "first_comments": [
      {
        "author": "github-actions[bot]",
        "created_at": "2024-11-03T02:02:50+00:00",
        "body": "This issue is stale because it has been open for 30 days with no activity. Remove stale label or comment or this will be closed in 7 days."
      },
      {
        "author": "github-actions[bot]",
        "created_at": "2024-11-11T01:58:44+00:00",
        "body": "This issue was closed because it has been inactive for 7 days since being marked as stale."
      }
    ],
    "url": "https://github.com/NVIDIA/NeMo/issues/10738"
  },
  {
    "repository": "NVIDIA/NeMo",
    "issue_number": 10715,
    "title": "Global shape mismatch for loaded ((1024, 768)) and expected ((512, 768)) tensor for key model.embedding.position_embeddings.weight",
    "body": "**Describe the bug**\nI followed the instructions in:\nhttps://docs.nvidia.com/nemo-framework/user-guide/latest/nemotoolkit/nlp/nemo_megatron/gpt/gpt_training.html\n\nthe i replace 1024 with 512\n\n```\npython src/examples/nlp/language_modeling/megatron_gpt_pretraining.py  \\\n    --config-path=conf \\\n    --config-name=megatron_gpt_config \\\n    trainer.devices=1 \\\n    trainer.num_nodes=1 \\\n    trainer.max_epochs=null \\\n    trainer.max_steps=30000 \\\n    trainer.val_check_interval=300 \\\n    trainer.log_every_n_steps=50 \\\n    trainer.limit_val_batches=50 \\\n    trainer.limit_test_batches=50 \\\n    trainer.accumulate_grad_batches=1 \\\n    trainer.precision=16 \\\n    model.micro_batch_size=6 \\\n    model.global_batch_size=192 \\\n    model.tensor_model_parallel_size=1 \\\n    model.pipeline_model_parallel_size=1 \\\n    model.max_position_embeddings=512 \\\n    model.encoder_seq_length=512 \\\n    model.hidden_size=768 \\\n    model.ffn_hidden_size=3072 \\\n    model.num_layers=12 \\\n    model.num_attention_heads=12 \\\n    model.init_method_std=0.021 \\\n    model.hidden_dropout=0.1 \\\n    model.layernorm_epsilon=1e-5 \\\n    model.tokenizer.vocab_file=data/tokenizer/gpt2-vocab.json \\\nmodel.tokenizer.merge_file=data/tokenizer/gpt2-merges.txt \\\n    model.data.data_prefix=[1.0,data/tokenized_data/hfbpe_gpt_training_data_text_document] \\\n    model.data.num_workers=2 \\\n    model.data.seq_length=512 \\\n    model.data.splits_string=\\'980,10,10\\' \\\n    model.megatron_amp_O2=False \\\n    model.optim.name=fused_adam \\\n    model.optim.lr=6e-4 \\\n    model.optim.betas=[0.9,0.95] \\\n    model.optim.weight_decay=0.1 \\\n    model.optim.sched.name=CosineAnnealing \\\n    model.optim.sched.warmup_steps=750 \\\n    model.optim.sched.constant_steps=80000 \\\n    model.optim.sched.min_lr=6e-5 \\\n    exp_manager.resume_if_exists=True \\\n    exp_manager.resume_ignore_no_checkpoint=True \\\n    exp_manager.create_checkpoint_callback=True \\\n    exp_manager.checkpoint_callback_params.monitor=val_loss \\\n    exp_manager.checkpoint_callback_params.save_top_k=3 \\\n    exp_manager.checkpoint_callback_params.mode=min \\\n    exp_manager.checkpoint_callback_params.always_save_nemo=False\n```\n\nIt get this error:\n```\nError executing job with overrides: ['trainer.devices=1', 'trainer.num_nodes=1', 'trainer.max_epochs=null', 'trainer.max_steps=30000', 'trainer.val_check_interval=300', 'trainer.log_every_n_steps=50', 'trainer.limit_val_batches=50', 'trainer.limit_test_batches=50', 'trainer.accumulate_grad_batches=1', 'trainer.precision=16', 'model.micro_batch_size=6', 'model.global_batch_size=192', 'model.tensor_model_parallel_size=1', 'model.pipeline_model_parallel_size=1', 'model.max_position_embeddings=512', 'model.encoder_seq_length=512', 'model.fp8_amax_history_len=512', 'model.hidden_size=768', 'model.ffn_hidden_size=3072', 'model.num_layers=12', 'model.num_attention_heads=12', 'model.init_method_std=0.021', 'model.hidden_dropout=0.1', 'model.layernorm_epsilon=1e-5', 'model.tokenizer.vocab_file=data/tokenizer/gpt2-vocab.json', 'model.tokenizer.merge_file=data/tokenizer/gpt2-merges.txt', 'model.data.data_prefix=[1.0,data/tokenized_data/hfbpe_gpt_training_data_text_document]', 'model.data.num_workers=2', 'model.data.seq_length=512', \"model.data.splits_string='980,10,10'\", 'model.megatron_amp_O2=False', 'model.optim.name=fused_adam', 'model.optim.lr=6e-4', 'model.optim.betas=[0.9,0.95]', 'model.optim.weight_decay=0.1', 'model.optim.sched.name=CosineAnnealing', 'model.optim.sched.warmup_steps=750', 'model.optim.sched.constant_steps=80000', 'model.optim.sched.min_lr=6e-5', 'exp_manager.resume_if_exists=True', 'exp_manager.resume_ignore_no_checkpoint=True', 'exp_manager.create_checkpoint_callback=True', 'exp_manager.checkpoint_callback_params.monitor=val_loss', 'exp_manager.checkpoint_callback_params.save_top_k=3', 'exp_manager.checkpoint_callback_params.mode=min', 'exp_manager.checkpoint_callback_params.always_save_nemo=False']\nTraceback (most recent call last):\n  File \"/app/src/examples/nlp/language_modeling/megatron_gpt_pretraining.py\", line 42, in main\n    trainer.fit(model)\n  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\", line 543, in fit\n    call._call_and_handle_interrupt(\n  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/call.py\", line 43, in _call_and_handle_interrupt\n    return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/strategies/launchers/subprocess_script.py\", line 105, in launch\n    return function(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\", line 579, in _fit_impl\n    self._run(model, ckpt_path=ckpt_path)\n  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\", line 973, in _run\n    self._checkpoint_connector._restore_modules_and_callbacks(ckpt_path)\n  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/checkpoint_connector.py\", line 397, in _restore_modules_and_callbacks\n    self.resume_start(checkpoint_path)\n  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/checkpoint_connector.py\", line 79, in resume_start\n    loaded_checkpoint = self.trainer.strategy.load_checkpoint(checkpoint_path)\n  File \"/opt/NeMo/nemo/collections/nlp/parts/nlp_overrides.py\", line 441, in load_checkpoint\n    return self.checkpoint_io.load_checkpoint(checkpoint_path, sharded_state_dict=checkpoint)\n  File \"/opt/NeMo/nemo/utils/callbacks/dist_ckpt_io.py\", line 78, in load_checkpoint\n    return dist_checkpointing.load(\n  File \"/opt/megatron-lm/megatron/core/dist_checkpointing/serialization.py\", line 135, in load\n    loaded_state_dict = sharded_strategy.load(sharded_state_dict, checkpoint_dir)\n  File \"/opt/megatron-lm/megatron/core/dist_checkpointing/strategies/tensorstore.py\", line 43, in load\n    dict_list_map_inplace(load_fn, sharded_state_dict)\n  File \"/opt/megatron-lm/megatron/core/dist_checkpointing/dict_utils.py\", line 180, in dict_list_map_inplace\n    x[k] = dict_list_map_inplace(f, v)\n  File \"/opt/megatron-lm/megatron/core/dist_checkpointing/dict_utils.py\", line 180, in dict_list_map_inplace\n    x[k] = dict_list_map_inplace(f, v)\n  File \"/opt/megatron-lm/megatron/core/dist_checkpointing/dict_utils.py\", line 184, in dict_list_map_inplace\n    return f(x)\n  File \"/opt/megatron-lm/megatron/core/dist_checkpointing/strategies/tensorstore.py\", line 80, in _load_from_array\n    x = _load_regular_chunk(sharded_tensor, checkpoint_dir)\n  File \"/opt/megatron-lm/megatron/core/dist_checkpointing/strategies/tensorstore.py\", line 107, in _load_regular_chunk\n    raise CheckpointingException(_msg)\nmegatron.core.dist_checkpointing.core.CheckpointingException: Global shape mismatch for loaded ((1024, 768)) and expected ((512, 768)) tensor for key model.embedding.position_embeddings.weight\n```\n\n**Environment details**\nA100\nnemo docker:\nnvcr.io/nvidia/nemo:24.05.01\n",
    "state": "closed",
    "created_at": "2024-10-02T17:40:32+00:00",
    "closed_at": "2024-11-10T02:00:52+00:00",
    "updated_at": "2024-11-12T07:12:17+00:00",
    "author": "Alireza3242",
    "author_type": "User",
    "comments_count": 4,
    "reactions_count": 0,
    "labels": "bug,stale",
    "milestone": null,
    "closed_by": "github-actions[bot]",
    "resolution_time_hours": 920.3388888888888,
    "first_comments": [
      {
        "author": "Alireza3242",
        "created_at": "2024-10-02T18:09:53+00:00",
        "body": "I find it:\nI have to delete this folder:\nnemo_experiments/megatron_gpt/checkpoints"
      },
      {
        "author": "github-actions[bot]",
        "created_at": "2024-11-02T01:57:56+00:00",
        "body": "This issue is stale because it has been open for 30 days with no activity. Remove stale label or comment or this will be closed in 7 days."
      },
      {
        "author": "github-actions[bot]",
        "created_at": "2024-11-10T02:00:51+00:00",
        "body": "This issue was closed because it has been inactive for 7 days since being marked as stale."
      },
      {
        "author": "Han-tj",
        "created_at": "2024-11-12T07:12:17+00:00",
        "body": "what is the folder nemo_experiments/megatron_gpt/checkpoints? And why delete it can work?"
      }
    ],
    "url": "https://github.com/NVIDIA/NeMo/issues/10715"
  },
  {
    "repository": "NVIDIA/NeMo",
    "issue_number": 10681,
    "title": "Using MSDD model with a different speaker embedding model",
    "body": "Hello, is it possible to replace the TitaNet embedding model that is used along with MSDD? and if yes, does that require retraining?\nI want to construct a pipeline with a different VAD and embedding model but still use the MSDD model",
    "state": "closed",
    "created_at": "2024-09-30T09:27:00+00:00",
    "closed_at": "2024-11-11T08:46:00+00:00",
    "updated_at": "2024-11-11T08:46:00+00:00",
    "author": "MahmoudAshraf97",
    "author_type": "User",
    "comments_count": 2,
    "reactions_count": 0,
    "labels": "stale",
    "milestone": null,
    "closed_by": "MahmoudAshraf97",
    "resolution_time_hours": 1007.3166666666667,
    "first_comments": [
      {
        "author": "tango4j",
        "created_at": "2024-10-08T20:38:17+00:00",
        "body": "It would still work to a certain degree without re-training, but you will get much better result if you retrain a new model.\nNot only MSDD model, optimizing the clustering algorithm on the scale length and scale weights on the new embedding  would also affect a lot on the performance."
      },
      {
        "author": "github-actions[bot]",
        "created_at": "2024-11-08T01:57:51+00:00",
        "body": "This issue is stale because it has been open for 30 days with no activity. Remove stale label or comment or this will be closed in 7 days."
      }
    ],
    "url": "https://github.com/NVIDIA/NeMo/issues/10681"
  },
  {
    "repository": "NVIDIA/NeMo",
    "issue_number": 10680,
    "title": "Unable to decode using canary 1b model",
    "body": "**Describe the bug**\n\nA clear and concise description of what the bug is.\n\nI am using nemo inside docker environment, with single GPU, unable to get the ASR ouput using canary 1b model\n\n```\nError executing job with overrides: ['pretrained_name=nvidia/canary-1b', 'audio_dir=/opt/workspace/es_test', 'output_filename=canary_output.txt', 'clean_groundtruth_text=True', 'langid=es', 'batch_size=1', 'compute_timestamps=False', 'compute_langs=False', 'cuda=0', 'amp=True', 'append_pred=False', 'pred_name_postfix=canary']\nTraceback (most recent call last):\n  File \"/opt/asr_ensemble/repo/nemo/examples/asr/transcribe_speech.py\", line 290, in main\n    asr_model.change_decoding_strategy(cfg.ctc_decoding)\n  File \"/usr/local/lib/python3.10/dist-packages/nemo/collections/asr/models/aed_multitask_models.py\", line 203, in change_decoding_strategy\n    self.decoding = MultiTaskDecoding(\n  File \"/usr/local/lib/python3.10/dist-packages/nemo/collections/asr/parts/submodules/multitask_decoding.py\", line 377, in __init__\n    super().__init__(\n  File \"/usr/local/lib/python3.10/dist-packages/nemo/collections/asr/parts/submodules/multitask_decoding.py\", line 124, in __init__\n    raise NotImplementedError(\"Greedy decoding is not implemented yet.\")\nNotImplementedError: Greedy decoding is not implemented yet.\n\n```\n\n\n**Steps/Code to reproduce bug**\n\n    python3 nemo/examples/asr/transcribe_speech.py \\\n    pretrained_name=\"nvidia/canary-1b\" \\\n    audio_dir=\"/opt/workspace/es_test\" \\\n    output_filename=\"canary_output.txt\" \\\n    clean_groundtruth_text=True \\\n    langid='es' \\\n    batch_size=1 \\\n    compute_timestamps=False \\\n    compute_langs=False \\\n    cuda=0 \\\n    amp=True \\\n    append_pred=False \\\n    pred_name_postfix=\"canary\"\n\n\n**Expected behavior**\n\nA clear and concise description of what you expected to happen.\n\n**Environment overview (please complete the following information)**\n\n - Environment location: [Bare-metal, Docker, Cloud(specify cloud provider - AWS, Azure, GCP, Collab)]\n - Method of NeMo install: [pip install or from source]. Please specify exact commands you used to install.\n - If method of install is [Docker], provide `docker pull` & `docker run` commands used\n\n**Environment details**\n\nIf NVIDIA docker image is used you don't need to specify these.\nOtherwise, please provide:\n- OS version Ubuntu 20\n- PyTorch version\n- Python version 3.10\n\n**Additional context**\n\nAdd any other context about the problem here.\nExample: GPU model\n",
    "state": "closed",
    "created_at": "2024-09-30T06:55:04+00:00",
    "closed_at": "2024-11-15T02:02:38+00:00",
    "updated_at": "2024-11-15T02:02:38+00:00",
    "author": "uni-saurabh-vyas",
    "author_type": "User",
    "comments_count": 3,
    "reactions_count": 0,
    "labels": "bug,stale",
    "milestone": null,
    "closed_by": "github-actions[bot]",
    "resolution_time_hours": 1099.1261111111112,
    "first_comments": [
      {
        "author": "pzelasko",
        "created_at": "2024-10-08T16:56:09+00:00",
        "body": "Which NeMo version is this? Greedy decoding for Canary should work if you install the latest 2.0 pre-release, or the latest `main` branch."
      },
      {
        "author": "github-actions[bot]",
        "created_at": "2024-11-08T01:57:52+00:00",
        "body": "This issue is stale because it has been open for 30 days with no activity. Remove stale label or comment or this will be closed in 7 days."
      },
      {
        "author": "github-actions[bot]",
        "created_at": "2024-11-15T02:02:37+00:00",
        "body": "This issue was closed because it has been inactive for 7 days since being marked as stale."
      }
    ],
    "url": "https://github.com/NVIDIA/NeMo/issues/10680"
  },
  {
    "repository": "NVIDIA/NeMo",
    "issue_number": 10679,
    "title": "When converting a checkpoint from Hugging Face, the checkpoint format conversion keeps getting CUDA out of memory",
    "body": "```\npython3 /opt/NeMo/scripts/checkpoint_converters/convert_llava_hf_to_nemo.py \\\n    --input_name_or_path llava-hf/llava-1.5-7b-hf \\\n    --output_path /workspace/checkpoints/llava-7b.nemo \\\n    --tokenizer_path /workspace/checkpoints/vicuna-7b-v1.5/tokenizer_neva.model\n```\nKeep reporting errors\n```\n[NeMo I 2024-09-30 05:28:51 convert_llava_hf_to_nemo:288] Running verifications ['query: how much protein should a female eat'] ...\n[rank0]: Traceback (most recent call last):\n[rank0]:   File \"/opt/NeMo/scripts/checkpoint_converters/convert_llava_hf_to_nemo.py\", line 331, in <module>\n[rank0]:     convert(args)\n[rank0]:   File \"/opt/NeMo/scripts/checkpoint_converters/convert_llava_hf_to_nemo.py\", line 295, in convert\n[rank0]:     model = model.cuda().eval()\n[rank0]:   File \"/opt/NeMo/nemo/core/classes/modelPT.py\", line 1963, in cuda\n[rank0]:     return super().cuda(device=device)\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/lightning_fabric/utilities/device_dtype_mixin.py\", line 76, in cuda\n[rank0]:     return super().cuda(device=device)\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 911, in cuda\n[rank0]:     return self._apply(lambda t: t.cuda(device))\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 802, in _apply\n[rank0]:     module._apply(fn)\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 802, in _apply\n[rank0]:     module._apply(fn)\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 802, in _apply\n[rank0]:     module._apply(fn)\n[rank0]:   [Previous line repeated 3 more times]\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 825, in _apply\n[rank0]:     param_applied = fn(param)\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 911, in <lambda>\n[rank0]:     return self._apply(lambda t: t.cuda(device))\n[rank0]: torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 192.00 MiB. GPU 0 has a total capacity of 47.29 GiB of which 169.00 MiB is free. Process 573844 has 46.78 GiB memory in use. Of the allocated memory 46.42 GiB is allocated by PyTorch, and 17.42 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n```\n\nI am using two NVIDIA RTX 5880 48G GPUs, each with 32GB of memory. When I use only one GPU, the memory is fully utilized. However, when I use both GPUs, only one GPU's memory is fully utilized, while the other GPU seems to be underutilized. Why is this happening? Additionally, why does data type conversion consume so much GPU memory?\n",
    "state": "closed",
    "created_at": "2024-09-30T05:36:02+00:00",
    "closed_at": "2024-11-08T01:57:53+00:00",
    "updated_at": "2024-11-08T01:57:54+00:00",
    "author": "changg10",
    "author_type": "User",
    "comments_count": 3,
    "reactions_count": 0,
    "labels": "stale",
    "milestone": null,
    "closed_by": "github-actions[bot]",
    "resolution_time_hours": 932.3641666666666,
    "first_comments": [
      {
        "author": "changg10",
        "created_at": "2024-09-30T05:37:20+00:00",
        "body": "And I often have this warning, please how to solve it \n`zarr distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).\n`"
      },
      {
        "author": "github-actions[bot]",
        "created_at": "2024-10-31T01:59:58+00:00",
        "body": "This issue is stale because it has been open for 30 days with no activity. Remove stale label or comment or this will be closed in 7 days."
      },
      {
        "author": "github-actions[bot]",
        "created_at": "2024-11-08T01:57:53+00:00",
        "body": "This issue was closed because it has been inactive for 7 days since being marked as stale."
      }
    ],
    "url": "https://github.com/NVIDIA/NeMo/issues/10679"
  },
  {
    "repository": "NVIDIA/NeMo",
    "issue_number": 10677,
    "title": "Punctuation and Capitalization Model: how to add custom Punctuation marks to prepare data script?",
    "body": "Dear Team,\n\nPlease help me to add custom Punctuation marks to following script.\n\npython examples/nlp/token_classification/data/prepare_data_for_punctuation_capitalization.py \\\n       -s <PATH/TO/THE/SOURCE/FILE> \\\n       -o <PATH/TO/THE/OUTPUT/DIRECTORY>\n       -p ? --marks ?\n\nI can not add.\nThank you in advance!",
    "state": "closed",
    "created_at": "2024-09-29T14:55:11+00:00",
    "closed_at": "2024-11-09T01:55:49+00:00",
    "updated_at": "2024-11-09T01:55:50+00:00",
    "author": "ican24",
    "author_type": "User",
    "comments_count": 5,
    "reactions_count": 0,
    "labels": "stale",
    "milestone": null,
    "closed_by": "github-actions[bot]",
    "resolution_time_hours": 971.0105555555556,
    "first_comments": [
      {
        "author": "ican24",
        "created_at": "2024-09-30T19:57:12+00:00",
        "body": "It seems there is a bug in  create_text_and_labels function of \n.local/lib/python3.10/site-packages/nemo/collections/nlp/data/token_classification\n\nI had tried to run it directly \n`\ncreate_text_and_labels(\"/tmp\", \"/punct/src/test.txt\", \"՞,․՜՝«»՛․։…\")\n`\nBut no any result!\n"
      },
      {
        "author": "ican24",
        "created_at": "2024-09-30T20:37:20+00:00",
        "body": "The reason is an absent of punctuation marks (punct_marks) in \"remove_punctuation\" function\n`\ndef remove_punctuation(word: str):\n`\n\nThe software developer forgot to use them\n\n```\nRemoves all punctuation marks from a word except for '\nthat is often a part of word: don't, it's, and so on\n\nall_punct_marks = string.punctuation.replace(\"'\", '')\n\n```"
      },
      {
        "author": "ican24",
        "created_at": "2024-10-01T03:59:32+00:00",
        "body": "Finally my troubles with this product ended successfully after 10 days of struggling and I am ready to close this issue, but  many questions remain open.\nFirstly it is hard and long to install this modul which demands NVIDIA's Transformer Engine.\nIt requires CUDA and CUDNN, but how can we intall them on the usage machines without GPU? \nIt is a serious trouble, because we are not always ready to buy machine with expensive GPU-s to use this function/utility, which is not a core.\nI remind you that we are able to use NEMO ASR in none-GPU environment.\n "
      },
      {
        "author": "github-actions[bot]",
        "created_at": "2024-11-01T02:04:07+00:00",
        "body": "This issue is stale because it has been open for 30 days with no activity. Remove stale label or comment or this will be closed in 7 days."
      },
      {
        "author": "github-actions[bot]",
        "created_at": "2024-11-09T01:55:49+00:00",
        "body": "This issue was closed because it has been inactive for 7 days since being marked as stale."
      }
    ],
    "url": "https://github.com/NVIDIA/NeMo/issues/10677"
  },
  {
    "repository": "NVIDIA/NeMo",
    "issue_number": 10675,
    "title": "NeMo 2.0 llm sharded_state_dict error",
    "body": "**Describe the bug**\n\n```\nTraining epoch 0, iteration 196/3999 | lr: 2.954e-05 | consumed_samples: 4728 | global_batch_size: 24 | global_step: 196 | reduced_train_loss: 10.84 | train_step_timing in s: 0.9765\nTraining epoch 0, iteration 197/3999 | lr: 2.969e-05 | consumed_samples: 4752 | global_batch_size: 24 | global_step: 197 | reduced_train_loss: 10.84 | train_step_timing in s: 0.9826\nTraining epoch 0, iteration 198/3999 | lr: 2.984e-05 | consumed_samples: 4776 | global_batch_size: 24 | global_step: 198 | reduced_train_loss: 10.84 | train_step_timing in s: 0.9742\nTraining epoch 0, iteration 199/3999 | lr: 2.999e-05 | consumed_samples: 4800 | global_batch_size: 24 | global_step: 199 | reduced_train_loss: 10.85 | train_step_timing in s: 1.085\n[NeMo W 2024-09-29 08:11:31 nemo_logging:349] /home/usr/.local/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:384: `ModelCheckpoint(monitor='val_loss')` could not find the monitored key in the returned metrics: ['lr', 'consumed_samples', 'global_batch_size', 'global_step', 'step', 'reduced_train_loss', 'grad_norm', 'train_step_timing in s', 'epoch']. HINT: Did you call `log('val_loss', value)` in the `LightningModule`?\n    \nEpoch 0, global step 199: 'val_loss' was not in top 10\n[rank0]: Traceback (most recent call last):\n[rank0]:   File \"/home/usr/.local/bin/nemorun\", line 8, in <module>\n[rank0]:     sys.exit(app())\n[rank0]:   File \"/home/usr/.local/lib/python3.10/site-packages/typer/main.py\", line 338, in __call__\n[rank0]:     raise e\n[rank0]:   File \"/home/usr/.local/lib/python3.10/site-packages/typer/main.py\", line 321, in __call__\n[rank0]:     return get_command(self)(*args, **kwargs)\n[rank0]:   File \"/home/usr/.local/lib/python3.10/site-packages/click/core.py\", line 1157, in __call__\n[rank0]:     return self.main(*args, **kwargs)\n[rank0]:   File \"/home/usr/.local/lib/python3.10/site-packages/typer/core.py\", line 728, in main\n[rank0]:     return _main(\n[rank0]:   File \"/home/usr/.local/lib/python3.10/site-packages/typer/core.py\", line 197, in _main\n[rank0]:     rv = self.invoke(ctx)\n[rank0]:   File \"/home/usr/.local/lib/python3.10/site-packages/click/core.py\", line 1688, in invoke\n[rank0]:     return _process_result(sub_ctx.command.invoke(sub_ctx))\n[rank0]:   File \"/home/usr/.local/lib/python3.10/site-packages/click/core.py\", line 1688, in invoke\n[rank0]:     return _process_result(sub_ctx.command.invoke(sub_ctx))\n[rank0]:   File \"/home/usr/.local/lib/python3.10/site-packages/click/core.py\", line 1434, in invoke\n[rank0]:     return ctx.invoke(self.callback, **ctx.params)\n[rank0]:   File \"/home/usr/.local/lib/python3.10/site-packages/click/core.py\", line 783, in invoke\n[rank0]:     return __callback(*args, **kwargs)\n[rank0]:   File \"/home/usr/.local/lib/python3.10/site-packages/typer/main.py\", line 703, in wrapper\n[rank0]:     return callback(**use_params)\n[rank0]:   File \"/home/usr/code/NeMo-Run/src/nemo_run/cli/api.py\", line 790, in command\n[rank0]:     self.cli_execute(fn, ctx.args, type)\n[rank0]:   File \"/home/usr/code/NeMo-Run/src/nemo_run/cli/api.py\", line 842, in cli_execute\n[rank0]:     self._execute_task(fn, filtered_args)\n[rank0]:   File \"/home/usr/code/NeMo-Run/src/nemo_run/cli/api.py\", line 892, in _execute_task\n[rank0]:     run_task()\n[rank0]:   File \"/home/usr/code/NeMo-Run/src/nemo_run/cli/api.py\", line 871, in run_task\n[rank0]:     run.run(\n[rank0]:   File \"/home/usr/code/NeMo-Run/src/nemo_run/run/api.py\", line 65, in run\n[rank0]:     direct_run_fn(fn_or_script, dryrun=dryrun)\n[rank0]:   File \"/home/usr/code/NeMo-Run/src/nemo_run/run/task.py\", line 77, in direct_run_fn\n[rank0]:     built_fn()\n[rank0]:   File \"/home/usr/code/NeMo/nemo/collections/llm/api.py\", line 127, in pretrain\n[rank0]:     return train(\n[rank0]:   File \"/home/usr/code/NeMo/nemo/collections/llm/api.py\", line 85, in train\n[rank0]:     trainer.fit(model, data)\n[rank0]:   File \"/home/usr/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py\", line 538, in fit\n[rank0]:     call._call_and_handle_interrupt(\n[rank0]:   File \"/home/usr/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py\", line 46, in _call_and_handle_interrupt\n[rank0]:     return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)\n[rank0]:   File \"/home/usr/.local/lib/python3.10/site-packages/pytorch_lightning/strategies/launchers/subprocess_script.py\", line 105, in launch\n[rank0]:     return function(*args, **kwargs)\n[rank0]:   File \"/home/usr/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py\", line 574, in _fit_impl\n[rank0]:     self._run(model, ckpt_path=ckpt_path)\n[rank0]:   File \"/home/usr/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py\", line 981, in _run\n[rank0]:     results = self._run_stage()\n[rank0]:   File \"/home/usr/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py\", line 1025, in _run_stage\n[rank0]:     self.fit_loop.run()\n[rank0]:   File \"/home/usr/.local/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py\", line 205, in run\n[rank0]:     self.advance()\n[rank0]:   File \"/home/usr/.local/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py\", line 363, in advance\n[rank0]:     self.epoch_loop.run(self._data_fetcher)\n[rank0]:   File \"/home/usr/.local/lib/python3.10/site-packages/pytorch_lightning/loops/training_epoch_loop.py\", line 140, in run\n[rank0]:     self.advance(data_fetcher)\n[rank0]:   File \"/home/usr/.local/lib/python3.10/site-packages/pytorch_lightning/loops/training_epoch_loop.py\", line 269, in advance\n[rank0]:     call._call_callback_hooks(trainer, \"on_train_batch_end\", batch_output, batch, batch_idx)\n[rank0]:   File \"/home/usr/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py\", line 218, in _call_callback_hooks\n[rank0]:     fn(trainer, trainer.lightning_module, *args, **kwargs)\n[rank0]:   File \"/home/usr/.local/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py\", line 317, in on_train_batch_end\n[rank0]:     self._save_last_checkpoint(trainer, monitor_candidates)\n[rank0]:   File \"/home/usr/.local/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py\", line 696, in _save_last_checkpoint\n[rank0]:     self._save_checkpoint(trainer, filepath)\n[rank0]:   File \"/home/usr/code/NeMo/nemo/lightning/pytorch/callbacks/model_checkpoint.py\", line 449, in _save_checkpoint\n[rank0]:     trainer.save_checkpoint(ckpt_filepath, save_weights_only, storage_options=storage_options)\n[rank0]:   File \"/home/usr/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py\", line 1365, in save_checkpoint\n[rank0]:     self.strategy.save_checkpoint(checkpoint, filepath, storage_options=storage_options)\n[rank0]:   File \"/home/usr/code/NeMo/nemo/lightning/pytorch/strategies/megatron_strategy.py\", line 660, in save_checkpoint\n[rank0]:     checkpoint[\"optimizer\"] = [self.optimizer_sharded_state_dict()]\n[rank0]:   File \"/home/usr/code/NeMo/nemo/lightning/pytorch/strategies/megatron_strategy.py\", line 638, in optimizer_sharded_state_dict\n[rank0]:     return _strategy_lib.optimizer_sharded_state_dict(\n[rank0]:   File \"/home/usr/code/NeMo/nemo/lightning/_strategy_lib.py\", line 455, in optimizer_sharded_state_dict\n[rank0]:     return optimizer.sharded_state_dict(\n[rank0]:   File \"/home/usr/code/NeMo/nemo/lightning/pytorch/optim/megatron.py\", line 114, in sharded_state_dict\n[rank0]:     state_dict = self.mcore_optimizer.sharded_state_dict(\n[rank0]:   File \"/home/usr/code/Megatron-LM/megatron/core/optimizer/distrib_optimizer.py\", line 950, in sharded_state_dict\n[rank0]:     param_state = self.sharded_param_state_fs_model_space(\n[rank0]:   File \"/home/usr/code/Megatron-LM/megatron/core/optimizer/distrib_optimizer.py\", line 1161, in sharded_param_state_fs_model_space\n[rank0]:     dtype=state_ten.dtype,\n[rank0]: AttributeError: 'NoneType' object has no attribute 'dtype'\n[rank1]: Traceback (most recent call last):\n[rank1]:   File \"/home/usr/.local/bin/nemorun\", line 8, in <module>\n[rank1]:     sys.exit(app())\n[rank1]:   File \"/home/usr/.local/lib/python3.10/site-packages/typer/main.py\", line 338, in __call__\n[rank1]:     raise e\n[rank1]:   File \"/home/usr/.local/lib/python3.10/site-packages/typer/main.py\", line 321, in __call__\n[rank1]:     return get_command(self)(*args, **kwargs)\n[rank1]:   File \"/home/usr/.local/lib/python3.10/site-packages/click/core.py\", line 1157, in __call__\n[rank1]:     return self.main(*args, **kwargs)\n[rank1]:   File \"/home/usr/.local/lib/python3.10/site-packages/typer/core.py\", line 728, in main\n[rank1]:     return _main(\n[rank1]:   File \"/home/usr/.local/lib/python3.10/site-packages/typer/core.py\", line 197, in _main\n[rank1]:     rv = self.invoke(ctx)\n[rank1]:   File \"/home/usr/.local/lib/python3.10/site-packages/click/core.py\", line 1688, in invoke\n[rank1]:     return _process_result(sub_ctx.command.invoke(sub_ctx))\n[rank1]:   File \"/home/usr/.local/lib/python3.10/site-packages/click/core.py\", line 1688, in invoke\n[rank1]:     return _process_result(sub_ctx.command.invoke(sub_ctx))\n[rank1]:   File \"/home/usr/.local/lib/python3.10/site-packages/click/core.py\", line 1434, in invoke\n[rank1]:     return ctx.invoke(self.callback, **ctx.params)\n[rank1]:   File \"/home/usr/.local/lib/python3.10/site-packages/click/core.py\", line 783, in invoke\n[rank1]:     return __callback(*args, **kwargs)\n[rank1]:   File \"/home/usr/.local/lib/python3.10/site-packages/typer/main.py\", line 703, in wrapper\n[rank1]:     return callback(**use_params)\n[rank1]:   File \"/home/usr/code/NeMo-Run/src/nemo_run/cli/api.py\", line 790, in command\n[rank1]:     self.cli_execute(fn, ctx.args, type)\n[rank1]:   File \"/home/usr/code/NeMo-Run/src/nemo_run/cli/api.py\", line 842, in cli_execute\n[rank1]:     self._execute_task(fn, filtered_args)\n[rank1]:   File \"/home/usr/code/NeMo-Run/src/nemo_run/cli/api.py\", line 892, in _execute_task\n[rank1]:     run_task()\n[rank1]:   File \"/home/usr/code/NeMo-Run/src/nemo_run/cli/api.py\", line 871, in run_task\n[rank1]:     run.run(\n[rank1]:   File \"/home/usr/code/NeMo-Run/src/nemo_run/run/api.py\", line 65, in run\n[rank1]:     direct_run_fn(fn_or_script, dryrun=dryrun)\n[rank1]:   File \"/home/usr/code/NeMo-Run/src/nemo_run/run/task.py\", line 77, in direct_run_fn\n[rank1]:     built_fn()\n[rank1]:   File \"/home/usr/code/NeMo/nemo/collections/llm/api.py\", line 127, in pretrain\n[rank1]:     return train(\n[rank1]:   File \"/home/usr/code/NeMo/nemo/collections/llm/api.py\", line 85, in train\n[rank1]:     trainer.fit(model, data)\n[rank1]:   File \"/home/usr/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py\", line 538, in fit\n[rank1]:     call._call_and_handle_interrupt(\n[rank1]:   File \"/home/usr/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py\", line 46, in _call_and_handle_interrupt\n[rank1]:     return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)\n[rank1]:   File \"/home/usr/.local/lib/python3.10/site-packages/pytorch_lightning/strategies/launchers/subprocess_script.py\", line 105, in launch\n[rank1]:     return function(*args, **kwargs)\n[rank1]:   File \"/home/usr/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py\", line 574, in _fit_impl\n[rank1]:     self._run(model, ckpt_path=ckpt_path)\n[rank1]:   File \"/home/usr/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py\", line 981, in _run\n[rank1]:     results = self._run_stage()\n[rank1]:   File \"/home/usr/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py\", line 1025, in _run_stage\n[rank1]:     self.fit_loop.run()\n[rank1]:   File \"/home/usr/.local/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py\", line 205, in run\n[rank1]:     self.advance()\n[rank1]:   File \"/home/usr/.local/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py\", line 363, in advance\n[rank1]:     self.epoch_loop.run(self._data_fetcher)\n[rank1]:   File \"/home/usr/.local/lib/python3.10/site-packages/pytorch_lightning/loops/training_epoch_loop.py\", line 140, in run\n[rank1]:     self.advance(data_fetcher)\n[rank1]:   File \"/home/usr/.local/lib/python3.10/site-packages/pytorch_lightning/loops/training_epoch_loop.py\", line 269, in advance\n[rank1]:     call._call_callback_hooks(trainer, \"on_train_batch_end\", batch_output, batch, batch_idx)\n[rank1]:   File \"/home/usr/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py\", line 218, in _call_callback_hooks\n[rank1]:     fn(trainer, trainer.lightning_module, *args, **kwargs)\n[rank1]:   File \"/home/usr/.local/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py\", line 317, in on_train_batch_end\n[rank1]:     self._save_last_checkpoint(trainer, monitor_candidates)\n[rank1]:   File \"/home/usr/.local/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py\", line 696, in _save_last_checkpoint\n[rank1]:     self._save_checkpoint(trainer, filepath)\n[rank1]:   File \"/home/usr/code/NeMo/nemo/lightning/pytorch/callbacks/model_checkpoint.py\", line 449, in _save_checkpoint\n[rank1]:     trainer.save_checkpoint(ckpt_filepath, save_weights_only, storage_options=storage_options)\n[rank1]:   File \"/home/usr/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py\", line 1365, in save_checkpoint\n[rank1]:     self.strategy.save_checkpoint(checkpoint, filepath, storage_options=storage_options)\n[rank1]:   File \"/home/usr/code/NeMo/nemo/lightning/pytorch/strategies/megatron_strategy.py\", line 660, in save_checkpoint\n[rank1]:     checkpoint[\"optimizer\"] = [self.optimizer_sharded_state_dict()]\n[rank1]:   File \"/home/usr/code/NeMo/nemo/lightning/pytorch/strategies/megatron_strategy.py\", line 638, in optimizer_sharded_state_dict\n[rank1]:     return _strategy_lib.optimizer_sharded_state_dict(\n[rank1]:   File \"/home/usr/code/NeMo/nemo/lightning/_strategy_lib.py\", line 455, in optimizer_sharded_state_dict\n[rank1]:     return optimizer.sharded_state_dict(\n[rank1]:   File \"/home/usr/code/NeMo/nemo/lightning/pytorch/optim/megatron.py\", line 114, in sharded_state_dict\n[rank1]:     state_dict = self.mcore_optimizer.sharded_state_dict(\n[rank1]:   File \"/home/usr/code/Megatron-LM/megatron/core/optimizer/distrib_optimizer.py\", line 950, in sharded_state_dict\n[rank1]:     param_state = self.sharded_param_state_fs_model_space(\n[rank1]:   File \"/home/usr/code/Megatron-LM/megatron/core/optimizer/distrib_optimizer.py\", line 1161, in sharded_param_state_fs_model_space\n[rank1]:     dtype=state_ten.dtype,\n[rank1]: AttributeError: 'NoneType' object has no attribute 'dtype'\n```\n\n**Steps/Code to reproduce bug**\n\n```\nnemorun llm pretrain --factory llama3_8b trainer.max_steps=4000 trainer.num_nodes=1 trainer.devices=2 data.seq_length=128 data.global_batch_size=24 model.config.seq_length=128 model.config.num_layers=4 model.config.hidden_size=256 model.config.ffn_hidden_size=1024 log.log_dir=exp_0929_n1d2_mini_fixckptsave trainer.strategy.save_ckpt_format=torch -y\n```\n\n\nDEBUG: master_param, dict_keys(['fp32_param', 'exp_avg', 'exp_avg_sq', 'master_param'])\nDEBUG: master_param, dict_keys(['fp32_param', 'exp_avg', 'exp_avg_sq', 'master_param'])\n",
    "state": "closed",
    "created_at": "2024-09-29T08:20:47+00:00",
    "closed_at": "2024-09-30T00:26:16+00:00",
    "updated_at": "2024-09-30T00:26:18+00:00",
    "author": "lifeiteng",
    "author_type": "User",
    "comments_count": 1,
    "reactions_count": 0,
    "labels": "bug",
    "milestone": null,
    "closed_by": "lifeiteng",
    "resolution_time_hours": 16.09138888888889,
    "first_comments": [
      {
        "author": "lifeiteng",
        "created_at": "2024-09-30T00:26:16+00:00",
        "body": "install TE from source\n\nhttps://github.com/NVIDIA/TransformerEngine/pull/1130"
      }
    ],
    "url": "https://github.com/NVIDIA/NeMo/issues/10675"
  },
  {
    "repository": "NVIDIA/NeMo",
    "issue_number": 10615,
    "title": "Punctuation and Capitalization Model not working",
    "body": "Dear Team,\n\nI am trying to test  \"Punctuation and Capitalization Model\" Quick Start Guide  \nfrom\nhttps://docs.nvidia.com/nemo-framework/user-guide/latest/nemotoolkit/nlp/punctuation_and_capitalization.html\n\n```\nfrom nemo.collections.nlp.models import PunctuationCapitalizationModel\n\n# to get the list of pre-trained models\nPunctuationCapitalizationModel.list_available_models()\n\n# Download and load the pre-trained BERT-based model\nmodel = PunctuationCapitalizationModel.from_pretrained(\"punctuation_en_bert\")\n\n# try the model on a few examples\nmodel.add_punctuation_capitalization(['how are you', 'great how about you'])\n\n```\nEach time I am receiving error \n\n```\nImportError: cannot import name 'MMapIndexedDataset' from 'megatron.core.datasets.indexed_dataset' (/home/deep/.local/lib/python3.10/site-packages/megatron/core/datasets/indexed_dataset.py)\n\n```\n\nMy attempts to change megatron-core version ended resultness.\n \nThe environment configuration shows blow:\n\nNVIDIA-SMI 550.54.14\nDriver Version: 550.54.14\nCUDA Version: 12.4\n\nnvcc -V :\nnvcc: NVIDIA (R) Cuda compiler driver\nCopyright (c) 2005-2024 NVIDIA Corporation\nBuilt on Tue_Feb_27_16:19:38_PST_2024\nCuda compilation tools, release 12.4, V12.4.99\nBuild cuda_12.4.r12.4/compiler.33961263_0\n\ncuDNN: 8.9.6\ntorch 2.4.1\ntorchvision 0.19.1\n\n ",
    "state": "closed",
    "created_at": "2024-09-25T06:25:26+00:00",
    "closed_at": "2024-09-29T14:09:07+00:00",
    "updated_at": "2024-09-29T14:09:08+00:00",
    "author": "ican24",
    "author_type": "User",
    "comments_count": 2,
    "reactions_count": 0,
    "labels": "bug",
    "milestone": null,
    "closed_by": "ican24",
    "resolution_time_hours": 103.72805555555556,
    "first_comments": [
      {
        "author": "ican24",
        "created_at": "2024-09-26T16:41:30+00:00",
        "body": "I am wasting time on reinstallation ans re-configuration of all software components.\nUnfortunately yet there is no result.\nOnly the error was changed:\n```\n\nTraceback (most recent call last):\n  File \"/maroc/nemoasr/pc.py\", line 1, in <module>\n    from nemo.collections.nlp.models import PunctuationCapitalizationModel\n  File \"/maroc/NeMo/nemo/collections/nlp/__init__.py\", line 15, in <module>\n    from nemo.collections.nlp import data, losses, models, modules\n  File \"/maroc/NeMo/nemo/collections/nlp/data/__init__.py\", line 16, in <module>\n    from nemo.collections.nlp.data.entity_linking.entity_linking_dataset import EntityLinkingDataset\n  File \"/maroc/NeMo/nemo/collections/nlp/data/entity_linking/__init__.py\", line 15, in <module>\n    from nemo.collections.nlp.data.entity_linking.entity_linking_dataset import EntityLinkingDataset\n  File \"/maroc/NeMo/nemo/collections/nlp/data/entity_linking/entity_linking_dataset.py\", line 22, in <module>\n    from nemo.core.classes import Dataset\n  File \"/maroc/NeMo/nemo/core/__init__.py\", line 16, in <module>\n    from nemo.core.classes import *\n  File \"/maroc/NeMo/nemo/core/classes/__init__.py\", line 33, in <module>\n    from nemo.core.classes.modelPT import ModelPT\n  File \"/maroc/NeMo/nemo/core/classes/modelPT.py\", line 29, in <module>\n    from megatron.core.optimizer import OptimizerConfig, get_megatron_optimizer\n  File \"/home/deep/.local/lib/python3.10/site-packages/megatron/core/optimizer/__init__.py\", line 8, in <module>\n    from transformer_engine.pytorch.optimizers import FusedAdam as Adam\n  File \"/home/deep/.local/lib/python3.10/site-packages/transformer_engine/__init__.py\", line 10, in <module>\n    import transformer_engine.common\n  File \"/home/deep/.local/lib/python3.10/site-packages/transformer_engine/common/__init__.py\", line 118, in <module>\n    _TE_LIB_CTYPES = _load_library()\n  File \"/home/deep/.local/lib/python3.10/site-packages/transformer_engine/common/__init__.py\", line 89, in _load_library\n    return ctypes.CDLL(so_path, mode=ctypes.RTLD_GLOBAL)\n  File \"/usr/local/lib/python3.10/ctypes/__init__.py\", line 374, in __init__\n    self._handle = _dlopen(self._name, mode)\nOSError: /home/deep/.local/lib/python3.10/site-packages/transformer_engine/libtransformer_engine.so: undefined symbol: cudnnBackendExecute\n\n``` "
      },
      {
        "author": "ican24",
        "created_at": "2024-09-29T14:09:07+00:00",
        "body": "I had re-installed all CUDx and pip packages.\nIt is hard to tell all details.\nI can say that It is a horrible difficult and long history.\nHowever, it is working now."
      }
    ],
    "url": "https://github.com/NVIDIA/NeMo/issues/10615"
  },
  {
    "repository": "NVIDIA/NeMo",
    "issue_number": 10583,
    "title": "Unable to connect to network no Internet connection",
    "body": "**Describe the bug**\n\nA clear and concise description of what the bug is.\n\n**Steps/Code to reproduce bug**\n\nPlease list *minimal* steps or code snippet for us to be able to reproduce the bug.\n\nA  helpful guide on on how to craft a minimal bug report  http://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports. \n\n\n**Expected behavior**\n\nA clear and concise description of what you expected to happen.\n\n**Environment overview (please complete the following information)**\n\n - Environment location: Docker\n - Method of install: Please specify exact commands you used to install.\n - If method of install is [Docker], provide `docker pull` & `docker run` commands used\n\n**Additional context**\n\nAdd any other context about the problem here.\nExample: GPU model\n",
    "state": "closed",
    "created_at": "2024-09-23T15:38:31+00:00",
    "closed_at": "2024-09-25T18:14:38+00:00",
    "updated_at": "2024-09-25T18:14:39+00:00",
    "author": "Savage0725",
    "author_type": "User",
    "comments_count": 1,
    "reactions_count": 0,
    "labels": "bug",
    "milestone": null,
    "closed_by": "elliottnv",
    "resolution_time_hours": 50.60194444444444,
    "first_comments": [
      {
        "author": "elliottnv",
        "created_at": "2024-09-25T18:14:38+00:00",
        "body": "Please fill out the template and reopen. Thank you!"
      }
    ],
    "url": "https://github.com/NVIDIA/NeMo/issues/10583"
  },
  {
    "repository": "NVIDIA/NeMo",
    "issue_number": 10582,
    "title": "cosine annealing linear warmup lr=0 on first step",
    "body": "**Describe the bug**\n\nPlease see https://github.com/NVIDIA/NeMo/pull/10530\n\n**Steps/Code to reproduce bug**\n\nPlease see https://github.com/NVIDIA/NeMo/pull/10530\n\n**Expected behavior**\n\nLR != 0 on a first step\n\n**Environment overview (please complete the following information)**\n\n - Environment location: [Bare-metal, Docker, Cloud(specify cloud provider - AWS, Azure, GCP, Collab)]: Any\n - Method of NeMo install: [pip install or from source]. Please specify exact commands you used to install: Any\n - If method of install is [Docker], provide `docker pull` & `docker run` commands used: N/A\n\n**Environment details**\n\nIf NVIDIA docker image is used you don't need to specify these.\nOtherwise, please provide:\n- OS version: Any\n- PyTorch version: Any\n- Python version: Any\n\n**Additional context**\n\nAdd any other context about the problem here.\nExample: GPU model: Any\n",
    "state": "closed",
    "created_at": "2024-09-23T15:10:46+00:00",
    "closed_at": "2024-09-27T12:57:18+00:00",
    "updated_at": "2024-09-27T12:57:18+00:00",
    "author": "clumsy",
    "author_type": "User",
    "comments_count": 1,
    "reactions_count": 0,
    "labels": "bug",
    "milestone": null,
    "closed_by": "clumsy",
    "resolution_time_hours": 93.77555555555556,
    "first_comments": [
      {
        "author": "ericharper",
        "created_at": "2024-09-25T18:18:58+00:00",
        "body": "I'm not sure we want to modify that lr schedule as it's been used as-is for so long. Can you use your own implementation? NeMo supports users to use their own lr schedules.\n\nhttps://docs.nvidia.com/nemo-framework/user-guide/latest/nemotoolkit/core/core.html?highlight=learning%2520rate#learning-rate-schedulers\n"
      }
    ],
    "url": "https://github.com/NVIDIA/NeMo/issues/10582"
  }
]