[
  {
    "repository": "langchain-ai/langchain",
    "issue_number": 29435,
    "title": "How to records first_token_time for your runs?",
    "body": "\n### Discussed in https://github.com/langchain-ai/langchain/discussions/29414\n\n<div type='discussions-op-text'>\n\n<sup>Originally posted by **uznadeem** January 24, 2025</sup>\n### Checked other resources\n\n- [X] I added a very descriptive title to this question.\n- [X] I searched the LangChain documentation with the integrated search.\n- [X] I used the GitHub search to find a similar question and didn't find it.\n\n### Commit to Help\n\n- [X] I commit to help with one of those options ðŸ‘†\n\n### Example Code\n\n```python\ncurl -X POST \"https://api.smith.langchain.com/api/v1/runs\" \\\r\n-H \"Content-Type: application/json\" \\\r\n-H \"x-api-key: <API KEY HERE>\" \\\r\n-d '{\r\n  \"name\": \"uzair3\",\r\n  \"run_type\": \"llm\",\r\n  \"start_time\": \"2025-01-24T16:05:30Z\",\r\n  \"end_time\": \"2025-01-24T16:05:34Z\",\r\n  \"first_token_time\": \"2025-01-24T16:05:34Z\",\r\n  \"inputs\": { \r\n    \"system_message\": \"Your role is to answer questions accurately.\",\r\n    \"user_message\": \"What is the capital of France?\"\r\n  },\r\n  \"outputs\": {\r\n    \"response\": \"The capital of France is Paris.\"\r\n  }\r\n}'\n```\n\n\n### Description\n\n- We want to track `Time to First Token` for our runs.\r\n\r\n- From the [documentations](https://api.smith.langchain.com/redoc#tag/run/operation/trigger_rules_api_v1_runs_rules_trigger_post) I see we need to use `first_token_time` for that.\r\n- I had tried passing `\"first_token_time\": \"2025-01-24T16:05:34Z\",` with my request in multiple formats but it is recording it as `\"first_token_time\":null`\r\n\r\nHere is my output:\r\n\r\n```\r\ncurl -X GET \"https://api.smith.langchain.com/api/v1/runs/9a2c2694-1ae5-4949-8968-f2a41cb0f602\" \\\r\n-H \"Content-Type: application/json\" \\\r\n-H \"x-api-key: <API KEY HERE>\" \\\r\n\r\n\r\n{\"name\":\"uzair3\",\"inputs\":{\"system_message\":\"Your role is to answer questions accurately.\",\"user_message\":\"What is the capital of France?\"},\"inputs_preview\":\"Your role is to answer questions accurately.\",\"run_type\":\"llm\",\"start_time\":\"2025-01-24T16:05:30\",\"end_time\":\"2025-01-24T16:05:34\",\"extra\":null,\"error\":null,\"execution_order\":1,\"serialized\":null,\"outputs\":{\"response\":\"The capital of France is Paris.\"},\"outputs_preview\":\"The capital of France is Paris.\",\"parent_run_id\":null,\"manifest_id\":null,\"manifest_s3_id\":null,\"events\":[],\"tags\":[],\"inputs_s3_urls\":null,\"outputs_s3_urls\":null,\"s3_urls\":null,\"trace_id\":\"9a2c2694-1ae5-4949-8968-f2a41cb0f602\",\"dotted_order\":\"20250124T160530000000Z9a2c2694-1ae5-4949-8968-f2a41cb0f602\",\"id\":\"9a2c2694-1ae5-4949-8968-f2a41cb0f602\",\"status\":\"success\",\"child_run_ids\":null,\"direct_child_run_ids\":null,\"parent_run_ids\":[],\"feedback_stats\":null,\"reference_example_id\":null,\"total_tokens\":0,\"prompt_tokens\":0,\"completion_tokens\":0,\"total_cost\":null,\"prompt_cost\":null,\"completion_cost\":null,\"price_model_id\":null,\"first_token_time\":null,\"session_id\":\"6de1a1bb-654a-44d7-8eb2-48f13833ac2d\",\"app_path\":\"/o/1754faa3-0e76-408a-8517-996d9ab3fffb/projects/p/6de1a1bb-654a-44d7-8eb2-48f13833ac2d/r/9a2c2694-1ae5-4949-8968-f2a41cb0f602?trace_id=9a2c2694-1ae5-4949-8968-f2a41cb0f602&start_time=2025-01-24T16:05:30\",\"last_queued_at\":null,\"in_dataset\":false,\"share_token\":null,\"trace_tier\":\"shortlived\",\"trace_first_received_at\":\"2025-01-24T16:25:10.989689\",\"ttl_seconds\":1209600,\"trace_upgrade\":false,\"reference_dataset_id\":null}\r\n```\n\n### System Info\n\nCURL request</div>",
    "state": "closed",
    "created_at": "2025-01-27T10:38:50+00:00",
    "closed_at": "2025-01-29T11:15:56+00:00",
    "updated_at": "2025-01-29T11:15:58+00:00",
    "author": "uznadeem",
    "author_type": "User",
    "comments_count": 1,
    "reactions_count": 0,
    "labels": "",
    "milestone": null,
    "closed_by": "uznadeem",
    "resolution_time_hours": 48.61833333333333,
    "first_comments": [
      {
        "author": "uznadeem",
        "created_at": "2025-01-29T11:15:56+00:00",
        "body": "I have resolved it,\n\nWe would need to add an event to your API call to have the time to first token datetime displayed. Another issue to fix would be the timestamps where we have removed the Z and can add .000000.\n\nI'm including my data payload below where I used a random string for my new token in that new event:\n\n```\n{\n    \"name\": \"uznadeem001\",\n    \"run_type\": \"llm\",\n    \"start_time\": \"2025-01-23T10:39:29.000000\",\n    \"end_time\": \"2025-01-23T10:40:50.000000\",\n    \"inputs\": {\n        \"system_message\": \"Your role is to answer questions accurately.\",\n        \"user_message\": \"What is the capital of France?\"\n    },\n    \"outputs\": {\n        \"response\": \"The capital of France is Paris.\"\n    },\n    \"events\": [\n        {\n            \"name\": \"new_token\",\n            \"time\": \"2025-01-23T10:39:31.000000\",\n            \"kwargs\": {\n                \"token\": [\n                    \"1111111222222\"\n                ]\n            }\n        }\n    ]\n}\n```"
      }
    ],
    "url": "https://github.com/langchain-ai/langchain/issues/29435"
  },
  {
    "repository": "langchain-ai/langchain",
    "issue_number": 29418,
    "title": "Tool calling broken for Gemini with legacy agent",
    "body": "### Checked other resources\n\n- [x] I added a very descriptive title to this issue.\n- [x] I searched the LangChain documentation with the integrated search.\n- [x] I used the GitHub search to find a similar question and didn't find it.\n- [x] I am sure that this is a bug in LangChain rather than my code.\n- [x] The bug is not resolved by updating to the latest stable version of LangChain (or the specific integration package).\n\n### Example Code\n\n```\nfrom langchain_google_genai import ChatGoogleGenerativeAI\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain.agents.agent import AgentExecutor\nfrom langchain.agents import create_tool_calling_agent\n\nmodel = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash-exp\")\nprompt = ChatPromptTemplate.from_messages(\n        [\n            (\"system\", PROMPT_TEMPLATE),\n            (\"placeholder\", \"{chat_history}\"),\n            (\"human\", \"{input}\"),\n            (\"placeholder\", \"{agent_scratchpad}\"),\n        ]\n    )\ntools = [my_tool, ...]\n\nagent = create_tool_calling_agent(model, tools, prompt)\nagent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n\nresponse = agent_executor.invoke({\"input\": \"call my_tool\"})\n```\n\n### Error Message and Stack Trace (if applicable)\n\n`langchain_google_genai.chat_models.ChatGoogleGenerativeAIError: Invalid argument provided to Gemini: 400 * GenerateContentRequest.contents[2].parts[0].function_response.name: Name cannot be empty.`\n\n### Description\n\nTool calling with Gemini requires the tool name to be returned in the tool response, but for some reason `ToolMessage.name` is not being set.\n\n`ToolMessage.additional_kwargs` is set to: `{\"name\": \"my_tool\"}`, but this isn't used when building the message Parts to send to Gemini. \n\n### System Info\n\npython -m langchain_core.sys_info\n\nSystem Information\n------------------\n> OS:  Darwin\n> OS Version:  Darwin Kernel Version 23.6.0: Mon Jul 29 21:14:30 PDT 2024; root:xnu-10063.141.2~1/RELEASE_ARM64_T6000\n> Python Version:  3.11.9 (main, Jun 20 2024, 15:53:48) [Clang 15.0.0 (clang-1500.3.9.4)]\n\nPackage Information\n-------------------\n> langchain_core: 0.3.31\n> langchain: 0.3.15\n> langchain_community: 0.3.15\n> langsmith: 0.2.11\n> langchain_anthropic: 0.3.3\n> langchain_google_genai: 2.0.9\n> langchain_google_vertexai: 2.0.7\n> langchain_openai: 0.3.1\n> langchain_text_splitters: 0.3.5\n> langgraph_sdk: 0.1.51\n\nOptional packages not installed\n-------------------------------\n> langserve\n\nOther Dependencies\n------------------\n> aiohttp: 3.11.11\n> anthropic: 0.43.1\n> anthropic[vertexai]: Installed. No version info available.\n> async-timeout: Installed. No version info available.\n> dataclasses-json: 0.6.7\n> defusedxml: 0.7.1\n> filetype: 1.2.0\n> google-cloud-aiplatform: 1.77.0\n> google-cloud-storage: 2.19.0\n> google-generativeai: 0.8.4\n> httpx: 0.27.2\n> httpx-sse: 0.4.0\n> jsonpatch: 1.33\n> langchain-mistralai: Installed. No version info available.\n> langsmith-pyo3: Installed. No version info available.\n> numpy: 1.26.4\n> openai: 1.59.9\n> orjson: 3.10.15\n> packaging: 24.2\n> pydantic: 2.10.5\n> pydantic-settings: 2.7.1\n> PyYAML: 6.0.2\n> requests: 2.32.3\n> requests-toolbelt: 1.0.0\n> SQLAlchemy: 2.0.37\n> tenacity: 9.0.0\n> tiktoken: 0.8.0\n> typing-extensions: 4.12.2\n> zstandard: Installed. No version info available",
    "state": "closed",
    "created_at": "2025-01-24T21:23:59+00:00",
    "closed_at": "2025-01-29T10:08:31+00:00",
    "updated_at": "2025-01-29T10:08:33+00:00",
    "author": "Finndersen",
    "author_type": "User",
    "comments_count": 3,
    "reactions_count": 0,
    "labels": "ðŸ¤–:bug",
    "milestone": null,
    "closed_by": "Finndersen",
    "resolution_time_hours": 108.74222222222222,
    "first_comments": [
      {
        "author": "Finndersen",
        "created_at": "2025-01-24T22:06:57+00:00",
        "body": "This can be resolved by setting the `name` attribute when creating the `ToolMessage` [here](https://github.com/langchain-ai/langchain/blob/dbb6b7b103d9c32cea46d3848839a4c9cbb493c3/libs/langchain/langchain/agents/format_scratchpad/tools.py#L35)"
      },
      {
        "author": "Finndersen",
        "created_at": "2025-01-24T22:20:14+00:00",
        "body": "This issue may have actually been caused by this change https://github.com/langchain-ai/langchain-google/pull/671 in `langchain-google`"
      },
      {
        "author": "Finndersen",
        "created_at": "2025-01-29T10:08:31+00:00",
        "body": "I made a fix for  langchain-ai/langchain-google which should resolve this"
      }
    ],
    "url": "https://github.com/langchain-ai/langchain/issues/29418"
  },
  {
    "repository": "langchain-ai/langchain",
    "issue_number": 29416,
    "title": "ChatOpenAI o1: \"Unsupported parameter: 'parallel_tool_calls' is not supported with this model.\"",
    "body": "### Privileged issue\n\n- [x] I am a LangChain maintainer, or was asked directly by a LangChain maintainer to create an issue here.\n\n### Issue Content\n\nNoting for documentation purpose:\n\nPreviously calling ChatOpenAI(model=\"o1\").with_structured_output raised:\n```\nError code: 400 - {'error': {'message': \"Unsupported parameter: 'parallel_tool_calls' is not supported with this model.\",\n```\nâœ…  _Issue has been fixed in langchain-openai 0.3.2_. Please upgrade if you're seeing this.",
    "state": "closed",
    "created_at": "2025-01-24T18:08:12+00:00",
    "closed_at": "2025-01-24T18:08:34+00:00",
    "updated_at": "2025-01-24T18:08:35+00:00",
    "author": "baskaryan",
    "author_type": "User",
    "comments_count": 1,
    "reactions_count": 0,
    "labels": "",
    "milestone": null,
    "closed_by": "baskaryan",
    "resolution_time_hours": 0.006111111111111111,
    "first_comments": [
      {
        "author": "baskaryan",
        "created_at": "2025-01-24T18:08:34+00:00",
        "body": "Resolved in langchain-openai 0.3.2"
      }
    ],
    "url": "https://github.com/langchain-ai/langchain/issues/29416"
  },
  {
    "repository": "langchain-ai/langchain",
    "issue_number": 29374,
    "title": "Langchain-openai 0.3.1 bug: Overrides max_tokens with 100",
    "body": "### Checked other resources\n\n- [x] I added a very descriptive title to this issue.\n- [x] I searched the LangChain documentation with the integrated search.\n- [x] I used the GitHub search to find a similar question and didn't find it.\n- [x] I am sure that this is a bug in LangChain rather than my code.\n- [x] The bug is not resolved by updating to the latest stable version of LangChain (or the specific integration package).\n\n### Example Code\n\n**Code:**\n\n```\nfrom langchain_openai import ChatOpenAI\n\nmodel = ChatOpenAI(\n                base_url = url,\n                api_key = \"-\",\n                temperature=0.01,\n                max_tokens=2048,\n                default_headers = {\n                    \"x-api-key\": api_key\n                }\n            )\n\nprompt = \"\"\"\nWrite a long story about rocks and minerals.\n\n\"\"\"\n\nresponse = model.invoke(prompt)\n\n```\n\n\n**Output:**\n\n> AIMessage(content=\"In the heart of the Earth, where the heat and pressure were extreme, a world of rocks and minerals thrived. For billions of years, they had been forming, transforming, and evolving, creating a vast and wondrous landscape of geological wonders.\\n\\nAt the center of this world was a magnificent mountain range, its peaks reaching for the sky like giant's fists. The rocks that made up this range were ancient, their stories etched into their very fabric. Granite, basalt, and gne\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 100, 'prompt_tokens': 0, 'total_tokens': 100, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': '/data/llama3-70b-neuron', 'system_fingerprint': '2.0.2-native', 'finish_reason': 'length', 'logprobs': None}, id='run-5bc7f748-870c-4a56-963b-818f9755b8e2-0', usage_metadata={'input_tokens': 0, 'output_tokens': 100, 'total_tokens': 100, 'input_token_details': {}, 'output_token_details': {}})\n\n### Error Message and Stack Trace (if applicable)\n\n_No response_\n\n### Description\n\nThe latest version of langchain-openai (0.3.1) results in a max_token override with 100 tokens regardless of what you set the max_token limit as.\n\nAs displayed in the code, the max token limit is set to 2048 and yet the response maxes out at 100 tokens. I have tried all combinations and the issue keeps repeating because there is a bug with langchain-openai 0.3.1.\n\n**Issue can be resolved by using earlier versions of langchain-openai. (I am using langchain-openai 0.2.0 now and everything works just fine)**\n\n### System Info\n\n\nSystem Information\n------------------\n> OS:  Linux\n> OS Version:  #84~20.04.1-Ubuntu SMP Mon Nov 4 18:58:41 UTC 2024\n> Python Version:  3.12.3 (main, Nov  6 2024, 18:32:19) [GCC 13.2.0]\n\nPackage Information\n-------------------\n> langchain_core: 0.3.31\n> langchain: 0.3.8\n> langsmith: 0.1.133\n> langchain_openai: 0.3.1\n> langchain_text_splitters: 0.3.2\n\nOptional packages not installed\n-------------------------------\n> langserve\n\nOther Dependencies\n------------------\n> aiohttp: 3.9.5\n> async-timeout: Installed. No version info available.\n> httpx: 0.28.0\n> jsonpatch: 1.33\n> numpy: 1.26.4\n> openai: 1.60.0\n> orjson: 3.10.12\n> packaging: 24.1\n> pydantic: 2.8.2\n> PyYAML: 6.0.1\n> requests: 2.32.2\n> requests-toolbelt: 1.0.0\n> SQLAlchemy: 2.0.30\n> tenacity: 8.2.2\n> tiktoken: 0.7.0\n> typing-extensions: 4.11.0",
    "state": "closed",
    "created_at": "2025-01-23T11:33:30+00:00",
    "closed_at": "2025-01-23T15:11:48+00:00",
    "updated_at": "2025-01-23T15:11:50+00:00",
    "author": "Pranya-Chandratre",
    "author_type": "User",
    "comments_count": 1,
    "reactions_count": 0,
    "labels": "ðŸ¤–:bug,investigate",
    "milestone": null,
    "closed_by": "ccurme",
    "resolution_time_hours": 3.638333333333333,
    "first_comments": [
      {
        "author": "ccurme",
        "created_at": "2025-01-23T15:11:48+00:00",
        "body": "Between 0.2 and 0.3, ChatOpenAI was updated to send `max_completion_tokens` to the server, because the `max_tokens` parameter has been [deprecated](https://platform.openai.com/docs/api-reference/chat/create#chat-create-max_tokens).\n\nIf you are using another endpoint (i.e., not OpenAI) that does not support `max_completion_tokens`, you should use `BaseChatOpenAI` instead.\n```python\nfrom langchain_openai.chat_models.base import BaseChatOpenAI\n\nmodel = BaseChatOpenAI(model=\"gpt-4o-mini\", max_tokens=3)\n\nresponse = model.invoke(\"Hello!\")\nprint(response.content)\n```\n```\nHello! How\n```"
      }
    ],
    "url": "https://github.com/langchain-ai/langchain/issues/29374"
  },
  {
    "repository": "langchain-ai/langchain",
    "issue_number": 29372,
    "title": "AzureCosmosDBNoSqlVectorSearch + Indexing API = ContainerProxy.delete_item() missing 1 required positional argument: 'partition_key'",
    "body": "### Checked other resources\n\n- [x] I added a very descriptive title to this issue.\n- [x] I searched the LangChain documentation with the integrated search.\n- [x] I used the GitHub search to find a similar question and didn't find it.\n- [x] I am sure that this is a bug in LangChain rather than my code.\n- [x] The bug is not resolved by updating to the latest stable version of LangChain (or the specific integration package).\n\n### Example Code\n\nThe following code : \n```python\nfrom langchain.indexes import SQLRecordManager, IndexingResult, index\nfrom langchain_community.vectorstores.azure_cosmos_db_no_sql import (\n    AzureCosmosDBNoSqlVectorSearch\n)\n\n    vector_search = AzureCosmosDBNoSqlVectorSearch(\n        embedding=embeddings,\n        cosmos_client=cosmos_client,\n        database_name=cosmos_database,\n        container_name=cosmos_container,\n        vector_embedding_policy=vector_embedding_policy,\n        indexing_policy=indexing_policy,\n        cosmos_container_properties=cosmos_container_properties,\n        cosmos_database_properties={},\n        full_text_search_enabled=False,\n    )\n\n\n    namespace = f\"cosmosdb/{cosmos_database}/{cosmos_container}\"\n    record_manager = SQLRecordManager(\n        namespace=namespace,\n        db_url=\"sqlite:///record_manager_cache.db\"\n    )\n    record_manager.create_schema()\n\n    result = index(\n        splitted_docs,\n        record_manager,\n        vector_search,\n        cleanup=cleanup_mode, \n        source_id_key=\"source\"    \n    )\n```\n\n\n\n### Error Message and Stack Trace (if applicable)\n\nresults in ContainerProxy.delete_item() missing 1 required positional argument: 'partition_key'\n\n\n### Description\n\nWhen deleting a document using the index function (in langchain_core/indexing/api.py), the partition_key arg is not passed through\nLooking at source code, there's no way to pass this arg\nHow can I use Langchain Indexing API with AzureCosmosDBNoSqlVectorSearch\n\n### System Info\n\nazure-cosmos==4.9.0\nlangchain==0.3.14\nlangchain-community==0.3.14\nlangchain-core==0.3.29\nlangchain-openai==0.3.0\nlangchain-text-splitters==0.3.5",
    "state": "closed",
    "created_at": "2025-01-23T10:19:11+00:00",
    "closed_at": "2025-01-24T07:13:35+00:00",
    "updated_at": "2025-01-24T07:13:36+00:00",
    "author": "lorisalx",
    "author_type": "User",
    "comments_count": 2,
    "reactions_count": 0,
    "labels": "â±­: vector store,ðŸ¤–:bug",
    "milestone": null,
    "closed_by": "lorisalx",
    "resolution_time_hours": 20.906666666666666,
    "first_comments": [
      {
        "author": "lorisalx",
        "created_at": "2025-01-23T13:49:56+00:00",
        "body": "to give more context, I don't even know how the delete function of this AzureCosmosDBNoSqlVectorSearch can work since there is no PartitionKey provided here : \nhttps://github.com/langchain-ai/langchain/blame/f2ea62f63209130bfc56b1fe7d0fa7c299bbf352/libs/community/langchain_community/vectorstores/azure_cosmos_db_no_sql.py#L359\n\nand the delete_item require a mandatory parameter partition_key"
      },
      {
        "author": "lorisalx",
        "created_at": "2025-01-24T07:13:35+00:00",
        "body": "Issue fixed by providing a default partition_key parameter: PR #29382 "
      }
    ],
    "url": "https://github.com/langchain-ai/langchain/issues/29372"
  },
  {
    "repository": "langchain-ai/langchain",
    "issue_number": 29332,
    "title": "SSL certificate verification failed issue for Chatgroq",
    "body": "### URL\n\n_No response_\n\n### Checklist\n\n- [x] I added a very descriptive title to this issue.\n- [x] I included a link to the documentation page I am referring to (if applicable).\n\n### Issue with current documentation:\n\nI have created one sql agent and calling the llm using Chatgroq. I have also kept the goq api key in the environment and loading it in my solution. When i am using app.invoke(message) i am getting SSL error. Is there a way to by pass the SSL error i am getting. I have searched online and got some solution from stack overflow but it not worked.\n\n### Idea or request for content:\n\n_No response_",
    "state": "closed",
    "created_at": "2025-01-21T09:08:43+00:00",
    "closed_at": "2025-01-24T00:17:10+00:00",
    "updated_at": "2025-01-24T00:17:11+00:00",
    "author": "rajban94",
    "author_type": "User",
    "comments_count": 1,
    "reactions_count": 0,
    "labels": "",
    "milestone": null,
    "closed_by": "efriis",
    "resolution_time_hours": 63.14083333333333,
    "first_comments": [
      {
        "author": "efriis",
        "created_at": "2025-01-24T00:17:10+00:00",
        "body": "can't reproduce this unfortunately! If you want to file again or debug yourself, would recommend checking if you're using some kind of proxy, as well as if you get the same ssl error just using the groq sdk directly"
      }
    ],
    "url": "https://github.com/langchain-ai/langchain/issues/29332"
  },
  {
    "repository": "langchain-ai/langchain",
    "issue_number": 29294,
    "title": "Missing options in `OllamaEmbeddings` of `langchain_ollama.embeddings` after migration from `langchain_community.embeddings`",
    "body": "### Checked other resources\n\n- [x] I added a very descriptive title to this issue.\n- [x] I searched the LangChain documentation with the integrated search.\n- [x] I used the GitHub search to find a similar question and didn't find it.\n- [x] I am sure that this is a bug in LangChain rather than my code.\n- [x] The bug is not resolved by updating to the latest stable version of LangChain (or the specific integration package).\n\n### Example Code\n\nThere are no options to set parameters such as max tokens in OllamaEmbeddings imported from langchain_ollama.\nRunning the following code yields errors:\n\n```python\nfrom langchain_ollama import OllamaEmbeddings\n\nembedding_client = OllamaEmbeddings(\n    base_url=\"http://localhost:11434\",\n    model=\"nomic-embed-text\",\n    num_ctx=6144,\n)\n```\n\nPreviously, these were available:\n```python\nfrom langchain_community.embeddings import OllamaEmbeddings\n\nembedding_client = OllamaEmbeddings(\n    base_url=\"http://localhost:11434\",\n    model=\"nomic-embed-text\",\n    num_ctx=6144,\n)\n```\n\n### Error Message and Stack Trace (if applicable)\n\nIf the user tries to set the configurations like before, the following trace occurs:\n```\n(langchain) root@50aff4a67851:/workspace/libs/partners/ollama# python /workspace/libs/community/test.py \nTraceback (most recent call last):\n  File \"/workspace/libs/community/test.py\", line 9, in <module>\n    ollama_emb = OllamaEmbeddings(\n  File \"/root/miniconda3/envs/langchain/lib/python3.9/site-packages/pydantic/main.py\", line 214, in __init__\n    validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)\npydantic_core._pydantic_core.ValidationError: 1 validation error for OllamaEmbeddings\nnum_ctx\n  Extra inputs are not permitted [type=extra_forbidden, input_value=1024, input_type=int]\n    For further information visit https://errors.pydantic.dev/2.10/v/extra_forbidden\n```\n\n_**Note:** This error occurs because this functionality was removed but it is much needed._\n\n### Description\n\nI am trying to use `OllamaEmbeddings` imported from `langchain_ollama`. The issue arises that options that were previously available are (such as `num_ctx`) not available now.\n\n### Discussed in https://github.com/langchain-ai/langchain/discussions/29113\n\n<div type='discussions-op-text'>\n\n<sup>Originally posted by **Marsman1996** January  9, 2025</sup>\nFor `OllamaEmbeddings` from `langchain_community.embeddings`, I can use the following code to set max tokens:\n```python\nembedding_client = OllamaEmbeddings(\n    base_url=f\"http://localhost:11434\",\n    model=\"nomic-embed-text\",\n    num_ctx=6144,\n)\n```\n\nBut how to set max tokens for `OllamaEmbeddings` from `langchain_ollama.embeddings`?</div>\n\n### System Info\n\nSystem Information\n------------------\n> OS:  Linux\n> OS Version:  #41~22.04.2-Ubuntu SMP PREEMPT_DYNAMIC Mon Jun  3 11:32:55 UTC 2\n> Python Version:  3.9.21 (main, Dec 11 2024, 16:24:11) \n[GCC 11.2.0]\n\nPackage Information\n-------------------\n> langchain_core: 0.3.30\n> langchain: 0.3.14\n> langchain_community: 0.3.14\n> langsmith: 0.2.3\n> langchain_ollama: 0.2.2\n> langchain_tests: 0.3.8\n> langchain_text_splitters: 0.3.5\n\nOptional packages not installed\n-------------------------------\n> langserve\n\nOther Dependencies\n------------------\n> aiohttp: 3.11.11\n> async-timeout: 4.0.3\n> dataclasses-json: 0.6.7\n> httpx: 0.27.2\n> httpx-sse: 0.4.0\n> jsonpatch: 1.33\n> langsmith-pyo3: Installed. No version info available.\n> numpy: 1.26.4\n> ollama: 0.4.4\n> orjson: 3.10.12\n> packaging: 24.2\n> pydantic: 2.10.4\n> pydantic-settings: 2.7.0\n> pytest: 7.4.4\n> pytest-asyncio: 0.23.8\n> pytest-socket: 0.7.0\n> PyYAML: 6.0.2\n> requests: 2.32.3\n> requests-toolbelt: 1.0.0\n> SQLAlchemy: 2.0.36\n> syrupy: 4.8.0\n> tenacity: 9.0.0\n> typing-extensions: 4.12.2",
    "state": "closed",
    "created_at": "2025-01-19T04:54:51+00:00",
    "closed_at": "2025-01-20T18:00:42+00:00",
    "updated_at": "2025-01-20T18:00:44+00:00",
    "author": "SyedBaqarAbbas",
    "author_type": "User",
    "comments_count": 1,
    "reactions_count": 0,
    "labels": "ðŸ¤–:bug",
    "milestone": null,
    "closed_by": "SyedBaqarAbbas",
    "resolution_time_hours": 37.0975,
    "first_comments": [
      {
        "author": "SyedBaqarAbbas",
        "created_at": "2025-01-20T18:00:42+00:00",
        "body": "Issue was solved in #29296 "
      }
    ],
    "url": "https://github.com/langchain-ai/langchain/issues/29294"
  },
  {
    "repository": "langchain-ai/langchain",
    "issue_number": 29283,
    "title": "DeepSeek V3 Does Not Respect max_tokens Parameter in LangChain with ChatOpenAI()",
    "body": "### Checked other resources\n\n- [x] I added a very descriptive title to this issue.\n- [x] I searched the LangChain documentation with the integrated search.\n- [x] I used the GitHub search to find a similar question and didn't find it.\n- [x] I am sure that this is a bug in LangChain rather than my code.\n- [x] The bug is not resolved by updating to the latest stable version of LangChain (or the specific integration package).\n\n### Example Code\n\n**Steps to Reproduce:**\n\n1. Install the required libraries:\n   ```python\n   !pip install -qU langchain-openai\n   !pip install -qU langchain_community\n   !pip install -qU langchain_experimental\n   !pip install -qU langgraph\n   ```\n\n2. Initialize the model with a `max_tokens` limit:\n   ```python\n   from langchain_openai import ChatOpenAI\n   from kaggle_secrets import UserSecretsClient\n\n   llm_api_key = UserSecretsClient().get_secret(\"api-key-deepseek\")\n\n   # Initialize the model with a max_tokens limit (30 tokens)\n   model = ChatOpenAI(model=\"deepseek-chat\", temperature=1.0, openai_api_key=llm_api_key, openai_api_base='https://api.deepseek.com', max_tokens=30)\n   ```\n\n3. Send a message to the model:\n   ```python\n   response = model.invoke(\"Explain the theory of relativity in simple terms.\")\n   print(response.content)\n   ```\n\n### Error Message and Stack Trace (if applicable)\n\n**Actual Behavior:**  \n\nThe model generates a response significantly longer than 30 tokens, ignoring the `max_tokens` limit. For example:\n```\nThe theory of relativity is a fundamental concept in physics, primarily developed by Albert Einstein. It consists of two parts: the Special Theory of Relativity and the General Theory of Relativity. Here's a simplified explanation of both:\n\n### Special Theory of Relativity (1905)\n1. **Speed of Light**: The speed of light in a vacuum is always the same, no matter how fast an observer is moving. This speed is approximately 299,792 kilometers per second (about 186,282 miles per second).\n2. **Relativity of Simultaneity**: Events that appear simultaneous to one observer may not be simultaneous to another observer moving at a different speed.\n3. **Time Dilation**: Time passes more slowly for an object in motion compared to one at rest. This effect becomes noticeable at speeds close to the speed of light.\n4. **Length Contraction**: Objects in motion contract in the direction of motion as their speed approaches the speed of light.\n5. **Mass-Energy Equivalence**: Energy (E) and mass (m) are interchangeable, as described by the famous equation \\( E = mc^2 \\), where \\( c \\) is the speed of light.\n\n### General Theory of Relativity (1915)\n1. **Gravity as Curvature**: Gravity is not a force in the traditional sense but rather the result of the curvature of spacetime caused by mass and energy. Massive objects like planets and stars warp the fabric of spacetime, and this curvature affects the motion of objects.\n2. **Equivalence Principle**: The effects of gravity are locally indistinguishable from acceleration. For example, if you were in a closed elevator, you wouldn't be able to tell if you were being pulled by gravity or if the elevator were accelerating upward.\n3. **Gravitational Time Dilation**: Time runs slower in stronger gravitational fields. For instance, a clock on the surface of the Earth ticks more slowly than one in space.\n4. **Light Bending**: Light bends when it passes near a massive object, a phenomenon known as gravitational lensing.\n\n### Practical Implications\n- **GPS Systems**: The precise timing required for GPS satellites must account for both special and general relativistic effects to provide accurate location data.\n- **Black Holes**: General relativity predicts the existence of black holes, regions of spacetime where gravity is so strong that nothing, not even light, can escape.\n- **Cosmology**: The theory underpins our understanding of the universe's expansion and the behavior of galaxies and cosmic structures.\n\nIn essence, the theory of relativity revolutionized our understanding of space, time, and gravity, showing that they are interwoven in a four-dimensional fabric called spacetime.\n```\n\n### Description\n\n**Description:**  \n\nWhen using `ChatOpenAI()` with DeepSeek V3 in LangChain, the `max_tokens` parameter does not effectively limit the length of the model's output. Despite setting `max_tokens=30`, the model generates a response significantly longer than the specified limit. This behavior prevents users from controlling the response length, which is critical for applications requiring concise outputs.\n\n**Expected Behavior:**  \n\nThe model should generate a response containing no more than 30 tokens, as specified by the `max_tokens` parameter.\n\n### System Info\n\n**Environment:**\n\n- Python 3.x (Kaggle Notebook)\n- Libraries: `langchain-openai`, `langchain-community`, `langchain_experimental`, `langgraph`\n- Model: `deepseek-chat` (via DeepSeek API)",
    "state": "closed",
    "created_at": "2025-01-18T06:42:55+00:00",
    "closed_at": "2025-01-18T21:16:56+00:00",
    "updated_at": "2025-01-18T21:16:58+00:00",
    "author": "ksmooi",
    "author_type": "User",
    "comments_count": 1,
    "reactions_count": 0,
    "labels": "ðŸ¤–:bug,investigate",
    "milestone": null,
    "closed_by": "ccurme",
    "resolution_time_hours": 14.566944444444445,
    "first_comments": [
      {
        "author": "ccurme",
        "created_at": "2025-01-18T21:16:57+00:00",
        "body": "`ChatOpenAI` is intended to support the OpenAI API. They have [deprecated max_tokens](https://platform.openai.com/docs/api-reference/chat/create#chat-create-max_tokens). Try using `BaseChatOpenAI`, which accommodates many APIs that are similar to OpenAI. It still supports max_tokens.\n\n```python\nfrom langchain_openai.chat_models.base import BaseChatOpenAI\n\nllm = BaseChatOpenAI(...)\n```"
      }
    ],
    "url": "https://github.com/langchain-ai/langchain/issues/29283"
  },
  {
    "repository": "langchain-ai/langchain",
    "issue_number": 29282,
    "title": "DeepSeek V3 Does Not Support Structured Output in LangChain with ChatOpenAI()",
    "body": "### Checked other resources\n\n- [x] I added a very descriptive title to this issue.\n- [x] I searched the LangChain documentation with the integrated search.\n- [x] I used the GitHub search to find a similar question and didn't find it.\n- [x] I am sure that this is a bug in LangChain rather than my code.\n- [x] The bug is not resolved by updating to the latest stable version of LangChain (or the specific integration package).\n\n### Example Code\n\n**Steps to Reproduce:**\n\n1. Install the required libraries:\n   ```python\n   !pip install -qU langchain-openai\n   !pip install -qU langchain_community\n   !pip install -qU langchain_experimental\n   !pip install -qU langgraph\n   ```\n\n2. Initialize the model and define a Pydantic model for structured output:\n   ```python\n   from langchain_openai import ChatOpenAI\n   from kaggle_secrets import UserSecretsClient\n   from pydantic import BaseModel, Field\n\n   llm_api_key = UserSecretsClient().get_secret(\"api-key-deepseek\")\n   model = ChatOpenAI(model=\"deepseek-chat\", temperature=0, openai_api_key=llm_api_key, openai_api_base='https://api.deepseek.com')\n\n   # Define a Pydantic model for structured output\n   class Person(BaseModel):\n       name: str = Field(description=\"The name of the person\")\n       age: int = Field(description=\"The age of the person\")\n       email: str = Field(description=\"The email address of the person\")\n\n   # Query the model (correct)\n   response = model.invoke(\"Extract the name, age, and email of John Doe, who is 30 years old and has the email john.doe@example.com.\")\n   print(response)\n   ```\n\n3. Use `with_structured_output()` to enforce the Pydantic model and query the model with structured output:\n   ```python\n   structured_model = model.with_structured_output(Person)\n\n   # Query the model with structured output (incorrect)\n   response = structured_model.invoke(\"Extract the name, age, and email of John Doe, who is 30 years old and has the email john.doe@example.com.\")\n   print(response)\n   ```\n\n### Error Message and Stack Trace (if applicable)\n\n**Actual Behavior:**  \n\nThe model throws an `UnprocessableEntityError` indicating that the `response_format` type `json_schema` is unavailable:\n```\ncontent='Here is the extracted information:\\n\\n- **Name**: John Doe  \\n- **Age**: 30  \\n- **Email**: john.doe@example.com' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 30, 'prompt_tokens': 33, 'total_tokens': 63, 'completion_tokens_details': None, 'prompt_tokens_details': None, 'prompt_cache_hit_tokens': 0, 'prompt_cache_miss_tokens': 33}, 'model_name': 'deepseek-chat', 'system_fingerprint': 'fp_3a5770e1b4', 'finish_reason': 'stop', 'logprobs': None} id='run-d078cad9-42a0-4be0-9e92-a593002a8606-0' usage_metadata={'input_tokens': 33, 'output_tokens': 30, 'total_tokens': 63, 'input_token_details': {}, 'output_token_details': {}}\n\n---------------------------------------------------------------------------\nUnprocessableEntityError                  Traceback (most recent call last)\n<ipython-input-14-83b3c0097ccc> in <cell line: 18>()\n     16 \n     17 # Query the model with structured output\n---> 18 response = structured_model.invoke(\"Extract the name, age, and email of John Doe, who is 30 years old and has the email john.doe@example.com.\")\n     19 print(response)\n\n/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/base.py in invoke(self, input, config, **kwargs)\n   3018                 context.run(_set_config_context, config)\n   3019                 if i == 0:\n-> 3020                     input = context.run(step.invoke, input, config, **kwargs)\n   3021                 else:\n   3022                     input = context.run(step.invoke, input, config)\n\n/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/base.py in invoke(self, input, config, **kwargs)\n   5350         **kwargs: Optional[Any],\n   5351     ) -> Output:\n-> 5352         return self.bound.invoke(\n   5353             input,\n   5354             self._merge_configs(config),\n\n/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py in invoke(self, input, config, stop, **kwargs)\n    284         return cast(\n    285             ChatGeneration,\n--> 286             self.generate_prompt(\n    287                 [self._convert_input(input)],\n    288                 stop=stop,\n\n/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py in generate_prompt(self, prompts, stop, callbacks, **kwargs)\n    784     ) -> LLMResult:\n    785         prompt_messages = [p.to_messages() for p in prompts]\n--> 786         return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n    787 \n    788     async def agenerate_prompt(\n\n/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py in generate(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\n    641                 if run_managers:\n    642                     run_managers[i].on_llm_error(e, response=LLMResult(generations=[]))\n--> 643                 raise e\n    644         flattened_outputs = [\n    645             LLMResult(generations=[res.generations], llm_output=res.llm_output)  # type: ignore[list-item]\n\n/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py in generate(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\n    631             try:\n    632                 results.append(\n--> 633                     self._generate_with_cache(\n    634                         m,\n    635                         stop=stop,\n\n/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py in _generate_with_cache(self, messages, stop, run_manager, **kwargs)\n    849         else:\n    850             if inspect.signature(self._generate).parameters.get(\"run_manager\"):\n--> 851                 result = self._generate(\n    852                     messages, stop=stop, run_manager=run_manager, **kwargs\n    853                 )\n\n/usr/local/lib/python3.10/dist-packages/langchain_openai/chat_models/base.py in _generate(self, messages, stop, run_manager, **kwargs)\n    771             payload.pop(\"stream\")\n    772             try:\n--> 773                 response = self.root_client.beta.chat.completions.parse(**payload)\n    774             except openai.BadRequestError as e:\n    775                 _handle_openai_bad_request(e)\n\n/usr/local/lib/python3.10/dist-packages/openai/resources/beta/chat/completions.py in parse(self, messages, model, audio, response_format, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, seed, service_tier, stop, store, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\n    158             )\n    159 \n--> 160         return self._post(\n    161             \"/chat/completions\",\n    162             body=maybe_transform(\n\n/usr/local/lib/python3.10/dist-packages/openai/_base_client.py in post(self, path, cast_to, body, options, files, stream, stream_cls)\n   1281             method=\"post\", url=path, json_data=body, files=to_httpx_files(files), **options\n   1282         )\n-> 1283         return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n   1284 \n   1285     def patch(\n\n/usr/local/lib/python3.10/dist-packages/openai/_base_client.py in request(self, cast_to, options, remaining_retries, stream, stream_cls)\n    958             retries_taken = 0\n    959 \n--> 960         return self._request(\n    961             cast_to=cast_to,\n    962             options=options,\n\n/usr/local/lib/python3.10/dist-packages/openai/_base_client.py in _request(self, cast_to, options, retries_taken, stream, stream_cls)\n   1062 \n   1063             log.debug(\"Re-raising status error\")\n-> 1064             raise self._make_status_error_from_response(err.response) from None\n   1065 \n   1066         return self._process_response(\n\nUnprocessableEntityError: Failed to deserialize the JSON body into the target type: response_format: response_format.type `json_schema` is unavailable now at line 1 column 626\n```\n\n### Description\n\n**Description:**  \n\nWhen using `ChatOpenAI()` with DeepSeek V3 in LangChain, the `with_structured_output()` method fails to enforce structured output formats (e.g., Pydantic models). The model returns an error indicating that the `response_format` type `json_schema` is unavailable. This prevents the use of structured output functionality, which is critical for applications requiring consistent and predictable data formats.\n\n\n**Expected Behavior:**  \n\nThe model should return a structured output in the format defined by the Pydantic model:\n```python\nPerson(name=\"John Doe\", age=30, email=\"john.doe@example.com\")\n```\n\n### System Info\n\n**Environment:**\n\n- Python 3.x (Kaggle Notebook)\n- Libraries: `langchain-openai`, `langchain-community`, `langchain_experimental`, `langgraph`\n- Model: `deepseek-chat` (via DeepSeek API)",
    "state": "closed",
    "created_at": "2025-01-18T06:39:49+00:00",
    "closed_at": "2025-01-18T21:15:17+00:00",
    "updated_at": "2025-01-22T20:17:15+00:00",
    "author": "ksmooi",
    "author_type": "User",
    "comments_count": 2,
    "reactions_count": 0,
    "labels": "ðŸ¤–:bug,â±­:  models,investigate",
    "milestone": null,
    "closed_by": "ccurme",
    "resolution_time_hours": 14.591111111111111,
    "first_comments": [
      {
        "author": "ccurme",
        "created_at": "2025-01-18T21:15:17+00:00",
        "body": "`ChatOpenAI` is intended to support the OpenAI API. Try using `BaseChatOpenAI`, which accommodates many APIs that are similar to OpenAI. It uses tool calling for structured output by default.\n\n```python\nfrom langchain_openai.chat_models.base import BaseChatOpenAI\n\nllm = BaseChatOpenAI()\n```"
      },
      {
        "author": "mohit-217",
        "created_at": "2025-01-22T20:17:14+00:00",
        "body": "Deepseek provided the sample script. Go to link\nhttps://cdn.deepseek.com/api-docs/deepseek_langchain.py"
      }
    ],
    "url": "https://github.com/langchain-ai/langchain/issues/29282"
  },
  {
    "repository": "langchain-ai/langchain",
    "issue_number": 29270,
    "title": "OpenAIAssistantRunnable Async Doesn't Work",
    "body": "### Checked other resources\n\n- [x] I added a very descriptive title to this issue.\n- [x] I searched the LangChain documentation with the integrated search.\n- [x] I used the GitHub search to find a similar question and didn't find it.\n- [x] I am sure that this is a bug in LangChain rather than my code.\n- [x] The bug is not resolved by updating to the latest stable version of LangChain (or the specific integration package).\n\n### Example Code\n\nThe following code:\n```\nimport asyncio\nfrom openai import AsyncAzureOpenAI\nfrom langchain.agents.openai_assistant import OpenAIAssistantRunnable\nfrom utils.read_config import GPT4_API_DEPLOYMENT, GPT4_API_BASE, API_TOKEN\nload_dotenv(find_dotenv(), override=True)\n\nasync_client = AsyncAzureOpenAI(\n    azure_endpoint=GPT4_API_BASE,\n    api_key=API_TOKEN,\n    api_version=\"2024-05-01-preview\"\n\n)\nasync def call_code_interpreter():\n    agent = await OpenAIAssistantRunnable.acreate_assistant(\n        name=\"chatgpt\",\n        instructions=\"You are a very careful thinker. Think step by step before answering queries.\",\n        model=GPT4_API_DEPLOYMENT,\n        tools=[{\"type\": \"code_interpreter\"}],\n        async_client=async_client\n    )\n    return agent\n\nasync def main():\n    agent = await call_code_interpreter()\n    output = await agent.ainvoke({\"content\": \"\"\"\n    Plot a simple line graph with the following data:\n    x = [1, 2, 3, 4, 5]\n    y = [2, 3, 5, 7, 11]\n    \"\"\"})\n    print(output)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n\n\n### Error Message and Stack Trace (if applicable)\n\n```\nFile ~/miniforge3/envs/python3.12/lib/python3.12/site-packages/openai/_client.py:110, in OpenAI.__init__(self, api_key, organization, project, base_url, websocket_base_url, timeout, max_retries, default_headers, default_query, http_client, _strict_response_validation)\n    108     api_key = os.environ.get(\"OPENAI_API_KEY\")\n    109 if api_key is None:\n--> 110     raise OpenAIError(\n    111         \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n    112     )\n    113 self.api_key = api_key\n    115 if organization is None:\n\nOpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n```\n\n### Description\n\nThe API Key is already added in the Async Client as well as env. \n\nThe exact same code, but for non-async works without issues.\n\n### System Info\n\nSystem Information\n------------------\n> OS:  Linux\n> OS Version:  #1 SMP Tue Nov 5 00:21:55 UTC 2024\n> Python Version:  3.12.0 | packaged by conda-forge | (main, Oct  3 2023, 08:43:22) [GCC 12.3.0]\n\nPackage Information\n-------------------\n> langchain_core: 0.3.29\n> langchain: 0.3.14\n> langchain_community: 0.3.14\n> langsmith: 0.2.10\n> langchain_openai: 0.3.0\n> langchain_postgres: 0.0.12\n> langchain_sdk: 0.1.5\n> langchain_text_splitters: 0.3.5\n> langchain_unstructured: 0.1.6\n> langgraph_sdk: 0.1.51\n\nOptional packages not installed\n-------------------------------\n> langserve\n\nOther Dependencies\n------------------\n> aiohttp: 3.11.11\n> async-timeout: Installed. No version info available.\n> dataclasses-json: 0.6.7\n> httpx: 0.28.1\n> httpx-sse: 0.4.0\n> jsonpatch: 1.33\n> langchain_core>=0.3.0: Installed. No version info available.\n> langsmith-pyo3: Installed. No version info available.\n> numpy: 1.26.4\n> onnxruntime: 1.19.2\n> openai: 1.59.7\n> orjson: 3.10.14\n> packaging: 23.2\n> pgvector: 0.2.5\n> psycopg: 3.2.3\n> psycopg-pool: 3.2.4\n> pydantic: 2.9.2\n> pydantic-settings: 2.7.1\n> PyYAML: 6.0.2\n> requests: 2.32.3\n> requests-toolbelt: 1.0.0\n> SQLAlchemy: 2.0.37\n> sqlalchemy: 2.0.37\n> tenacity: 9.0.0\n> tiktoken: 0.8.0\n> typing-extensions: 4.12.2\n> unstructured-client: 0.27.0\n> unstructured[all-docs]: Installed. No version info available.\n> zstandard: Installed. No version info available.",
    "state": "closed",
    "created_at": "2025-01-17T15:58:05+00:00",
    "closed_at": "2025-01-18T01:25:19+00:00",
    "updated_at": "2025-01-18T01:25:19+00:00",
    "author": "GhimBoon",
    "author_type": "User",
    "comments_count": 2,
    "reactions_count": 0,
    "labels": "ðŸ¤–:bug,investigate",
    "milestone": null,
    "closed_by": "GhimBoon",
    "resolution_time_hours": 9.453888888888889,
    "first_comments": [
      {
        "author": "keenborder786",
        "created_at": "2025-01-17T20:42:51+00:00",
        "body": "@GhimBoon I ran the exact same code you provided and it works, please make sure you are loading the correct environment variables."
      },
      {
        "author": "GhimBoon",
        "created_at": "2025-01-18T01:25:19+00:00",
        "body": "Closing issue to troubleshoot further. Thank you for the confirmation."
      }
    ],
    "url": "https://github.com/langchain-ai/langchain/issues/29270"
  },
  {
    "repository": "langchain-ai/langchain",
    "issue_number": 29251,
    "title": "DOC: typo in tool_results_pass_to_model.ipynb (how-to)",
    "body": "### URL\n\nhttps://python.langchain.com/docs/how_to/tool_results_pass_to_model/\n\n### Checklist\n\n- [x] I added a very descriptive title to this issue.\n- [x] I included a link to the documentation page I am referring to (if applicable).\n\n### Issue with current documentation:\n\nIn documentation, word `cals` should be `calls`.\n>This guide will demonstrate how to use those tool **cals** to actually call a function and properly pass the results back to the model.\n\n\n\n### Idea or request for content:\n\n_No response_",
    "state": "closed",
    "created_at": "2025-01-16T12:39:47+00:00",
    "closed_at": "2025-01-16T16:05:30+00:00",
    "updated_at": "2025-01-16T16:05:30+00:00",
    "author": "Q-Bug4",
    "author_type": "User",
    "comments_count": 0,
    "reactions_count": 0,
    "labels": "ðŸ¤–:docs",
    "milestone": null,
    "closed_by": "ccurme",
    "resolution_time_hours": 3.428611111111111,
    "first_comments": [],
    "url": "https://github.com/langchain-ai/langchain/issues/29251"
  },
  {
    "repository": "langchain-ai/langchain",
    "issue_number": 29235,
    "title": "OpenAIEmbeddings - Pydantic dependency bug",
    "body": "### Checked other resources\n\n- [x] I added a very descriptive title to this issue.\n- [x] I searched the LangChain documentation with the integrated search.\n- [x] I used the GitHub search to find a similar question and didn't find it.\n- [x] I am sure that this is a bug in LangChain rather than my code.\n- [x] The bug is not resolved by updating to the latest stable version of LangChain (or the specific integration package).\n\n### Example Code\n\nfrom langchain_openai import OpenAIEmbeddings \n\n\n\n### Error Message and Stack Trace (if applicable)\n\n(base) zanedash@Zanes-MacBook-Air entfi % conda create -n langchain_setup\n python==3.12.3\nChannels:\n - defaults\nPlatform: osx-arm64\nCollecting package metadata (repodata.json): done\nSolving environment: done\n\n## Package Plan ##\n\n  environment location: /opt/miniconda3/envs/langchain_setup\n\n  added / updated specs:\n    - python==3.12.3\n\n\nThe following NEW packages will be INSTALLED:\n\n  bzip2              pkgs/main/osx-arm64::bzip2-1.0.8-h80987f9_6 \n  ca-certificates    pkgs/main/osx-arm64::ca-certificates-2024.12.31-hca03da5_0 \n  expat              pkgs/main/osx-arm64::expat-2.6.4-h313beb8_0 \n  libcxx             pkgs/main/osx-arm64::libcxx-14.0.6-h848a8c0_0 \n  libffi             pkgs/main/osx-arm64::libffi-3.4.4-hca03da5_1 \n  ncurses            pkgs/main/osx-arm64::ncurses-6.4-h313beb8_0 \n  openssl            pkgs/main/osx-arm64::openssl-3.0.15-h80987f9_0 \n  pip                pkgs/main/osx-arm64::pip-24.2-py312hca03da5_0 \n  python             pkgs/main/osx-arm64::python-3.12.3-h99e199e_1 \n  readline           pkgs/main/osx-arm64::readline-8.2-h1a28f6b_0 \n  setuptools         pkgs/main/osx-arm64::setuptools-75.1.0-py312hca03da5_0 \n  sqlite             pkgs/main/osx-arm64::sqlite-3.45.3-h80987f9_0 \n  tk                 pkgs/main/osx-arm64::tk-8.6.14-h6ba3021_0 \n  tzdata             pkgs/main/noarch::tzdata-2024b-h04d1e81_0 \n  wheel              pkgs/main/osx-arm64::wheel-0.44.0-py312hca03da5_0 \n  xz                 pkgs/main/osx-arm64::xz-5.4.6-h80987f9_1 \n  zlib               pkgs/main/osx-arm64::zlib-1.2.13-h18a0788_1 \n\n\nProceed ([y]/n)? y\n\n\nDownloading and Extracting Packages:\n\nPreparing transaction: done\nVerifying transaction: done\nExecuting transaction: done\n#\n# To activate this environment, use\n#\n#     $ conda activate langchain_setup\n#\n# To deactivate an active environment, use\n#\n#     $ conda deactivate\n\n(base) zanedash@Zanes-MacBook-Air entfi % conda activate langchain_setup\n(langchain_setup) zanedash@Zanes-MacBook-Air entfi % pip install uv\nCollecting uv\n  Downloading uv-0.5.18-py3-none-macosx_11_0_arm64.whl.metadata (11 kB)\nDownloading uv-0.5.18-py3-none-macosx_11_0_arm64.whl (14.0 MB)\n   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 14.0/14.0 MB 35.9 MB/s eta 0:00:00\nInstalling collected packages: uv\nSuccessfully installed uv-0.5.18\n(langchain_setup) zanedash@Zanes-MacBook-Air entfi % uv pip install langchain\nUsing Python 3.12.3 environment at: /opt/miniconda3/envs/langchain_setup\nResolved 35 packages in 550ms\nPrepared 19 packages in 413ms\nInstalled 35 packages in 69ms\n + aiohappyeyeballs==2.4.4\n + aiohttp==3.11.11\n + aiosignal==1.3.2\n + annotated-types==0.7.0\n + anyio==4.8.0\n + attrs==24.3.0\n + certifi==2024.12.14\n + charset-normalizer==3.4.1\n + frozenlist==1.5.0\n + h11==0.14.0\n + httpcore==1.0.7\n + httpx==0.28.1\n + idna==3.10\n + jsonpatch==1.33\n + jsonpointer==3.0.0\n + langchain==0.3.14\n + langchain-core==0.3.29\n + langchain-text-splitters==0.3.5\n + langsmith==0.2.10\n + multidict==6.1.0\n + numpy==2.2.1\n + orjson==3.10.14\n + packaging==24.2\n + propcache==0.2.1\n + pydantic==2.10.5\n + pydantic-core==2.27.2\n + pyyaml==6.0.2\n + requests==2.32.3\n + requests-toolbelt==1.0.0\n + sniffio==1.3.1\n + sqlalchemy==2.0.37\n + tenacity==9.0.0\n + typing-extensions==4.12.2\n + urllib3==2.3.0\n + yarl==1.18.3\n(langchain_setup) zanedash@Zanes-MacBook-Air entfi % uv pip install langchain-openai\nUsing Python 3.12.3 environment at: /opt/miniconda3/envs/langchain_setup\nResolved 30 packages in 295ms\nPrepared 5 packages in 114ms\nInstalled 7 packages in 9ms\n + distro==1.9.0\n + jiter==0.8.2\n + langchain-openai==0.3.0\n + openai==1.59.7\n + regex==2024.11.6\n + tiktoken==0.8.0\n + tqdm==4.67.1\n(langchain_setup) zanedash@Zanes-MacBook-Air entfi % cd platform/backend\n(langchain_setup) zanedash@Zanes-MacBook-Air backend % python testfile.py\n\n/opt/miniconda3/envs/workflow/lib/python3.12/site-packages/langchain_openai/chat_models/__init__.py:1: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n\nFor example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\nwith: `from pydantic import BaseModel`\nor the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet.  from pydantic.v1 import BaseModel\n\n  from langchain_openai.chat_models.azure import AzureChatOpenAI\n/opt/miniconda3/envs/workflow/lib/python3.12/site-packages/pydantic/_internal/_config.py:345: UserWarning: Valid config keys have changed in V2:\n* 'allow_population_by_field_name' has been renamed to 'populate_by_name'\n  warnings.warn(message, UserWarning)\nTraceback (most recent call last):\n  File \"/Users/zanedash/Documents/GitHub/entfi/platform/backend/testfile.py\", line 1, in <module>\n    from langchain_openai.embeddings import OpenAIEmbeddings \n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/miniconda3/envs/workflow/lib/python3.12/site-packages/langchain_openai/__init__.py\", line 1, in <module>\n    from langchain_openai.chat_models import AzureChatOpenAI, ChatOpenAI\n  File \"/opt/miniconda3/envs/workflow/lib/python3.12/site-packages/langchain_openai/chat_models/__init__.py\", line 1, in <module>\n    from langchain_openai.chat_models.azure import AzureChatOpenAI\n  File \"/opt/miniconda3/envs/workflow/lib/python3.12/site-packages/langchain_openai/chat_models/azure.py\", line 40, in <module>\n    from langchain_openai.chat_models.base import BaseChatOpenAI\n  File \"/opt/miniconda3/envs/workflow/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 297, in <module>\n    class BaseChatOpenAI(BaseChatModel):\n  File \"/opt/miniconda3/envs/workflow/lib/python3.12/site-packages/pydantic/_internal/_model_construction.py\", line 224, in __new__\n    complete_model_class(\n  File \"/opt/miniconda3/envs/workflow/lib/python3.12/site-packages/pydantic/_internal/_model_construction.py\", line 602, in complete_model_class\n    schema = cls.__get_pydantic_core_schema__(cls, handler)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/miniconda3/envs/workflow/lib/python3.12/site-packages/pydantic/main.py\", line 702, in __get_pydantic_core_schema__\n    return handler(source)\n           ^^^^^^^^^^^^^^^\n  File \"/opt/miniconda3/envs/workflow/lib/python3.12/site-packages/pydantic/_internal/_schema_generation_shared.py\", line 84, in __call__\n    schema = self._handler(source_type)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/miniconda3/envs/workflow/lib/python3.12/site-packages/pydantic/_internal/_generate_schema.py\", line 610, in generate_schema\n    schema = self._generate_schema_inner(obj)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/miniconda3/envs/workflow/lib/python3.12/site-packages/pydantic/_internal/_generate_schema.py\", line 879, in _generate_schema_inner\n    return self._model_schema(obj)\n           ^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/miniconda3/envs/workflow/lib/python3.12/site-packages/pydantic/_internal/_generate_schema.py\", line 691, in _model_schema\n    {k: self._generate_md_field_schema(k, v, decorators) for k, v in fields.items()},\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/miniconda3/envs/workflow/lib/python3.12/site-packages/pydantic/_internal/_generate_schema.py\", line 1071, in _generate_md_field_schema\n    common_field = self._common_field_schema(name, field_info, decorators)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/miniconda3/envs/workflow/lib/python3.12/site-packages/pydantic/_internal/_generate_schema.py\", line 1263, in _common_field_schema\n    schema = self._apply_annotations(\n             ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/miniconda3/envs/workflow/lib/python3.12/site-packages/pydantic/_internal/_generate_schema.py\", line 2056, in _apply_annotations\n    schema = get_inner_schema(source_type)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/miniconda3/envs/workflow/lib/python3.12/site-packages/pydantic/_internal/_schema_generation_shared.py\", line 84, in __call__\n    schema = self._handler(source_type)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/miniconda3/envs/workflow/lib/python3.12/site-packages/pydantic/_internal/_generate_schema.py\", line 2037, in inner_handler\n    schema = self._generate_schema_inner(obj)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/miniconda3/envs/workflow/lib/python3.12/site-packages/pydantic/_internal/_generate_schema.py\", line 884, in _generate_schema_inner\n    return self.match_type(obj)\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/miniconda3/envs/workflow/lib/python3.12/site-packages/pydantic/_internal/_generate_schema.py\", line 986, in match_type\n    return self._match_generic_type(obj, origin)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/miniconda3/envs/workflow/lib/python3.12/site-packages/pydantic/_internal/_generate_schema.py\", line 1014, in _match_generic_type\n    return self._union_schema(obj)\n           ^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/miniconda3/envs/workflow/lib/python3.12/site-packages/pydantic/_internal/_generate_schema.py\", line 1325, in _union_schema\n    choices.append(self.generate_schema(arg))\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/miniconda3/envs/workflow/lib/python3.12/site-packages/pydantic/_internal/_generate_schema.py\", line 612, in generate_schema\n    metadata_js_function = _extract_get_pydantic_json_schema(obj, schema)\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/miniconda3/envs/workflow/lib/python3.12/site-packages/pydantic/_internal/_generate_schema.py\", line 2395, in _extract_get_pydantic_json_schema\n    raise PydanticUserError(\npydantic.errors.PydanticUserError: The `__modify_schema__` method is not supported in Pydantic v2. Use `__get_pydantic_json_schema__` instead in class `SecretStr`.\n\n### Description\n\nWith fresh conda environment, installed langchain and langchain-openai and get a Pydantic compatibility error when try to import OpenAI Embeddings\n\nPlease fix ASAP, thank you\n\n### System Info\n\naiohappyeyeballs==2.4.4\naiohttp==3.11.11\naiosignal==1.3.2\nannotated-types==0.7.0\nanyio==4.8.0\nattrs==24.3.0\ncertifi==2024.12.14\ncharset-normalizer==3.4.1\ndistro==1.9.0\nfrozenlist==1.5.0\nh11==0.14.0\nhttpcore==1.0.7\nhttpx==0.28.1\nidna==3.10\njiter==0.8.2\njsonpatch==1.33\njsonpointer==3.0.0\nlangchain==0.3.14\nlangchain-core==0.3.29\nlangchain-openai==0.3.0\nlangchain-text-splitters==0.3.5\nlangsmith==0.1.147\nmultidict==6.1.0\nnumpy==2.2.1\nopenai==1.59.7\norjson==3.10.14\npackaging==24.2\npropcache==0.2.1\npydantic==2.10.5\npydantic_core==2.27.2\nPyYAML==6.0.2\nregex==2024.11.6\nrequests==2.32.3\nrequests-toolbelt==1.0.0\nsetuptools==75.1.0\nsniffio==1.3.1\nSQLAlchemy==2.0.37\ntenacity==8.5.0\ntiktoken==0.8.0\ntqdm==4.67.1\ntyping_extensions==4.12.2\nurllib3==2.3.0\nuv==0.5.18\nwheel==0.44.0\nyarl==1.18.3",
    "state": "closed",
    "created_at": "2025-01-15T16:37:39+00:00",
    "closed_at": "2025-01-15T19:59:48+00:00",
    "updated_at": "2025-01-15T19:59:49+00:00",
    "author": "ZaneDash",
    "author_type": "User",
    "comments_count": 2,
    "reactions_count": 0,
    "labels": "ðŸ¤–:bug,investigate",
    "milestone": null,
    "closed_by": "ZaneDash",
    "resolution_time_hours": 3.3691666666666666,
    "first_comments": [
      {
        "author": "SyedBaqarAbbas",
        "created_at": "2025-01-15T18:34:43+00:00",
        "body": "Hello @ZaneDash,\n\nI ran the following code and it worked without any issues:\n```python\nfrom langchain_openai import OpenAIEmbeddings\n```\n\nI am unable to replicate the issue. I just checked and the involved libraries (langchain, pydantic, langchain_openai, etc) all have the same version as yours. Could you share an elaborate way to reproduce the error since your stack trace contains initialization to other modules (`from langchain_openai.chat_models.azure import AzureChatOpenAI`)?\n\n**My set-up details from `python -m langchain_core.sys_info` are as below:**\n\nSystem Information\n------------------\n> OS:  Linux\n> OS Version:  #52~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Mon Dec  9 15:00:52 UTC 2\n> Python Version:  3.12.3 | packaged by Anaconda, Inc. | (main, May  6 2024, 19:46:43) [GCC 11.2.0]\n\nPackage Information\n-------------------\n> langchain_core: 0.3.29\n> langchain: 0.3.14\n> langsmith: 0.1.147\n> langchain_openai: 0.3.0\n> langchain_text_splitters: 0.3.5\n\nOptional packages not installed\n-------------------------------\n> langserve\n\nOther Dependencies\n------------------\n> aiohttp: 3.11.11\n> async-timeout: Installed. No version info available.\n> httpx: 0.28.1\n> jsonpatch: 1.33\n> langsmith-pyo3: Installed. No version info available.\n> numpy: 2.2.1\n> openai: 1.59.7\n> orjson: 3.10.14\n> packaging: 24.2\n> pydantic: 2.10.5\n> PyYAML: 6.0.2\n> requests: 2.32.3\n> requests-toolbelt: 1.0.0\n> SQLAlchemy: 2.0.37\n> tenacity: 8.5.0\n> tiktoken: 0.8.0\n> typing-extensions: 4.12.2"
      },
      {
        "author": "ZaneDash",
        "created_at": "2025-01-15T19:59:48+00:00",
        "body": "Had incorrect Python alias, closing"
      }
    ],
    "url": "https://github.com/langchain-ai/langchain/issues/29235"
  },
  {
    "repository": "langchain-ai/langchain",
    "issue_number": 29227,
    "title": "SQLDatabase in langchain_community.utilities raises Exception due to type mismatch when connecting with Snowflake",
    "body": "### Checked other resources\n\n- [x] I added a very descriptive title to this issue.\n- [x] I searched the LangChain documentation with the integrated search.\n- [x] I used the GitHub search to find a similar question and didn't find it.\n- [x] I am sure that this is a bug in LangChain rather than my code.\n- [x] The bug is not resolved by updating to the latest stable version of LangChain (or the specific integration package).\n\n### Example Code\n\nThe following code can be used to reproduce the error:\n\n\n```python\n\nfrom langchain_community.utilities import SQLDatabase\n\n# Fill in snowflake credentials\nSNOWFLAKE_ACCOUNT = \"\"\nSNOWFLAKE_USERNAME = \"\"\nSNOWFLAKE_PASSWORD = \"\"\nSNOWFLAKE_WAREHOUSE = \"\"\nSNOWFLAKE_DATABASE = \"\"\nSNOWFLAKE_SCHEMA = \"\"\nSNOWFLAKE_ROLE = \"\"  # Optional field\n\n# Build the connection string\nconnection_string = (\n    f\"snowflake://{SNOWFLAKE_USERNAME}:{SNOWFLAKE_PASSWORD}@{SNOWFLAKE_ACCOUNT}/\"\n    f\"{SNOWFLAKE_DATABASE}/{SNOWFLAKE_SCHEMA}?warehouse={SNOWFLAKE_WAREHOUSE}\"\n)\n\n# Add role if provided\nif SNOWFLAKE_ROLE:\n    connection_string += f\"&role={SNOWFLAKE_ROLE}\"\n\n# Connect to database using langchain's wrapper\nsql_db = SQLDatabase.from_uri(database_uri=connection_string)\n```\n\n### Error Message and Stack Trace (if applicable)\n\n### Error Description\nI encountered the following error:\n\n```\nTypeError: unsupported operand type(s) for +: 'dict_keys' and 'list'\n```\n\n### Relevant Library Versions\n- **snowflake-sqlalchemy**: 1.7.2  \n- **snowflake-connector-python**: 3.12.4  \n- **sqlalchemy**: 2.0.20  \n- **langchain_community**: 0.3.14  \n\n### Description\n\nI am trying to use the langchain's wrapper for sqlachemy (langchain_community.utilities.SQLDatabase) to connect to a snowflake database.\n\nWhile connecting, instead of a successfully connecting, I am facing the error `TypeError: unsupported operand type(s) for +: 'dict_keys' and 'list'`\n\n### System Info\n\nSystem Information\n------------------\n> OS:  Linux\n> OS Version:  #52~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Mon Dec  9 15:00:52 UTC 2\n> Python Version:  3.11.10 (main, Oct  3 2024, 07:29:13) [GCC 11.2.0]\n\nPackage Information\n-------------------\n> langchain_core: 0.3.29\n> langchain: 0.3.14\n> langchain_community: 0.3.14\n> langsmith: 0.1.125\n> langchain_google_genai: 2.0.7\n> langchain_openai: 0.2.2\n> langchain_text_splitters: 0.3.4\n> langchainhub: 0.1.21\n> langgraph_sdk: 0.1.48\n\nOptional packages not installed\n-------------------------------\n> langserve\n\nOther Dependencies\n------------------\n> aiohttp: 3.10.5\n> async-timeout: 4.0.3\n> dataclasses-json: 0.6.7\n> filetype: 1.2.0\n> google-generativeai: 0.8.3\n> httpx: 0.27.0\n> httpx-sse: 0.4.0\n> jsonpatch: 1.33\n> numpy: 1.26.4\n> openai: 1.42.0\n> orjson: 3.10.7\n> packaging: 24.1\n> pydantic: 2.8.2\n> pydantic-settings: 2.4.0\n> PyYAML: 6.0.2\n> requests: 2.32.3\n> SQLAlchemy: 2.0.20\n> tenacity: 8.5.0\n> tiktoken: 0.7.0\n> types-requests: 2.32.0.20240914\n> typing-extensions: 4.12.2",
    "state": "closed",
    "created_at": "2025-01-15T08:37:18+00:00",
    "closed_at": "2025-01-15T16:52:43+00:00",
    "updated_at": "2025-01-15T16:52:45+00:00",
    "author": "SyedBaqarAbbas",
    "author_type": "User",
    "comments_count": 1,
    "reactions_count": 0,
    "labels": "ðŸ¤–:bug",
    "milestone": null,
    "closed_by": "SyedBaqarAbbas",
    "resolution_time_hours": 8.256944444444445,
    "first_comments": [
      {
        "author": "SyedBaqarAbbas",
        "created_at": "2025-01-15T16:52:43+00:00",
        "body": "Fixed in #29229 "
      }
    ],
    "url": "https://github.com/langchain-ai/langchain/issues/29227"
  },
  {
    "repository": "langchain-ai/langchain",
    "issue_number": 29156,
    "title": "RankLLMRerank usage throws an error when used GPT (not only) when rank-llm version is > 0.12.8 ",
    "body": "### Checked other resources\r\n\r\n- [X] I added a very descriptive title to this issue.\r\n- [X] I searched the LangChain documentation with the integrated search.\r\n- [X] I used the GitHub search to find a similar question and didn't find it.\r\n- [X] I am sure that this is a bug in LangChain rather than my code.\r\n- [X] The bug is not resolved by updating to the latest stable version of LangChain (or the specific integration package).\r\n\r\n### Soluton\r\n\r\nIssue is resolved with the PR: #29154 \r\n\r\n### Example Code\r\n\r\nCode from the [tutorial](https://python.langchain.com/docs/integrations/document_transformers/rankllm-reranker/) and rank-llm installed with version > 0.12.8:\r\n\r\n```\r\nfrom langchain.retrievers.contextual_compression import ContextualCompressionRetriever\r\nfrom langchain_community.document_compressors.rankllm_rerank import RankLLMRerank\r\n\r\ncompressor = RankLLMRerank(top_n=3, model=\"gpt\", gpt_model=\"gpt-3.5-turbo\")\r\ncompression_retriever = ContextualCompressionRetriever(\r\n    base_compressor=compressor, base_retriever=retriever\r\n)\r\n```\r\n\r\n### Error Message and Stack Trace\r\n\r\n---------------------------------------------------------------------------\r\nModuleNotFoundError                       Traceback (most recent call last)\r\nFile ~/Library/Caches/pypoetry/virtualenvs/nlp-rag-NMztfdt9-py3.10/lib/python3.10/site-packages/langchain_community/document_compressors/rankllm_rerank.py:69, in RankLLMRerank.validate_environment(cls, values)\r\n     [68](https://file+.vscode-resource.vscode-cdn.net/.../notebooks/~/Library/Caches/pypoetry/virtualenvs/nlp-rag-NMztfdt9-py3.10/lib/python3.10/site-packages/langchain_community/document_compressors/rankllm_rerank.py:68) elif model_enum == ModelType.GPT:\r\n---> [69](https://file+.vscode-resource.vscode-cdn.net/Users/.../notebooks/~/Library/Caches/pypoetry/virtualenvs/nlp-rag-NMztfdt9-py3.10/lib/python3.10/site-packages/langchain_community/document_compressors/rankllm_rerank.py:69)     from rank_llm.rerank.rank_gpt import SafeOpenai\r\n     [70](https://file+.vscode-resource.vscode-cdn.net/.../notebooks/~/Library/Caches/pypoetry/virtualenvs/nlp-rag-NMztfdt9-py3.10/lib/python3.10/site-packages/langchain_community/document_compressors/rankllm_rerank.py:70)     from rank_llm.rerank.reranker import Reranker\r\n\r\nModuleNotFoundError: No module named '**rank_llm.rerank.rank_gpt**'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nImportError                               Traceback (most recent call last)\r\nCell In[4], [line 8](vscode-notebook-cell:?execution_count=4&line=8)\r\n      [3](vscode-notebook-cell:?execution_count=4&line=3) from core.retrieval import retriever_rank_gpt\r\n      [5](vscode-notebook-cell:?execution_count=4&line=5) import rank_llm\r\n      [7](vscode-notebook-cell:?execution_count=4&line=7) retrieval_chain = query_translation(prompt_template=multi_query_prompt_template, retriever=\r\n----> [8](vscode-notebook-cell:?execution_count=4&line=8) retriever_rank_gpt(retriever=retriever)\r\n      [9](vscode-notebook-cell:?execution_count=4&line=9)                                     )\r\n     [11](vscode-notebook-cell:?execution_count=4&line=11) docs = retrieval_chain.invoke({\"question\": \"What is task decomposition for LLM agents?\"})\r\n     [13](vscode-notebook-cell:?execution_count=4&line=13) pretty_print_docs(docs)\r\n\r\nFile ~/.../src/core/retrieval.py:55, in retriever_rank_gpt(retriever)\r\n     [52](https://file+.vscode-resource.vscode-cdn.net/.../notebooks/~/.../src/core/retrieval.py:52) def retriever_rank_gpt(\r\n     [53](https://file+.vscode-resource.vscode-cdn.net/.../notebooks/~/.../src/core/retrieval.py:53)     retriever: VectorStoreRetriever,\r\n     [54](https://file+.vscode-resource.vscode-cdn.net/.../notebooks/~/.../src/core/retrieval.py:54) ) -> VectorStoreRetriever:\r\n...\r\n     [86](https://file+.vscode-resource.vscode-cdn.net/.../notebooks/~/Library/Caches/pypoetry/virtualenvs/nlp-rag-NMztfdt9-py3.10/lib/python3.10/site-packages/langchain_community/document_compressors/rankllm_rerank.py:86)             \"Please install it with `pip install rank_llm`.\"\r\n     [87](https://file+.vscode-resource.vscode-cdn.net/.../notebooks/~/Library/Caches/pypoetry/virtualenvs/nlp-rag-NMztfdt9-py3.10/lib/python3.10/site-packages/langchain_community/document_compressors/rankllm_rerank.py:87)         )\r\n     [89](https://file+.vscode-resource.vscode-cdn.net/.../notebooks/~/Library/Caches/pypoetry/virtualenvs/nlp-rag-NMztfdt9-py3.10/lib/python3.10/site-packages/langchain_community/document_compressors/rankllm_rerank.py:89) return values\r\n\r\nImportError: Could not import rank_llm python package. Please install it with `pip install rank_llm`.\r\n\r\n### Description\r\n\r\nThe RankLLMRerank integration in LangChain, designed to rerank documents, throws an error when used with GPT (or other models) if the rank-llm package version exceeds 0.12.8. This issue arises due to breaking changes introduced in newer versions of the rank-llm package, now all of the files like rank_gpt, vicuna_reranker or zephyr_reranker were moved to the submodule `rank_llm.rerank.listwise`.\r\n\r\n### System Info\r\n\r\nSystem Information\r\n------------------\r\n> OS:  Darwin\r\n> OS Version:  Darwin Kernel Version 23.4.0: Wed Feb 21 21:45:49 PST 2024; root:xnu-10063.101.15~2/RELEASE_ARM64_T6020\r\n> Python Version:  3.10.14 (main, Mar 19 2024, 21:46:16) [Clang 15.0.0 (clang-1500.3.9.4)]\r\n\r\nPackage Information\r\n-------------------\r\n> langchain_core: 0.3.29\r\n> langchain: 0.3.14\r\n> langchain_community: 0.3.14\r\n> langsmith: 0.2.4\r\n> langchain_anthropic: 0.3.1\r\n> langchain_aws: 0.2.2\r\n> langchain_chroma: 0.2.0\r\n> langchain_experimental: 0.3.2\r\n> langchain_fireworks: 0.2.6\r\n> langchain_google_vertexai: 2.0.5\r\n> langchain_groq: 0.2.3\r\n> langchain_mistralai: 0.2.4\r\n> langchain_openai: 0.3.0\r\n> langchain_text_splitters: 0.3.5\r\n> langchain_together: 0.2.0\r\n> langchain_unstructured: 0.1.5\r\n> langchainhub: 0.1.21\r\n> langgraph_sdk: 0.1.34\r\n\r\nOptional packages not installed\r\n-------------------------------\r\n> langserve\r\n\r\nOther Dependencies\r\n------------------\r\n> aiohttp: 3.11.11\r\n> anthropic: 0.40.0\r\n> anthropic[vertexai]: Installed. No version info available.\r\n> async-timeout: 4.0.3\r\n> boto3: 1.35.42\r\n> chromadb: 0.5.15\r\n> dataclasses-json: 0.6.7\r\n> defusedxml: 0.7.1\r\n> fastapi: 0.115.3\r\n> fireworks-ai: 0.15.7\r\n> google-cloud-aiplatform: 1.70.0\r\n> google-cloud-storage: 2.18.2\r\n> groq: 0.11.0\r\n> httpx: 0.28.1\r\n> httpx-sse: 0.4.0\r\n> jsonpatch: 1.33\r\n> langsmith-pyo3: Installed. No version info available.\r\n> numpy: 1.26.4\r\n> openai: 1.57.4\r\n> orjson: 3.10.12\r\n> packaging: 24.2\r\n> pydantic: 2.10.4\r\n> pydantic-settings: 2.7.0\r\n> PyYAML: 6.0.2\r\n> rank-llm: 0.20.3\r\n> requests: 2.32.3\r\n> requests-toolbelt: 1.0.0\r\n> SQLAlchemy: 2.0.36\r\n> tenacity: 9.0.0\r\n> tiktoken: 0.8.0\r\n> tokenizers: 0.20.1\r\n> types-requests: 2.32.0.20241016\r\n> typing-extensions: 4.12.2\r\n> unstructured-client: 0.25.9\r\n> unstructured[all-docs]: Installed. No version info available.",
    "state": "closed",
    "created_at": "2025-01-11T23:37:06+00:00",
    "closed_at": "2025-01-13T15:59:10+00:00",
    "updated_at": "2025-01-13T15:59:10+00:00",
    "author": "tymzar",
    "author_type": "User",
    "comments_count": 0,
    "reactions_count": 0,
    "labels": "ðŸ¤–:bug",
    "milestone": null,
    "closed_by": "tymzar",
    "resolution_time_hours": 40.367777777777775,
    "first_comments": [],
    "url": "https://github.com/langchain-ai/langchain/issues/29156"
  },
  {
    "repository": "langchain-ai/langchain",
    "issue_number": 29121,
    "title": "langchain_community is missing a dependency",
    "body": "### Checked other resources\r\n\r\n- [X] I added a very descriptive title to this issue.\r\n- [X] I searched the LangChain documentation with the integrated search.\r\n- [X] I used the GitHub search to find a similar question and didn't find it.\r\n- [X] I am sure that this is a bug in LangChain rather than my code.\r\n- [X] The bug is not resolved by updating to the latest stable version of LangChain (or the specific integration package).\r\n\r\n### Example Code\r\n\r\npip show langchain_community | grep Requires\r\n\r\n### Error Message and Stack Trace (if applicable)\r\n\r\n_No response_\r\n\r\n### Description\r\n\r\nlangchain_community has a dependency on unstructured that is not captured. If you look at the document_loaders/unstructured.py file, you will see several imports of unstructured. It is not captured in the dependencies:\r\n\r\n$ pip3 show langchain_community | grep Requires\r\nRequires: SQLAlchemy, langsmith, PyYAML, dataclasses-json, tenacity, langchain-core, langchain, numpy, aiohttp, requests\r\n\r\nEven though the system info below is RHEL 8 era, the current code shows poetry.lock does not have unstructured listed anywhere. This causes crashes on downloads.\r\n\r\n### System Info\r\n\r\nSystem Information\r\n------------------\r\n> OS:  Linux\r\n> OS Version:  #1 SMP PREEMPT_DYNAMIC Sun Nov 17 18:52:19 UTC 2024\r\n> Python Version:  3.9.20 (main, Sep 26 2024, 20:59:47) \r\n[GCC 8.5.0 20210514 (Red Hat 8.5.0-22)]\r\n\r\nPackage Information\r\n-------------------\r\n> langchain_core: 0.2.43\r\n> langchain: 0.2.9\r\n> langchain_community: 0.2.9\r\n> langsmith: 0.1.147\r\n> langchain_text_splitters: 0.2.4\r\n\r\nOptional packages not installed\r\n-------------------------------\r\n> langgraph\r\n> langserve\r\n\r\nOther Dependencies\r\n------------------\r\n> aiohttp: 3.11.11\r\n> async-timeout: 4.0.3\r\n> dataclasses-json: 0.6.7\r\n> httpx: 0.28.1\r\n> jsonpatch: 1.33\r\n> langsmith-pyo3: Installed. No version info available.\r\n> numpy: 1.26.4\r\n> orjson: 3.10.14\r\n> packaging: 24.2\r\n> pydantic: 2.10.4\r\n> PyYAML: 6.0.2\r\n> requests: 2.32.3\r\n> requests-toolbelt: 1.0.0\r\n> SQLAlchemy: 2.0.36\r\n> tenacity: 8.5.0\r\n> typing-extensions: 4.12.2",
    "state": "closed",
    "created_at": "2025-01-10T04:04:07+00:00",
    "closed_at": "2025-01-10T15:20:58+00:00",
    "updated_at": "2025-01-10T15:20:59+00:00",
    "author": "stevegrubb",
    "author_type": "User",
    "comments_count": 1,
    "reactions_count": 0,
    "labels": "ðŸ¤–:bug,investigate",
    "milestone": null,
    "closed_by": "ccurme",
    "resolution_time_hours": 11.280833333333334,
    "first_comments": [
      {
        "author": "ccurme",
        "created_at": "2025-01-10T15:20:58+00:00",
        "body": "`unstructured` and almost all other third party integration dependencies are [deliberately omitted](https://python.langchain.com/docs/concepts/architecture/#langchain-community) from the explicit dependencies of `langchain-community`. They are often not imported until a class is instantiated and we make an effort to raise an informative error message when missing. Due to the number of integrations in `langchain-community`, it would not be practical to manage all their dependencies simultaneously.\r\n\r\nTo mitigate this, we recommend new integrations be implemented as [separate packages](https://python.langchain.com/docs/contributing/how_to/integrations/). There is an existing package for [langchain-unstructured](https://pypi.org/project/langchain-unstructured/) that explicitly depends on `unstructured`. This package is recommended over the abstractions in `langchain-community`."
      }
    ],
    "url": "https://github.com/langchain-ai/langchain/issues/29121"
  },
  {
    "repository": "langchain-ai/langchain",
    "issue_number": 29108,
    "title": " UnstructuredMarkdownLoader#zipfile.BadZipFile: File is not a zip file",
    "body": "### Checked other resources\n\n- [X] I added a very descriptive title to this issue.\n- [X] I searched the LangChain documentation with the integrated search.\n- [X] I used the GitHub search to find a similar question and didn't find it.\n- [X] I am sure that this is a bug in LangChain rather than my code.\n- [X] The bug is not resolved by updating to the latest stable version of LangChain (or the specific integration package).\n\n### Example Code\n\nfrom langchain_community.document_loaders import UnstructuredMarkdownLoader\r\n\r\nloader = UnstructuredMarkdownLoader(\"./README.md\")\r\ndocuments = loader.load()\n\n### Error Message and Stack Trace (if applicable)\n\n line 11, in <module>\r\n    documents = loader.load()\r\n                ^^^^^^^^^^^^^\r\n  File \"/Users/ryan/Desktop/code/llmops/llmops-api/.venv/lib/python3.12/site-packages/langchain_core/document_loaders/base.py\", line 31, in load\r\n    return list(self.lazy_load())\r\n           ^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/ryan/Desktop/code/llmops/llmops-api/.venv/lib/python3.12/site-packages/langchain_community/document_loaders/unstructured.py\", line 107, in lazy_load\r\n    elements = self._get_elements()\r\n               ^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/ryan/Desktop/code/llmops/llmops-api/.venv/lib/python3.12/site-packages/langchain_community/document_loaders/markdown.py\", line 86, in _get_elements\r\n    return partition_md(filename=self.file_path, **self.unstructured_kwargs)  # type: ignore[arg-type]\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/ryan/Desktop/code/llmops/llmops-api/.venv/lib/python3.12/site-packages/unstructured/partition/md.py\", line 78, in partition_md\r\n    return partition_html(\r\n           ^^^^^^^^^^^^^^^\r\n  File \"/Users/ryan/Desktop/code/llmops/llmops-api/.venv/lib/python3.12/site-packages/unstructured/partition/common/metadata.py\", line 162, in wrapper\r\n    elements = func(*args, **kwargs)\r\n               ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/ryan/Desktop/code/llmops/llmops-api/.venv/lib/python3.12/site-packages/unstructured/chunking/dispatch.py\", line 74, in wrapper\r\n    elements = func(*args, **kwargs)\r\n               ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/ryan/Desktop/code/llmops/llmops-api/.venv/lib/python3.12/site-packages/unstructured/partition/html/partition.py\", line 91, in partition_html\r\n    return list(_HtmlPartitioner.iter_elements(opts))\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/ryan/Desktop/code/llmops/llmops-api/.venv/lib/python3.12/site-packages/unstructured/partition/html/partition.py\", line 189, in iter_elements\r\n    yield from cls(opts)._iter_elements()\r\n  File \"/Users/ryan/Desktop/code/llmops/llmops-api/.venv/lib/python3.12/site-packages/unstructured/partition/html/partition.py\", line 202, in _iter_elements\r\n    for e in elements_iter:\r\n  File \"/Users/ryan/Desktop/code/llmops/llmops-api/.venv/lib/python3.12/site-packages/unstructured/partition/html/parser.py\", line 359, in iter_elements\r\n    yield from block_item.iter_elements()\r\n  File \"/Users/ryan/Desktop/code/llmops/llmops-api/.venv/lib/python3.12/site-packages/unstructured/partition/html/parser.py\", line 354, in iter_elements\r\n    yield from self._element_from_text_or_tail(self.text or \"\", q, self._ElementCls)\r\n  File \"/Users/ryan/Desktop/code/llmops/llmops-api/.venv/lib/python3.12/site-packages/unstructured/partition/html/parser.py\", line 384, in _element_from_text_or_tail\r\n    yield from element_accum.flush(ElementCls)\r\n  File \"/Users/ryan/Desktop/code/llmops/llmops-api/.venv/lib/python3.12/site-packages/unstructured/partition/html/parser.py\", line 252, in flush\r\n    ElementCls = derive_element_type_from_text(normalized_text)\r\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/ryan/Desktop/code/llmops/llmops-api/.venv/lib/python3.12/site-packages/unstructured/partition/html/parser.py\", line 884, in derive_element_type_from_text\r\n    if is_possible_narrative_text(text):\r\n       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/ryan/Desktop/code/llmops/llmops-api/.venv/lib/python3.12/site-packages/unstructured/partition/text_type.py\", line 74, in is_possible_narrative_text\r\n    if exceeds_cap_ratio(text, threshold=cap_threshold):\r\n       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/ryan/Desktop/code/llmops/llmops-api/.venv/lib/python3.12/site-packages/unstructured/partition/text_type.py\", line 270, in exceeds_cap_ratio\r\n    if sentence_count(text, 3) > 1:\r\n       ^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/ryan/Desktop/code/llmops/llmops-api/.venv/lib/python3.12/site-packages/unstructured/partition/text_type.py\", line 219, in sentence_count\r\n    sentences = sent_tokenize(text)\r\n                ^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/ryan/Desktop/code/llmops/llmops-api/.venv/lib/python3.12/site-packages/unstructured/nlp/tokenize.py\", line 56, in sent_tokenize\r\n    _download_nltk_packages_if_not_present()\r\n  File \"/Users/ryan/Desktop/code/llmops/llmops-api/.venv/lib/python3.12/site-packages/unstructured/nlp/tokenize.py\", line 41, in _download_nltk_packages_if_not_present\r\n    tagger_available = check_for_nltk_package(\r\n                       ^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/ryan/Desktop/code/llmops/llmops-api/.venv/lib/python3.12/site-packages/unstructured/nlp/tokenize.py\", line 29, in check_for_nltk_package\r\n    nltk.find(f\"{package_category}/{package_name}\", paths=paths)\r\n  File \"/Users/ryan/Desktop/code/llmops/llmops-api/.venv/lib/python3.12/site-packages/nltk/data.py\", line 551, in find\r\n    return find(modified_name, paths)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/ryan/Desktop/code/llmops/llmops-api/.venv/lib/python3.12/site-packages/nltk/data.py\", line 538, in find\r\n    return ZipFilePathPointer(p, zipentry)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/ryan/Desktop/code/llmops/llmops-api/.venv/lib/python3.12/site-packages/nltk/data.py\", line 391, in __init__\r\n    zipfile = OpenOnDemandZipFile(os.path.abspath(zipfile))\r\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/ryan/Desktop/code/llmops/llmops-api/.venv/lib/python3.12/site-packages/nltk/data.py\", line 1020, in __init__\r\n    zipfile.ZipFile.__init__(self, filename)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/zipfile/__init__.py\", line 1349, in __init__\r\n    self._RealGetContents()\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/zipfile/__init__.py\", line 1416, in _RealGetContents\r\n    raise BadZipFile(\"File is not a zip file\")\r\nzipfile.BadZipFile: File is not a zip file\r\n\r\nProcess finished with exit code 1\r\n\n\n### Description\n\nzipfile.BadZipFile: File is not a zip file\r\n\n\n### System Info\n\nMarkdown==3.7\r\nunstructured==0.16.12\r\nopenpyxl==3.1.5\r\nnltk==3.9.1\r\nlangchain-community==0.3.14\r\n\r\n",
    "state": "closed",
    "created_at": "2025-01-09T07:41:18+00:00",
    "closed_at": "2025-01-09T07:56:32+00:00",
    "updated_at": "2025-01-09T07:56:32+00:00",
    "author": "ryanSir",
    "author_type": "User",
    "comments_count": 1,
    "reactions_count": 0,
    "labels": "ðŸ¤–:bug",
    "milestone": null,
    "closed_by": "ryanSir",
    "resolution_time_hours": 0.2538888888888889,
    "first_comments": [
      {
        "author": "ryanSir",
        "created_at": "2025-01-09T07:56:29+00:00",
        "body": "I have solved this problem with download third packages\r\n\r\nimport nltk\r\nnltk.download()"
      }
    ],
    "url": "https://github.com/langchain-ai/langchain/issues/29108"
  },
  {
    "repository": "langchain-ai/langchain",
    "issue_number": 29090,
    "title": "UnstructuredHTMLLoader fail when given `Path` type document",
    "body": "### Checked other resources\n\n- [X] I added a very descriptive title to this issue.\n- [X] I searched the LangChain documentation with the integrated search.\n- [X] I used the GitHub search to find a similar question and didn't find it.\n- [X] I am sure that this is a bug in LangChain rather than my code.\n- [X] The bug is not resolved by updating to the latest stable version of LangChain (or the specific integration package).\n\n### Example Code\n\n```python\r\nfrom pathlib import Path\r\nfrom langchain_community.document_loaders import UnstructuredHTMLLoader\r\n\r\ndocument = Path(\"./test.html\")\r\nloader = UnstructuredHTMLLoader(document, mode=\"elements\", strategy=\"fast\")\r\nloader.load()\r\n```\n\n### Error Message and Stack Trace (if applicable)\n\n```\r\nTraceback (most recent call last):\r\n  File \"/home/Marsman1996/afgen/test/./hello.py\", line 6, in <module>\r\n    loader.load()\r\n    ~~~~~~~~~~~^^\r\n  File \"/home/Marsman1996/afgen/test/.venv/lib/python3.13/site-packages/langchain_core/document_loaders/base.py\", line 31, in load\r\n    return list(self.lazy_load())\r\n  File \"/home/Marsman1996/afgen/test/.venv/lib/python3.13/site-packages/langchain_community/document_loaders/unstructured.py\", line 107, in lazy_load\r\n    elements = self._get_elements()\r\n  File \"/home/Marsman1996/afgen/test/.venv/lib/python3.13/site-packages/langchain_community/document_loaders/html.py\", line 33, in _get_elements\r\n    return partition_html(filename=self.file_path, **self.unstructured_kwargs)  # type: ignore[arg-type]\r\n  File \"/home/Marsman1996/afgen/test/.venv/lib/python3.13/site-packages/unstructured/partition/common/metadata.py\", line 162, in wrapper\r\n    elements = func(*args, **kwargs)\r\n  File \"/home/Marsman1996/afgen/test/.venv/lib/python3.13/site-packages/unstructured/chunking/dispatch.py\", line 74, in wrapper\r\n    elements = func(*args, **kwargs)\r\n  File \"/home/Marsman1996/afgen/test/.venv/lib/python3.13/site-packages/unstructured/partition/html/partition.py\", line 91, in partition_html\r\n    return list(_HtmlPartitioner.iter_elements(opts))\r\n  File \"/home/Marsman1996/afgen/test/.venv/lib/python3.13/site-packages/unstructured/partition/html/partition.py\", line 189, in iter_elements\r\n    yield from cls(opts)._iter_elements()\r\n  File \"/home/Marsman1996/afgen/test/.venv/lib/python3.13/site-packages/unstructured/partition/html/partition.py\", line 203, in _iter_elements\r\n    e.metadata.last_modified = self._opts.last_modified\r\n                               ^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/Marsman1996/afgen/test/.venv/lib/python3.13/site-packages/unstructured/utils.py\", line 154, in __get__\r\n    value = self._fget(obj)\r\n  File \"/home/Marsman1996/afgen/test/.venv/lib/python3.13/site-packages/unstructured/partition/html/partition.py\", line 160, in last_modified\r\n    if not self._file_path or is_temp_file_path(self._file_path)\r\n                              ~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\r\n  File \"/home/Marsman1996/afgen/test/.venv/lib/python3.13/site-packages/unstructured/utils.py\", line 68, in is_temp_file_path\r\n    return file_path.startswith(tempfile.gettempdir())\r\n           ^^^^^^^^^^^^^^^^^^^^\r\nAttributeError: 'PosixPath' object has no attribute 'startswith'\r\n```\n\n### Description\n\nI'm trying to use langchain to parse HTML file and give a `Path` type variable according to the [doc](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.html.UnstructuredHTMLLoader.html#langchain_community.document_loaders.html.UnstructuredHTMLLoader.__init__). \r\nThe doc says `UnstructuredHTMLLoader` could receive `file_path: str | List[str] | Path | List[Path]`.\r\nHowever, actually it could only deal with the `str` type input...\n\n### System Info\n\n```\r\n$ python -m langchain_core.sys_info                      \r\n\r\nSystem Information\r\n------------------\r\n> OS:  Linux\r\n> OS Version:  #52-Ubuntu SMP PREEMPT_DYNAMIC Thu Dec  5 13:09:44 UTC 2024\r\n> Python Version:  3.13.0 (main, Oct 16 2024, 03:23:02) [Clang 18.1.8 ]\r\n\r\nPackage Information\r\n-------------------\r\n> langchain_core: 0.3.29\r\n> langchain: 0.3.14\r\n> langchain_community: 0.3.14\r\n> langsmith: 0.2.10\r\n> langchain_text_splitters: 0.3.5\r\n\r\nOptional packages not installed\r\n-------------------------------\r\n> langserve\r\n\r\nOther Dependencies\r\n------------------\r\n> aiohttp: 3.11.11\r\n> async-timeout: Installed. No version info available.\r\n> dataclasses-json: 0.6.7\r\n> httpx: 0.28.1\r\n> httpx-sse: 0.4.0\r\n> jsonpatch: 1.33\r\n> langsmith-pyo3: Installed. No version info available.\r\n> numpy: 1.26.4\r\n> orjson: 3.10.13\r\n> packaging: 24.2\r\n> pydantic: 2.9.2\r\n> pydantic-settings: 2.7.1\r\n> PyYAML: 6.0.2\r\n> requests: 2.32.3\r\n> requests-toolbelt: 1.0.0\r\n> SQLAlchemy: 2.0.36\r\n> tenacity: 9.0.0\r\n> typing-extensions: 4.12.2\r\n> zstandard: Installed. No version info available.\r\n```",
    "state": "closed",
    "created_at": "2025-01-08T11:19:21+00:00",
    "closed_at": "2025-01-08T15:19:29+00:00",
    "updated_at": "2025-01-08T15:19:29+00:00",
    "author": "Marsman1996",
    "author_type": "User",
    "comments_count": 0,
    "reactions_count": 0,
    "labels": "ðŸ¤–:bug",
    "milestone": null,
    "closed_by": "ccurme",
    "resolution_time_hours": 4.002222222222223,
    "first_comments": [],
    "url": "https://github.com/langchain-ai/langchain/issues/29090"
  },
  {
    "repository": "langchain-ai/langchain",
    "issue_number": 29088,
    "title": "Labels are not included when include_labels=True is set in ConfluenceLoader",
    "body": "### Checked other resources\r\n\r\n- [X] I added a very descriptive title to this issue.\r\n- [X] I searched the LangChain documentation with the integrated search.\r\n- [X] I used the GitHub search to find a similar question and didn't find it.\r\n- [X] I am sure that this is a bug in LangChain rather than my code.\r\n- [X] The bug is not resolved by updating to the latest stable version of LangChain (or the specific integration package).\r\n\r\n### Example Code\r\n\r\n```py\r\nfrom langchain_community.document_loaders import ConfluenceLoader\r\n\r\n# Confluence API settings\r\nCONFLUENCE_HOST = \"https://your-confluence-instance.atlassian.net\"\r\nCONFLUENCE_USERNAME = \"your-email@example.com\"\r\nCONFLUENCE_API_TOKEN = \"your-confluence-api-token\"\r\n\r\n# Initialize the loader with include_labels=True\r\nloader = ConfluenceLoader(\r\n    url=CONFLUENCE_HOST,\r\n    username=CONFLUENCE_USERNAME,\r\n    api_token=CONFLUENCE_API_TOKEN,\r\n    cql=\"type=page AND space=YOUR_SPACE_KEY\",\r\n    include_labels=True\r\n)\r\n\r\n# Load documents\r\ntry:\r\n    documents = loader.load()\r\n    for doc in documents:\r\n        print(f\"Title: {doc.metadata['title']}\")\r\n        print(f\"Labels: {doc.metadata.get('labels', 'No labels found')}\")\r\n```\r\n\r\nExpected Behavior\r\nWhen include_labels=True is set in ConfluenceLoader, the metadata dictionary of each document should include a key labels with the associated labels of the Confluence page.\r\n\r\nActual Behavior\r\nThe labels field is missing in the metadata of the loaded documents, even when include_labels=True is explicitly set.\r\n\r\n### Error Message and Stack Trace (if applicable)\r\n\r\n_No response_\r\n\r\n### Description\r\n\r\n**Description**\r\nI am using ConfluenceLoader to load Confluence documents with the include_labels parameter set to True. The loader successfully fetches the document metadata, but the labels field is not included in the metadata dictionary of the documents. This issue occurs consistently when using CQL queries.\r\n\r\n**Expected Output**: Metadata should include a labels key with a list of labels.\r\n\r\n**Actual Output**: Metadata does not include the labels key.\r\n\r\n### System Info\r\n\r\n- langchain                0.3.14\r\n- langchain-community      0.3.14\r\n- langchain-core           0.3.29\r\n- langchain-tests          0.3.7 \r\n- langchain-text-splitters 0.3.5           ",
    "state": "closed",
    "created_at": "2025-01-08T10:37:42+00:00",
    "closed_at": "2025-01-08T15:16:41+00:00",
    "updated_at": "2025-01-08T15:16:41+00:00",
    "author": "zenoengine",
    "author_type": "User",
    "comments_count": 0,
    "reactions_count": 0,
    "labels": "ðŸ¤–:bug",
    "milestone": null,
    "closed_by": "ccurme",
    "resolution_time_hours": 4.6497222222222225,
    "first_comments": [],
    "url": "https://github.com/langchain-ai/langchain/issues/29088"
  },
  {
    "repository": "langchain-ai/langchain",
    "issue_number": 29061,
    "title": "TavilySearch returns irrelevant results with low relevance scores, but Langchain doesn't return the relevance scores",
    "body": "### Checked other resources\r\n\r\n- [X] I added a very descriptive title to this issue.\r\n- [X] I searched the LangChain documentation with the integrated search.\r\n- [X] I used the GitHub search to find a similar question and didn't find it.\r\n- [X] I am sure that this is a bug in LangChain rather than my code.\r\n- [X] The bug is not resolved by updating to the latest stable version of LangChain (or the specific integration package).\r\n\r\n### Example Code\r\n\r\nTavily returns the following content for my query:\r\n\r\n```\r\n{\r\n    \"query\": \"SOME QUERY\",\r\n    \"follow_up_questions\": null,\r\n    \"answer\": null,\r\n    \"images\": [],\r\n    \"results\": [\r\n        {\r\n            \"title\": \"TITLE 1\",\r\n            \"url\": \"URL 1\",\r\n            \"content\": \"CONTENT 1\",\r\n            \"score\": 0.18894346,\r\n            \"raw_content\": null\r\n        },\r\n        {\r\n            \"title\": \"TITLE 2\",\r\n            \"url\": \"URL 2\",\r\n            \"content\": \"CONTENT 2\",\r\n            \"score\": 0.15776997,\r\n            \"raw_content\": null\r\n        },\r\n        {\r\n            \"title\": \"TITLE 3\",\r\n            \"url\": \"URL 3\",\r\n            \"content\": \"CONTENT 3\",\r\n            \"score\": 0.0416598,\r\n            \"raw_content\": null\r\n        }\r\n    ],\r\n    \"response_time\": 1.88\r\n}\r\n```\r\n\r\nThese returned contents have very low relevance scores < 0.2. I basically need these scores to filter out irrelevant documents from this result.\r\n\r\nFor the same query, TavilySearchResults of Langchain returns the following:\r\n\r\n```\r\n[\r\n        {\r\n            \"url\": \"URL 1\",\r\n            \"content\": \"CONTENT 1\"\r\n        },\r\n        {\r\n            \"url\": \"URL 2\",\r\n            \"content\": \"CONTENT 2\"\r\n        },\r\n        {\r\n            \"url\": \"URL 3\",\r\n            \"content\": \"CONTENT 3\"\r\n        }\r\n]\r\n```\r\n\r\n\r\n\r\n### Error Message and Stack Trace (if applicable)\r\n\r\n_No response_\r\n\r\n### Description\r\n\r\nTavily returns the following content for my query:\r\n\r\n```\r\n{\r\n    \"query\": \"SOME QUERY\",\r\n    \"follow_up_questions\": null,\r\n    \"answer\": null,\r\n    \"images\": [],\r\n    \"results\": [\r\n        {\r\n            \"title\": \"TITLE 1\",\r\n            \"url\": \"URL 1\",\r\n            \"content\": \"CONTENT 1\",\r\n            \"score\": 0.18894346,\r\n            \"raw_content\": null\r\n        },\r\n        {\r\n            \"title\": \"TITLE 2\",\r\n            \"url\": \"URL 2\",\r\n            \"content\": \"CONTENT 2\",\r\n            \"score\": 0.15776997,\r\n            \"raw_content\": null\r\n        },\r\n        {\r\n            \"title\": \"TITLE 3\",\r\n            \"url\": \"URL 3\",\r\n            \"content\": \"CONTENT 3\",\r\n            \"score\": 0.0416598,\r\n            \"raw_content\": null\r\n        }\r\n    ],\r\n    \"response_time\": 1.88\r\n}\r\n```\r\n\r\nFor the same query, TavilySearchResults of Langchain returns the following:\r\n\r\n```\r\n[\r\n        {\r\n            \"url\": \"URL 1\",\r\n            \"content\": \"CONTENT 1\"\r\n        },\r\n        {\r\n            \"url\": \"URL 2\",\r\n            \"content\": \"CONTENT 2\"\r\n        },\r\n        {\r\n            \"url\": \"URL 3\",\r\n            \"content\": \"CONTENT 3\"\r\n        }\r\n]\r\n```\r\n\r\nI basically need scores to filter out irrelevant documents from this result\r\n\r\n### System Info\r\n\r\n> OS:  Darwin\r\n> OS Version:  Darwin Kernel Version 23.6.0: Fri Jul  5 17:56:41 PDT 2024; root:xnu-10063.141.1~2/RELEASE_ARM64_T6000\r\n> Python Version:  3.11.9 (main, Aug  4 2024, 22:27:51) [Clang 15.0.0 (clang-1500.3.9.4)]\r\n\r\nPackage Information\r\n-------------------\r\n> langchain_core: 0.3.29\r\n> langchain: 0.2.12\r\n> langchain_community: 0.2.11\r\n> langsmith: 0.1.125\r\n> langchain_chroma: 0.1.2\r\n> langchain_google_genai: 2.0.7\r\n> langchain_huggingface: 0.1.0\r\n> langchain_openai: 0.1.20\r\n> langchain_text_splitters: 0.2.2\r\n> langgraph_cli: 0.1.52\r\n> langgraph_sdk: 0.1.27\r\n\r\nOptional packages not installed\r\n-------------------------------\r\n> langserve\r\n\r\nOther Dependencies\r\n------------------\r\n> aiohttp: 3.10.0\r\n> async-timeout: Installed. No version info available.\r\n> chromadb: 0.5.5\r\n> click: 8.1.7\r\n> dataclasses-json: 0.6.7\r\n> fastapi: 0.112.0\r\n> filetype: 1.2.0\r\n> google-generativeai: 0.8.3\r\n> httpx: 0.27.0\r\n> httpx-sse: 0.4.0\r\n> huggingface-hub: 0.24.5\r\n> jsonpatch: 1.33\r\n> numpy: 1.26.4\r\n> openai: 1.38.0\r\n> orjson: 3.10.6\r\n> packaging: 24.1\r\n> pydantic: 2.9.2\r\n> PyYAML: 6.0.1\r\n> requests: 2.32.3\r\n> sentence-transformers: 3.1.1\r\n> SQLAlchemy: 2.0.31\r\n> tenacity: 8.5.0\r\n> tiktoken: 0.7.0\r\n> tokenizers: 0.19.1\r\n> transformers: 4.44.2\r\n> typing-extensions: 4.12.2",
    "state": "closed",
    "created_at": "2025-01-07T06:18:48+00:00",
    "closed_at": "2025-01-08T11:28:24+00:00",
    "updated_at": "2025-01-08T11:28:57+00:00",
    "author": "kbatsuren",
    "author_type": "User",
    "comments_count": 4,
    "reactions_count": 0,
    "labels": "ðŸ¤–:bug",
    "milestone": null,
    "closed_by": "kbatsuren",
    "resolution_time_hours": 29.16,
    "first_comments": [
      {
        "author": "kbatsuren",
        "created_at": "2025-01-07T06:26:00+00:00",
        "body": "Instead of including the scores in the output, another solution could be that TavilySearchResults may filter out the documents for a given threshold. "
      },
      {
        "author": "kbatsuren",
        "created_at": "2025-01-07T08:58:01+00:00",
        "body": "@rotemweiss57 Could you have a look at this issue?"
      },
      {
        "author": "keenborder786",
        "created_at": "2025-01-07T12:35:47+00:00",
        "body": "@kbatsuren this is because of the following [clean_results](https://vscode.dev/github/keenborder786/langchain/blob/master3/envs/langchain/lib/python3.12/site-packages/langchain_community/utilities/tavily_search.py#L175). You can get the raw results from same output as follow:\r\n\r\n```python\r\nfrom langchain_community.tools.tavily_search import TavilySearchResults\r\nimport os\r\nos.environ[\"TAVILY_API_KEY\"] = \"tvly-xxxxxxxxxx\"\r\nresults,raw_results = TavilySearchResults()._run(\"who is the current president of pakistan?\")\r\nprint(\"Results:\", results)\r\nprint(\"raw_results:\", raw_results)\r\n\r\n\r\n\r\n```"
      },
      {
        "author": "kbatsuren",
        "created_at": "2025-01-08T00:26:18+00:00",
        "body": "@keenborder786 Thanks for the quick reply. My technical limitation is that many of my codes chained TavilySearch as a tool that calls the invoke function. Is there any way to keep this tool in the chain?"
      }
    ],
    "url": "https://github.com/langchain-ai/langchain/issues/29061"
  },
  {
    "repository": "langchain-ai/langchain",
    "issue_number": 29060,
    "title": " max_tokens param in ChatOpenAI() can't be processed",
    "body": "### Checked other resources\n\n- [X] I added a very descriptive title to this issue.\n- [X] I searched the LangChain documentation with the integrated search.\n- [X] I used the GitHub search to find a similar question and didn't find it.\n- [X] I am sure that this is a bug in LangChain rather than my code.\n- [X] The bug is not resolved by updating to the latest stable version of LangChain (or the specific integration package).\n\n### Example Code\n\n```python\r\nfrom langchain_community.chat_models import ChatOpenAI\r\n# from langchain_openai.chat_models import ChatOpenAI\r\nqwen_local_llm = ChatOpenAI(model=QWEN_API_LOCAL_MODEL_NAME,\r\n                            openai_api_key=OPENAI_API_KEY,\r\n                            openai_api_base=OPENAI_API_BASE,\r\n                            max_tokens=4096,\r\n                            temperature=0.01,request_timeout=600)\r\nprompt_context_str = \"\"\"ä½ æ˜¯ä¸€ä¸ªæå–ç®—æ³•ä¸“å®¶ï¼Œä»…ä»Žæ–‡æœ¬ä¸­æå–ç›¸å…³ä¿¡æ¯ã€‚.......\"\"\"\r\nfull_response = \"\"\r\nfor chunk in qwen_local_llm.stream(prompt_context_str):\r\n    full_response += chunk.content\r\nprint(full_response)  ```\n\n### Error Message and Stack Trace (if applicable)\n\nWhen the returned result exceeds 1,024 tokens, it will be truncated.\r\n\n\n### Description\n\nThe max_tokens parameter of the ChatOpenAI object introduced in the way of from langchain_openai.chat_models import ChatOpenAI doesn't take effect. While the max_tokens function of the ChatOpenAI object introduced in the way of from langchain_community.chat_models import ChatOpenAI works normally. The default value of max_tokens for the former is 1024.\r\n\n\n### System Info\n\nSystem Information\r\n------------------\r\n> OS:  Darwin\r\n> OS Version:  Darwin Kernel Version 24.2.0: Fri Dec  6 18:40:14 PST 2024; root:xnu-11215.61.5~2/RELEASE_ARM64_T8103\r\n> Python Version:  3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:47:18) \r\n[Clang 18.1.8 ]\r\n\r\nPackage Information\r\n-------------------\r\n> langchain_core: 0.3.29\r\n> langchain: 0.3.14\r\n> langchain_community: 0.3.14\r\n> langsmith: 0.2.10\r\n> langchain_chroma: 0.1.4\r\n> langchain_ollama: 0.2.2\r\n> langchain_openai: 0.2.14\r\n> langchain_text_splitters: 0.3.4\r\n\r\nOptional packages not installed\r\n-------------------------------\r\n> langserve\r\n",
    "state": "closed",
    "created_at": "2025-01-07T06:14:59+00:00",
    "closed_at": "2025-01-08T06:24:33+00:00",
    "updated_at": "2025-01-08T06:24:33+00:00",
    "author": "newispk",
    "author_type": "User",
    "comments_count": 2,
    "reactions_count": 0,
    "labels": "ðŸ¤–:bug,investigate",
    "milestone": null,
    "closed_by": "newispk",
    "resolution_time_hours": 24.159444444444443,
    "first_comments": [
      {
        "author": "ccurme",
        "created_at": "2025-01-07T14:44:27+00:00",
        "body": "OpenAI has [deprecated](https://platform.openai.com/docs/api-reference/chat/create#chat-create-max_tokens) the `max_tokens` parameter in favor of `max_completion_tokens`. The API you are interacting with via the OpenAI SDK has not kept up with OpenAI's changes.\r\n\r\nCan you try using `BaseChatOpenAI` instead? This supports `max_tokens`:\r\n```python\r\nfrom langchain_openai.chat_models.base import BaseChatOpenAI\r\n\r\nllm = BaseChatOpenAI(model=\"gpt-4o-mini\")\r\nllm.invoke(\"hi, how are you?\", max_tokens=3)\r\n```"
      },
      {
        "author": "newispk",
        "created_at": "2025-01-08T00:54:51+00:00",
        "body": "> OpenAI has [deprecated](https://platform.openai.com/docs/api-reference/chat/create#chat-create-max_tokens) the `max_tokens` parameter in favor of `max_completion_tokens`. The API you are interacting with via the OpenAI SDK has not kept up with OpenAI's changes.\r\n> \r\n> Can you try using `BaseChatOpenAI` instead? This supports `max_tokens`:\r\n> \r\n> ```python\r\n> from langchain_openai.chat_models.base import BaseChatOpenAI\r\n> \r\n> llm = BaseChatOpenAI(model=\"gpt-4o-mini\")\r\n> llm.invoke(\"hi, how are you?\", max_tokens=3)\r\n> ```\r\n\r\nThanks it worked for me!"
      }
    ],
    "url": "https://github.com/langchain-ai/langchain/issues/29060"
  },
  {
    "repository": "langchain-ai/langchain",
    "issue_number": 29053,
    "title": "DOC: HTMLSemanticPreservingSplitter import error. It is missing in langchain_text_splitters.html module",
    "body": "### URL\n\nhttps://python.langchain.com/docs/how_to/split_html/#using-htmlheadertextsplitter\n\n### Checklist\n\n- [X] I added a very descriptive title to this issue.\n- [X] I included a link to the documentation page I am referring to (if applicable).\n\n### Issue with current documentation:\n\nneither of the imports are working:\r\n\r\nDocumentatuion: from langchain_text_splitters import HTMLSemanticPreservingSplitter\r\n\r\nAPI reference: from langchain_text_splitters.html import HTMLSemanticPreservingSplitter\n\n### Idea or request for content:\n\nImportError: cannot import name 'HTMLSemanticPreservingSplitter' from 'langchain_text_splitters.html' (/usr/local/lib/python3.10/dist-packages/langchain_text_splitters/html.py)\r\n\r\nImportError: cannot import name 'HTMLSemanticPreservingSplitter' from 'langchain_text_splitters' (/usr/local/lib/python3.10/dist-packages/langchain_text_splitters/__init__.py)\r\n",
    "state": "closed",
    "created_at": "2025-01-07T02:24:55+00:00",
    "closed_at": "2025-01-07T14:48:31+00:00",
    "updated_at": "2025-01-07T15:01:36+00:00",
    "author": "Bavalpreet",
    "author_type": "User",
    "comments_count": 1,
    "reactions_count": 0,
    "labels": "ðŸ¤–:docs,investigate",
    "milestone": null,
    "closed_by": "ccurme",
    "resolution_time_hours": 12.393333333333333,
    "first_comments": [
      {
        "author": "ccurme",
        "created_at": "2025-01-07T15:01:35+00:00",
        "body": "Hi, thanks for this. That splitter is new and was just released in `langchain-text-splitters==0.3.5`. It was added to the documentation before the release, apologies for that oversight."
      }
    ],
    "url": "https://github.com/langchain-ai/langchain/issues/29053"
  },
  {
    "repository": "langchain-ai/langchain",
    "issue_number": 29032,
    "title": "DOC: No Document for DeepSeek with LangChain",
    "body": "### URL\n\nhttps://www.deepseek.com/\n\n### Checklist\n\n- [X] I added a very descriptive title to this issue.\n- [X] I included a link to the documentation page I am referring to (if applicable).\n\n### Issue with current documentation:\n\ni have see no document for using langchain and deepseek \r\n\r\nis there any way to use with  this model?\n\n### Idea or request for content:\n\n_No response_",
    "state": "closed",
    "created_at": "2025-01-05T11:16:41+00:00",
    "closed_at": "2025-01-16T19:44:37+00:00",
    "updated_at": "2025-01-16T19:44:37+00:00",
    "author": "alm0ra",
    "author_type": "User",
    "comments_count": 3,
    "reactions_count": 0,
    "labels": "ðŸ¤–:docs",
    "milestone": null,
    "closed_by": "alm0ra",
    "resolution_time_hours": 272.46555555555557,
    "first_comments": [
      {
        "author": "Nydaym",
        "created_at": "2025-01-09T01:23:25+00:00",
        "body": "You can use OpenAI-compatible mode to utilize Deepseek"
      },
      {
        "author": "Shajeel-Afzal",
        "created_at": "2025-01-16T19:26:11+00:00",
        "body": "@alm0ra did you find any solution? I also want to integrate DeepSeek in my applications that are using LangChain!"
      },
      {
        "author": "alm0ra",
        "created_at": "2025-01-16T19:44:07+00:00",
        "body": "@Shajeel-Afzal  yes , integrated easily\n\n```python\nfrom django.conf import settings\nfrom langchain_openai.chat_models import ChatOpenAI\n\n\nclass DeepSeekService:\n    def __init__(self):\n        self.llm = ChatOpenAI(\n            model=\"deepseek-chat\",\n            api_key=settings.DEEPSEEK_API_KEY,\n            openai_api_base=\"https://api.deepseek.com\",\n        )\n```"
      }
    ],
    "url": "https://github.com/langchain-ai/langchain/issues/29032"
  },
  {
    "repository": "langchain-ai/langchain",
    "issue_number": 29020,
    "title": "pinecone: integration tests are not passing",
    "body": "### Privileged issue\r\n\r\n- [X] I am a LangChain maintainer, or was asked directly by a LangChain maintainer to create an issue here.\r\n\r\n### Issue Content\r\n\r\n[test_embeddings.test_vector_store](https://github.com/langchain-ai/langchain/blob/master/libs/partners/pinecone/tests/integration_tests/test_embeddings.py#L67) fails (likely due to consistency issues in the index)\r\n[test_vector_stores.py](https://github.com/langchain-ai/langchain/blob/master/libs/partners/pinecone/tests/integration_tests/test_vectorstores.py) hangs indefinitely (new issue in last few months)\r\n\r\nit's possible these are compatibility issues with a recent version of pinecone-client. Currently tests are running against a serverless index - it's possible that's not correct anymore?\r\n\r\nThese are preventing the current package from being released\r\n\r\n\r\nTo run integration tests, set pinecone credentials `PINECONE_API_KEY, PINECONE_INDEX, PINECONE_ENVIRONMENT` and\r\n```\r\ncd libs/partners/pinecone\r\npoetry install --with lint,typing,test,test_integration --sync\r\nmake integration_tests\r\n```",
    "state": "closed",
    "created_at": "2025-01-03T22:25:33+00:00",
    "closed_at": "2025-01-07T21:24:55+00:00",
    "updated_at": "2025-01-07T21:24:55+00:00",
    "author": "efriis",
    "author_type": "User",
    "comments_count": 1,
    "reactions_count": 0,
    "labels": "help wanted,â±­: vector store,investigate",
    "milestone": null,
    "closed_by": "efriis",
    "resolution_time_hours": 94.98944444444444,
    "first_comments": [
      {
        "author": "efriis",
        "created_at": "2025-01-07T21:24:55+00:00",
        "body": "fixed"
      }
    ],
    "url": "https://github.com/langchain-ai/langchain/issues/29020"
  },
  {
    "repository": "langchain-ai/langchain",
    "issue_number": 29010,
    "title": "DOC: example for FewShotChatMessagePromptTemplate lacks the prefix",
    "body": "### URL\n\nhttps://github.com/langchain-ai/langchain/blob/8d7daa59fb6896778a9c9186c8cd1e9a8276e88a/libs/core/langchain_core/prompts/few_shot.py#L285\n\n### Checklist\n\n- [X] I added a very descriptive title to this issue.\n- [X] I included a link to the documentation page I am referring to (if applicable).\n\n### Issue with current documentation:\n\nReviewing the docs for `FewShotChatMessagePromptTemplate` in https://github.com/langchain-ai/langchain/blob/8d7daa59fb6896778a9c9186c8cd1e9a8276e88a/libs/core/langchain_core/prompts/few_shot.py#L285, I see that the example where we want to create\r\n```\r\nSystem: You are a helpful AI Assistant\r\n        Human: What is 2+2?\r\n        AI: 4\r\n        Human: What is 2+3?\r\n        AI: 5\r\n        Human: What is 4+4?\r\n```\r\nThe proposed code is\r\n```\r\nfrom langchain_core.prompts import (\r\n                FewShotChatMessagePromptTemplate,\r\n                ChatPromptTemplate\r\n            )\r\n\r\n            examples = [\r\n                {\"input\": \"2+2\", \"output\": \"4\"},\r\n                {\"input\": \"2+3\", \"output\": \"5\"},\r\n            ]\r\n\r\n            example_prompt = ChatPromptTemplate.from_messages(\r\n                [('human', '{input}'), ('ai', '{output}')]\r\n            )\r\n\r\n            few_shot_prompt = FewShotChatMessagePromptTemplate(\r\n                examples=examples,\r\n                # This is a prompt template used to format each individual example.\r\n                example_prompt=example_prompt,\r\n            )\r\n\r\n            final_prompt = ChatPromptTemplate.from_messages(\r\n                [\r\n                    ('system', 'You are a helpful AI Assistant'),\r\n                    few_shot_prompt,\r\n                    ('human', '{input}'),\r\n                ]\r\n            )\r\n            final_prompt.format(input=\"What is 4+4?\")\r\n```\r\nThis creates\r\n```\r\nSystem: You are a helpful AI Assistant\r\nHuman: 2+2\r\nAI: 4\r\nHuman: 2+3\r\nAI: 5\r\nHuman: What is 4+4?\r\n```\n\n### Idea or request for content:\n\nFix to \r\n```\r\nexample_prompt = ChatPromptTemplate.from_messages(\r\n    [('human', 'What is {input}?'), ('ai', '{output}')]\r\n)\r\n```",
    "state": "closed",
    "created_at": "2025-01-03T16:05:39+00:00",
    "closed_at": "2025-01-06T20:29:16+00:00",
    "updated_at": "2025-01-06T20:29:17+00:00",
    "author": "marctorsoc",
    "author_type": "User",
    "comments_count": 0,
    "reactions_count": 0,
    "labels": "ðŸ¤–:docs",
    "milestone": null,
    "closed_by": "efriis",
    "resolution_time_hours": 76.39361111111111,
    "first_comments": [],
    "url": "https://github.com/langchain-ai/langchain/issues/29010"
  },
  {
    "repository": "langchain-ai/langchain",
    "issue_number": 28997,
    "title": "Unknown model: gpt-4o-2024-05-13-cached",
    "body": "### Checked other resources\n\n- [X] I added a very descriptive title to this issue.\n- [X] I searched the LangChain documentation with the integrated search.\n- [X] I used the GitHub search to find a similar question and didn't find it.\n- [X] I am sure that this is a bug in LangChain rather than my code.\n- [X] The bug is not resolved by updating to the latest stable version of LangChain (or the specific integration package).\n\n### Example Code\n\n```python\r\nfrom langchain_openai import AzureChatOpenAI\r\nfrom pydantic import BaseModel, Field\r\n\r\nchat: AzureChatOpenAI = AzureChatOpenAI(\r\n    azure_deployment=\"gpt-4o-2024-05-13\",\r\n    azure_endpoint=\"<azure_endpoint>\",\r\n    api_key=\"<api_key>\",\r\n    api_version=\"2024-10-01-preview\",\r\n    temperature=1,\r\n    top_p=0.95,\r\n    frequency_penalty=0,\r\n    presence_penalty=0,\r\n    verbose=settings.DEBUG,\r\n    response_format={\"type\": \"json_object\"},\r\n)\r\n```\n\n### Error Message and Stack Trace (if applicable)\n\nWARNING:langchain_core.callbacks.manager:Error in OpenAICallbackHandler.on_llm_end callback: ValueError('Unknown model: gpt-4o-2024-05-13-cached. Please provide a valid OpenAI model name.Known models are: o1-preview, o1-preview-cached, o1-preview-2024-09-12, o1-preview-2024-09-12-cached, o1-preview-completion, o1-preview-2024-09-12-completion, o1-mini, o1-mini-cached, o1-mini-2024-09-12, o1-mini-2024-09-12-cached, o1-mini-completion, o1-mini-2024-09-12-completion, gpt-4o-mini, gpt-4o-mini-cached, gpt-4o-mini-2024-07-18, gpt-4o-mini-2024-07-18-cached, gpt-4o-mini-completion, gpt-4o-mini-2024-07-18-completion, gpt-4o, gpt-4o-cached, gpt-4o-2024-05-13, gpt-4o-2024-08-06, gpt-4o-2024-08-06-cached, gpt-4o-2024-11-20, gpt-4o-completion, gpt-4o-2024-05-13-completion, gpt-4o-2024-08-06-completion, gpt-4o-2024-11-20-completion, gpt-4, gpt-4-0314, gpt-4-0613, gpt-4-32k, gpt-4-32k-0314, gpt-4-32k-0613, gpt-4-vision-preview, gpt-4-1106-preview, gpt-4-0125-preview, gpt-4-turbo-preview, gpt-4-turbo, gpt-4-turbo-2024-04-09, gpt-4-completion, gpt-4-0314-completion, gpt-4-0613-completion, gpt-4-32k-completion, gpt-4-32k-0314-completion, gpt-4-32k-0613-completion, gpt-4-vision-preview-completion, gpt-4-1106-preview-completion, gpt-4-0125-preview-completion, gpt-4-turbo-preview-completion, gpt-4-turbo-completion, gpt-4-turbo-2024-04-09-completion, gpt-3.5-turbo, gpt-3.5-turbo-0125, gpt-3.5-turbo-0301, gpt-3.5-turbo-0613, gpt-3.5-turbo-1106, gpt-3.5-turbo-instruct, gpt-3.5-turbo-16k, gpt-3.5-turbo-16k-0613, gpt-3.5-turbo-completion, gpt-3.5-turbo-0125-completion, gpt-3.5-turbo-0301-completion, gpt-3.5-turbo-0613-completion, gpt-3.5-turbo-1106-completion, gpt-3.5-turbo-instruct-completion, gpt-3.5-turbo-16k-completion, gpt-3.5-turbo-16k-0613-completion, gpt-35-turbo, gpt-35-turbo-0125, gpt-35-turbo-0301, gpt-35-turbo-0613, gpt-35-turbo-instruct, gpt-35-turbo-16k, gpt-35-turbo-16k-0613, gpt-35-turbo-completion, gpt-35-turbo-0125-completion, gpt-35-turbo-0301-completion, gpt-35-turbo-0613-completion, gpt-35-turbo-instruct-completion, gpt-35-turbo-16k-completion, gpt-35-turbo-16k-0613-completion, text-ada-001, ada, text-babbage-001, babbage, text-curie-001, curie, text-davinci-003, text-davinci-002, code-davinci-002, babbage-002-finetuned, davinci-002-finetuned, gpt-3.5-turbo-0613-finetuned, gpt-3.5-turbo-1106-finetuned, gpt-3.5-turbo-0125-finetuned, gpt-4o-mini-2024-07-18-finetuned, babbage-002-finetuned-completion, davinci-002-finetuned-completion, gpt-3.5-turbo-0613-finetuned-completion, gpt-3.5-turbo-1106-finetuned-completion, gpt-3.5-turbo-0125-finetuned-completion, gpt-4o-mini-2024-07-18-finetuned-completion, babbage-002-azure-finetuned, davinci-002-azure-finetuned, gpt-35-turbo-0613-azure-finetuned, babbage-002-azure-finetuned-completion, davinci-002-azure-finetuned-completion, gpt-35-turbo-0613-azure-finetuned-completion, ada-finetuned-legacy, babbage-finetuned-legacy, curie-finetuned-legacy, davinci-finetuned-legacy')\n\n### Description\n\n* I'm trying to use the 'langchain' library to chat with Azure OpenAI.\r\n* I expect to see correct JSON object. \r\n* Instead, it does, but also print the Unknown model Error.\n\n### System Info\n\nSystem Information\r\n------------------\r\n> OS:  Darwin\r\n> OS Version:  Darwin Kernel Version 24.1.0: Thu Oct 10 21:03:15 PDT 2024; root:xnu-11215.41.3~2/RELEASE_ARM64_T6000\r\n> Python Version:  3.11.11 | packaged by conda-forge | (main, Dec  5 2024, 14:21:42) [Clang 18.1.8 ]\r\n\r\nPackage Information\r\n-------------------\r\n> langchain_core: 0.3.28\r\n> langchain: 0.3.13\r\n> langchain_community: 0.3.13\r\n> langsmith: 0.2.6\r\n> langchain_ollama: 0.2.2\r\n> langchain_openai: 0.2.14\r\n> langchain_postgres: 0.0.12\r\n> langchain_text_splitters: 0.3.4\r\n\r\nOptional packages not installed\r\n-------------------------------\r\n> langserve\r\n\r\nOther Dependencies\r\n------------------\r\n> aiohttp: 3.11.11\r\n> async-timeout: 5.0.1\r\n> dataclasses-json: 0.6.7\r\n> httpx: 0.27.2\r\n> httpx-sse: 0.4.0\r\n> jsonpatch: 1.33\r\n> langsmith-pyo3: Installed. No version info available.\r\n> numpy: 1.26.4\r\n> ollama: 0.4.5\r\n> openai: 1.58.1\r\n> orjson: 3.10.13\r\n> packaging: 24.2\r\n> pgvector: 0.2.5\r\n> psycopg: 3.2.3\r\n> psycopg-pool: 3.2.4\r\n> pydantic: 2.10.4\r\n> pydantic-settings: 2.7.1\r\n> PyYAML: 6.0.2\r\n> requests: 2.32.3\r\n> requests-toolbelt: 1.0.0\r\n> sqlalchemy: 2.0.36\r\n> SQLAlchemy: 2.0.36\r\n> tenacity: 9.0.0\r\n> tiktoken: 0.8.0\r\n> typing-extensions: 4.12.2\r\n> zstandard: Installed. No version info available.",
    "state": "closed",
    "created_at": "2025-01-03T02:16:36+00:00",
    "closed_at": "2025-01-03T15:14:08+00:00",
    "updated_at": "2025-01-23T05:42:30+00:00",
    "author": "Oscaner",
    "author_type": "User",
    "comments_count": 5,
    "reactions_count": 0,
    "labels": "ðŸ¤–:bug,investigate",
    "milestone": null,
    "closed_by": "ccurme",
    "resolution_time_hours": 12.95888888888889,
    "first_comments": [
      {
        "author": "JanPalasek",
        "created_at": "2025-01-03T09:22:14+00:00",
        "body": "The problem is that `OpenAICallbackHandler.on_llm_end` calls `get_openai_token_cost_for_model`, even if there are no tokens cached. If the model is not cacheable, `get_openai_token_cost_for_model` raises an error. This should not happen, since we did not want any caching in the first-place and thus the caching tokens is zero, so there is not even a point in computing the cost."
      },
      {
        "author": "ziqizhang",
        "created_at": "2025-01-12T21:51:04+00:00",
        "body": "Can confirm that I also get this error, when I use custom fine-tuned models. Is there a fix?"
      },
      {
        "author": "ccurme",
        "created_at": "2025-01-13T17:42:38+00:00",
        "body": "> Can confirm that I also get this error, when I use custom fine-tuned models. Is there a fix?\r\n\r\n@ziqizhang this should be fixed in `langchain-community==0.3.14`. Are you running into the issue after upgrade?"
      },
      {
        "author": "ziqizhang",
        "created_at": "2025-01-13T19:05:08+00:00",
        "body": "Yes, I am using langchain community 0.3.14 already..."
      },
      {
        "author": "PotatoSpudowski",
        "created_at": "2025-01-23T05:42:29+00:00",
        "body": "I still face this issue with `langchain-community==0.3.14`\n@ccurme what do you suggest we do to override this behaviour as a quick fix till this gets fixed for good? "
      }
    ],
    "url": "https://github.com/langchain-ai/langchain/issues/28997"
  },
  {
    "repository": "langchain-ai/langchain",
    "issue_number": 28989,
    "title": "SlackToolkit Problem - sample code does not work",
    "body": "### Discussed in https://github.com/langchain-ai/langchain/discussions/28566\r\n\r\n<div type='discussions-op-text'>\r\n\r\n<sup>Originally posted by **edwardgonen** December  6, 2024</sup>\r\n### Checked other resources\r\n\r\n- [X] I added a very descriptive title to this question.\r\n- [X] I searched the LangChain documentation with the integrated search.\r\n- [X] I used the GitHub search to find a similar question and didn't find it.\r\n\r\n### Commit to Help\r\n\r\n- [X] I commit to help with one of those options ðŸ‘†\r\n\r\n### Example Code\r\n\r\n```python\r\nfrom langchain_community.agent_toolkits import SlackToolkit\r\n\r\ntoolkit = SlackToolkit()\r\n```\r\n\r\n\r\n### Description\r\n\r\nI have the \r\nSLACK_USER_TOKEN=<my token> set correctly and installed all the needed packages including slack_sdk\r\nBut the SlackToolkin() gives exception:\r\nException has occurred: PydanticUserError\r\n`SlackToolkit` is not fully defined; you should define `WebClient`, then call `SlackToolkit.model_rebuild()`.\r\n\r\nFor further information visit https://errors.pydantic.dev/2.10/u/class-not-fully-defined\r\n  File \"/Users/user/Projects/slack/main.py\", line 13, in <module>\r\n    toolkit = SlackToolkit()\r\n              ^^^^^^^^^^^^^^\r\npydantic.errors.PydanticUserError: `SlackToolkit` is not fully defined; you should define `WebClient`, then call `SlackToolkit.model_rebuild()`.\r\n\r\nFor further information visit https://errors.pydantic.dev/2.10/u/class-not-fully-defined\r\n\r\nI've tried to add WebClient but then fail on get_tools().\r\n\r\nCan anyone help?\r\n\r\n### System Info\r\n\r\nlangchain==0.3.8\r\nlangchain-anthropic==0.3.0\r\nlangchain-community==0.3.8\r\nlangchain-core==0.3.21\r\nlangchain-experimental==0.3.3\r\nlangchain-openai==0.2.10\r\nlangchain-text-splitters==0.3.2\r\n\r\n\r\nMAC\r\n\r\nPython 3.12.3\r\n</div>",
    "state": "closed",
    "created_at": "2025-01-02T16:09:33+00:00",
    "closed_at": "2025-01-02T16:14:18+00:00",
    "updated_at": "2025-01-03T15:21:46+00:00",
    "author": "ccurme",
    "author_type": "User",
    "comments_count": 0,
    "reactions_count": 0,
    "labels": "ðŸ¤–:bug",
    "milestone": null,
    "closed_by": "ccurme",
    "resolution_time_hours": 0.07916666666666666,
    "first_comments": [],
    "url": "https://github.com/langchain-ai/langchain/issues/28989"
  },
  {
    "repository": "langchain-ai/langchain",
    "issue_number": 28966,
    "title": "DOC: The arguments used in the function declaration differ from those written in the function description",
    "body": "### URL\n\nhttps://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.base.BasePromptTemplate.html#langchain_core.prompts.base.BasePromptTemplate.partial\n\n### Checklist\n\n- [X] I added a very descriptive title to this issue.\n- [X] I included a link to the documentation page I am referring to (if applicable).\n\n### Issue with current documentation:\n\nclass : BasePromptTemplate\r\nfunction : partial\r\nIssue : The arguments used in the function declaration differ from those written in the function description\r\n```\r\n    def partial(self, **kwargs: Union[str, Callable[[], str]]) -> BasePromptTemplate:\r\n        \"\"\"Return a partial of the prompt template.\r\n\r\n        Args:\r\n            kwargs: Union[str, Callable[[], str], partial variables to set.\r\n\r\n        Returns:\r\n            BasePromptTemplate: A partial of the prompt template.\r\n        \"\"\"\r\n```\r\n\r\nSource Code : https://github.com/langchain-ai/langchain/blob/master/libs/core/langchain_core/prompts/base.py#L267 #\n\n### Idea or request for content:\n\nadd ']' at the end of Union \r\n\r\nkwargs: Union[str, Callable[[], str], partial variables to set.\r\n=> kwargs: Union[str, Callable[[], str]**_]_**, partial variables to set.",
    "state": "closed",
    "created_at": "2024-12-30T05:43:18+00:00",
    "closed_at": "2025-01-14T06:15:03+00:00",
    "updated_at": "2025-01-14T06:15:03+00:00",
    "author": "skaghzz",
    "author_type": "User",
    "comments_count": 0,
    "reactions_count": 0,
    "labels": "ðŸ¤–:docs",
    "milestone": null,
    "closed_by": "skaghzz",
    "resolution_time_hours": 360.52916666666664,
    "first_comments": [],
    "url": "https://github.com/langchain-ai/langchain/issues/28966"
  },
  {
    "repository": "langchain-ai/langchain",
    "issue_number": 28934,
    "title": "ChatOllama does not parse yfinance output correctly",
    "body": "### Checked other resources\n\n- [X] I added a very descriptive title to this issue.\n- [X] I searched the LangChain documentation with the integrated search.\n- [X] I used the GitHub search to find a similar question and didn't find it.\n- [X] I am sure that this is a bug in LangChain rather than my code.\n- [X] The bug is not resolved by updating to the latest stable version of LangChain (or the specific integration package).\n\n### Example Code\n\n\r\nfrom dotenv import load_dotenv\r\nimport os\r\n\r\nload_dotenv()\r\n\r\nfrom langchain_ollama import ChatOllama \r\nfrom langchain_openai import ChatOpenAI\r\nfrom langchain_groq import ChatGroq\r\nfrom langchain_core.output_parsers import StrOutputParser\r\nfrom langchain_core.runnables import RunnablePassthrough \r\nfrom langchain_core.prompts import ChatPromptTemplate\r\n\r\nfrom langchain_core.tools import tool\r\nimport yfinance as yf\r\n\r\nllm = ChatOllama(model='llama3.1', temperature=0)\r\n#llm = ChatGroq(model='llama-3.1-8b-instant', temperature =0)\r\n#llm = ChatOpenAI(model='gpt-4o-mini', temperature = 0)\r\n\r\nfrom langchain_core.tools import tool, StructuredTool\r\nfrom datetime import date\r\n\r\n@tool\r\ndef company_information(ticker: str) -> dict:\r\n    \"\"\"Use this tool to retrieve company information like address, industry, sector, company officers, business summary, website,\r\n       marketCap, current price, ebitda, total debt, total revenue, debt-to-equity, etc.\"\"\"\r\n    \r\n    ticker_obj = yf.Ticker(ticker)\r\n    ticker_info = ticker_obj.get_info()\r\n\r\n    return ticker_info\r\n\r\n@tool\r\ndef last_dividend_and_earnings_date(ticker: str) -> dict:\r\n    \"\"\"\r\n    Use this tool to retrieve company's last dividend date and earnings release dates.\r\n    It does not provide information about historical dividend yields.\r\n    \"\"\"\r\n    ticker_obj = yf.Ticker(ticker)\r\n    \r\n    return ticker_obj.get_calendar()\r\n\r\n@tool\r\ndef summary_of_mutual_fund_holders(ticker: str) -> dict:\r\n    \"\"\"\r\n    Use this tool to retrieve company's top mutual fund holders. \r\n    It also returns their percentage of share, stock count and value of holdings.\r\n    \"\"\"\r\n    ticker_obj = yf.Ticker(ticker)\r\n    mf_holders = ticker_obj.get_mutualfund_holders()\r\n    \r\n    return mf_holders.to_dict(orient=\"records\")\r\n\r\n@tool\r\ndef summary_of_institutional_holders(ticker: str) -> dict:\r\n    \"\"\"\r\n    Use this tool to retrieve company's top institutional holders. \r\n    It also returns their percentage of share, stock count and value of holdings.\r\n    \"\"\"\r\n    ticker_obj = yf.Ticker(ticker)   \r\n    inst_holders = ticker_obj.get_institutional_holders()\r\n    \r\n    return inst_holders.to_dict(orient=\"records\")\r\n\r\n@tool\r\ndef stock_grade_updrages_downgrades(ticker: str) -> dict:\r\n    \"\"\"\r\n    Use this to retrieve grade ratings upgrades and downgrades details of particular stock.\r\n    It'll provide name of firms along with 'To Grade' and 'From Grade' details. Grade date is also provided.\r\n    \"\"\"\r\n    ticker_obj = yf.Ticker(ticker)\r\n    \r\n    curr_year = date.today().year\r\n    \r\n    upgrades_downgrades = ticker_obj.get_upgrades_downgrades()\r\n    upgrades_downgrades = upgrades_downgrades.loc[upgrades_downgrades.index > f\"{curr_year}-01-01\"]\r\n    upgrades_downgrades = upgrades_downgrades[upgrades_downgrades[\"Action\"].isin([\"up\", \"down\"])]\r\n    \r\n    return upgrades_downgrades.to_dict(orient=\"records\")\r\n\r\n@tool\r\ndef stock_splits_history(ticker: str) -> dict:\r\n    \"\"\"\r\n    Use this tool to retrieve company's historical stock splits data.\r\n    \"\"\"\r\n    ticker_obj = yf.Ticker(ticker)\r\n    hist_splits = ticker_obj.get_splits()\r\n    \r\n    return hist_splits.to_dict()\r\n\r\n@tool\r\ndef stock_news(ticker: str) -> dict:\r\n    \"\"\"\r\n    Use this to retrieve latest news articles discussing particular stock ticker.\r\n    \"\"\"\r\n    ticker_obj = yf.Ticker(ticker)\r\n    \r\n    return ticker_obj.get_news()\r\n\r\n\r\nfrom langchain.agents import AgentExecutor\r\nfrom langchain.agents import create_tool_calling_agent\r\n\r\n#from langchain.prompts import ChatPromptTemplate\r\nfrom langchain_core.prompts import MessagesPlaceholder\r\n\r\ntools = [\r\n         company_information,\r\n         last_dividend_and_earnings_date,\r\n         stock_splits_history,\r\n         summary_of_mutual_fund_holders,\r\n         summary_of_institutional_holders, \r\n         stock_grade_updrages_downgrades,\r\n         stock_news\r\n]\r\nprompt = ChatPromptTemplate.from_messages(\r\n    [\r\n        (\r\n            \"system\",\r\n            \"You are a helpful assistant. Try to answer user query using available tools. Parse the input carefully.\",\r\n        ),\r\n        MessagesPlaceholder(variable_name=\"messages\"),\r\n        MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\r\n    ]\r\n)\r\n\r\nagent = create_tool_calling_agent(llm, tools, prompt)\r\n\r\nagent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\r\n\r\nfrom langchain_core.messages import HumanMessage\r\n\r\nresp = agent_executor.invoke({\"messages\": [HumanMessage(content=\"What is address of Nike?\")]})\r\n\n\n### Error Message and Stack Trace (if applicable)\n\n_No response_\n\n### Description\n\nI am testing local LLMs with ChatOllama and using llama3.1 for answering output from yfinance. I found that it cannot answer simple question like \"What is Nike's address?\". It can answer longer questions okay, just not short question. \r\n\r\nIt works pretty well with ChatOpenAI() and ChatGroq() with the same model. You can reproduce the issue with the attached code. \n\n### System Info\n\nSystem Information\r\n------------------\r\n> OS:  Windows\r\n> OS Version:  10.0.22631\r\n> Python Version:  3.12.8 (tags/v3.12.8:2dc476b, Dec  3 2024, 19:30:04) [MSC v.1942 64 bit (AMD64)]\r\n\r\nPackage Information\r\n-------------------\r\n> langchain_core: 0.3.28\r\n> langchain: 0.3.12\r\n> langchain_community: 0.3.12\r\n> langsmith: 0.2.3\r\n> langchain_groq: 0.2.2\r\n> langchain_huggingface: 0.1.0\r\n> langchain_ollama: 0.2.1\r\n> langchain_openai: 0.2.12\r\n> langchain_text_splitters: 0.3.3\r\n> langgraph_sdk: 0.1.45\r\n\r\nOptional packages not installed\r\n-------------------------------\r\n> langserve\r\n\r\nOther Dependencies\r\n------------------\r\n> aiohttp: 3.11.10\r\n> async-timeout: Installed. No version info available.\r\n> dataclasses-json: 0.6.7\r\n> groq: 0.13.1\r\n> httpx: 0.27.2\r\n> httpx-sse: 0.4.0\r\n> huggingface-hub: 0.25.1\r\n> jsonpatch: 1.33\r\n> langsmith-pyo3: Installed. No version info available.\r\n> numpy: 1.26.4\r\n> ollama: 0.4.4\r\n> openai: 1.58.1\r\n> orjson: 3.10.12\r\n> packaging: 24.2\r\n> pydantic: 2.9.2\r\n> pydantic-settings: 2.7.0\r\n> PyYAML: 6.0.2\r\n> requests: 2.32.3\r\n> requests-toolbelt: 1.0.0\r\n> sentence-transformers: 3.3.1\r\n> SQLAlchemy: 2.0.36\r\n> tenacity: 9.0.0\r\n> tiktoken: 0.8.0\r\n> tokenizers: 0.21.0\r\n> transformers: 4.47.0\r\n> typing-extensions: 4.12.2",
    "state": "closed",
    "created_at": "2024-12-27T02:33:24+00:00",
    "closed_at": "2025-01-06T01:18:33+00:00",
    "updated_at": "2025-01-06T01:18:34+00:00",
    "author": "dlin95123",
    "author_type": "User",
    "comments_count": 3,
    "reactions_count": 0,
    "labels": "ðŸ¤–:bug,â±­:  models",
    "milestone": null,
    "closed_by": "dlin95123",
    "resolution_time_hours": 238.7525,
    "first_comments": [
      {
        "author": "keenborder786",
        "created_at": "2024-12-27T10:29:32+00:00",
        "body": "You need to update your system Prompt and make it more comprehensive, something like this should work:\r\n\r\n```python\r\n\r\n\r\n\r\n## System Prompt for Agent\r\n\r\nYou are a financial insights assistant equipped with specialized tools for retrieving and analyzing stock market data and company information. You have access to the following tools to assist users effectively:\r\n\r\n1. **`company_information(ticker: str)`**  \r\n   Retrieve comprehensive company details, including address, industry, sector, officers, business summary, website, financial metrics (e.g., market cap, EBITDA, total debt, revenue, debt-to-equity ratio), and current price.\r\n\r\n2. **`last_dividend_and_earnings_date(ticker: str)`**  \r\n   Get the most recent dividend payment date and earnings release dates for a company.\r\n\r\n3. **`summary_of_mutual_fund_holders(ticker: str)`**  \r\n   Provide a summary of the top mutual fund holders, including their percentage of shares, stock count, and the value of their holdings.\r\n\r\n4. **`summary_of_institutional_holders(ticker: str)`**  \r\n   Offer insights into the top institutional holders of a company, detailing their percentage of shares, stock count, and the value of their holdings.\r\n\r\n5. **`stock_grade_updrages_downgrades(ticker: str)`**  \r\n   Retrieve the latest grade rating upgrades and downgrades for a stock, including the firms involved, the updated grades, previous grades, and the dates of these actions.\r\n\r\n6. **`stock_splits_history(ticker: str)`**  \r\n   Access the historical stock split data for a company.\r\n\r\n7. **`stock_news(ticker: str)`**  \r\n   Fetch the latest news articles discussing the specified stock ticker.\r\n\r\n### Role and Instructions\r\nYou act as a professional financial assistant for users, providing accurate and timely information based on their queries. Your role involves:\r\n- Understanding user needs and selecting the appropriate tool.\r\n- Explaining the information retrieved clearly and concisely.\r\n- Providing actionable insights or clarifications when necessary.\r\n\r\nWhen a user mentions a stock ticker (e.g., \"AAPL\" for Apple Inc.), ensure:\r\n- You confirm the ticker's validity before processing.\r\n- You provide responses in a structured and professional tone.\r\n\r\nAlways prioritize user understanding by avoiding jargon and ensuring outputs are directly relevant to their inquiries.\r\n\r\n\r\n```"
      },
      {
        "author": "dlin95123",
        "created_at": "2024-12-27T21:13:11+00:00",
        "body": "Your modified system prompt yields the same results. It does not improve the results. Note that ChatGroq yields correct answers for both system prompts (my original and your modified) with the same LLM model, Llama3.1:8b, as used for ChatOllama. With ChatGroq, it answers the address correctly. But with ChatOllama, it yields the following answer:\r\n\r\nBased on the provided JSON data, here are some key statistics and insights about Nike, Inc. (NKE):\r\n\r\n**Financials**\r\n\r\n* Market Capitalization: $113.8 billion\r\n* Enterprise Value: $116.6 billion\r\n* Total Debt: $121.3 billion\r\n* Cash and Cash Equivalents: $10.2 billion\r\n* Earnings Per Share (EPS): $3.24 (trailing), $3.23 (forward)\r\n* Revenue Growth: -0.104% (quarter-over-quarter)\r\n\r\n**Valuation**\r\n\r\n* Price-to-Earnings (P/E) Ratio: 23.59 (trailing), 29.07 (forward)\r\n* Price-to-Book (P/B) Ratio: 7.96\r\n* Dividend Yield: 0.0208%\r\n* Forward P/E Ratio: 29.07\r\n\r\n**Growth**\r\n\r\n* Earnings Growth: -0.255% (quarter-over-quarter)\r\n* Revenue Growth: -0.104% (quarter-over-quarter)\r\n\r\n**Operational Performance**\r\n\r\n* Gross Margin: 44.97%\r\n* Operating Margin: 10.43%\r\n* Return on Equity (ROE): 37.98%\r\n* Free Cash Flow: $6.57 billion\r\n\r\n**Analyst Estimates**\r\n\r\n* Average Analyst Rating: Buy\r\n* Recommendation Mean: 2.18\r\n* Number of Analysts: 34\r\n\r\n**Industry and Market Positioning**\r\n\r\n* Industry: Consumer Discretionary\r\n* Sub-Industry: Apparel & Footwear Retailers\r\n* Market Capitalization Rank: #1 in Consumer Discretionary, #4 in Apparel & Footwear Retailers\r\n\r\nPlease note that these statistics are based on the provided JSON data and may not reflect the current market situation or future performance of Nike, Inc. (NKE)."
      },
      {
        "author": "dlin95123",
        "created_at": "2025-01-06T01:18:33+00:00",
        "body": "It turns out that ChatOllama has an additional parameter, num_ctx. The default is only 2048. After increasing it to larger numbers, ChatOllama works correctly too. "
      }
    ],
    "url": "https://github.com/langchain-ai/langchain/issues/28934"
  },
  {
    "repository": "langchain-ai/langchain",
    "issue_number": 28933,
    "title": "community: Raise KeyError: 'request' in module langchain_community.llms.tongyi ",
    "body": "### Checked other resources\n\n- [X] I added a very descriptive title to this issue.\n- [X] I searched the LangChain documentation with the integrated search.\n- [X] I used the GitHub search to find a similar question and didn't find it.\n- [X] I am sure that this is a bug in LangChain rather than my code.\n- [X] The bug is not resolved by updating to the latest stable version of LangChain (or the specific integration package).\n\n### Example Code\n\n```python\r\n        content = None\r\n        err = None\r\n        try:\r\n            content = self.chain.invoke(input=dict(topic=topic,\r\n                                         section=section,\r\n                                         topic_description=topic_description.strip(),\r\n                                         section_description=section_description.strip(),\r\n                                         formatted_text=formatted_text\r\n                                         ))\r\n        except Exception as e:\r\n            logger.error(f\"{self.__class__} run error: {repr(e)}\")\r\n            logger.error(traceback.format_exc())\r\n            err = e\r\n```\n\n### Error Message and Stack Trace (if applicable)\n\n\r\n<img width=\"988\" alt=\"image\" src=\"https://github.com/user-attachments/assets/529ff319-80f7-451b-a4a8-7aab477cb4ce\" />\r\n\r\n<img width=\"975\" alt=\"image\" src=\"https://github.com/user-attachments/assets/0c63e7d8-840b-443d-9dca-78bc24aecbec\" />\r\n\n\n### Description\n\n1. I'm trying to use Tongyi llm model implemented in langchain_community.llms to do sth.\r\n2. When I call chain.invoke(*arg, **kwargs), Sometimes it raise Error: KeyError: 'request' for unknown reason. \r\n4. It seems like the api_base_url by default is not stable., but I'm not sure.\n\n### System Info\n\n------------------\r\n> OS:  Darwin\r\n> OS Version:  Darwin Kernel Version 24.2.0: Fri Dec  6 18:56:34 PST 2024; root:xnu-11215.61.5~2/RELEASE_ARM64_T6020\r\n> Python Version:  3.11.10 (main, Oct  3 2024, 02:26:51) [Clang 14.0.6 ]\r\n\r\nPackage Information\r\n-------------------\r\n> langchain_core: 0.3.22\r\n> langchain: 0.3.10\r\n> langchain_community: 0.3.10\r\n> langsmith: 0.1.147\r\n> langchain_openai: 0.2.11\r\n> langchain_text_splitters: 0.3.2\r\n\r\nOptional packages not installed\r\n-------------------------------\r\n> langserve\r\n\r\nOther Dependencies\r\n------------------\r\n> aiohttp: 3.10.10\r\n> async-timeout: Installed. No version info available.\r\n> dataclasses-json: 0.6.7\r\n> httpx: 0.27.2\r\n> httpx-sse: 0.4.0\r\n> jsonpatch: 1.33\r\n> langsmith-pyo3: Installed. No version info available.\r\n> numpy: 1.26.4\r\n> openai: 1.54.3\r\n> orjson: 3.10.12\r\n> packaging: 23.2\r\n> pydantic: 2.9.2\r\n> pydantic-settings: 2.6.1\r\n> PyYAML: 6.0.2\r\n> requests: 2.32.3\r\n> requests-toolbelt: 1.0.0\r\n> SQLAlchemy: 2.0.36\r\n> tenacity: 8.5.0\r\n> tiktoken: 0.8.0\r\n> typing-extensions: 4.12.2\r\n",
    "state": "closed",
    "created_at": "2024-12-26T15:50:17+00:00",
    "closed_at": "2024-12-27T10:45:52+00:00",
    "updated_at": "2024-12-27T10:45:52+00:00",
    "author": "Coolgiserz",
    "author_type": "User",
    "comments_count": 3,
    "reactions_count": 0,
    "labels": "ðŸ¤–:bug",
    "milestone": null,
    "closed_by": "Coolgiserz",
    "resolution_time_hours": 18.926388888888887,
    "first_comments": [
      {
        "author": "keenborder786",
        "created_at": "2024-12-26T18:23:20+00:00",
        "body": "@Coolgiserz this might be a problem `dashscope` package itself."
      },
      {
        "author": "Coolgiserz",
        "created_at": "2024-12-27T07:29:08+00:00",
        "body": "Thanks for your reply. \r\nAfter conducting some debugging, Iâ€™ve discovered that the dashscope package seems to be masking actual user-related errors. Instead of exposing the specific issues related to the userâ€™s actions or environment, it throws a generic internal processing exception.\r\n![image](https://github.com/user-attachments/assets/94d4f5f4-cc89-40df-a512-f555d76deb50)\r\n\r\nI will further investigate this behavior and provide more details if needed. "
      },
      {
        "author": "Coolgiserz",
        "created_at": "2024-12-27T10:45:52+00:00",
        "body": "![image](https://github.com/user-attachments/assets/d2a05c49-31da-4ffd-8e00-c79b0b9fd2fd)\r\nSometimes raise KeyError while reaching request rate limit."
      }
    ],
    "url": "https://github.com/langchain-ai/langchain/issues/28933"
  },
  {
    "repository": "langchain-ai/langchain",
    "issue_number": 28923,
    "title": "Tongyi llm call error with model \"qwen-long\"",
    "body": "### Checked other resources\n\n- [X] I added a very descriptive title to this issue.\n- [X] I searched the LangChain documentation with the integrated search.\n- [X] I used the GitHub search to find a similar question and didn't find it.\n- [X] I am sure that this is a bug in LangChain rather than my code.\n- [X] The bug is not resolved by updating to the latest stable version of LangChain (or the specific integration package).\n\n### Example Code\n\n```python\r\nfrom langchain_community.llms.tongyi import Tongyi\r\nimport os\r\n\r\ntongyi = Tongyi(model=\"qwen-turbo\", api_key=api_key)\r\n\r\nresponse = tongyi.invoke(\"Who are you?\")\r\n\r\nprint(response)\r\n\r\n```\n\n### Error Message and Stack Trace (if applicable)\n\n```python\r\nTraceback (most recent call last):\r\n  File \"/Users/feng/Work/my/tests/qwen/api_test.py\", line 33, in <module>\r\n    response = tongyi.invoke(\"Who are you?\")\r\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/feng/Work/my/tests/qwen/.venv/lib/python3.12/site-packages/langchain_core/language_models/llms.py\", line 390, in invoke\r\n    self.generate_prompt(\r\n  File \"/Users/feng/Work/my/tests/qwen/.venv/lib/python3.12/site-packages/langchain_core/language_models/llms.py\", line 755, in generate_prompt\r\n    return self.generate(prompt_strings, stop=stop, callbacks=callbacks, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/feng/Work/my/tests/qwen/.venv/lib/python3.12/site-packages/langchain_core/language_models/llms.py\", line 950, in generate\r\n    output = self._generate_helper(\r\n             ^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/feng/Work/my/tests/qwen/.venv/lib/python3.12/site-packages/langchain_core/language_models/llms.py\", line 792, in _generate_helper\r\n    raise e\r\n  File \"/Users/feng/Work/my/tests/qwen/.venv/lib/python3.12/site-packages/langchain_core/language_models/llms.py\", line 779, in _generate_helper\r\n    self._generate(\r\n  File \"/Users/feng/Work/my/tests/qwen/.venv/lib/python3.12/site-packages/langchain_community/llms/tongyi.py\", line 327, in _generate\r\n    [Generation(**self._generation_from_qwen_resp(completion))]\r\n     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/feng/Work/my/tests/qwen/.venv/lib/python3.12/site-packages/langchain_core/load/serializable.py\", line 125, in __init__\r\n    super().__init__(*args, **kwargs)\r\n  File \"/Users/feng/Work/my/tests/qwen/.venv/lib/python3.12/site-packages/pydantic/main.py\", line 214, in __init__\r\n    validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)\r\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\npydantic_core._pydantic_core.ValidationError: 1 validation error for Generation\r\ntext\r\n  Input should be a valid string [type=string_type, input_value=None, input_type=NoneType]\r\n    For further information visit https://errors.pydantic.dev/2.10/v/string_type\r\n\r\n```\n\n### Description\n\nI tried other models listed [here](https://help.aliyun.com/zh/model-studio/getting-started/models?spm=a2c4g.11186623.help-menu-2400256.d_0_2.41f9253aV5dLgB)  [qwen-max, qwen-turbo, qwen-plus] which are all working fine\r\n\r\n\r\nI debugged and stepped into the source code and found that [this line](https://github.com/langchain-ai/langchain/blob/5991b45a88fe60fe511c7a64a0a5f1dbb2410b08/libs/community/langchain_community/llms/tongyi.py#L454)  should be the root cause as for \"qwen-long\" the api returns \"choices\" instead of \"text\" which results in a Nonetype\r\n\r\nyou can debug and replicate it like\r\n```python\r\nfrom langchain_community.llms.tongyi import Tongyi\r\nimport os\r\n\r\noriginal_generate = Tongyi._generate\r\n\r\ndef debug_generate(self, prompts, stop=None, run_manager=None, **kwargs):\r\n    breakpoint()  \r\n    return original_generate(self, prompts, stop, run_manager, **kwargs)\r\n\r\n# Patch the method\r\nTongyi._generate = debug_generate\r\napi_key = os.getenv(\"DASHSCOPE_API_KEY\")\r\n\r\ntongyi = Tongyi(model=\"qwen-turbo\", api_key=api_key)\r\n\r\nresponse = tongyi.invoke(\"Who are you?\")\r\n\r\nprint(response)\r\n```\r\nThis issue could potentially be resolved by checking both resp[\"output\"][\"text\"] and resp[\"output\"][\"choices\"] for the response. However, itâ€™s unclear if this inconsistency stems from the Tongyi server. The moduleâ€™s maintainer might be able to provide clarification.\n\n### System Info\n\n```\r\nSystem Information\r\n------------------\r\n> OS:  Darwin\r\n> OS Version:  Darwin Kernel Version 24.1.0: Thu Oct 10 21:03:15 PDT 2024; root:xnu-11215.41.3~2/RELEASE_ARM64_T6000\r\n> Python Version:  3.12.5 (main, Aug 14 2024, 04:32:18) [Clang 18.1.8 ]\r\n\r\nPackage Information\r\n-------------------\r\n> langchain_core: 0.3.28\r\n> langchain: 0.3.13\r\n> langchain_community: 0.3.13\r\n> langsmith: 0.2.6\r\n> langchain_text_splitters: 0.3.4\r\n\r\nOptional packages not installed\r\n-------------------------------\r\n> langserve\r\n\r\nOther Dependencies\r\n------------------\r\n> aiohttp: 3.11.11\r\n> async-timeout: Installed. No version info available.\r\n> dataclasses-json: 0.6.7\r\n> httpx: 0.28.1\r\n> httpx-sse: 0.4.0\r\n> jsonpatch: 1.33\r\n> langsmith-pyo3: Installed. No version info available.\r\n> numpy: 2.2.1\r\n> orjson: 3.10.12\r\n> packaging: 24.2\r\n> pydantic: 2.10.4\r\n> pydantic-settings: 2.7.0\r\n> PyYAML: 6.0.2\r\n> requests: 2.32.3\r\n> requests-toolbelt: 1.0.0\r\n> SQLAlchemy: 2.0.36\r\n> tenacity: 9.0.0\r\n> typing-extensions: 4.12.2\r\n> zstandard: Installed. No version info available.\r\n```",
    "state": "closed",
    "created_at": "2024-12-25T22:48:02+00:00",
    "closed_at": "2024-12-27T13:57:42+00:00",
    "updated_at": "2024-12-27T13:57:52+00:00",
    "author": "niuguy",
    "author_type": "User",
    "comments_count": 4,
    "reactions_count": 0,
    "labels": "",
    "milestone": null,
    "closed_by": "niuguy",
    "resolution_time_hours": 39.16111111111111,
    "first_comments": [
      {
        "author": "keenborder786",
        "created_at": "2024-12-26T18:55:28+00:00",
        "body": "qwen-long might be a chat-based model therefore it is returning the output in `output.choices`. Can you try the same call with `langchain_community.chat_models.tongyi.ChatTongyi` and it should work."
      },
      {
        "author": "niuguy",
        "created_at": "2024-12-27T00:14:24+00:00",
        "body": "@keenborder786 Thanks for your response. However, is it possible to use this model consistently with other models? as I would allow users to switch models in my app. Additionally, qwen-long is supposed to handle extremely long prompts, making it ideal for one-shot invocations."
      },
      {
        "author": "keenborder786",
        "created_at": "2024-12-27T10:11:49+00:00",
        "body": "Why don't you just use `langchain_community.chat_models.tongyi.ChatTongyi`"
      },
      {
        "author": "niuguy",
        "created_at": "2024-12-27T13:55:55+00:00",
        "body": "OK I see what you mean, yes ChatTongyi works for all. Thanks "
      }
    ],
    "url": "https://github.com/langchain-ai/langchain/issues/28923"
  },
  {
    "repository": "langchain-ai/langchain",
    "issue_number": 28901,
    "title": "bedrock prepare input not work properly for all models",
    "body": "### Checked other resources\n\n- [X] I added a very descriptive title to this issue.\n- [X] I searched the LangChain documentation with the integrated search.\n- [X] I used the GitHub search to find a similar question and didn't find it.\n- [X] I am sure that this is a bug in LangChain rather than my code.\n- [X] The bug is not resolved by updating to the latest stable version of LangChain (or the specific integration package).\n\n### Example Code\n\n\r\nfrom langchain_community.chat_models import BedrockChat\r\n\r\nconfig = {\r\n    \"model_id\": \"amazon.nova-lite-v1:0\",\r\n    'model_kwargs': {\r\n        \"max_tokens\": 1000,\r\n        \"temperature\": 0.7\r\n    }\r\n}\r\n\r\nllm = BedrockChat(**config)\r\n\r\nllm.invoke('hi')\n\n### Error Message and Stack Trace (if applicable)\n\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.9/site-packages/langchain_community/llms/bedrock.py\", line 547, in _prepare_input_and_invoke\r\n    response = self.client.invoke_model(**request_options)\r\n  File \"/usr/local/lib/python3.9/site-packages/botocore/client.py\", line 569, in _api_call\r\n    return self._make_api_call(operation_name, kwargs)\r\n  File \"/usr/local/lib/python3.9/site-packages/botocore/client.py\", line 1023, in _make_api_call\r\n    raise error_class(parsed_response, operation_name)\r\nbotocore.errorfactory.ValidationException: An error occurred (ValidationException) when calling the InvokeModel operation: Malformed input request: #: required key [messages] not found, please reformat your input and try again.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\n  File \"/usr/local/lib/python3.9/site-packages/langchain_core/language_models/chat_models.py\", line 286, in invoke\r\n    self.generate_prompt(\r\n  File \"/usr/local/lib/python3.9/site-packages/langchain_core/language_models/chat_models.py\", line 786, in generate_prompt\r\n    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\r\n  File \"/usr/local/lib/python3.9/site-packages/langchain_core/language_models/chat_models.py\", line 643, in generate\r\n    raise e\r\n  File \"/usr/local/lib/python3.9/site-packages/langchain_core/language_models/chat_models.py\", line 633, in generate\r\n    self._generate_with_cache(\r\n  File \"/usr/local/lib/python3.9/site-packages/langchain_core/language_models/chat_models.py\", line 851, in _generate_with_cache\r\n    result = self._generate(\r\n  File \"/usr/local/lib/python3.9/site-packages/langchain_community/chat_models/bedrock.py\", line 299, in _generate\r\n    completion, usage_info = self._prepare_input_and_invoke(\r\n  File \"/usr/local/lib/python3.9/site-packages/langchain_community/llms/bedrock.py\", line 554, in _prepare_input_and_invoke\r\n    raise ValueError(f\"Error raised by bedrock service: {e}\")\r\nValueError: Error raised by bedrock service: An error occurred (ValidationException) when calling the InvokeModel operation: Malformed input request: #: required key [messages] not found, please reformat your input and try again.\n\n### Description\n\nthe problem ins on prepare input of langchain_community.llms.bedrock.py lane 112 aprox\r\n    @classmethod\r\n    def prepare_input(\r\n\r\nthe body is not seted correctly for this model it works for titan but not for the nova for example\n\n### System Info\n\npip freeze\r\naiohappyeyeballs==2.4.4\r\naiohttp==3.11.11\r\naiosignal==1.3.2\r\nannotated-types==0.7.0\r\nanyio==4.7.0\r\nasync-timeout==4.0.3\r\nattrs==24.3.0\r\nboto3==1.35.85\r\nbotocore==1.35.85\r\ncachetools==5.5.0\r\ncertifi==2024.12.14\r\ncharset-normalizer==3.4.0\r\ndataclasses-json==0.6.7\r\nexceptiongroup==1.2.2\r\nfiletype==1.2.0\r\nfrozenlist==1.5.0\r\ngoogle-ai-generativelanguage==0.6.10\r\ngoogle-api-core==2.24.0\r\ngoogle-api-python-client==2.156.0\r\ngoogle-auth==2.37.0\r\ngoogle-auth-httplib2==0.2.0\r\ngoogle-generativeai==0.8.3\r\ngoogleapis-common-protos==1.66.0\r\ngreenlet==3.1.1\r\ngrpcio==1.68.1\r\ngrpcio-status==1.68.1\r\nh11==0.14.0\r\nhttpcore==1.0.7\r\nhttplib2==0.22.0\r\nhttpx==0.28.1\r\nhttpx-sse==0.4.0\r\nidna==3.10\r\njmespath==1.0.1\r\njsonpatch==1.33\r\njsonpointer==3.0.0\r\nlangchain==0.3.13\r\nlangchain-community==0.3.13\r\nlangchain-core==0.3.28\r\nlangchain-google-genai==2.0.7\r\nlangchain-text-splitters==0.3.4\r\nlanggraph==0.2.60\r\nlanggraph-checkpoint==2.0.9\r\nlanggraph-sdk==0.1.48\r\nlangsmith==0.2.4\r\nmarshmallow==3.23.2\r\nmsgpack==1.1.0\r\nmultidict==6.1.0\r\nmypy-extensions==1.0.0\r\nnumpy==1.26.4\r\norjson==3.10.12\r\npackaging==24.2\r\npropcache==0.2.1\r\nproto-plus==1.25.0\r\nprotobuf==5.29.2\r\npyasn1==0.6.1\r\npyasn1_modules==0.4.1\r\npydantic==2.10.4\r\npydantic-settings==2.7.0\r\npydantic_core==2.27.2\r\npyparsing==3.2.0\r\npython-dateutil==2.9.0.post0\r\npython-dotenv==1.0.1\r\nPyYAML==6.0.2\r\nrequests==2.32.3\r\nrequests-toolbelt==1.0.0\r\nrsa==4.9\r\ns3transfer==0.10.4\r\nsix==1.17.0\r\nsniffio==1.3.1\r\nSQLAlchemy==2.0.36\r\ntenacity==9.0.0\r\ntqdm==4.67.1\r\ntyping-inspect==0.9.0\r\ntyping_extensions==4.12.2\r\nuritemplate==4.1.1\r\nurllib3==1.26.20\r\nyarl==1.18.3",
    "state": "closed",
    "created_at": "2024-12-24T09:34:33+00:00",
    "closed_at": "2024-12-24T20:26:33+00:00",
    "updated_at": "2024-12-24T20:26:33+00:00",
    "author": "anglisano",
    "author_type": "User",
    "comments_count": 1,
    "reactions_count": 0,
    "labels": "ðŸ¤–:bug,investigate",
    "milestone": null,
    "closed_by": "efriis",
    "resolution_time_hours": 10.866666666666667,
    "first_comments": [
      {
        "author": "efriis",
        "created_at": "2024-12-24T20:26:33+00:00",
        "body": "this integration is deprecated. Would recommend\r\n\r\n```\r\n%pip install langchain-aws\r\nfrom langchain_aws import ChatBedrock\r\n```\r\n\r\nand if you see the same issue, please file an issue for the aws folks to see at https://github.com/langchain-ai/langchain-aws !"
      }
    ],
    "url": "https://github.com/langchain-ai/langchain/issues/28901"
  },
  {
    "repository": "langchain-ai/langchain",
    "issue_number": 28900,
    "title": "message of type RemoveMessage is not handled in langchain-",
    "body": "### Checked other resources\n\n- [X] I added a very descriptive title to this issue.\n- [X] I searched the LangChain documentation with the integrated search.\n- [X] I used the GitHub search to find a similar question and didn't find it.\n- [X] I am sure that this is a bug in LangChain rather than my code.\n- [X] The bug is not resolved by updating to the latest stable version of LangChain (or the specific integration package).\n\n### Example Code\n\n```\r\nfrom langchain_openai.chat_models.base import _convert_message_to_dict\r\nfrom langchain_core.messages import RemoveMessage\r\n_convert_message_to_dict(RemoveMessage(id=\"1\"))\r\n```\n\n### Error Message and Stack Trace (if applicable)\n\n```\r\nTraceback (most recent call last):\r\n  File \"/Users/projects/my-chat/main.py\", line 23, in <module>\r\n    _convert_message_to_dict(RemoveMessage(id=\"1\"))\r\n  File \"/Users/.pyenv/versions/chat/lib/python3.11/site-packages/langchain_openai/chat_models/base.py\", line 257, in _convert_message_to_dict\r\n    raise TypeError(f\"Got unknown type {message}\")\r\nTypeError: Got unknown type content='' additional_kwargs={} response_metadata={} id='1'\r\n```\n\n### Description\n\nIn langchain_openai.chat_models.base (and more specifically in langchain_openai.chat_models.base._convert_message_to_dict) there is no handling of messages of type RemoveMessage and it fails over unknown type\n\n### System Info\n\n```\r\nSystem Information\r\n------------------\r\n> OS:  Darwin\r\n> OS Version:  Darwin Kernel Version 23.6.0: Thu Sep 12 23:35:10 PDT 2024; root:xnu-10063.141.1.701.1~1/RELEASE_ARM64_T6030\r\n> Python Version:  3.11.6 (main, Jun 24 2024, 09:02:31) [Clang 15.0.0 (clang-1500.3.9.4)]\r\n\r\nPackage Information\r\n-------------------\r\n> langchain_core: 0.3.28\r\n> langchain: 0.3.13\r\n> langchain_community: 0.3.13\r\n> langsmith: 0.2.4\r\n> langchain_experimental: 0.3.4\r\n> langchain_openai: 0.2.14\r\n> langchain_text_splitters: 0.3.4\r\n> langgraph_api: 0.0.15\r\n> langgraph_cli: 0.1.65\r\n> langgraph_license: Installed. No version info available.\r\n> langgraph_platform: Installed. No version info available.\r\n> langgraph_sdk: 0.1.48\r\n> langgraph_storage: Installed. No version info available.\r\n\r\nOptional packages not installed\r\n-------------------------------\r\n> langserve\r\n\r\nOther Dependencies\r\n------------------\r\n> aiohttp: 3.11.11\r\n> async-timeout: Installed. No version info available.\r\n> click: 8.1.8\r\n> cryptography: 43.0.3\r\n> dataclasses-json: 0.6.7\r\n> httpx: 0.28.1\r\n> httpx-sse: 0.4.0\r\n> jsonpatch: 1.33\r\n> jsonschema-rs: 0.25.1\r\n> langgraph: 0.2.60\r\n> langgraph-checkpoint: 2.0.9\r\n> langsmith-pyo3: Installed. No version info available.\r\n> numpy: 1.26.4\r\n> openai: 1.58.1\r\n> orjson: 3.10.12\r\n> packaging: 24.2\r\n> pydantic: 2.10.4\r\n> pydantic-settings: 2.7.0\r\n> pyjwt: 2.10.1\r\n> python-dotenv: 1.0.1\r\n> PyYAML: 6.0.2\r\n> requests: 2.32.3\r\n> requests-toolbelt: 1.0.0\r\n> SQLAlchemy: 2.0.36\r\n> sse-starlette: 2.1.3\r\n> starlette: 0.41.3\r\n> structlog: 24.4.0\r\n> tenacity: 8.5.0\r\n> tiktoken: 0.8.0\r\n> typing-extensions: 4.12.2\r\n> uvicorn: 0.34.0\r\n> watchfiles: 1.0.3\r\n\r\n```",
    "state": "closed",
    "created_at": "2024-12-24T08:25:59+00:00",
    "closed_at": "2024-12-27T19:35:59+00:00",
    "updated_at": "2024-12-27T19:35:59+00:00",
    "author": "daher928",
    "author_type": "User",
    "comments_count": 10,
    "reactions_count": 0,
    "labels": "ðŸ¤–:bug,investigate,â±­:  core",
    "milestone": null,
    "closed_by": "daher928",
    "resolution_time_hours": 83.16666666666667,
    "first_comments": [
      {
        "author": "daher928",
        "created_at": "2024-12-24T08:47:42+00:00",
        "body": "@hwchase17 Hello Harrison, could you please take a lot at this issue?"
      },
      {
        "author": "efriis",
        "created_at": "2024-12-24T20:24:35+00:00",
        "body": "Assigning to @vbarda ! I believe the `RemoveMessage` was intended as a langgraph feature, and we may want to add this as a feature of `BaseChatModel` to remove this confusion.\r\n\r\nI don't think this is a regression (i.e. this always behaved this way). Let me know if that seems right to you!"
      },
      {
        "author": "daher928",
        "created_at": "2024-12-24T22:00:42+00:00",
        "body": "@efriis  It happens when I try to remove messages from graph state in LangGraph and using Azure chat model\n\nI would appreciate fixing it as soon as possible! :)\n\nThanks!"
      },
      {
        "author": "daher928",
        "created_at": "2024-12-27T13:44:41+00:00",
        "body": "Hi @vbarda @efriis ,\r\nAny updates regarding this issue ?"
      },
      {
        "author": "eyurtsev",
        "created_at": "2024-12-27T14:46:28+00:00",
        "body": "Hi @daher928 it's holidays season so many of the maintainers are out of office for another week.\n\nThe issue looks like usage error rather than a bug from the given explanation and code snippet.\n\nRemove message should never be sent to a chat model. It's a message type that's used only by the persistence layer of langgraph. If you can share an example of the code you're using with langgraph.\n"
      }
    ],
    "url": "https://github.com/langchain-ai/langchain/issues/28900"
  },
  {
    "repository": "langchain-ai/langchain",
    "issue_number": 28870,
    "title": "C:\\Windows\\system32>LANGCHAIN_TRACING_V2=true 'LANGCHAIN_TRACING_V2' is not recognized as an internal or external command, operable program or batch file.",
    "body": "### URL\n\n_No response_\n\n### Checklist\n\n- [X] I added a very descriptive title to this issue.\n- [X] I included a link to the documentation page I am referring to (if applicable).\n\n### Issue with current documentation:\n\nC:\\Windows\\system32>LANGCHAIN_TRACING_V2=true\r\n'LANGCHAIN_TRACING_V2' is not recognized as an internal or external command,\r\noperable program or batch file.\n\n### Idea or request for content:\n\nC:\\Windows\\system32>LANGCHAIN_TRACING_V2=true\r\n'LANGCHAIN_TRACING_V2' is not recognized as an internal or external command,\r\noperable program or batch file.",
    "state": "closed",
    "created_at": "2024-12-21T19:37:51+00:00",
    "closed_at": "2024-12-23T20:37:24+00:00",
    "updated_at": "2024-12-23T20:37:24+00:00",
    "author": "gandhidakshina",
    "author_type": "User",
    "comments_count": 2,
    "reactions_count": 0,
    "labels": "",
    "milestone": null,
    "closed_by": "ccurme",
    "resolution_time_hours": 48.9925,
    "first_comments": [
      {
        "author": "keenborder786",
        "created_at": "2024-12-21T21:06:02+00:00",
        "body": "Sorry Can you elaborate your issue?"
      },
      {
        "author": "ccurme",
        "created_at": "2024-12-23T20:37:24+00:00",
        "body": "I think this issue is regarding how to set environment variables on Windows machines and is unrelated to LangChain."
      }
    ],
    "url": "https://github.com/langchain-ai/langchain/issues/28870"
  },
  {
    "repository": "langchain-ai/langchain",
    "issue_number": 28866,
    "title": "Error in RAG tutorials",
    "body": "### Checked other resources\r\n\r\n- [X] I added a very descriptive title to this issue.\r\n- [X] I searched the LangChain documentation with the integrated search.\r\n- [X] I used the GitHub search to find a similar question and didn't find it.\r\n- [X] I am sure that this is a bug in LangChain rather than my code.\r\n- [X] The bug is not resolved by updating to the latest stable version of LangChain (or the specific integration package).\r\n\r\n### Example Code\r\n\r\nget error when run the rag tutorials code\r\n\r\n```py\r\nimport dotenv\r\nfrom langchain import hub\r\nfrom langchain_community.document_loaders import TextLoader\r\nfrom langchain_core.documents import Document\r\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\r\nfrom langgraph.graph import START, StateGraph\r\nfrom typing_extensions import List, TypedDict\r\nfrom langchain_community.vectorstores import FAISS\r\nfrom langchain_openai import OpenAIEmbeddings\r\nfrom langchain_community.docstore.in_memory import InMemoryDocstore\r\nfrom langchain_openai import ChatOpenAI\r\nimport faiss\r\n\r\ndotenv.load_dotenv()\r\n\r\n# Load and chunk contents of the blog\r\ndocs = list[Document]\r\nloader = TextLoader(\"../devs/ragtest/input/mmkg_rag.txt\")\r\ndocs = loader.load()\r\n\r\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=1200, chunk_overlap=100)\r\nall_splits = text_splitter.split_documents(docs)\r\n\r\nembeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\r\nindex = faiss.IndexFlatL2()\r\nvector_store = FAISS(\r\n    embedding_function=embeddings,\r\n    index=index,\r\n    docstore=InMemoryDocstore(),\r\n    index_to_docstore_id={},\r\n)\r\n# Index chunks\r\n_ = vector_store.add_documents(documents=all_splits)\r\n\r\nllm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.0)\r\n\r\n# Define prompt for question-answering\r\nprompt = hub.pull(\"rlm/rag-prompt\")\r\n\r\n\r\n# Define state for application\r\nclass State(TypedDict):\r\n    question: str\r\n    context: List[Document]\r\n    answer: str\r\n\r\n\r\n# Define application steps\r\ndef retrieve(state: State):\r\n    retrieved_docs = vector_store.similarity_search(state[\"question\"])\r\n    return {\"context\": retrieved_docs}\r\n\r\n\r\ndef generate(state: State):\r\n    docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])\r\n    messages = prompt.invoke({\"question\": state[\"question\"], \"context\": docs_content})\r\n    response = llm.invoke(messages)\r\n    return {\"answer\": response.content}\r\n\r\n\r\n# Compile application and test\r\ngraph_builder = StateGraph(State).add_sequence([retrieve, generate])\r\ngraph_builder.add_edge(START, \"retrieve\")\r\ngraph = graph_builder.compile()\r\n\r\n\r\nresponse = graph.invoke({\"question\": \"What components are in MMKG_RAG?\"})\r\nprint(response[\"answer\"])\r\n```\r\n\r\n### Error Message and Stack Trace (if applicable)\r\n\r\n```txt\r\nTraceback (most recent call last):\r\n  File \"~\\workspace\\projects\\xxx\\repos\\graphrag\\repeat\\nativerag.py\", line 33, in <module>\r\n    _ = vector_store.add_documents(documents=all_splits)\r\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"~\\miniconda3\\envs\\py311\\Lib\\site-packages\\langchain_core\\vectorstores\\base.py\", line 287, in add_documents  \r\n    return self.add_texts(texts, metadatas, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"~\\miniconda3\\envs\\py311\\Lib\\site-packages\\langchain_community\\vectorstores\\faiss.py\", line 341, in add_texts\r\n    return self.__add(texts, embeddings, metadatas=metadatas, ids=ids)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"~\\miniconda3\\envs\\py311\\Lib\\site-packages\\langchain_community\\vectorstores\\faiss.py\", line 313, in __add    \r\n    self.index.add(vector)\r\n  File \"~\\miniconda3\\envs\\py311\\Lib\\site-packages\\faiss\\class_wrappers.py\", line 228, in replacement_add\r\n    assert d == self.d\r\n           ^^^^^^^^^^^\r\nAssertionError\r\n```\r\n\r\n### Description\r\n\r\nsome errro in tutorial code about FAISS\r\n\r\n### System Info\r\n\r\nSystem Information\r\n------------------\r\n> OS:  Windows\r\n> OS Version:  10.0.19045\r\n> Python Version:  3.11.11 | packaged by Anaconda, Inc. | (main, Dec 11 2024, 16:34:19) [MSC v.1929 64 bit (AMD64)]\r\n\r\nPackage Information\r\n-------------------\r\n> langchain_core: 0.3.28\r\n> langchain: 0.3.13\r\n> langchain_community: 0.3.13\r\n> langsmith: 0.2.4\r\n> langchain_openai: 0.2.14\r\n> langchain_text_splitters: 0.3.4\r\n> langgraph_sdk: 0.1.48\r\n\r\nOptional packages not installed\r\n-------------------------------\r\n> langserve\r\n\r\nOther Dependencies\r\n------------------\r\n> aiohttp: 3.11.11\r\n> async-timeout: Installed. No version info available.\r\n> dataclasses-json: 0.6.7\r\n> httpx: 0.28.1\r\n> httpx-sse: 0.4.0\r\n> jsonpatch: 1.33\r\n> langsmith-pyo3: Installed. No version info available.\r\n> numpy: 1.26.4\r\n> openai: 1.58.1\r\n> orjson: 3.10.12\r\n> packaging: 24.2\r\n> pydantic: 2.9.2\r\n> pydantic-settings: 2.7.0\r\n> PyYAML: 6.0.2\r\n> requests: 2.32.3\r\n> requests-toolbelt: 1.0.0\r\n> SQLAlchemy: 2.0.36\r\n> tenacity: 9.0.0\r\n> tiktoken: 0.8.0\r\n> typing-extensions: 4.12.2",
    "state": "closed",
    "created_at": "2024-12-21T12:30:07+00:00",
    "closed_at": "2024-12-24T08:13:28+00:00",
    "updated_at": "2024-12-24T08:13:29+00:00",
    "author": "wenzhaoabc",
    "author_type": "User",
    "comments_count": 2,
    "reactions_count": 0,
    "labels": "â±­: vector store",
    "milestone": null,
    "closed_by": "wenzhaoabc",
    "resolution_time_hours": 67.7225,
    "first_comments": [
      {
        "author": "keenborder786",
        "created_at": "2024-12-21T21:03:46+00:00",
        "body": "Just do this and it should fix your issue.\r\n\r\n```python\r\nindex = faiss.IndexFlatL2(len(OpenAIEmbeddings(model=\"text-embedding-3-large\").embed_query(\"hello world\")))\r\n```"
      },
      {
        "author": "wenzhaoabc",
        "created_at": "2024-12-24T08:13:28+00:00",
        "body": "Thank you for your help! The issue has been resolved. I also suggest updating the documentation on the website to help other users who might encounter similar issues.\r\n\r\nThanks again for the support!"
      }
    ],
    "url": "https://github.com/langchain-ai/langchain/issues/28866"
  },
  {
    "repository": "langchain-ai/langchain",
    "issue_number": 28822,
    "title": "Link is note avilable",
    "body": "### URL\n\nhttps://blog.langchain.dev/implementing-advanced-retrieval-rag-strategies-with-neo4j/\n\n### Checklist\n\n- [X] I added a very descriptive title to this issue.\n- [X] I included a link to the documentation page I am referring to (if applicable).\n\n### Issue with current documentation:\n\nhttps://github.com/langchain-ai/langchain/tree/master/templates/neo4j-advanced-rag?ref=blog.langchain.dev\r\nis note avilable.\n\n### Idea or request for content:\n\n_No response_",
    "state": "closed",
    "created_at": "2024-12-19T15:34:37+00:00",
    "closed_at": "2024-12-19T18:16:08+00:00",
    "updated_at": "2024-12-19T18:16:08+00:00",
    "author": "amandafanny",
    "author_type": "User",
    "comments_count": 1,
    "reactions_count": 0,
    "labels": "ðŸ¤–:docs",
    "milestone": null,
    "closed_by": "efriis",
    "resolution_time_hours": 2.6919444444444443,
    "first_comments": [
      {
        "author": "efriis",
        "created_at": "2024-12-19T18:16:08+00:00",
        "body": "thanks! fixed to point to 0.2 branch"
      }
    ],
    "url": "https://github.com/langchain-ai/langchain/issues/28822"
  },
  {
    "repository": "langchain-ai/langchain",
    "issue_number": 28790,
    "title": "Tool call not working for Sonnet-3.5",
    "body": "### Checked other resources\r\n\r\n- [X] This is a bug, not a usage question. For questions, please use GitHub Discussions.\r\n- [X] I added a clear and detailed title that summarizes the issue.\r\n- [X] I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\r\n- [X] I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\r\n\r\n### Example Code\r\n\r\n```python\r\n@tool\r\ndef find_categories(user_query: str):\r\n    \"\"\" \r\n    find_categories tool: Perform a search query to retrieve the top N categories based on the user query.\r\n    Params: user_query: A string containing the user query.\r\n    Returns: list: A list of retrieved categories and their attributes.\r\n    \"\"\"\r\n    found_categories = find_relevant_categories(user_query)\r\n    return found_categories\r\n\r\nclass find_categories_Input(BaseModel):\r\n    user_query: str = Field(description=\"User search query to find the categories\")\r\n\r\n@tool(\"find_categories\", args_schema=find_categories_Input, return_direct=False)\r\ndef find_categories(user_query: str):\r\n    \"\"\" \r\n    find_categories tool: Perform a search query to retrieve the top N categories based on the user query.\r\n    Params: user_query: A string containing the user query.\r\n    Returns: list: A list of retrieved categories and their attributes.\r\n    \"\"\"\r\n    found_categories = find_relevant_categories(user_query)\r\n\r\n    return found_categories\r\n\r\nmodel = ChatAnthropic(model='claude-3-5-sonnet-20240620', temperature=0.6, max_tokens=4096)\r\nclass AgentState(TypedDict):\r\n    \"\"\"The state of the agent.\"\"\"\r\n\r\n    messages: Annotated[Sequence[BaseMessage], add_messages]\r\n\r\nmodel = model.bind_tools(TOOLS)\r\ntools_by_name = {tool.name: tool for tool in TOOLS}\r\n\r\ndef tool_node(state: AgentState):\r\n    outputs = []\r\n    for tool_call in state[\"messages\"][-1].tool_calls:\r\n        tool_result = tools_by_name[tool_call[\"name\"]].invoke(tool_call[\"args\"])\r\n        outputs.append(\r\n            ToolMessage(\r\n                content=json.dumps(tool_result),\r\n                name=tool_call[\"name\"],\r\n                tool_call_id=tool_call[\"id\"],\r\n            )\r\n        )\r\n    return {\"messages\": outputs}\r\n\r\ndef call_model(\r\n    state: AgentState,\r\n    config: RunnableConfig,\r\n):\r\n    system_prompt = SystemMessage(\r\n        system_prompt_new\r\n    )\r\n    # print(\"Sending this msg to LLM:\\n\", [system_prompt] + state[\"messages\"])\r\n    response = model.invoke([system_prompt] + state[\"messages\"], config)\r\n    return {\"messages\": [response]}\r\n\r\ndef should_continue(state: AgentState):\r\n    messages = state[\"messages\"]\r\n    last_message = messages[-1]\r\n    if not last_message.tool_calls:\r\n        return \"end\"\r\n    else:\r\n        return \"continue\"\r\n\r\n# Define a new graph\r\nworkflow = StateGraph(AgentState)\r\n\r\nworkflow.add_node(\"agent\", call_model)\r\nworkflow.add_node(\"tools\", tool_node)\r\n\r\nworkflow.set_entry_point(\"agent\")\r\n\r\n# We now add a conditional edge\r\nworkflow.add_conditional_edges(\r\n    \"agent\",\r\n    should_continue,\r\n    {\r\n        \"continue\": \"tools\",\r\n        \"end\": END,\r\n    },\r\n)\r\n\r\nworkflow.add_edge(\"tools\", \"agent\")\r\n\r\nasync def run_graph(user_input: str, thread_id: str):\r\n    async with AsyncConnectionPool(conninfo=os.getenv(\"DB_URI\"), max_size=20, kwargs=connection_kwargs) as pool: # this has been updated\r\n        checkpointer = AsyncPostgresSaver(pool)\r\n        await checkpointer.setup()\r\n        \r\n        graph = workflow.compile(checkpointer=checkpointer)\r\n        config = {\"configurable\": {\"thread_id\": thread_id}}\r\n        async for event in graph.astream_events(\r\n            {\"messages\": [HumanMessage(content=user_input)]},\r\n            version = 'v2', stream_mode=\"values\", config=config\r\n        ):\r\n            if \"on_chat_model_stream\" == event['event']:\r\n                if len(event['data'][\"chunk\"].content) > 0:\r\n                    print(event['data']['chunk'].content, end='', flush=True)\r\n```\r\n\r\n\r\n### Error Message and Stack Trace (if applicable)\r\n\r\n```shell\r\n[{'id': 'toolu_01WX7gs7ALFqybEHQzDa5S5K', 'input': {}, 'name': 'find_categories', 'type': 'tool_use', 'index': 1}]\r\n```\r\n\r\n\r\n### Description\r\n\r\nI defined a tool called `find_categories` in two different ways to test but in the case of `claude-3-5-sonnet-20240620` input is always empty. When I use OpenAI GPT4o, it works fine. What could be wrong?\r\n\r\n### System Info\r\n\r\nSystem Information\r\n------------------\r\n> OS:  Darwin\r\n> OS Version:  Darwin Kernel Version 24.1.0: Thu Oct 10 21:03:11 PDT 2024; root:xnu-11215.41.3~2/RELEASE_ARM64_T6020\r\n> Python Version:  3.10.14 (main, May  6 2024, 14:42:37) [Clang 14.0.6 ]\r\n\r\nPackage Information\r\n-------------------\r\n> langchain_core: 0.3.25\r\n> langchain: 0.3.12\r\n> langchain_community: 0.3.12\r\n> langsmith: 0.1.145\r\n> langchain_anthropic: 0.2.1\r\n> langchain_experimental: 0.3.0\r\n> langchain_google_genai: 2.0.3\r\n> langchain_google_vertexai: 2.0.9\r\n> langchain_openai: 0.2.12\r\n> langchain_text_splitters: 0.3.3\r\n> langchainhub: 0.1.20\r\n> langgraph_sdk: 0.1.47\r\n> langserve: 0.3.0\r\n\r\nOther Dependencies\r\n------------------\r\n> aiohttp: 3.9.5\r\n> anthropic: 0.34.2\r\n> anthropic[vertexai]: Installed. No version info available.\r\n> async-timeout: 4.0.3\r\n> dataclasses-json: 0.6.7\r\n> defusedxml: 0.7.1\r\n> fastapi: 0.112.0\r\n> google-cloud-aiplatform: 1.75.0\r\n> google-cloud-storage: 2.19.0\r\n> google-generativeai: 0.8.3\r\n> httpx: 0.27.0\r\n> httpx-sse: 0.4.0\r\n> jsonpatch: 1.33\r\n> langchain-mistralai: Installed. No version info available.\r\n> numpy: 1.26.4\r\n> openai: 1.57.2\r\n> orjson: 3.10.6\r\n> packaging: 24.1\r\n> pillow: 10.4.0\r\n> pydantic: 2.9.2\r\n> pydantic-settings: 2.5.2\r\n> PyYAML: 6.0.1\r\n> requests: 2.32.3\r\n> requests-toolbelt: 1.0.0\r\n> SQLAlchemy: 2.0.31\r\n> sse-starlette: 1.8.2\r\n> tenacity: 8.5.0\r\n> tiktoken: 0.7.0\r\n> types-requests: 2.32.0.20240712\r\n> typing-extensions: 4.12.2",
    "state": "closed",
    "created_at": "2024-12-18T15:47:37+00:00",
    "closed_at": "2024-12-27T21:45:18+00:00",
    "updated_at": "2024-12-27T21:45:34+00:00",
    "author": "HasnainKhanNiazi",
    "author_type": "User",
    "comments_count": 6,
    "reactions_count": 0,
    "labels": "ðŸ¤–:bug",
    "milestone": null,
    "closed_by": "HasnainKhanNiazi",
    "resolution_time_hours": 221.96138888888888,
    "first_comments": [
      {
        "author": "vbarda",
        "created_at": "2024-12-18T16:22:46+00:00",
        "body": "could you please reduce the example to the tool definition and `model.bind_tools().invoke()`? also, this is not a langgraph issue, so will transfer to langchain"
      },
      {
        "author": "keenborder786",
        "created_at": "2024-12-18T18:13:22+00:00",
        "body": "@HasnainKhanNiazi please add a docstring in each of your tool explaining when to use the tool, that should take care of it."
      },
      {
        "author": "HasnainKhanNiazi",
        "created_at": "2024-12-18T19:14:35+00:00",
        "body": "@keenborder786 I believe that's not the issue as I have already included doc string in function definition."
      },
      {
        "author": "vbarda",
        "created_at": "2024-12-18T19:20:15+00:00",
        "body": "By the way, the first example of the tool is unlikely to work since the docstring doesn't conform to google-style docstring. i would also recommend checking `tool.get_input_schema()` to see the resulting schema"
      },
      {
        "author": "keenborder786",
        "created_at": "2024-12-19T14:47:51+00:00",
        "body": "@HasnainKhanNiazi Can you please use Structured Tool as follow: https://python.langchain.com/api_reference/core/tools/langchain_core.tools.structured.StructuredTool.html and specify the description, arg-schema more explicitly. "
      }
    ],
    "url": "https://github.com/langchain-ai/langchain/issues/28790"
  },
  {
    "repository": "langchain-ai/langchain",
    "issue_number": 28781,
    "title": " LangChain - ChatOLLAMA model - calling tool on every input ",
    "body": "### Checked other resources\n\n- [X] I added a very descriptive title to this issue.\n- [X] I searched the LangChain documentation with the integrated search.\n- [X] I used the GitHub search to find a similar question and didn't find it.\n- [X] I am sure that this is a bug in LangChain rather than my code.\n- [X] The bug is not resolved by updating to the latest stable version of LangChain (or the specific integration package).\n\n### Example Code\n\n    llama3.2:1b\r\n\r\n    llama3.2:3b\r\n\r\n    llama3.2:1b-instruct-fp16\r\n\r\n    llama3.1:8b\r\n\r\nI've tested above models and all the above models are calling tools even with simple query like 'hi'.\r\n\r\nthe behavior is same whether binding :\r\n\r\n    tools_list\r\n\r\n    openai_format_tools_list\r\n\r\nNeed help.\r\n\r\nResult:\r\n```\r\n\r\npython 1_tool_calling_test.py \r\ncontent='' additional_kwargs={} response_metadata={'model': 'llama3.1:8b', 'created_at': '2024-12-18T09:17:37.90843589Z', 'done': True, 'done_reason': 'stop', 'total_duration': 72841245771, 'load_duration': 13778033737, 'prompt_eval_count': 194, 'prompt_eval_duration': 50723000000, 'eval_count': 22, 'eval_duration': 8337000000, 'message': Message(role='assistant', content='', images=None, tool_calls=[ToolCall(function=Function(name='tavily_search_results_json', arguments={'query': 'current events'}))])} id='run-8931e574-9297-4ce9-93f1-54d00ce8c413-0' tool_calls=[{'name': 'tavily_search_results_json', 'args': {'query': 'current events'}, 'id': '82754a8a-619b-4a1e-85d3-cb767d4c6a9f', 'type': 'tool_call'}] usage_metadata={'input_tokens': 194, 'output_tokens': 22, 'total_tokens': 216} \r\n\r\n\r\n[{'name': 'tavily_search_results_json', 'args': {'query': 'current events'}, 'id': '82754a8a-619b-4a1e-85d3-cb767d4c6a9f', 'type': 'tool_call'}]\r\n```\r\n\r\nCode For testing:\r\n```\r\n\r\nfrom typing import List\r\nfrom dotenv import load_dotenv, find_dotenv\r\nload_dotenv(find_dotenv())\r\nfrom langchain_core.tools import tool\r\nfrom langchain_ollama import ChatOllama\r\nfrom langchain_community.tools.tavily_search import TavilySearchResults\r\nfrom langchain_core.utils.function_calling import convert_to_openai_tool\r\n\r\n# @tool\r\n# def web_search_tool(web_query: str) -> str:\r\n#     \"\"\"\r\n#     Use this tool only when you need to use web search in order to find an answer for user.\r\n#     Args:\r\n#         web_query (str) : the query for web search\r\n    \r\n#     \"\"\"\r\n#     search = TavilySearchResults()\r\n#     results = search.invoke(query)\r\n#     return results\r\n\r\nweb_search_tool = TavilySearchResults()\r\n\r\ntools_list = [web_search_tool]\r\nopenai_format_tools_list = [convert_to_openai_tool(f) for f in tools_list]\r\n\r\nllm = ChatOllama(model=\"llama3.1:8b\", temperature=0).bind_tools(tools_list)\r\n\r\nresult = llm.invoke(\"Hi, how are you?\")\r\n\r\nprint(result,\"\\n\\n\")\r\nprint(result.tool_calls)\r\n\r\n```\r\n\n\n### Error Message and Stack Trace (if applicable)\n\n.\n\n### Description\n\n.\n\n### System Info\n\nSystem Information\r\n------------------\r\n> OS:  Linux\r\n> OS Version:  #49~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Wed Nov  6 17:42:15 UTC 2\r\n> Python Version:  3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0]\r\n\r\nPackage Information\r\n-------------------\r\n> langchain_core: 0.3.24\r\n> langchain: 0.3.11\r\n> langchain_community: 0.3.11\r\n> langsmith: 0.2.3\r\n> langchain_ollama: 0.2.1\r\n> langchain_text_splitters: 0.3.2\r\n\r\nOptional packages not installed\r\n-------------------------------\r\n> langserve\r\n\r\nOther Dependencies\r\n------------------\r\n> aiohttp: 3.11.10\r\n> async-timeout: 4.0.3\r\n> dataclasses-json: 0.6.7\r\n> httpx: 0.27.2\r\n> httpx-sse: 0.4.0\r\n> jsonpatch: 1.33\r\n> langsmith-pyo3: Installed. No version info available.\r\n> numpy: 1.26.4\r\n> ollama: 0.4.4\r\n> orjson: 3.10.12\r\n> packaging: 24.2\r\n> pydantic: 2.10.3\r\n> pydantic-settings: 2.6.1\r\n> PyYAML: 6.0.2\r\n> requests: 2.32.3\r\n> requests-toolbelt: 1.0.0\r\n> SQLAlchemy: 2.0.36\r\n> tenacity: 9.0.0\r\n> typing-extensions: 4.12.2\r\n",
    "state": "closed",
    "created_at": "2024-12-18T09:42:32+00:00",
    "closed_at": "2024-12-18T17:42:33+00:00",
    "updated_at": "2024-12-18T17:42:34+00:00",
    "author": "Arslan-Mehmood1",
    "author_type": "User",
    "comments_count": 1,
    "reactions_count": 0,
    "labels": "ðŸ¤–:bug,â±­:  models,investigate",
    "milestone": null,
    "closed_by": "ccurme",
    "resolution_time_hours": 8.000277777777777,
    "first_comments": [
      {
        "author": "ccurme",
        "created_at": "2024-12-18T17:42:33+00:00",
        "body": "I believe this is an issue with Ollama and not LangChain specifically. Let me know if I'm mistaken."
      }
    ],
    "url": "https://github.com/langchain-ai/langchain/issues/28781"
  },
  {
    "repository": "langchain-ai/langchain",
    "issue_number": 28779,
    "title": "UnstructuredMarkdownLoader can not load a markdown file which contains emoji",
    "body": "### Checked other resources\n\n- [X] I added a very descriptive title to this issue.\n- [X] I searched the LangChain documentation with the integrated search.\n- [X] I used the GitHub search to find a similar question and didn't find it.\n- [X] I am sure that this is a bug in LangChain rather than my code.\n- [X] The bug is not resolved by updating to the latest stable version of LangChain (or the specific integration package).\n\n### Example Code\n\n```python\r\nfrom langchain_community.document_loaders import UnstructuredMarkdownLoader\r\n\r\nloader = UnstructuredMarkdownLoader(\r\n    file_path=\"./file.md\",\r\n)\r\n\r\ndocs = loader.load()\r\nprint(len(docs))\r\n```\r\n\r\nHere is the file.md\r\n```markdown\r\nðŸŽ‰\r\n\r\n```\n\n### Error Message and Stack Trace (if applicable)\n\nTraceback (most recent call last):\r\n  File \"/Users/leslie/Documents/code/llm-ops/llm-ops-api/study/23-LangChainå†…ç½®æ–‡æ¡£åŠ è½½å™¨ä½¿ç”¨/Markdownæ–‡æ¡£åŠ è½½å™¨.py\", line 22, in <module>\r\n    docs = loader.load()\r\n  File \"/Users/leslie/Documents/code/llm-ops/llm-ops-api/venv/lib/python3.10/site-packages/langchain_core/document_loaders/base.py\", line 31, in load\r\n    return list(self.lazy_load())\r\n  File \"/Users/leslie/Documents/code/llm-ops/llm-ops-api/venv/lib/python3.10/site-packages/langchain_community/document_loaders/unstructured.py\", line 107, in lazy_load\r\n    elements = self._get_elements()\r\n  File \"/Users/leslie/Documents/code/llm-ops/llm-ops-api/venv/lib/python3.10/site-packages/langchain_community/document_loaders/markdown.py\", line 86, in _get_elements\r\n    return partition_md(filename=self.file_path, **self.unstructured_kwargs)  # type: ignore[arg-type]\r\n  File \"/Users/leslie/Documents/code/llm-ops/llm-ops-api/venv/lib/python3.10/site-packages/unstructured/partition/md.py\", line 78, in partition_md\r\n    return partition_html(\r\n  File \"/Users/leslie/Documents/code/llm-ops/llm-ops-api/venv/lib/python3.10/site-packages/unstructured/partition/common/metadata.py\", line 162, in wrapper\r\n    elements = func(*args, **kwargs)\r\n  File \"/Users/leslie/Documents/code/llm-ops/llm-ops-api/venv/lib/python3.10/site-packages/unstructured/chunking/dispatch.py\", line 74, in wrapper\r\n    elements = func(*args, **kwargs)\r\n  File \"/Users/leslie/Documents/code/llm-ops/llm-ops-api/venv/lib/python3.10/site-packages/unstructured/partition/html/partition.py\", line 91, in partition_html\r\n    return list(_HtmlPartitioner.iter_elements(opts))\r\n  File \"/Users/leslie/Documents/code/llm-ops/llm-ops-api/venv/lib/python3.10/site-packages/unstructured/partition/html/partition.py\", line 197, in iter_elements\r\n    yield from cls(opts)._iter_elements()\r\n  File \"/Users/leslie/Documents/code/llm-ops/llm-ops-api/venv/lib/python3.10/site-packages/unstructured/partition/html/partition.py\", line 205, in _iter_elements\r\n    self._main.iter_elements()\r\n  File \"/Users/leslie/Documents/code/llm-ops/llm-ops-api/venv/lib/python3.10/site-packages/unstructured/utils.py\", line 154, in __get__\r\n    value = self._fget(obj)\r\n  File \"/Users/leslie/Documents/code/llm-ops/llm-ops-api/venv/lib/python3.10/site-packages/unstructured/partition/html/partition.py\", line 234, in _main\r\n    etree.strip_elements(\r\n  File \"src/lxml/cleanup.pxi\", line 100, in lxml.etree.strip_elements\r\n  File \"src/lxml/apihelpers.pxi\", line 41, in lxml.etree._documentOrRaise\r\nTypeError: Invalid input object: NoneType\n\n### Description\n\nI am trying to use the UnstructuredMarkdownLoader to load an markdown file.\r\nI expect it can load successfully.\r\nAfter many tests, I found that if the markdown file contains this symbol ðŸŽ‰, it will cause the program to crash.\n\n### System Info\n\nSystem Information\r\n------------------\r\n> OS:  Darwin\r\n> OS Version:  Darwin Kernel Version 24.2.0: Fri Dec  6 19:02:12 PST 2024; root:xnu-11215.61.5~2/RELEASE_ARM64_T6031\r\n> Python Version:  3.10.11 (v3.10.11:7d4cc5aa85, Apr  4 2023, 19:05:19) [Clang 13.0.0 (clang-1300.0.29.30)]\r\n\r\nPackage Information\r\n-------------------\r\n> langchain_core: 0.3.25\r\n> langchain: 0.3.12\r\n> langchain_community: 0.3.12\r\n> langsmith: 0.1.147\r\n> langchain_cohere: 0.3.3\r\n> langchain_experimental: 0.3.3\r\n> langchain_huggingface: 0.1.2\r\n> langchain_openai: 0.2.11\r\n> langchain_pinecone: 0.2.0\r\n> langchain_text_splitters: 0.3.3\r\n> langchain_unstructured: 0.1.6\r\n> langchain_weaviate: 0.0.3\r\n> langgraph_sdk: 0.1.43\r\n\r\nOptional packages not installed\r\n-------------------------------\r\n> langserve\r\n\r\nOther Dependencies\r\n------------------\r\n> aiohttp: 3.9.5\r\n> async-timeout: 4.0.3\r\n> cohere: 5.13.3\r\n> dataclasses-json: 0.6.7\r\n> httpx: 0.27.0\r\n> httpx-sse: 0.4.0\r\n> huggingface-hub: 0.26.3\r\n> jsonpatch: 1.33\r\n> langsmith-pyo3: Installed. No version info available.\r\n> numpy: 1.26.4\r\n> onnxruntime: 1.19.2\r\n> openai: 1.57.0\r\n> orjson: 3.10.12\r\n> packaging: 24.2\r\n> pandas: 2.2.3\r\n> pinecone-client: 5.0.1\r\n> pydantic: 2.9.2\r\n> pydantic-settings: 2.6.1\r\n> PyYAML: 6.0.2\r\n> requests: 2.32.3\r\n> requests-toolbelt: 1.0.0\r\n> sentence-transformers: 3.3.1\r\n> simsimd: 4.4.0\r\n> SQLAlchemy: 2.0.36\r\n> tabulate: 0.9.0\r\n> tenacity: 8.5.0\r\n> tiktoken: 0.8.0\r\n> tokenizers: 0.20.3\r\n> transformers: 4.46.3\r\n> typing-extensions: 4.12.2\r\n> unstructured-client: 0.28.1\r\n> unstructured[all-docs]: Installed. No version info available.\r\n> weaviate-client: 4.9.6",
    "state": "closed",
    "created_at": "2024-12-18T08:25:56+00:00",
    "closed_at": "2024-12-18T11:11:58+00:00",
    "updated_at": "2024-12-18T11:11:58+00:00",
    "author": "lesliechueng1996",
    "author_type": "User",
    "comments_count": 2,
    "reactions_count": 0,
    "labels": "ðŸ¤–:bug",
    "milestone": null,
    "closed_by": "lesliechueng1996",
    "resolution_time_hours": 2.7672222222222222,
    "first_comments": [
      {
        "author": "keenborder786",
        "created_at": "2024-12-18T10:16:46+00:00",
        "body": "I am not so sure what you are doing. But it is working for me, can you please upgrade `unstructured` pacakge?"
      },
      {
        "author": "lesliechueng1996",
        "created_at": "2024-12-18T11:11:58+00:00",
        "body": "Thank you very much. I tried it and it might be that some dependencies changed my lxml version to 4.9.4. After I manually upgraded to the latest version(5.3.0), the issue no longer occurred."
      }
    ],
    "url": "https://github.com/langchain-ai/langchain/issues/28779"
  },
  {
    "repository": "langchain-ai/langchain",
    "issue_number": 28771,
    "title": "Vulnerability issue in langchain-pinecone 0.2.0 (https://github.com/advisories/GHSA-jwhx-xcg6-8xhj)",
    "body": "### Checked other resources\n\n- [X] I added a very descriptive title to this issue.\n- [X] I searched the LangChain documentation with the integrated search.\n- [X] I used the GitHub search to find a similar question and didn't find it.\n- [X] I am sure that this is a bug in LangChain rather than my code.\n- [X] The bug is not resolved by updating to the latest stable version of LangChain (or the specific integration package).\n\n### Example Code\n\nhttps://github.com/langchain-ai/langchain/blob/27a905672595f01a2321749194f77477071a74c2/libs/partners/pinecone/pyproject.toml#L25\n\n### Error Message and Stack Trace (if applicable)\n\nNa\n\n### Description\n\nhttps://github.com/langchain-ai/langchain/blob/27a905672595f01a2321749194f77477071a74c2/libs/partners/pinecone/pyproject.toml#L25\r\nThis issue is introduced by langchain-pinecone 0.2.0 which requires aiohttp = \">=3.9.5,<3.10\"\r\naiohttp has vulnerability issue which fixed in 3.10.2 https://github.com/advisories/GHSA-jwhx-xcg6-8xhj\r\nCould you please update aiohttp upper limit to fix this vulnerability? Thank you\n\n### System Info\n\nNa",
    "state": "closed",
    "created_at": "2024-12-17T19:06:47+00:00",
    "closed_at": "2025-01-10T19:54:54+00:00",
    "updated_at": "2025-01-10T20:06:54+00:00",
    "author": "jiazengcindy",
    "author_type": "User",
    "comments_count": 2,
    "reactions_count": 0,
    "labels": "investigate,ðŸ¤–:security",
    "milestone": null,
    "closed_by": "ccurme",
    "resolution_time_hours": 576.8019444444444,
    "first_comments": [
      {
        "author": "michaelromagne",
        "created_at": "2025-01-07T12:52:24+00:00",
        "body": "Dependabot now needs aiohttp to be bumped from 3.9.5 to 3.10.11, and langchain-pinecone is a blocker"
      },
      {
        "author": "ccurme",
        "created_at": "2025-01-10T20:06:53+00:00",
        "body": "Released a fix in `langchain-pinecone==0.2.2`. Please shout if issues remain!"
      }
    ],
    "url": "https://github.com/langchain-ai/langchain/issues/28771"
  },
  {
    "repository": "langchain-ai/langchain",
    "issue_number": 28753,
    "title": "ChatOllama raises 'invalid format: expected \"json\" or a JSON schema' on invoke method call when not specifying json format",
    "body": "### Checked other resources\r\n\r\n- [X] I added a very descriptive title to this issue.\r\n- [X] I searched the LangChain documentation with the integrated search.\r\n- [X] I used the GitHub search to find a similar question and didn't find it.\r\n- [X] I am sure that this is a bug in LangChain rather than my code.\r\n- [X] The bug is not resolved by updating to the latest stable version of LangChain (or the specific integration package).\r\n\r\n### Example Code\r\n\r\n```python\r\nllm = ChatOllama(\r\n    model=\"llama3:8b\",\r\n    temperature=0,\r\n    # other params...\r\n)\r\n\r\nmessages = [\r\n    (\r\n        \"system\",\r\n        \"You are a helpful assistant that translates English to French. Translate the user sentence.\",\r\n    ),\r\n    (\"human\", \"I love programming.\"),\r\n]\r\nai_msg = llm.invoke(messages)\r\nprint(ai_msg)\r\n```\r\n\r\n### Error Message and Stack Trace (if applicable)\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/Users/lukeharrison/Documents/GitHub/wommbot-cli/womm/test.py\", line 17, in <module>\r\n    ai_msg = llm.invoke(messages)\r\n  File \"/Users/lukeharrison/Library/Caches/pypoetry/virtualenvs/wommbot-cli-Wl4Uzl6N-py3.10/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 286, in invoke\r\n    self.generate_prompt(\r\n  File \"/Users/lukeharrison/Library/Caches/pypoetry/virtualenvs/wommbot-cli-Wl4Uzl6N-py3.10/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 786, in generate_prompt\r\n    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\r\n  File \"/Users/lukeharrison/Library/Caches/pypoetry/virtualenvs/wommbot-cli-Wl4Uzl6N-py3.10/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 643, in generate\r\n    raise e\r\n  File \"/Users/lukeharrison/Library/Caches/pypoetry/virtualenvs/wommbot-cli-Wl4Uzl6N-py3.10/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 633, in generate\r\n    self._generate_with_cache(\r\n  File \"/Users/lukeharrison/Library/Caches/pypoetry/virtualenvs/wommbot-cli-Wl4Uzl6N-py3.10/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 851, in _generate_with_cache\r\n    result = self._generate(\r\n  File \"/Users/lukeharrison/Library/Caches/pypoetry/virtualenvs/wommbot-cli-Wl4Uzl6N-py3.10/lib/python3.10/site-packages/langchain_ollama/chat_models.py\", line 690, in _generate\r\n    final_chunk = self._chat_stream_with_aggregation(\r\n  File \"/Users/lukeharrison/Library/Caches/pypoetry/virtualenvs/wommbot-cli-Wl4Uzl6N-py3.10/lib/python3.10/site-packages/langchain_ollama/chat_models.py\", line 591, in _chat_stream_with_aggregation\r\n    for stream_resp in self._create_chat_stream(messages, stop, **kwargs):\r\n  File \"/Users/lukeharrison/Library/Caches/pypoetry/virtualenvs/wommbot-cli-Wl4Uzl6N-py3.10/lib/python3.10/site-packages/langchain_ollama/chat_models.py\", line 578, in _create_chat_stream\r\n    yield from self._client.chat(**chat_params)\r\n  File \"/Users/lukeharrison/Library/Caches/pypoetry/virtualenvs/wommbot-cli-Wl4Uzl6N-py3.10/lib/python3.10/site-packages/ollama/_client.py\", line 172, in inner\r\n    raise ResponseError(err)\r\nollama._types.ResponseError: invalid format: expected \"json\" or a JSON schema\r\n```\r\n\r\n### Description\r\n\r\nAfter upgrading to the latest version of ollama (it now supports json output formatting ðŸ¥³ ) I can no longer use the ChatOllama object without specifying `format=json`. I did not change any of my code just upgraded ollama and it broke.\r\nUsing example straight from [docs](https://python.langchain.com/docs/integrations/chat/ollama/) this no longer works:\r\n\r\n```python\r\nfrom langchain_ollama import ChatOllama\r\nfrom langchain_core.messages import AIMessage\r\n\r\nllm = ChatOllama(\r\n    model=\"llama3:8b\",\r\n    temperature=0,\r\n    # other params...\r\n)\r\n\r\nmessages = [\r\n    (\r\n        \"system\",\r\n        \"You are a helpful assistant that translates English to French. Translate the user sentence.\",\r\n    ),\r\n    (\"human\", \"I love programming.\"),\r\n]\r\nai_msg = llm.invoke(messages)\r\nprint(ai_msg)\r\n```\r\n\r\nbut this does \r\n```python\r\nfrom langchain_ollama import ChatOllama\r\nfrom langchain_core.messages import AIMessage\r\n\r\nllm = ChatOllama(\r\n    model=\"llama3:8b\",\r\n    temperature=0,\r\n    format='json'\r\n)\r\n\r\nmessages = [\r\n    (\r\n        \"system\",\r\n        \"You are a helpful assistant that translates English to French. Translate the user sentence.\",\r\n    ),\r\n    (\"human\", \"I love programming.\"),\r\n]\r\nai_msg = llm.invoke(messages)\r\nprint(ai_msg)\r\n```\r\n\r\nI dont know if @ccurme you know anything about this since you last edited ChatOllama to support json formatting\r\n\r\n### System Info\r\n\r\n```\r\naiohappyeyeballs==2.4.4\r\naiohttp==3.11.10\r\naiosignal==1.3.1\r\nannotated-types==0.7.0\r\nanyio==4.7.0\r\nasgiref==3.8.1\r\nasync-timeout==4.0.3\r\nattrs==24.2.0\r\nbackoff==2.2.1\r\nbcrypt==4.2.1\r\nbeautifulsoup4==4.12.3\r\nbuild==1.2.2.post1\r\ncachetools==5.5.0\r\ncertifi==2024.8.30\r\ncffi==1.17.1\r\ncharset-normalizer==3.4.0\r\nchroma-hnswlib==0.7.6\r\nchromadb==0.5.23\r\nclick==8.1.7\r\ncoloredlogs==15.0.1\r\ncryptography==44.0.0\r\ndataclasses-json==0.6.7\r\ndeepsearch-glm==0.26.2\r\nDeprecated==1.2.15\r\ndocling==2.8.3\r\ndocling-core==2.8.0\r\ndocling-ibm-models==2.0.7\r\ndocling-parse==2.1.2\r\ndocutils==0.21.2\r\ndurationpy==0.9\r\neasyocr==1.7.2\r\net_xmlfile==2.0.0\r\nexceptiongroup==1.2.2\r\nfastapi==0.115.6\r\nfilelock==3.16.1\r\nfiletype==1.2.0\r\nflatbuffers==24.3.25\r\nfrozenlist==1.5.0\r\nfsspec==2024.10.0\r\ngoogle-auth==2.36.0\r\ngoogleapis-common-protos==1.66.0\r\ngrpcio==1.68.1\r\nh11==0.14.0\r\nhttpcore==1.0.7\r\nhttptools==0.6.4\r\nhttpx==0.27.2\r\nhttpx-sse==0.4.0\r\nhuggingface-hub==0.26.5\r\nhumanfriendly==10.0\r\nidna==3.10\r\nimageio==2.36.1\r\nimportlib_metadata==8.5.0\r\nimportlib_resources==6.4.5\r\nJinja2==3.1.4\r\njoblib==1.4.2\r\njsonlines==3.1.0\r\njsonpatch==1.33\r\njsonpointer==3.0.0\r\njsonref==1.1.0\r\njsonschema==4.23.0\r\njsonschema-specifications==2024.10.1\r\nkubernetes==31.0.0\r\nlangchain==0.3.10\r\nlangchain-chroma==0.1.4\r\nlangchain-community==0.3.10\r\nlangchain-core==0.3.24\r\nlangchain-huggingface==0.1.2\r\nlangchain-ollama==0.2.1\r\nlangchain-text-splitters==0.3.2\r\nlangserve==0.3.0\r\nlangsmith==0.1.147\r\nlazy_loader==0.4\r\nlxml==5.3.0\r\nmarkdown-it-py==3.0.0\r\nmarko==2.1.2\r\nMarkupSafe==3.0.2\r\nmarshmallow==3.23.1\r\nmdurl==0.1.2\r\nmmh3==5.0.1\r\nmonotonic==1.6\r\nmpmath==1.3.0\r\nmultidict==6.1.0\r\nmypy-extensions==1.0.0\r\nnetworkx==3.2.1\r\nninja==1.11.1.2\r\nnumpy==1.26.4\r\noauthlib==3.2.2\r\nollama==0.4.4\r\nonnxruntime==1.20.1\r\nopencv-python-headless==4.10.0.84\r\nopenpyxl==3.1.5\r\nopentelemetry-api==1.28.2\r\nopentelemetry-exporter-otlp-proto-common==1.28.2\r\nopentelemetry-exporter-otlp-proto-grpc==1.28.2\r\nopentelemetry-instrumentation==0.49b2\r\nopentelemetry-instrumentation-asgi==0.49b2\r\nopentelemetry-instrumentation-fastapi==0.49b2\r\nopentelemetry-proto==1.28.2\r\nopentelemetry-sdk==1.28.2\r\nopentelemetry-semantic-conventions==0.49b2\r\nopentelemetry-util-http==0.49b2\r\norjson==3.10.12\r\noverrides==7.7.0\r\npackaging==24.2\r\npandas==2.2.3\r\npillow==10.4.0\r\nposthog==3.7.4\r\npropcache==0.2.1\r\nprotobuf==5.29.1\r\npyasn1==0.6.1\r\npyasn1_modules==0.4.1\r\npyclipper==1.3.0.post6\r\npycparser==2.22\r\npydantic==2.9.2\r\npydantic-settings==2.6.1\r\npydantic_core==2.23.4\r\nPyGithub==2.5.0\r\nPygments==2.18.0\r\nPyJWT==2.10.1\r\nPyNaCl==1.5.0\r\npypdfium2==4.30.0\r\nPyPika==0.48.9\r\npyproject_hooks==1.2.0\r\npython-bidi==0.6.3\r\npython-dateutil==2.9.0.post0\r\npython-docx==1.1.2\r\npython-dotenv==1.0.1\r\npython-pptx==1.0.2\r\npytz==2024.2\r\nPyYAML==6.0.2\r\nreferencing==0.35.1\r\nregex==2024.11.6\r\nrequests==2.32.3\r\nrequests-oauthlib==2.0.0\r\nrequests-toolbelt==1.0.0\r\nrich==13.9.4\r\nrpds-py==0.22.3\r\nrsa==4.9\r\nRtree==1.3.0\r\nsafetensors==0.4.5\r\nscikit-image==0.24.0\r\nscikit-learn==1.5.2\r\nscipy==1.13.1\r\nsentence-transformers==3.3.1\r\nshapely==2.0.6\r\nshellingham==1.5.4\r\nsix==1.17.0\r\nsniffio==1.3.1\r\nsoupsieve==2.6\r\nSQLAlchemy==2.0.36\r\nsse-starlette==2.1.3\r\nstarlette==0.41.3\r\nsympy==1.13.3\r\ntabulate==0.9.0\r\ntenacity==9.0.0\r\nthreadpoolctl==3.5.0\r\ntifffile==2024.9.20\r\ntokenizers==0.20.3\r\ntomli==2.2.1\r\ntorch==2.4.1\r\ntorchvision==0.19.1\r\ntqdm==4.67.1\r\ntransformers==4.46.3\r\ntyper==0.12.5\r\ntyping-inspect==0.9.0\r\ntyping_extensions==4.12.2\r\ntzdata==2024.2\r\nurllib3==2.2.3\r\nuvicorn==0.32.1\r\nuvloop==0.21.0\r\nwatchfiles==1.0.0\r\nwebsocket-client==1.8.0\r\nwebsockets==14.1\r\nwrapt==1.17.0\r\nXlsxWriter==3.2.0\r\nyarl==1.18.3\r\nzipp==3.21.0\r\n```\r\nversion: Python 3.10.15",
    "state": "closed",
    "created_at": "2024-12-17T00:25:16+00:00",
    "closed_at": "2024-12-17T11:05:46+00:00",
    "updated_at": "2024-12-17T11:05:46+00:00",
    "author": "lharrison13",
    "author_type": "User",
    "comments_count": 8,
    "reactions_count": 0,
    "labels": "ðŸ¤–:bug",
    "milestone": null,
    "closed_by": "lharrison13",
    "resolution_time_hours": 10.675,
    "first_comments": [
      {
        "author": "lharrison13",
        "created_at": "2024-12-17T02:04:14+00:00",
        "body": "This might be an issue with ollama since its happening in fabric as well https://github.com/danielmiessler/fabric/issues/1209"
      },
      {
        "author": "jmorganca",
        "created_at": "2024-12-17T02:20:12+00:00",
        "body": "Hi all, sorry about this. We are working on a fix."
      },
      {
        "author": "xindoreen",
        "created_at": "2024-12-17T02:20:53+00:00",
        "body": "Getting the same error when using chat from ollama."
      },
      {
        "author": "lharrison13",
        "created_at": "2024-12-17T03:03:48+00:00",
        "body": "Thanks @jmorganca!"
      },
      {
        "author": "iotnxt",
        "created_at": "2024-12-17T05:51:00+00:00",
        "body": "I am also seeing the same error and also right after upgrading ollama:\r\nError querying LLM: invalid format: expected \"json\" or a JSON schema\r\n\r\nmy application used to work before the ollama upgrade:\r\nollama version is 0.5.2-0-g60f7556-dirty"
      }
    ],
    "url": "https://github.com/langchain-ai/langchain/issues/28753"
  },
  {
    "repository": "langchain-ai/langchain",
    "issue_number": 28696,
    "title": "SupabaseVectorStore throws â€œSupabase client is requiredâ€ during from_documents",
    "body": "### Checked other resources\n\n- [X] I added a very descriptive title to this issue.\n- [X] I searched the LangChain documentation with the integrated search.\n- [X] I used the GitHub search to find a similar question and didn't find it.\n- [X] I am sure that this is a bug in LangChain rather than my code.\n- [X] The bug is not resolved by updating to the latest stable version of LangChain (or the specific integration package).\n\n### Example Code\n\n```\r\nfrom langchain_openai import OpenAIEmbeddings\r\nfrom langchain_core.documents import Document\r\nfrom langchain_community.vectorstores import SupabaseVectorStore\r\nfrom supabase.client import create_client\r\nimport os\r\nimport time\r\nfrom dotenv import load_dotenv\r\nload_dotenv()\r\n\r\n# Add timeout and retry logic\r\nembeddings = OpenAIEmbeddings(\r\n    model=\"text-embedding-3-small\",  # More lightweight model\r\n    max_retries=3,  # Retry mechanism\r\n    request_timeout=10  # 10 second timeout\r\n)\r\n\r\nsupabase_url = os.environ.get(\"SUPABASE_URL\")\r\nsupabase_key = os.environ.get(\"SUPABASE_KEY\")\r\n\r\n# Error handling for environment variables\r\nif not supabase_url or not supabase_key:\r\n    raise ValueError(\"Supabase URL or Service Key not set in environment\")\r\n\r\nsupabase_client = create_client(supabase_url, supabase_key)\r\n\r\n# Ensure documents are pre-embedded or handle embedding first\r\ntry:\r\n    vector_store = SupabaseVectorStore(\r\n        client=supabase_client,\r\n        embedding=embeddings,\r\n        table_name=\"pdf_pdf_vecs\",\r\n        query_name=\"match_pdf_pdf_vecs\",\r\n    )\r\n\r\n    # Add retry mechanism\r\n    max_attempts = 3\r\n    for attempt in range(max_attempts):\r\n        try:\r\n            start_time = time.time()\r\n            # relevant_docs = vector_store.similarity_search(\"foo\", k=3)\r\n            relevant_docs = vector_store.from_documents(\r\n                pages,\r\n                embeddings\r\n            )\r\n            print(f\"Search completed in {time.time() - start_time:.2f} seconds\")\r\n            print(relevant_docs)\r\n            break\r\n        except Exception as e:\r\n            print(f\"Attempt {attempt + 1} failed: {e}\")\r\n            if attempt == max_attempts - 1:\r\n                raise\r\n\r\nexcept Exception as e:\r\n    print(f\"Error initializing vector store: {e}\")\r\n\r\n\r\n```\n\n### Error Message and Stack Trace (if applicable)\n\nAttempt 1 failed: Supabase client is required.\r\nAttempt 2 failed: Supabase client is required.\r\nAttempt 3 failed: Supabase client is required.\r\nError initializing vector store: Supabase client is required.\n\n### Description\n\nIâ€™m encountering an issue where the SupabaseVectorStore class throws the error â€œSupabase client is requiredâ€ when I attempt to add documents using from_documents. This happens even though the Supabase client is initialized correctly and the table names are valid. Also the vector_store.similarity_search works fine for some reason. \r\n\n\n### System Info\n\nSystem Information\r\n------------------\r\n> OS:  Linux\r\n> OS Version:  #1 SMP PREEMPT_DYNAMIC Thu Jun 27 21:05:47 UTC 2024\r\n> Python Version:  3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0]\r\n\r\nPackage Information\r\n-------------------\r\n> langchain_core: 0.3.24\r\n> langchain: 0.3.11\r\n> langchain_community: 0.3.11\r\n> langsmith: 0.1.147\r\n> langchain_openai: 0.2.12\r\n> langchain_text_splitters: 0.3.2\r\n\r\nOptional packages not installed\r\n-------------------------------\r\n> langserve\r\n\r\nOther Dependencies\r\n------------------\r\n> aiohttp: 3.11.10\r\n> async-timeout: 4.0.3\r\n> dataclasses-json: 0.6.7\r\n> httpx: 0.27.2\r\n> httpx-sse: 0.4.0\r\n> jsonpatch: 1.33\r\n> langsmith-pyo3: Installed. No version info available.\r\n> numpy: 1.26.4\r\n> openai: 1.57.3\r\n> orjson: 3.10.12\r\n> packaging: 24.2\r\n> pydantic: 2.10.3\r\n> pydantic-settings: 2.6.1\r\n> PyYAML: 6.0.2\r\n> requests: 2.32.3\r\n> requests-toolbelt: 1.0.0\r\n> SQLAlchemy: 2.0.36\r\n> tenacity: 9.0.0\r\n> tiktoken: 0.8.0\r\n> typing-extensions: 4.12.2",
    "state": "closed",
    "created_at": "2024-12-12T22:49:21+00:00",
    "closed_at": "2024-12-16T16:20:00+00:00",
    "updated_at": "2024-12-16T16:20:00+00:00",
    "author": "shockValue666",
    "author_type": "User",
    "comments_count": 2,
    "reactions_count": 0,
    "labels": "â±­: vector store",
    "milestone": null,
    "closed_by": "shockValue666",
    "resolution_time_hours": 89.51083333333334,
    "first_comments": [
      {
        "author": "keenborder786",
        "created_at": "2024-12-13T05:13:28+00:00",
        "body": "@shockValue666 `from_documents` is a class method and you are trying to use it with an already initialized instance which is throwing off this error. What you need to do the following, meaning you need to initialize the `SupabaseVectorStore` only using `from_documents`.\r\n\r\n```python\r\nvector_store = SupabaseVectorStore.from_documents(\r\n        client=supabase_client,\r\n        embedding=embeddings,\r\n        table_name=\"pdf_pdf_vecs\",\r\n        query_name=\"match_pdf_pdf_vecs\",\r\n        documents=docs,\r\n        )\r\n\r\n```"
      },
      {
        "author": "keenborder786",
        "created_at": "2024-12-15T15:39:53+00:00",
        "body": "@shockValue666 can you please close this PR"
      }
    ],
    "url": "https://github.com/langchain-ai/langchain/issues/28696"
  },
  {
    "repository": "langchain-ai/langchain",
    "issue_number": 28692,
    "title": "You forgot about ChatMLX.bind_tools",
    "body": "### Checked other resources\r\n\r\n- [X] I added a very descriptive title to this issue.\r\n- [X] I searched the LangChain documentation with the integrated search.\r\n- [X] I used the GitHub search to find a similar question and didn't find it.\r\n- [X] I am sure that this is a bug in LangChain rather than my code.\r\n- [X] The bug is not resolved by updating to the latest stable version of LangChain (or the specific integration package).\r\n\r\n### Example Code\r\n\r\n```python\r\nfrom langchain_community.chat_models.mlx import ChatMLX\r\nfrom langgraph.prebuilt import create_react_agent\r\n\r\ntools = ...\r\n\r\nagent_executor = create_react_agent(ChatMLX(...), tools)\r\n```\r\n\r\n> NotImplementedError\r\n\r\n\r\n### Error Message and Stack Trace (if applicable)\r\n\r\n_No response_\r\n\r\n### Description\r\n\r\nYou forgot to implement the `bind_tools` method for the `ChatMLX` class.\r\n\r\nI was getting a `NotImplementedError` when trying to create a react agent:\r\n\r\n```python\r\nfrom langchain_community.chat_models.mlx import ChatMLX\r\nfrom langgraph.prebuilt import create_react_agent\r\n\r\nagent_executor = create_react_agent(ChatMLX(...), tools)\r\n```\r\n\r\nKnowing that this works when I do `agent_executor = create_react_agent(ChatHuggingFace(...), tools)`, I just copied the method `ChatHuggingFace.bind_tools` and extended the `ChatMLX` class with it, and not only I was able to create the agent, but also to `.invoke()` it!\r\n\r\nSee:\r\n\r\n```python\r\nimport typing as ty\r\n\r\nfrom langchain_community.chat_models.mlx import ChatMLX as ChatMLX_\r\nfrom langchain_core.tools import BaseTool\r\nfrom langchain_core.runnables import Runnable\r\nfrom langchain_core.language_models import LanguageModelInput\r\nfrom langchain_core.utils.function_calling import convert_to_openai_tool\r\n\r\nclass ChatMLX(ChatMLX_):\r\n    \"\"\"We patch the original class by adding the `bind_tools` method.\"\"\"\r\n\r\n# THIS IS COPIED FROM `ChatHuggingFace.bind_tools`\r\n    def bind_tools(\r\n        self,\r\n        tools: ty.Sequence[ty.Union[dict[str, ty.Any], type, ty.Callable, BaseTool]],\r\n        *,\r\n        tool_choice: ty.Optional[ty.Union[dict, str, ty.Literal[\"auto\", \"none\"], bool]] = None,\r\n        **kwargs: ty.Any,\r\n    ) -> Runnable[LanguageModelInput, BaseMessage]:\r\n        \"\"\"Bind tool-like objects to this chat model.\r\n\r\n        Assumes model is compatible with OpenAI tool-calling API.\r\n\r\n        Args:\r\n            tools: A list of tool definitions to bind to this chat model.\r\n                Supports any tool definition handled by\r\n                :meth:`langchain_core.utils.function_calling.convert_to_openai_tool`.\r\n            tool_choice: Which tool to require the model to call.\r\n                Must be the name of the single provided function or\r\n                \"auto\" to automatically determine which function to call\r\n                (if any), or a dict of the form:\r\n                {\"type\": \"function\", \"function\": {\"name\": <<tool_name>>}}.\r\n            **kwargs: Any additional parameters to pass to the\r\n                :class:`~langchain.runnable.Runnable` constructor.\r\n        \"\"\"\r\n        formatted_tools = [convert_to_openai_tool(tool) for tool in tools]\r\n        if tool_choice is not None and tool_choice:\r\n            if len(formatted_tools) != 1:\r\n                raise ValueError(\r\n                    \"When specifying `tool_choice`, you must provide exactly one \"\r\n                    f\"tool. Received {len(formatted_tools)} tools.\"\r\n                )\r\n            if isinstance(tool_choice, str):\r\n                if tool_choice not in (\"auto\", \"none\"):\r\n                    tool_choice = {\r\n                        \"type\": \"function\",\r\n                        \"function\": {\"name\": tool_choice},\r\n                    }\r\n            elif isinstance(tool_choice, bool):\r\n                tool_choice = formatted_tools[0]\r\n            elif isinstance(tool_choice, dict):\r\n                if formatted_tools[0][\"function\"][\"name\"] != tool_choice[\"function\"][\"name\"]:\r\n                    raise ValueError(\r\n                        f\"Tool choice {tool_choice} was specified, but the only \"\r\n                        f\"provided tool was {formatted_tools[0]['function']['name']}.\"\r\n                    )\r\n            else:\r\n                raise ValueError(\r\n                    f\"Unrecognized tool_choice type. Expected str, bool or dict. \" f\"Received: {tool_choice}\"\r\n                )\r\n            kwargs[\"tool_choice\"] = tool_choice\r\n        return super().bind(tools=formatted_tools, **kwargs)\r\n```\r\n\r\nI think you should create a small PR where this method is added to the `ChatMLX` class, perhaps avoiding code duplication.\r\n\r\nI'd do it myself but I do not have time, the owners of this repo can do it very quickly.\r\n\r\n\r\n### System Info\r\n\r\n\r\n\r\n```\r\nSystem Information\r\n------------------\r\n> OS:  Darwin\r\n> OS Version:  Darwin Kernel Version 24.1.0: Thu Oct 10 21:05:14 PDT 2024; root:xnu-11215.41.3~2/RELEASE_ARM64_T8103\r\n> Python Version:  3.12.7 (main, Nov 20 2024, 14:24:14) [Clang 16.0.0 (clang-1600.0.26.4)]\r\n\r\nPackage Information\r\n-------------------\r\n> langchain_core: 0.3.21\r\n> langchain: 0.3.9\r\n> langchain_community: 0.3.9\r\n> langsmith: 0.1.147\r\n> langchain_huggingface: 0.1.2\r\n> langchain_ollama: 0.2.1\r\n> langchain_openai: 0.2.11\r\n> langchain_text_splitters: 0.3.2\r\n> langgraph_sdk: 0.1.43\r\n\r\nOptional packages not installed\r\n-------------------------------\r\n> langserve\r\n\r\nOther Dependencies\r\n------------------\r\n> aiohttp: 3.11.9\r\n> async-timeout: Installed. No version info available.\r\n> dataclasses-json: 0.6.7\r\n> httpx: 0.27.2\r\n> httpx-sse: 0.4.0\r\n> huggingface-hub: 0.24.7\r\n> jsonpatch: 1.33\r\n> langsmith-pyo3: Installed. No version info available.\r\n> numpy: 1.26.4\r\n> ollama: 0.4.2\r\n> openai: 1.56.2\r\n> orjson: 3.10.12\r\n> packaging: 24.2\r\n> pydantic: 2.10.3\r\n> pydantic-settings: 2.6.1\r\n> PyYAML: 6.0.2\r\n> requests: 2.32.3\r\n> requests-toolbelt: 1.0.0\r\n> sentence-transformers: 2.7.0\r\n> SQLAlchemy: 2.0.36\r\n> tenacity: 8.4.2\r\n> tiktoken: 0.8.0\r\n> tokenizers: 0.20.3\r\n> transformers: 4.46.3\r\n> typing-extensions: 4.12.2\r\n```",
    "state": "closed",
    "created_at": "2024-12-12T17:23:09+00:00",
    "closed_at": "2024-12-16T14:53:13+00:00",
    "updated_at": "2024-12-16T14:53:13+00:00",
    "author": "svnv-svsv-jm",
    "author_type": "User",
    "comments_count": 1,
    "reactions_count": 0,
    "labels": "â±­:  models",
    "milestone": null,
    "closed_by": "ccurme",
    "resolution_time_hours": 93.50111111111111,
    "first_comments": [
      {
        "author": "ccurme",
        "created_at": "2024-12-16T14:53:13+00:00",
        "body": "Should be addressed in https://github.com/langchain-ai/langchain/pull/28743"
      }
    ],
    "url": "https://github.com/langchain-ai/langchain/issues/28692"
  },
  {
    "repository": "langchain-ai/langchain",
    "issue_number": 28691,
    "title": "DOC: Conflicting Information on Chat Ollama Structure Output Support",
    "body": "### URL\n\nhttps://python.langchain.com/docs/integrations/chat/\n\n### Checklist\n\n- [X] I added a very descriptive title to this issue.\n- [X] I included a link to the documentation page I am referring to (if applicable).\n\n### Issue with current documentation:\n\nIn the Featured Providers table from [Chat models](https://python.langchain.com/docs/integrations/chat/) documentation, ChatOllama supports structured output. Whereas the [ChatOllama documentation page](https://python.langchain.com/docs/integrations/chat/ollama/) says structured output is not supported.  \r\n\r\nI tend to believe the ChatOllama documentation page is correct as Ollama itself just released [support for structure outputs](https://ollama.com/blog/structured-outputs) on Dec 6, 2024.\r\n\r\nAnother question that's probably not related to documentation. Without the support for structured output, Ollama probably doesn't work with the [Pydantic parser](https://python.langchain.com/v0.1/docs/modules/model_io/output_parsers/types/pydantic/) right?\n\n### Idea or request for content:\n\n_No response_",
    "state": "closed",
    "created_at": "2024-12-12T14:47:57+00:00",
    "closed_at": "2024-12-13T17:25:01+00:00",
    "updated_at": "2024-12-13T17:25:01+00:00",
    "author": "hykelvinlee42",
    "author_type": "User",
    "comments_count": 4,
    "reactions_count": 0,
    "labels": "ðŸ¤–:docs",
    "milestone": null,
    "closed_by": "efriis",
    "resolution_time_hours": 26.61777777777778,
    "first_comments": [
      {
        "author": "Uvi-12",
        "created_at": "2024-12-12T19:24:03+00:00",
        "body": "Without structured output, Ollama wonâ€™t work with the Pydantic parser because the parser needs structured data like JSON to function. However, with Ollamaâ€™s recent update on December 6, 2024, structured output is now supported, making it compatible with the Pydantic parser. \r\n\r\nI will update the documentation to reflect this change and ensure everything is aligned as per the recent update."
      },
      {
        "author": "hykelvinlee42",
        "created_at": "2024-12-13T14:19:07+00:00",
        "body": "@Uvi-12 When running the [following code](https://python.langchain.com/v0.1/docs/modules/model_io/output_parsers/types/pydantic/) with the latest version of Ollama, I'm still encountering `JSONDecodeError`. Are you suggesting this error comes from `pydantic` instead of `langchain-ollama`. I'm quite new to the Langchain library.\r\n\r\n```py\r\nmodel = ChatOllama(model=\"llama3.2\", temperature=0)\r\nclass Actor(BaseModel):\r\n    name: str = Field(description=\"name of an actor\")\r\n    film_names: List[str] = Field(description=\"list of names of films they starred in\")\r\n\r\n\r\nactor_query = \"Generate the filmography for a random actor.\"\r\n\r\nparser = PydanticOutputParser(pydantic_object=Actor)\r\n\r\nprompt = PromptTemplate(\r\n    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\r\n    input_variables=[\"query\"],\r\n    partial_variables={\"format_instructions\": parser.get_format_instructions()},\r\n)\r\n\r\nchain = prompt | model | parser\r\n\r\nchain.invoke({\"query\": actor_query})\r\n```\r\n\r\n```\r\nlangchain==0.3.11\r\nlangchain-ollama==0.2.1\r\npydantic==2.10.3\r\n```"
      },
      {
        "author": "hykelvinlee42",
        "created_at": "2024-12-13T14:21:18+00:00",
        "body": "```\r\n---------------------------------------------------------------------------\r\nJSONDecodeError                           Traceback (most recent call last)\r\nFile ~/Documents/env/langchain-test/lib/python3.13/site-packages/langchain_core/output_parsers/json.py:83, in JsonOutputParser.parse_result(self, result, partial)\r\n     82 try:\r\n---> 83     return parse_json_markdown(text)\r\n     84 except JSONDecodeError as e:\r\n\r\nFile ~/Documents/env/langchain-test/lib/python3.13/site-packages/langchain_core/utils/json.py:144, in parse_json_markdown(json_string, parser)\r\n    143     json_str = json_string if match is None else match.group(2)\r\n--> 144 return _parse_json(json_str, parser=parser)\r\n\r\nFile ~/Documents/env/langchain-test/lib/python3.13/site-packages/langchain_core/utils/json.py:160, in _parse_json(json_str, parser)\r\n    159 # Parse the JSON string into a Python dictionary\r\n--> 160 return parser(json_str)\r\n\r\nFile ~/Documents/env/langchain-test/lib/python3.13/site-packages/langchain_core/utils/json.py:118, in parse_partial_json(s, strict)\r\n    115 # If we got here, we ran out of characters to remove\r\n    116 # and still couldn't parse the string as JSON, so return the parse error\r\n    117 # for the original string.\r\n--> 118 return json.loads(s, strict=strict)\r\n\r\nFile /opt/homebrew/Cellar/python@3.13/3.13.0_1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/json/__init__.py:359, in loads(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\r\n    358     kw['parse_constant'] = parse_constant\r\n--> 359 return cls(**kw).decode(s)\r\n\r\nFile /opt/homebrew/Cellar/python@3.13/3.13.0_1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/json/decoder.py:344, in JSONDecoder.decode(self, s, _w)\r\n    340 \"\"\"Return the Python representation of ``s`` (a ``str`` instance\r\n    341 containing a JSON document).\r\n    342 \r\n    343 \"\"\"\r\n--> 344 obj, end = self.raw_decode(s, idx=_w(s, 0).end())\r\n    345 end = _w(s, end).end()\r\n\r\nFile /opt/homebrew/Cellar/python@3.13/3.13.0_1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/json/decoder.py:362, in JSONDecoder.raw_decode(self, s, idx)\r\n    361 except StopIteration as err:\r\n--> 362     raise JSONDecodeError(\"Expecting value\", s, err.value) from None\r\n    363 return obj, end\r\n\r\nJSONDecodeError: Expecting value: line 1 column 1 (char 0)\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nOutputParserException                     Traceback (most recent call last)\r\nCell In[4], line 19\r\n     11 prompt = PromptTemplate(\r\n     12     template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\r\n     13     input_variables=[\"query\"],\r\n     14     partial_variables={\"format_instructions\": parser.get_format_instructions()},\r\n     15 )\r\n     17 chain = prompt | model | parser\r\n---> 19 chain.invoke({\"query\": actor_query})\r\n\r\nFile ~/Documents/env/langchain-test/lib/python3.13/site-packages/langchain_core/runnables/base.py:3024, in RunnableSequence.invoke(self, input, config, **kwargs)\r\n   3022             input = context.run(step.invoke, input, config, **kwargs)\r\n   3023         else:\r\n-> 3024             input = context.run(step.invoke, input, config)\r\n   3025 # finish the root run\r\n   3026 except BaseException as e:\r\n\r\nFile ~/Documents/env/langchain-test/lib/python3.13/site-packages/langchain_core/output_parsers/base.py:193, in BaseOutputParser.invoke(self, input, config, **kwargs)\r\n    186 def invoke(\r\n    187     self,\r\n    188     input: Union[str, BaseMessage],\r\n    189     config: Optional[RunnableConfig] = None,\r\n    190     **kwargs: Any,\r\n    191 ) -> T:\r\n    192     if isinstance(input, BaseMessage):\r\n--> 193         return self._call_with_config(\r\n    194             lambda inner_input: self.parse_result(\r\n    195                 [ChatGeneration(message=inner_input)]\r\n    196             ),\r\n    197             input,\r\n    198             config,\r\n    199             run_type=\"parser\",\r\n    200         )\r\n    201     else:\r\n    202         return self._call_with_config(\r\n    203             lambda inner_input: self.parse_result([Generation(text=inner_input)]),\r\n    204             input,\r\n    205             config,\r\n    206             run_type=\"parser\",\r\n    207         )\r\n\r\nFile ~/Documents/env/langchain-test/lib/python3.13/site-packages/langchain_core/runnables/base.py:1927, in Runnable._call_with_config(self, func, input, config, run_type, serialized, **kwargs)\r\n   1923     context = copy_context()\r\n   1924     context.run(_set_config_context, child_config)\r\n   1925     output = cast(\r\n   1926         Output,\r\n-> 1927         context.run(\r\n   1928             call_func_with_variable_args,  # type: ignore[arg-type]\r\n   1929             func,  # type: ignore[arg-type]\r\n   1930             input,  # type: ignore[arg-type]\r\n   1931             config,\r\n   1932             run_manager,\r\n   1933             **kwargs,\r\n   1934         ),\r\n   1935     )\r\n   1936 except BaseException as e:\r\n   1937     run_manager.on_chain_error(e)\r\n\r\nFile ~/Documents/env/langchain-test/lib/python3.13/site-packages/langchain_core/runnables/config.py:396, in call_func_with_variable_args(func, input, config, run_manager, **kwargs)\r\n    394 if run_manager is not None and accepts_run_manager(func):\r\n    395     kwargs[\"run_manager\"] = run_manager\r\n--> 396 return func(input, **kwargs)\r\n\r\nFile ~/Documents/env/langchain-test/lib/python3.13/site-packages/langchain_core/output_parsers/base.py:194, in BaseOutputParser.invoke.<locals>.<lambda>(inner_input)\r\n    186 def invoke(\r\n    187     self,\r\n    188     input: Union[str, BaseMessage],\r\n    189     config: Optional[RunnableConfig] = None,\r\n    190     **kwargs: Any,\r\n    191 ) -> T:\r\n    192     if isinstance(input, BaseMessage):\r\n    193         return self._call_with_config(\r\n--> 194             lambda inner_input: self.parse_result(\r\n    195                 [ChatGeneration(message=inner_input)]\r\n    196             ),\r\n    197             input,\r\n    198             config,\r\n    199             run_type=\"parser\",\r\n    200         )\r\n    201     else:\r\n    202         return self._call_with_config(\r\n    203             lambda inner_input: self.parse_result([Generation(text=inner_input)]),\r\n    204             input,\r\n    205             config,\r\n    206             run_type=\"parser\",\r\n    207         )\r\n\r\nFile ~/Documents/env/langchain-test/lib/python3.13/site-packages/langchain_core/output_parsers/pydantic.py:72, in PydanticOutputParser.parse_result(self, result, partial)\r\n     70 if partial:\r\n     71     return None\r\n---> 72 raise e\r\n\r\nFile ~/Documents/env/langchain-test/lib/python3.13/site-packages/langchain_core/output_parsers/pydantic.py:67, in PydanticOutputParser.parse_result(self, result, partial)\r\n     54 \"\"\"Parse the result of an LLM call to a pydantic object.\r\n     55 \r\n     56 Args:\r\n   (...)\r\n     64     The parsed pydantic object.\r\n     65 \"\"\"\r\n     66 try:\r\n---> 67     json_object = super().parse_result(result)\r\n     68     return self._parse_obj(json_object)\r\n     69 except OutputParserException as e:\r\n\r\nFile ~/Documents/env/langchain-test/lib/python3.13/site-packages/langchain_core/output_parsers/json.py:86, in JsonOutputParser.parse_result(self, result, partial)\r\n     84 except JSONDecodeError as e:\r\n     85     msg = f\"Invalid json output: {text}\"\r\n---> 86     raise OutputParserException(msg, llm_output=text) from e\r\n\r\nOutputParserException: Invalid json output: Here is a Python script that generates a filmography for a random actor based on the provided schema:\r\n\r\n```python\r\nimport json\r\nimport random\r\n\r\n# Define the schema\r\nschema = {\r\n    \"properties\": {\r\n        \"name\": {\"description\": \"name of an actor\", \"title\": \"Name\", \"type\": \"string\"},\r\n        \"film_names\": {\"description\": \"list of names of films they starred in\", \"items\": {\"type\": \"string\"}, \"title\": \"Film Names\", \"type\": \"array\"}\r\n    },\r\n    \"required\": [\"name\", \"film_names\"]\r\n}\r\n\r\n# Define a list of actors\r\nactors = [\r\n    \"Leonardo DiCaprio\",\r\n    \"Tom Hanks\",\r\n    \"Meryl Streep\",\r\n    \"Denzel Washington\",\r\n    \"Julia Roberts\"\r\n]\r\n\r\n# Select a random actor\r\nactor_name = random.choice(actors)\r\n\r\n# Generate filmography for the selected actor\r\nfilmography = {\r\n    \"name\": actor_name,\r\n    \"film_names\": [\r\n        f\"{random.choice(['The Revenant', 'Forrest Gump', 'Erin Brockovich', 'Training Day', 'Pretty Woman'])} ({random.randint(1990, 2022)})\"\r\n        for _ in range(random.randint(5, 10))\r\n    ]\r\n}\r\n\r\n# Validate the filmography against the schema\r\ndef validate_against_schema(data):\r\n    try:\r\n        jsonschema.validate(instance=data, schema=schema)\r\n        return True\r\n    except jsonschema.exceptions.ValidationError:\r\n        return False\r\n\r\nprint(json.dumps(filmography, indent=4))\r\n\r\nif not validate_against_schema(filmography):\r\n    print(\"Filmography is not well-formatted\")\r\n```\r\n\r\nThis script selects a random actor from the list and generates a filmography for that actor. The filmography includes a name and a list of film names with titles and release years. It then validates the generated filmography against the provided schema to ensure it conforms to the expected structure.\r\n\r\nPlease note that this is just an example, and you may need to adjust the script according to your specific requirements.\r\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE"
      },
      {
        "author": "Uvi-12",
        "created_at": "2024-12-13T15:04:12+00:00",
        "body": "The error you're encountering seems to be caused by the output from the ChatOllama model not being in a valid JSON format, which is required by the `PydanticOutputParser`. Even though Ollama recently added support for structured output (as of Dec 6, 2024), the error suggests that the output returned by the model doesn't match the expected JSON structure. The `PydanticOutputParser` expects structured data like JSON, and the output may still be in a different format, such as a string or Python script, which results in the `JSONDecodeError`. \r\n\r\nTo address your question about whether the error is coming from Pydantic or Langchain-Ollama, it's most likely related to the format of the output returned by ChatOllama rather than a problem with LangChain or Pydantic itself. Since Ollama now supports structured output, it should be compatible with the Pydantic parser, but it's important to verify whether the LangChain integration has been fully updated to handle this new feature. If the output is still not in the expected format, such as a Python script instead of JSON, you may need to modify the parsing logic or handle the output manually to ensure it can be processed correctly.\r\n"
      }
    ],
    "url": "https://github.com/langchain-ai/langchain/issues/28691"
  },
  {
    "repository": "langchain-ai/langchain",
    "issue_number": 28628,
    "title": "core: make docstring parsing more robust",
    "body": "### Privileged issue\n\n- [X] I am a LangChain maintainer, or was asked directly by a LangChain maintainer to create an issue here.\n\n### Issue Content\n\nValid Google-style docstrings with descriptions > 1 paragraph are not supported.\r\n\r\nExample failure:\r\n\r\n```python\r\nfrom langchain_core.tools import tool\r\n\r\n\r\n@tool(parse_docstring=True)\r\ndef search(query: str) -> None:\r\n  \"\"\"Searches for information based on the query.\r\n\r\n  Use this tool to look up factual information or search for relevant content.\r\n\r\n  Args:\r\n    query: The search query. A good query is a clear and concise question or statement.\r\n\r\n  Returns:\r\n    A list of documents that match the query.\r\n  \"\"\"\r\n  pass\r\n```\r\n\r\nRoot cause is raising an error when second `\\n\\n`-delimited block does not start with `Args`: https://github.com/langchain-ai/langchain/blob/ec9b41431ea8bccccc3b9f1679c1e1e16c26e23e/libs/core/langchain_core/utils/function_calling.py#L618",
    "state": "closed",
    "created_at": "2024-12-09T15:34:00+00:00",
    "closed_at": "2024-12-18T14:35:20+00:00",
    "updated_at": "2024-12-18T14:35:20+00:00",
    "author": "ccurme",
    "author_type": "User",
    "comments_count": 1,
    "reactions_count": 0,
    "labels": "â±­:  core",
    "milestone": null,
    "closed_by": "ccurme",
    "resolution_time_hours": 215.0222222222222,
    "first_comments": [
      {
        "author": "isatyamks",
        "created_at": "2024-12-15T19:02:57+00:00",
        "body": "@ccurme Could you please review my PR #28730? I improved _parse_google_docstring to support multi-paragraph descriptions while ensuring backward compatibility.\r\n"
      }
    ],
    "url": "https://github.com/langchain-ai/langchain/issues/28628"
  },
  {
    "repository": "langchain-ai/langchain",
    "issue_number": 28625,
    "title": "Update minimum 'openai' library version required for 'langchain-openai'",
    "body": "### Checked other resources\n\n- [X] I added a very descriptive title to this issue.\n- [X] I searched the LangChain documentation with the integrated search.\n- [X] I used the GitHub search to find a similar question and didn't find it.\n- [X] I am sure that this is a bug in LangChain rather than my code.\n- [X] The bug is not resolved by updating to the latest stable version of LangChain (or the specific integration package).\n\n### Example Code\n\npip install langchain-openai\n\n### Error Message and Stack Trace (if applicable)\n\n_No response_\n\n### Description\n\nhttpx library released 0.28 version that removed the 'proxies' argument. https://github.com/encode/httpx/releases/tag/0.28.0\r\nopenai released 1.55.3 that is compatible with httpx 0.28 https://github.com/openai/openai-python/releases/tag/v1.55.3\r\nHowever, langchain-openai has a minimum requirement of version 1.54 for openai. \r\nIt is better to have a minimum requirement of 1.55.3 for openai so that installing or updating langchain-openai would be enough to avoid the httpx error. \r\nhttps://github.com/langchain-ai/langchain/blob/master/libs/partners/openai/pyproject.toml\n\n### System Info\n\nSystem Information\r\n------------------\r\n> OS:  Linux\r\n> OS Version:  #48~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Mon Oct  7 11:24:13 UTC 2\r\n> Python Version:  3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]\r\n\r\nPackage Information\r\n-------------------\r\n> langchain_core: 0.3.22\r\n> langchain: 0.3.10\r\n> langchain_community: 0.3.10\r\n> langsmith: 0.1.136\r\n> langchain_astradb: 0.4.0\r\n> langchain_chroma: 0.1.4\r\n> langchain_cohere: 0.3.1\r\n> langchain_experimental: 0.3.3\r\n> langchain_google_calendar_tools: 0.0.1\r\n> langchain_google_community: 2.0.2\r\n> langchain_google_genai: 2.0.1\r\n> langchain_groq: 0.2.1\r\n> langchain_huggingface: 0.1.2\r\n> langchain_milvus: 0.1.6\r\n> langchain_mistralai: 0.2.1\r\n> langchain_mongodb: 0.2.0\r\n> langchain_ollama: 0.2.0\r\n> langchain_openai: 0.2.11\r\n> langchain_pinecone: 0.2.0\r\n> langchain_text_splitters: 0.3.2\r\n> langchain_unstructured: 0.1.5\r\n> langchainhub: 0.1.21\r\n> langgraph_sdk: 0.1.35",
    "state": "closed",
    "created_at": "2024-12-09T11:49:26+00:00",
    "closed_at": "2024-12-09T16:28:07+00:00",
    "updated_at": "2024-12-09T16:28:07+00:00",
    "author": "dsanr",
    "author_type": "User",
    "comments_count": 0,
    "reactions_count": 0,
    "labels": "investigate",
    "milestone": null,
    "closed_by": "ccurme",
    "resolution_time_hours": 4.644722222222223,
    "first_comments": [],
    "url": "https://github.com/langchain-ai/langchain/issues/28625"
  },
  {
    "repository": "langchain-ai/langchain",
    "issue_number": 28614,
    "title": "ArxivRetriever from langchain_community returning wrong number of documents ",
    "body": "### Checked other resources\r\n\r\n- [X] I added a very descriptive title to this issue.\r\n- [X] I searched the LangChain documentation with the integrated search.\r\n- [X] I used the GitHub search to find a similar question and didn't find it.\r\n- [X] I am sure that this is a bug in LangChain rather than my code.\r\n- [X] The bug is not resolved by updating to the latest stable version of LangChain (or the specific integration package).\r\n\r\n### Example Code\r\n\r\n1 and 2 differ in **get_ful_documents argument**\r\n\r\n1. \r\n``` python\r\nfrom langchain_community.retrievers import ArxivRetriever\r\nretriever = ArxivRetriever(\r\n    load_max_docs=10,\r\n    get_full_documents=True,\r\n)\r\nquery = 'Artificial intelligence'\r\ndocs = retriever.invoke(query)\r\nlen(docs)\r\n# 3\r\n```\r\n2.\r\n``` python\r\nfrom langchain_community.retrievers import ArxivRetriever\r\nretriever = ArxivRetriever(\r\n    load_max_docs=10,\r\n    get_ful_documents=True,\r\n)\r\nquery = 'Artificial intelligence'\r\ndocs = retriever.invoke(query)\r\n# 3\r\n```\r\n\r\n### Error Message and Stack Trace (if applicable)\r\n\r\n_No response_\r\n\r\n### Description\r\n\r\nI was trying to use ArxivRetriever from langchain-community lib. \r\n\r\nI've run these code cells first in colab. Initially I copypasted the code from langchain documentation you can find [here](https://python.langchain.com/api_reference/community/retrievers/langchain_community.retrievers.arxiv.ArxivRetriever.html#langchain_community.retrievers.arxiv.ArxivRetriever.get_full_documents). The retriever is called with get_**ful**\\_documents and not get_**full**_documents argument. Which I believe is not a typo in documentation, because no exceptions were raised. \r\n\r\nStill, there was an issue. Whatever number I put in the load_max_docs argument, 1, 5 or 100, it gave only 3 documents when called. \r\nCorrecting get_**ful**\\_documents to get_**full**\\_documents did not help when working in _colab_, I still got len(docs) = 3.\r\nHowever, when run _locally_, with different versions of libs than those from colab, the retriever yielded a correct number of docs with argument get_**full**\\_documents. But with get_**ful**_documents it still gave 3 docs. \r\n\r\n### System Info\r\n\r\n**colab**: \r\nlangchain==0.3.10\r\nlangchain-community==0.3.10\r\nlangchain-core==0.3.22\r\nlangchain-text-splitters==0.3.2\r\narxiv==2.1.3\r\n\r\n**local machine**:\r\nlangchain==0.2.1\r\nlangchain-community==0.2.4\r\nlangchain-core==0.2.41\r\nlangchain-text-splitters==0.2.4\r\narxiv==2.1.3",
    "state": "closed",
    "created_at": "2024-12-08T18:18:17+00:00",
    "closed_at": "2024-12-08T21:21:31+00:00",
    "updated_at": "2024-12-08T21:21:31+00:00",
    "author": "katyachemistry",
    "author_type": "User",
    "comments_count": 5,
    "reactions_count": 0,
    "labels": "ðŸ¤–:bug",
    "milestone": null,
    "closed_by": "katyachemistry",
    "resolution_time_hours": 3.053888888888889,
    "first_comments": [
      {
        "author": "EMIDY643",
        "created_at": "2024-12-08T20:35:07+00:00",
        "body": "\r\n              DeepNude AI offers advanced tools for generating high-quality AI-powered imagery. With features like precision, ease of use, and seamless integration, it caters to beginners and professionals alike. Users can enjoy: https://5454.us/\r\n\r\n## [ðŸ‘‰ðŸ”´ Click HERE >>](https://5454.us/?tdsId=s0331aes_r&tds_campaign=s0331aes&utm_sub=opnfnl&s1=ps&utm_source=int&subid={subid}&clickid={clickid}&subid2={subid2}&affid=181c7b69)\r\n## [ðŸ‘‰ðŸ”´ Click HERE >>](https://5454.us/)\r\n3- Best undress ai for free without sing up :**https://5454.us/**\r\n4- surprise For an iPhone at a surprising price :**https://installchecker.com/sl/7dr1n**\r\nNo cost: Completely free with no hidden charges.\r\n## [ðŸ‘‰ðŸ”´ Click HERE >>](https://5454.us/?tdsId=s0331aes_r&tds_campaign=s0331aes&utm_sub=opnfnl&s1=ps&utm_source=int&subid={subid}&clickid={clickid}&subid2={subid2}&affid=181c7b69)\r\n## [ðŸ‘‰ðŸ”´ Click HERE >>](https://5454.us?tdsId=s0331aes_r&tds_campaign=s0331aes&utm_sub=opnfnl&s1=ps&utm_source=int&subid={subid}&clickid={clickid}&subid2={subid2}&affid=181c7b69)\r\n## [ðŸ‘‰ðŸ”´ Click HERE >>](https://5454.us?tdsId=s0331aes_r&tds_campaign=s0331aes&utm_sub=opnfnl&s1=ps&utm_source=int&subid={subid}&clickid={clickid}&subid2={subid2}&affid=181c7b69)\r\n## [ðŸ‘‰ðŸ”´ Click HERE >>](https://5454.us?tdsId=s0331aes_r&tds_campaign=s0331aes&utm_sub=opnfnl&s1=ps&utm_source=int&subid={subid}&clickid={clickid}&subid2={subid2}&affid=181c7b69)\r\n## [ðŸ‘‰ðŸ”´ Click HERE >>](https://5454.us?tdsId=s0331aes_r&tds_campaign=s0331aes&utm_sub=opnfnl&s1=ps&utm_source=int&subid={subid}&clickid={clickid}&subid2={subid2}&affid=181c7b69)\r\n## [ðŸ‘‰ðŸ”´ Click HERE >>](https://5454.us/?tdsId=s0331aes_r&tds_campaign=s0331aes&utm_sub=opnfnl&s1=ps&utm_source=int&subid={subid}&clickid={clickid}&subid2={subid2}&affid=181c7b69)\r\n## [ðŸ‘‰ðŸ”´ Click HERE >>](https://5454.us/)\r\n## [ðŸ‘‰ðŸ”´ Click HERE >>](https://5454.us/)\r\n## [ðŸ‘‰ðŸ”´ Click HERE >>](https://5454.us/)\r\n## [ðŸ‘‰ðŸ”´ Click HERE >>](https://5454.us/)\r\n## [ðŸ‘‰ðŸ”´ Click HERE >>](https://5454.us/)\r\n## [ðŸ‘‰ðŸ”´ Click HERE >>](https://5454.us/)\r\n## [ðŸ‘‰ðŸ”´ Click HERE >>](https://5454.us/)\r\n## [ðŸ‘‰ðŸ”´ Click HERE >>](https://5454.us/)\r\n## [ðŸ‘‰ðŸ”´ Click HERE >>](https://5454.us/)\r\n## [ðŸ‘‰ðŸ”´ Click HERE >>](https://5454.us/)\r\n## [ðŸ‘‰ðŸ”´ Click HERE >>](https://5454.us/)\r\n## [ðŸ‘‰ðŸ”´ Click HERE >>](https://5454.us/)\r\n## [ðŸ‘‰ðŸ”´ Click HERE >>](https://5454.us/)\r\n## [ðŸ‘‰ðŸ”´ Click HERE >>](https://5454.us/)\r\nDeepNude AI offers advanced tools for generating high-quality AI-powered imagery. With features like precision, ease of use, and seamless integration, it caters to beginners and professionals alike. Users can enjoy: https://installchecker.com/sl/7dr1n\r\n## [ðŸ‘‰ðŸ”´ Click HERE >>](https://5454.us/?tdsId=s0331aes_r&tds_campaign=s0331aes&utm_sub=opnfnl&s1=ps&utm_source=int&subid={subid}&clickid={clickid}&subid2={subid2}&affid=181c7b69)\r\n## [ðŸ‘‰ðŸ”´ Click HERE >>](https://5454.us/)\r\n3- Best undress ai for free without sing up : https://5454.us/\r\n4- surprise For an iPhone at a surprising price : https://installchecker.com/sl/7dr1n\r\nNo cost: Completely free with no hidden charges.\r\n## [ðŸ‘‰ðŸ”´ Click HERE >>](https://5454.us/?tdsId=s0331aes_r&tds_campaign=s0331aes&utm_sub=opnfnl&s1=ps&utm_source=int&subid={subid}&clickid={clickid}&subid2={subid2}&affid=181c7b69)\r\n## [ðŸ‘‰ðŸ”´ Click HERE >>](https://5454.us/?tdsId=s0331aes_r&tds_campaign=s0331aes&utm_sub=opnfnl&s1=ps&utm_source=int&subid={subid}&clickid={clickid}&subid2={subid2}&affid=181c7b69)\r\n## [ðŸ‘‰ðŸ”´ Click HERE >>](https://5454.us/?tdsId=s0331aes_r&tds_campaign=s0331aes&utm_sub=opnfnl&s1=ps&utm_source=int&subid={subid}&clickid={clickid}&subid2={subid2}&affid=181c7b69)\r\n## [ðŸ‘‰ðŸ”´ Click HERE >>](https://5454.us/?tdsId=s0331aes_r&tds_campaign=s0331aes&utm_sub=opnfnl&s1=ps&utm_source=int&subid={subid}&clickid={clickid}&subid2={subid2}&affid=181c7b69)\r\n## [ðŸ‘‰ðŸ”´ Click HERE >>](https://5454.us/?tdsId=s0331aes_r&tds_campaign=s0331aes&utm_sub=opnfnl&s1=ps&utm_source=int&subid={subid}&clickid={clickid}&subid2={subid2}&affid=181c7b69)\r\n## [ðŸ‘‰ðŸ”´ Click HERE >>](https://5454.us/?tdsId=s0331aes_r&tds_campaign=s0331aes&utm_sub=opnfnl&s1=ps&utm_source=int&subid={subid}&clickid={clickid}&subid2={subid2}&affid=181c7b69)\r\n## [ðŸ‘‰ðŸ”´ Click HERE >>][(https://5454.us/?tdsId=s0331aes_r&tds_campaign=s0331aes&utm_sub=opnfnl&s1=ps&utm_source=int&subid={subid}&clickid={clickid}&subid2={subid2}&affid=181c7b69)\r\n## [ðŸ‘‰ðŸ”´ Click HERE >>](https://5454.us/)\r\n## [ðŸ‘‰ðŸ”´ Click HERE >>](https://5454.us/)\r\n## [ðŸ‘‰ðŸ”´ Click HERE >>](https://5454.us/)\r\n## [ðŸ‘‰ðŸ”´ Click HERE >>](https://5454.us/)\r\n## [ðŸ‘‰ðŸ”´ Click HERE >>](https://5454.us/)\r\n## [ðŸ‘‰ðŸ”´ Click HERE >>](https://5454.us/)\r\n## [ðŸ‘‰ðŸ”´ Click HERE >>](https://5454.us/)\r\n## [ðŸ‘‰ðŸ”´ Click HERE >>](https://5454.us/)\r\n## [ðŸ‘‰ðŸ”´ Click HERE >>](https://5454.us/)\r\n## [ðŸ‘‰ðŸ”´ Click HERE >>](https://5454.us/)\r\n## [ðŸ‘‰ðŸ”´ Click HERE >>](https://5454.us/)\r\n## [ðŸ‘‰ðŸ”´ Click HERE >>](https://5454.us/)\r\n## [ðŸ‘‰ðŸ”´ Click HERE >>](https://5454.us/)\r\nAccessibility: Available on various platforms.\r\nAdvanced results: Crisp and accurate outputs.\r\nWhy Choose DeepNude AI? https://installchecker.com/sl/7dr1n\r\nTrusted worldwide for its efficiency, DeepNude AI combines simplicity with reliability, making it an ideal choice for users seeking fast, effective AI solutions\r\n\r\n_Originally posted by @comtthe in https://github.com/tidys/CocosCreatorPlugins/issues/33#issuecomment-2525654463_\r\n            "
      },
      {
        "author": "keenborder786",
        "created_at": "2024-12-08T21:03:18+00:00",
        "body": "@katyachemistry, you need to use `top_k_results` which basically is how many documents do you need to fetch which you want (minimum number of documents). On the other hand, `load_max_docs ` is bascially if you are fetching more than x docs, this will act as a upper limit meaning query will never fetch more than this amount (maximum number of docs)"
      },
      {
        "author": "keenborder786",
        "created_at": "2024-12-08T21:05:53+00:00",
        "body": "@katyachemistry check this: https://python.langchain.com/api_reference/community/utilities/langchain_community.utilities.arxiv.ArxivAPIWrapper.html"
      },
      {
        "author": "keenborder786",
        "created_at": "2024-12-08T21:10:55+00:00",
        "body": "Their is a slight issue where `load_max_docs` is not being used but you want `top_k_results` regardless "
      },
      {
        "author": "katyachemistry",
        "created_at": "2024-12-08T21:18:37+00:00",
        "body": "> Their is a slight issue where `load_max_docs` is not being used but you want `top_k_results` regardless\r\n\r\nI see. top_k_results works, thank you."
      }
    ],
    "url": "https://github.com/langchain-ai/langchain/issues/28614"
  },
  {
    "repository": "langchain-ai/langchain",
    "issue_number": 28607,
    "title": "langchain_ollama: ChatOllama default value of `format` causes error",
    "body": "### Checked other resources\n\n- [X] I added a very descriptive title to this issue.\n- [X] I searched the LangChain documentation with the integrated search.\n- [X] I used the GitHub search to find a similar question and didn't find it.\n- [X] I am sure that this is a bug in LangChain rather than my code.\n- [X] The bug is not resolved by updating to the latest stable version of LangChain (or the specific integration package).\n\n### Example Code\n\n```python\r\nfrom langchain_ollama import ChatOllama\r\n\r\nllm = ChatOllama(\r\n    model=\"qwen2.5:72b-instruct\",\r\n    base_url=\"http://127.0.0.1:11434\",\r\n    temperature=0,\r\n    num_ctx=32000,  # increased max characters, otherwise system prompts will be ignored\r\n    num_predict=2048,\r\n)\r\n\r\nresult = llm.invoke(\"hello\")\r\nprint(result.content)\r\n```\n\n### Error Message and Stack Trace (if applicable)\n\n```\r\nTraceback (most recent call last):\r\n  File \"ollama-bug.py\", line 11, in <module>\r\n    result = llm.invoke(\"hello\")\r\n             ^^^^^^^^^^^^^^^^^^^\r\n  File \"lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 286, in invoke\r\n    self.generate_prompt(\r\n  File \"lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 786, in generate_prompt\r\n    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 643, in generate\r\n    raise e\r\n  File \"lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 633, in generate\r\n    self._generate_with_cache(\r\n  File \"lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 851, in _generate_with_cache\r\n    result = self._generate(\r\n             ^^^^^^^^^^^^^^^\r\n  File \"lib/python3.11/site-packages/langchain_ollama/chat_models.py\", line 691, in _generate\r\n    final_chunk = self._chat_stream_with_aggregation(\r\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"lib/python3.11/site-packages/langchain_ollama/chat_models.py\", line 592, in _chat_stream_with_aggregation\r\n    for stream_resp in self._create_chat_stream(messages, stop, **kwargs):\r\n  File \"lib/python3.11/site-packages/langchain_ollama/chat_models.py\", line 579, in _create_chat_stream\r\n    yield from self._client.chat(**chat_params)\r\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"lib/python3.11/site-packages/ollama/_client.py\", line 336, in chat\r\n    json=ChatRequest(\r\n         ^^^^^^^^^^^^\r\n  File \"lib/python3.11/site-packages/pydantic/main.py\", line 214, in __init__\r\n    validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)\r\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\npydantic_core._pydantic_core.ValidationError: 2 validation errors for ChatRequest\r\nformat.literal['json']\r\n  Input should be 'json' [type=literal_error, input_value='', input_type=str]\r\n    For further information visit https://errors.pydantic.dev/2.10/v/literal_error\r\nformat.dict[str,any]\r\n  Input should be a valid dictionary [type=dict_type, input_value='', input_type=str]\r\n    For further information visit https://errors.pydantic.dev/2.10/v/dict_type\r\n```\n\n### Description\n\nI used the default value of `format` for `ChatOllama` from the newest `langchain_ollama`, I found that the default value \"\" defined below\r\n\r\nhttps://github.com/langchain-ai/langchain/blob/dd0085a9ff3ced8dcf904369ad394eb71d726ac0/libs/partners/ollama/langchain_ollama/chat_models.py#L388\r\n\r\nwill cause the error above.\r\n\r\nIf I use the `format=\"json\"` option, it could work, but it will lead to json result in a high probability which is not what I expected.\r\n\r\nI think the correct way to declare this parameter should be\r\n```python\r\nformat: Optional[Literal[\"json\"]] = None\r\n```\r\nwhich could give me the correct plain text response.\n\n### System Info\n\nSystem Information\r\n------------------\r\n> OS:  Darwin\r\n> OS Version:  Darwin Kernel Version 24.1.0: Thu Oct 10 21:03:15 PDT 2024; root:xnu-11215.41.3~2/RELEASE_ARM64_T6000\r\n> Python Version:  3.11.9 (main, Apr 19 2024, 11:43:47) [Clang 14.0.6 ]\r\n\r\nPackage Information\r\n-------------------\r\n> langchain_core: 0.3.22\r\n> langchain: 0.3.10\r\n> langchain_community: 0.3.10\r\n> langsmith: 0.1.129\r\n> langchain_experimental: 0.3.2\r\n> langchain_ollama: 0.2.1\r\n> langchain_openai: 0.2.11\r\n> langchain_text_splitters: 0.3.0\r\n> langgraph_api: 0.0.6\r\n> langgraph_cli: 0.1.61\r\n> langgraph_license: Installed. No version info available.\r\n> langgraph_sdk: 0.1.43\r\n> langgraph_storage: Installed. No version info available.\r\n> langserve: 0.3.0\r\n\r\nOther Dependencies\r\n------------------\r\n> aiohttp: 3.10.5\r\n> async-timeout: 4.0.3\r\n> click: 8.1.7\r\n> cryptography: 43.0.3\r\n> dataclasses-json: 0.6.7\r\n> fastapi: 0.115.6\r\n> httpx: 0.27.2\r\n> httpx-sse: 0.4.0\r\n> jsonpatch: 1.33\r\n> jsonschema-rs: 0.25.1\r\n> langgraph: 0.2.56\r\n> langgraph-checkpoint: 2.0.8\r\n> numpy: 1.26.4\r\n> ollama: 0.4.3\r\n> openai: 1.52.1\r\n> orjson: 3.10.11\r\n> packaging: 24.1\r\n> pydantic: 2.10.3\r\n> pydantic-settings: 2.5.2\r\n> pyjwt: 2.10.1\r\n> python-dotenv: 1.0.1\r\n> PyYAML: 6.0.2\r\n> requests: 2.32.3\r\n> SQLAlchemy: 2.0.34\r\n> sse-starlette: 2.1.3\r\n> starlette: 0.41.3\r\n> structlog: 24.4.0\r\n> tenacity: 8.5.0\r\n> tiktoken: 0.7.0\r\n> typing-extensions: 4.12.2\r\n> uvicorn: 0.32.1\r\n> watchfiles: 1.0.0",
    "state": "closed",
    "created_at": "2024-12-07T06:53:04+00:00",
    "closed_at": "2024-12-13T11:01:43+00:00",
    "updated_at": "2024-12-17T03:56:52+00:00",
    "author": "LogCreative",
    "author_type": "User",
    "comments_count": 18,
    "reactions_count": 0,
    "labels": "ðŸ¤–:bug",
    "milestone": null,
    "closed_by": "LogCreative",
    "resolution_time_hours": 148.14416666666668,
    "first_comments": [
      {
        "author": "samyIO",
        "created_at": "2024-12-07T08:35:19+00:00",
        "body": "Omg thanks! I have the same issue. I'm debugging my python environment since hours! Which version is free of this bug?"
      },
      {
        "author": "LogCreative",
        "created_at": "2024-12-07T08:41:53+00:00",
        "body": "> Omg thanks! I have the same issue. I'm debugging my python environment since hours! Which version is free of this bug?\r\n\r\nIt seems the code has not been fixed yetðŸ‘€, the referenced code is the latest."
      },
      {
        "author": "samyIO",
        "created_at": "2024-12-07T08:43:38+00:00",
        "body": "Yea.. i thought maybe i could use an older version. But i don't know when the bug started."
      },
      {
        "author": "LogCreative",
        "created_at": "2024-12-07T08:57:38+00:00",
        "body": "> Yea.. i thought maybe i could use an older version. But i don't know when the bug started.\r\n\r\nI changed the package file directlyðŸ˜.\r\n\r\nThe `ChatOllama` from `langchain_ollama` is new; and the one from `langchain_community.chat_models` has been marked as deprecated and does not support tool calling."
      },
      {
        "author": "juliojm13",
        "created_at": "2024-12-07T13:30:37+00:00",
        "body": "@samyIO Thank you for oppening this issue!\r\n\r\n---\r\nI have the same problem since yesterday after your latest release.\r\nAny estimation to resolve this bug?"
      }
    ],
    "url": "https://github.com/langchain-ai/langchain/issues/28607"
  },
  {
    "repository": "langchain-ai/langchain",
    "issue_number": 28569,
    "title": "d",
    "body": null,
    "state": "closed",
    "created_at": "2024-12-06T12:08:08+00:00",
    "closed_at": "2024-12-06T12:09:26+00:00",
    "updated_at": "2024-12-06T12:09:26+00:00",
    "author": "CerenSurmeli",
    "author_type": "User",
    "comments_count": 0,
    "reactions_count": 0,
    "labels": "",
    "milestone": null,
    "closed_by": "CerenSurmeli",
    "resolution_time_hours": 0.021666666666666667,
    "first_comments": [],
    "url": "https://github.com/langchain-ai/langchain/issues/28569"
  },
  {
    "repository": "langchain-ai/langchain",
    "issue_number": 28560,
    "title": "Validation Error: __init__() got an unexpected keyword argument 'proxies' in OpenAIEmbeddings",
    "body": "### Checked other resources\n\n- [X] I added a very descriptive title to this issue.\n- [X] I searched the LangChain documentation with the integrated search.\n- [X] I used the GitHub search to find a similar question and didn't find it.\n- [X] I am sure that this is a bug in LangChain rather than my code.\n- [X] The bug is not resolved by updating to the latest stable version of LangChain (or the specific integration package).\n\n### Example Code\n\nfrom langchain_openai import OpenAIEmbeddings\r\n\r\ntry:\r\n    embeddings = OpenAIEmbeddings(openai_api_key=\"sk-...\")\r\n    print(\"Embeddings initialized successfully!\")\r\nexcept Exception as e:\r\n    print(\"Error initializing OpenAIEmbeddings:\", e)\r\n\n\n### Error Message and Stack Trace (if applicable)\n\nError initializing OpenAIEmbeddings: 1 validation error for OpenAIEmbeddings\r\n__root__\r\n  __init__() got an unexpected keyword argument 'proxies' (type=type_error)\n\n### Description\n\nI encountered a ValidationError when initializing OpenAIEmbeddings from the langchain-openai package. The error indicates that an unexpected keyword argument proxies is being passed during initialization, even though I am not explicitly providing it.\n\n### System Info\n\nProvide details about your environment:\r\n\r\nOS: Windows 10 64-bit\r\nPython Version: 3.9.0\r\nLangChain Version: 0.2.2\r\nLangChain-OpenAI Version: 0.1.8\r\nOpenAI Version: 1.31.1\r\nPydantic Version: 1.10.12",
    "state": "closed",
    "created_at": "2024-12-06T01:29:18+00:00",
    "closed_at": "2024-12-06T19:40:40+00:00",
    "updated_at": "2025-01-13T19:32:58+00:00",
    "author": "HernanOsorio",
    "author_type": "User",
    "comments_count": 2,
    "reactions_count": 0,
    "labels": "ðŸ¤–:bug,investigate",
    "milestone": null,
    "closed_by": "ccurme",
    "resolution_time_hours": 18.189444444444444,
    "first_comments": [
      {
        "author": "ccurme",
        "created_at": "2024-12-06T19:40:40+00:00",
        "body": "Last week `httpx` [removed a deprecated proxies argument](https://github.com/encode/httpx/releases/tag/0.28.0).\r\n\r\nThe `openai` SDK then released [1.56.1](https://github.com/openai/openai-python/releases/tag/v1.56.1) to remove the use of `proxies`.\r\n\r\nYou will need to either update your OpenAI SDK to the latest version or install `httpx==0.27`.\r\n\r\nLet me know if this does not resolve your issue."
      },
      {
        "author": "UtkuBulkan",
        "created_at": "2025-01-13T19:32:56+00:00",
        "body": "No it does not solve the issue."
      }
    ],
    "url": "https://github.com/langchain-ai/langchain/issues/28560"
  },
  {
    "repository": "langchain-ai/langchain",
    "issue_number": 28543,
    "title": "ollama: support streaming tool calls",
    "body": "### Privileged issue\n\n- [X] I am a LangChain maintainer, or was asked directly by a LangChain maintainer to create an issue here.\n\n### Issue Content\n\nOllama now supports streaming tool calls, we should update `langchain-ollama` to support as well\r\n\r\nhttps://github.com/ollama/ollama/releases/tag/v0.4.6",
    "state": "closed",
    "created_at": "2024-12-05T18:08:29+00:00",
    "closed_at": "2024-12-10T17:54:38+00:00",
    "updated_at": "2024-12-10T17:54:38+00:00",
    "author": "ccurme",
    "author_type": "User",
    "comments_count": 3,
    "reactions_count": 0,
    "labels": "â±­:  models",
    "milestone": null,
    "closed_by": "ccurme",
    "resolution_time_hours": 119.76916666666666,
    "first_comments": [
      {
        "author": "keenborder786",
        "created_at": "2024-12-05T23:52:37+00:00",
        "body": "@ccurme I am working on this"
      },
      {
        "author": "keenborder786",
        "created_at": "2024-12-06T01:30:41+00:00",
        "body": "@ccurme Isn't this already implemented if you see the [following](https://github.com/langchain-ai/langchain/blob/18386c16c7201e54dfaacbc18eed51da7965a7a5/libs/partners/ollama/langchain_ollama/chat_models.py#L724). The tool call is being returned in streaming response. I also tested it with the following on Ollama 0.4.7:\r\n\r\n```python\r\nfrom langchain_ollama.chat_models import ChatOllama\r\nfrom langchain_openai.chat_models import ChatOpenAI\r\nfrom pydantic import BaseModel\r\nclass AdditionTool(BaseModel):\r\n    int_1:int\r\n    int_2:int\r\n\r\nllm = ChatOllama(model=\"llama3.1\")\r\nllm = llm.bind_tools(tools = [AdditionTool])\r\nresult = llm.stream(\"Add the two numbers: 1,2\")\r\n\r\nfor r in result:\r\n    print(r)\r\n\r\n```\r\n\r\nAnd it returned me the following response:\r\n\r\n\r\n```\r\ncontent='' additional_kwargs={} response_metadata={'model': 'llama3.1', 'created_at': '2024-12-06T01:08:45.445958544Z', 'message': {'role': 'assistant', 'content': '', 'tool_calls': [{'function': {'name': 'AdditionTool', 'arguments': {'int_1': '1', 'int_2': '2'}}}]}, 'done_reason': 'stop', 'done': True, 'total_duration': 116583623721, 'load_duration': 64097729072, 'prompt_eval_count': 176, 'prompt_eval_duration': 26549000000, 'eval_count': 28, 'eval_duration': 25890000000} id='run-abd152a9-2d2f-47bf-92dd-08a560e9fc0a' tool_calls=[{'name': 'AdditionTool', 'args': {'int_1': 1, 'int_2': 2}, 'id': 'f6051a75-9315-45d1-abe8-62d04b786931', 'type': 'tool_call'}] usage_metadata={'input_tokens': 176, 'output_tokens': 28, 'total_tokens': 204} tool_call_chunks=[{'name': 'AdditionTool', 'args': '{\"int_1\": 1, \"int_2\": 2}', 'id': 'f6051a75-9315-45d1-abe8-62d04b786931', 'index': None, 'type': 'tool_call_chunk'}]\r\n\r\n```"
      },
      {
        "author": "ccurme",
        "created_at": "2024-12-10T17:54:05+00:00",
        "body": "@keenborder786 current behavior is to override the `stream` parameter if tools are included. Updating this in https://github.com/langchain-ai/langchain/pull/28654."
      }
    ],
    "url": "https://github.com/langchain-ai/langchain/issues/28543"
  },
  {
    "repository": "langchain-ai/langchain",
    "issue_number": 28532,
    "title": "DOC: current doc about MLX-LM is wrong",
    "body": "### URL\n\nhttps://python.langchain.com/docs/integrations/chat/mlx/\n\n### Checklist\n\n- [X] I added a very descriptive title to this issue.\n- [X] I included a link to the documentation page I am referring to (if applicable).\n\n### Issue with current documentation:\n\nRan all cells in the notebook and the very last one got me an error:\r\n\r\n`TemplateError: System role not supported`\n\n### Idea or request for content:\n\nProvide a working example.",
    "state": "closed",
    "created_at": "2024-12-05T14:37:08+00:00",
    "closed_at": "2024-12-23T14:51:45+00:00",
    "updated_at": "2024-12-23T14:51:45+00:00",
    "author": "svnv-svsv-jm",
    "author_type": "User",
    "comments_count": 1,
    "reactions_count": 0,
    "labels": "ðŸ¤–:docs",
    "milestone": null,
    "closed_by": "ccurme",
    "resolution_time_hours": 432.24361111111114,
    "first_comments": [
      {
        "author": "ZhangShenao",
        "created_at": "2024-12-23T06:55:06+00:00",
        "body": "Try to fix this in https://github.com/langchain-ai/langchain/pull/28884"
      }
    ],
    "url": "https://github.com/langchain-ai/langchain/issues/28532"
  },
  {
    "repository": "langchain-ai/langchain",
    "issue_number": 28531,
    "title": "AttributeError: 'HumanMessage' object has no attribute 'get'",
    "body": "### Checked other resources\n\n- [X] I added a very descriptive title to this issue.\n- [X] I searched the LangChain documentation with the integrated search.\n- [X] I used the GitHub search to find a similar question and didn't find it.\n- [X] I am sure that this is a bug in LangChain rather than my code.\n- [X] The bug is not resolved by updating to the latest stable version of LangChain (or the specific integration package).\n\n### Example Code\n\nThe following code raises the error `AttributeError: 'HumanMessage' object has no attribute 'get'`\r\n```python\r\nfrom langchain_core.messages import HumanMessage, AIMessage, ToolMessage\r\nfrom pydantic import BaseModel\r\nfrom typing import List\r\n\r\nfrom datetime import datetime\r\nfrom typing import Any, Generator, Tuple, List\r\nfrom uuid import UUID, uuid4\r\n\r\nfrom langchain_core.messages import AIMessage, HumanMessage\r\n\r\nclass ChatMessage(BaseModel):\r\n    chat_id: UUID\r\n    message_id: UUID\r\n    brain_id: UUID | None\r\n    msg: AIMessage | HumanMessage\r\n    message_time: datetime\r\n    metadata: dict[str, Any]\r\n\r\n\r\nclass ChatHistory:\r\n\r\n    def __init__(self, chat_id: UUID, brain_id: UUID | None) -> None:\r\n        \"\"\"Init a new ChatHistory object.\r\n\r\n        Args:\r\n            chat_id (UUID): A unique identifier for the chat session.\r\n            brain_id (UUID | None): An optional identifier for the brain associated with the chat.\r\n        \"\"\"\r\n        self.id = chat_id\r\n        self.brain_id = brain_id\r\n        # TODO(@aminediro): maybe use a deque() instead ?\r\n        self._msgs: list[ChatMessage] = []\r\n\r\n    def get_chat_history(self, newest_first: bool = False) -> List[ChatMessage]:\r\n        \"\"\"\r\n        Retrieves the chat history, optionally sorted in reverse chronological order.\r\n\r\n        Args:\r\n            newest_first (bool, optional): If True, returns the messages in reverse order (newest first). Defaults to False.\r\n\r\n        Returns:\r\n            List[ChatMessage]: A sorted list of chat messages.\r\n        \"\"\"\r\n        history = sorted(self._msgs, key=lambda msg: msg.message_time)\r\n        if newest_first:\r\n            return history[::-1]\r\n        return history\r\n\r\n    def __len__(self):\r\n        return len(self._msgs)\r\n\r\n    def append(\r\n        self, langchain_msg: AIMessage | HumanMessage, metadata: dict[str, Any] = {}\r\n    ):\r\n        \"\"\"\r\n        Appends a new message to the chat history.\r\n\r\n        Args:\r\n            langchain_msg (AIMessage | HumanMessage): The message content (either an AI or Human message).\r\n            metadata (dict[str, Any], optional): Additional metadata related to the message. Defaults to an empty dictionary.\r\n        \"\"\"\r\n        chat_msg = ChatMessage(\r\n            chat_id=self.id,\r\n            message_id=uuid4(),\r\n            brain_id=self.brain_id,\r\n            msg=langchain_msg,\r\n            message_time=datetime.now(),\r\n            metadata=metadata,\r\n        )\r\n        self._msgs.append(chat_msg)\r\n\r\n    def iter_pairs(self) -> Generator[Tuple[HumanMessage, AIMessage], None, None]:\r\n        \"\"\"\r\n        Iterates over the chat history in pairs, returning a HumanMessage followed by an AIMessage.\r\n\r\n        Yields:\r\n            Tuple[HumanMessage, AIMessage]: Pairs of human and AI messages.\r\n\r\n        Raises:\r\n            AssertionError: If the messages in the pair are not in the expected order (i.e., a HumanMessage followed by an AIMessage).\r\n        \"\"\"\r\n        # Reverse the chat_history, newest first\r\n        it = iter(self.get_chat_history(newest_first=True))\r\n        for ai_message, human_message in zip(it, it, strict=False):\r\n            assert isinstance(\r\n                human_message.msg, HumanMessage\r\n            ), f\"msg {human_message} is not HumanMessage\"\r\n            assert isinstance(\r\n                ai_message.msg, AIMessage\r\n            ), f\"msg {human_message} is not AIMessage\"\r\n            yield (human_message.msg, ai_message.msg)\r\n\r\n    def to_list(self) -> List[HumanMessage | AIMessage]:\r\n        \"\"\"\r\n        Converts the chat history into a list of raw HumanMessage or AIMessage objects.\r\n\r\n        Returns:\r\n            list[HumanMessage | AIMessage]: A list of messages in their raw form, without metadata.\r\n        \"\"\"\r\n\r\n        return [_msg.msg for _msg in self._msgs]\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    human_msg = HumanMessage(content=\"i'm human\")\r\n    ai_msg = AIMessage(content=\"i'm AI\")\r\n    tool_msg = ToolMessage(content=\"i'm tool\", tool_call_id=\"123\")\r\n\r\n    chat_history = ChatHistory(chat_id=uuid4(), brain_id=uuid4())\r\n    chat_history.append(human_msg)\r\n    chat_history.append(ai_msg)\r\n\r\n```\n\n### Error Message and Stack Trace (if applicable)\n\n```\r\nTraceback (most recent call last):\r\n  File \"/Users/jchevall/Coding/quivr-enterprise/backend/core/examples/tmp/test_temp.py\", line 110, in <module>\r\n    chat_history.append(human_msg)\r\n  File \"/Users/jchevall/Coding/quivr-enterprise/backend/core/examples/tmp/test_temp.py\", line 62, in append\r\n    chat_msg = ChatMessage(\r\n               ^^^^^^^^^^^^\r\n  File \"/Users/jchevall/Coding/quivr-enterprise/backend/core/examples/tmp/.venv/lib/python3.11/site-packages/pydantic/main.py\", line 212, in __init__\r\n    validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)\r\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/jchevall/Coding/quivr-enterprise/backend/core/examples/tmp/.venv/lib/python3.11/site-packages/langchain_core/messages/ai.py\", line 202, in _backwards_compat_tool_calls\r\n    check_additional_kwargs = not any(\r\n                                  ^^^^\r\n  File \"/Users/jchevall/Coding/quivr-enterprise/backend/core/examples/tmp/.venv/lib/python3.11/site-packages/langchain_core/messages/ai.py\", line 203, in <genexpr>\r\n    values.get(k)\r\n    ^^^^^^^^^^\r\n  File \"/Users/jchevall/Coding/quivr-enterprise/backend/core/examples/tmp/.venv/lib/python3.11/site-packages/pydantic/main.py\", line 856, in __getattr__\r\n    raise AttributeError(f'{type(self).__name__!r} object has no attribute {item!r}')\r\nAttributeError: 'HumanMessage' object has no attribute 'get'\r\n```\n\n### Description\n\nThis error appeared when upgrading to langchain > 0.3.\r\n\r\nFollowing the solution in #26967, I created a new, clean venv and reinstalled the dependencies, but I still get the error.\n\n### System Info\n\nOutput of `python -m langchain_core.sys_info`\r\n```\r\nSystem Information\r\n------------------\r\n> OS:  Darwin\r\n> OS Version:  Darwin Kernel Version 23.5.0: Wed May  1 20:17:33 PDT 2024; root:xnu-10063.121.3~5/RELEASE_ARM64_T6031\r\n> Python Version:  3.11.9 (main, Aug 14 2024, 04:17:21) [Clang 18.1.8 ]\r\n\r\nPackage Information\r\n-------------------\r\n> langchain_core: 0.3.21\r\n> langchain: 0.3.9\r\n> langchain_community: 0.3.9\r\n> langsmith: 0.1.139\r\n> langchain_anthropic: 0.3.0\r\n> langchain_cohere: 0.3.3\r\n> langchain_experimental: 0.3.3\r\n> langchain_openai: 0.2.11\r\n> langchain_text_splitters: 0.3.2\r\n> langgraph_sdk: 0.1.35\r\n\r\nOptional packages not installed\r\n-------------------------------\r\n> langserve\r\n\r\nOther Dependencies\r\n------------------\r\n> aiohttp: 3.10.10\r\n> anthropic: 0.39.0\r\n> async-timeout: Installed. No version info available.\r\n> cohere: 5.11.3\r\n> dataclasses-json: 0.6.7\r\n> defusedxml: 0.7.1\r\n> httpx: 0.27.2\r\n> httpx-sse: 0.4.0\r\n> jsonpatch: 1.33\r\n> numpy: 1.26.3\r\n> openai: 1.54.4\r\n> orjson: 3.10.11\r\n> packaging: 23.2\r\n> pandas: 2.2.3\r\n> pydantic: 2.9.2\r\n> pydantic-settings: 2.6.1\r\n> PyYAML: 6.0.2\r\n> requests: 2.32.3\r\n> requests-toolbelt: 1.0.0\r\n> SQLAlchemy: 2.0.36\r\n> tabulate: 0.9.0\r\n> tenacity: 8.5.0\r\n> tiktoken: 0.8.0\r\n> typing-extensions: 4.12.2\r\n```\r\n\r\nOutput of `pip freeze`\r\n```\r\naiofiles==23.2.1\r\naiohappyeyeballs==2.4.3\r\naiohttp==3.10.10\r\naiosignal==1.3.1\r\namqp==5.2.0\r\nannotated-types==0.7.0\r\nanthropic==0.39.0\r\nanyascii==0.3.2\r\nanyio==4.6.2.post1\r\nappnope==0.1.4\r\nasttokens==2.4.1\r\nasyncer==0.0.7\r\nasyncpg==0.30.0\r\nattrs==24.2.0\r\nbackoff==2.2.1\r\nbeautifulsoup4==4.12.3\r\nbidict==0.23.1\r\nbilliard==4.2.1\r\nblack==24.10.0\r\ncachetools==5.5.0\r\ncelery==5.4.0\r\ncertifi==2022.12.7\r\ncffi==1.17.1\r\ncfgv==3.4.0\r\nchainlit==1.3.2\r\nchardet==5.2.0\r\ncharset-normalizer==2.1.1\r\nchevron==0.14.0\r\nclick==8.1.7\r\nclick-didyoumean==0.3.1\r\nclick-plugins==1.1.1\r\nclick-repl==0.3.0\r\ncohere==5.11.3\r\ncolorama==0.4.6\r\ncolorlog==6.9.0\r\ncomm==0.2.2\r\ncontourpy==1.3.0\r\ncoverage==7.6.4\r\ncryptography==43.0.3\r\ncycler==0.12.1\r\ndataclasses-json==0.6.7\r\ndebugpy==1.8.7\r\ndecorator==5.1.1\r\ndefusedxml==0.7.1\r\nDeprecated==1.2.14\r\ndeprecation==2.1.0\r\ndiff-match-patch==20241021\r\ndirtyjson==1.0.8\r\ndistlib==0.3.9\r\ndistro==1.9.0\r\ndocx2txt==0.8\r\ndropbox==12.0.2\r\necdsa==0.19.0\r\nemoji==2.14.0\r\net_xmlfile==2.0.0\r\nexecnet==2.1.1\r\nexecuting==2.1.0\r\nfaiss-cpu==1.9.0\r\nfastapi==0.115.5\r\nfastavro==1.9.7\r\nfilelock==3.13.1\r\nfiletype==1.2.0\r\nflake8==7.1.1\r\nflake8-black==0.3.6\r\nflower==2.0.1\r\nfonttools==4.54.1\r\nfpdf2==2.8.1\r\nfrozenlist==1.5.0\r\nfsspec==2024.2.0\r\ngoogle-api-core==2.22.0\r\ngoogle-api-python-client==2.151.0\r\ngoogle-auth==2.35.0\r\ngoogle-auth-httplib2==0.2.0\r\ngoogle-auth-oauthlib==1.2.1\r\ngoogleapis-common-protos==1.65.0\r\ngotrue==2.10.0\r\ngprof2dot==2024.6.6\r\ngreenlet==3.1.1\r\ngrpcio==1.67.1\r\nh11==0.14.0\r\nh2==4.1.0\r\nh5py==3.10.0\r\nhiredis==3.0.0\r\nhpack==4.0.0\r\nhttpcore==1.0.6\r\nhttplib2==0.22.0\r\nhttpx==0.27.2\r\nhttpx-sse==0.4.0\r\nhuggingface-hub==0.26.2\r\nhumanize==4.11.0\r\nhyperframe==6.0.1\r\nidentify==2.6.1\r\nidna==3.4\r\nimportlib_metadata==8.4.0\r\niniconfig==2.0.0\r\nipykernel==6.29.5\r\nipython==8.29.0\r\njedi==0.19.1\r\nJinja2==3.1.3\r\njiter==0.7.0\r\njoblib==1.4.2\r\njsonpatch==1.33\r\njsonpath-python==1.0.6\r\njsonpointer==3.0.0\r\njsonschema==4.23.0\r\njsonschema-specifications==2024.10.1\r\njupyter_client==8.6.3\r\njupyter_core==5.7.2\r\nkiwisolver==1.4.7\r\nkombu==5.4.2\r\nlangchain==0.3.9\r\nlangchain-anthropic==0.3.0\r\nlangchain-cohere==0.3.3\r\nlangchain-community==0.3.9\r\nlangchain-core==0.3.21\r\nlangchain-experimental==0.3.3\r\nlangchain-openai==0.2.11\r\nlangchain-text-splitters==0.3.2\r\nlangdetect==1.0.9\r\nlanggraph==0.2.44\r\nlanggraph-checkpoint==2.0.2\r\nlanggraph-sdk==0.1.35\r\nlangsmith==0.1.139\r\nLazify==0.4.0\r\nlitellm==1.51.3\r\nliteralai==0.0.623\r\nllama-cloud==0.1.4\r\nllama-index==0.11.23\r\nllama-index-agent-openai==0.3.4\r\nllama-index-cli==0.3.1\r\nllama-index-core==0.11.23\r\nllama-index-embeddings-openai==0.2.5\r\nllama-index-indices-managed-llama-cloud==0.4.0\r\nllama-index-legacy==0.9.48.post3\r\nllama-index-llms-openai==0.2.16\r\nllama-index-multi-modal-llms-openai==0.2.3\r\nllama-index-program-openai==0.2.0\r\nllama-index-question-gen-openai==0.2.0\r\nllama-index-readers-file==0.3.0\r\nllama-index-readers-llama-parse==0.3.0\r\nllama-parse==0.5.13\r\nllvmlite==0.43.0\r\nloguru==0.7.2\r\nlxml==5.3.0\r\nmarkdown-it-py==3.0.0\r\nmarkdownify==0.13.1\r\nMarkupSafe==3.0.1\r\nmarshmallow==3.23.1\r\nmarshmallow-enum==1.5.1\r\nmatplotlib==3.9.2\r\nmatplotlib-inline==0.1.7\r\nmccabe==0.7.0\r\nmdurl==0.1.2\r\nmegaparse==0.0.43\r\nmegaparse-sdk==0.1.7\r\nmonotonic==1.6\r\nmplcursors==0.6\r\nmpmath==1.3.0\r\nmsal==1.31.0\r\nmsgpack==1.1.0\r\nmultidict==6.1.0\r\nmypy==1.13.0\r\nmypy-extensions==1.0.0\r\nnats-py==2.9.0\r\nnest-asyncio==1.6.0\r\nnetworkx==3.2.1\r\nnltk==3.9.1\r\nnodeenv==1.9.1\r\nnotion-client==2.2.1\r\nnumba==0.60.0\r\nnumpy==1.26.3\r\noauthlib==3.2.2\r\nopenai==1.54.4\r\nopencv-python==4.10.0.84\r\nopenpyxl==3.1.5\r\nopentelemetry-api==1.27.0\r\nopentelemetry-exporter-otlp==1.27.0\r\nopentelemetry-exporter-otlp-proto-common==1.27.0\r\nopentelemetry-exporter-otlp-proto-grpc==1.27.0\r\nopentelemetry-exporter-otlp-proto-http==1.27.0\r\nopentelemetry-instrumentation==0.48b0\r\nopentelemetry-proto==1.27.0\r\nopentelemetry-sdk==1.27.0\r\nopentelemetry-semantic-conventions==0.48b0\r\norjson==3.10.11\r\npackaging==23.2\r\npandas==2.2.3\r\nparameterized==0.9.0\r\nparso==0.8.4\r\npathspec==0.12.1\r\npdf2image==1.17.0\r\npexpect==4.9.0\r\npgvector==0.3.6\r\npillow==10.2.0\r\nplatformdirs==4.3.6\r\nplaywright==1.48.0\r\npluggy==1.5.0\r\nply==3.11\r\npostgrest==0.17.2\r\nposthog==3.7.0\r\npre_commit==4.0.1\r\nprometheus_client==0.21.0\r\nprompt_toolkit==3.0.48\r\npropcache==0.2.0\r\nproto-plus==1.25.0\r\nprotobuf==4.25.5\r\npsutil==6.1.0\r\npsycopg2-binary==2.9.10\r\nptyprocess==0.7.0\r\npure_eval==0.2.3\r\npy-cpuinfo==9.0.0\r\npyasn1==0.6.1\r\npyasn1_modules==0.4.1\r\npyclipper==1.3.0.post6\r\npycodestyle==2.12.1\r\npycparser==2.22\r\npycryptodome==3.21.0\r\npydantic==2.9.2\r\npydantic-settings==2.6.1\r\npydantic_core==2.23.4\r\npyee==12.0.0\r\npyflakes==3.2.0\r\nPygments==2.18.0\r\npyinstrument==5.0.0\r\nPyJWT==2.9.0\r\npyparsing==3.2.0\r\npypdf==5.1.0\r\npypdfium2==4.30.0\r\npyproject-api==1.6.1\r\npytest==8.3.3\r\npytest-asyncio==0.24.0\r\npytest-benchmark==5.1.0\r\npytest-cov==6.0.0\r\npytest-dotenv==0.5.2\r\npytest-mock==3.14.0\r\npytest-profiling==1.7.0\r\npytest-xdist==3.6.1\r\npython-dateutil==2.9.0.post0\r\npython-doctr==0.10.0\r\npython-dotenv==1.0.1\r\npython-engineio==4.10.1\r\npython-iso639==2024.10.22\r\npython-jose==3.3.0\r\npython-json-logger==2.0.7\r\npython-magic==0.4.27\r\npython-multipart==0.0.9\r\npython-socketio==5.11.4\r\npytz==2024.2\r\nPyYAML==6.0.2\r\npyzmq==26.2.0\r\nRapidFuzz==3.10.1\r\nrealtime==2.0.6\r\nredis==5.2.0\r\nreferencing==0.35.1\r\nregex==2024.9.11\r\nrequests==2.32.3\r\nrequests-oauthlib==2.0.0\r\nrequests-toolbelt==1.0.0\r\nresend==2.4.0\r\nrich==13.9.4\r\nrpds-py==0.20.1\r\nrsa==4.9\r\nruff==0.7.2\r\nsafetensors==0.4.5\r\nscikit-learn==1.5.2\r\nscipy==1.14.1\r\nsentencepiece==0.2.0\r\nsentry-sdk==2.18.0\r\nshapely==2.0.6\r\nsimple-websocket==1.1.0\r\nsix==1.16.0\r\nsniffio==1.3.1\r\nsoupsieve==2.6\r\nSQLAlchemy==2.0.36\r\nsqlmodel==0.0.22\r\nstack-data==0.6.3\r\nstarlette==0.41.2\r\nstone==3.3.1\r\nstorage3==0.8.2\r\nstriprtf==0.0.26\r\nstructlog==24.4.0\r\nsupabase==2.9.1\r\nsupafunc==0.6.2\r\nsympy==1.13.1\r\nsyncer==2.0.3\r\ntabulate==0.9.0\r\ntenacity==8.5.0\r\nthreadpoolctl==3.5.0\r\ntiktoken==0.8.0\r\ntokenizers==0.20.1\r\ntomli==2.1.0\r\ntorch==2.4.0\r\ntorchvision==0.19.0\r\ntornado==6.4.1\r\ntox==4.15.1\r\ntqdm==4.66.5\r\ntraitlets==5.14.3\r\ntransformers==4.46.1\r\ntypes-cffi==1.16.0.20240331\r\ntypes-pyOpenSSL==24.1.0.20240722\r\ntypes-PyYAML==6.0.12.20240917\r\ntypes-redis==4.6.0.20241004\r\ntypes-requests==2.31.0.6\r\ntypes-setuptools==75.2.0.20241025\r\ntypes-urllib3==1.26.25.14\r\ntyping-inspect==0.9.0\r\ntyping_extensions==4.12.2\r\ntzdata==2024.2\r\nUnidecode==1.3.8\r\nunstructured==0.15.0\r\nunstructured-client==0.8.1\r\nuptrace==1.27.0\r\nuritemplate==4.1.1\r\nurllib3==1.26.13\r\nuvicorn==0.25.0\r\nuvloop==0.21.0\r\nvine==5.1.0\r\nvirtualenv==20.27.1\r\nwatchfiles==0.20.0\r\nwcwidth==0.2.13\r\nwebsockets==13.1\r\nwrapt==1.16.0\r\nwsproto==1.2.0\r\nyarl==1.17.1\r\nzipp==3.20.2\r\n```",
    "state": "closed",
    "created_at": "2024-12-05T13:48:26+00:00",
    "closed_at": "2024-12-12T01:38:49+00:00",
    "updated_at": "2024-12-13T19:25:34+00:00",
    "author": "jacopo-chevallard",
    "author_type": "User",
    "comments_count": 8,
    "reactions_count": 0,
    "labels": "ðŸ¤–:bug,â±­:  pydantic",
    "milestone": null,
    "closed_by": "efriis",
    "resolution_time_hours": 155.83972222222224,
    "first_comments": [
      {
        "author": "pranitsawant80",
        "created_at": "2024-12-07T07:48:00+00:00",
        "body": "```\r\nfrom langchain_core.messages import HumanMessage, AIMessage\r\nfrom pydantic import BaseModel\r\nfrom typing import List, Any, Generator, Tuple, Dict\r\nfrom datetime import datetime\r\nfrom uuid import UUID, uuid4\r\n\r\nclass ChatMessage(BaseModel):\r\n    chat_id: UUID\r\n    message_id: UUID\r\n    brain_id: UUID | None\r\n    msg_type: str\r\n#############################################################\r\n    content: Dict[str, Any]\r\n@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ \r\n    message_time: datetime\r\n    metadata: Dict[str, Any]\r\n\r\nclass ChatHistory:\r\n    def __init__(self, chat_id: UUID, brain_id: UUID | None) -> None:\r\n        self.id = chat_id\r\n        self.brain_id = brain_id\r\n        self._msgs: List[ChatMessage] = []\r\n\r\n    def get_chat_history(self, newest_first: bool = False) -> List[ChatMessage]:\r\n        history = sorted(self._msgs, key=lambda msg: msg.message_time)\r\n        return history[::-1] if newest_first else history\r\n\r\n    def __len__(self):\r\n        return len(self._msgs)\r\n\r\n    def append(self, langchain_msg: AIMessage | HumanMessage, metadata: Dict[str, Any] = {}):\r\n#####################################################\r\n        msg_type = langchain_msg.__class__.__name__\r\n        **content = {\r\n            \"content\": langchain_msg.content\r\n          \r\n        }\r\n@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\r\n        chat_msg = ChatMessage(\r\n            chat_id=self.id,\r\n            message_id=uuid4(),\r\n            brain_id=self.brain_id,\r\n            msg_type=msg_type,\r\n            content=content,\r\n            message_time=datetime.now(),\r\n            metadata=metadata,\r\n        )\r\n        self._msgs.append(chat_msg)\r\n##################################################################################\r\n    def iter_pairs(self) -> Generator[Tuple[HumanMessage, AIMessage], None, None]:\r\n        it = iter(self.get_chat_history(newest_first=True))\r\n        for ai_message, human_message in zip(it, it, strict=False):\r\n            if isinstance(human_message.content['content'], HumanMessage) and isinstance(ai_message.content['content'], AIMessage):\r\n                yield (human_message.content['content'], ai_message.content['content'])\r\n\r\n    def to_list(self) -> List[HumanMessage | AIMessage]:\r\n        return [_msg.content['content'] for _msg in self._msgs]\r\n@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\r\nif __name__ == \"__main__\":\r\n    human_msg = HumanMessage(content=\"I'm human\")\r\n    ai_msg = AIMessage(content=\"I'm AI\")\r\n\r\n    chat_history = ChatHistory(chat_id=uuid4(), brain_id=uuid4())\r\n    chat_history.append(human_msg)\r\n    chat_history.append(ai_msg)\r\n\r\n\r\n```\r\n\r\nCheck this it might help..\r\n\r\nThe issue with langchain core   undersite packages..in ai.py file \r\n\r\nI tried installing the newly updated version for langchain core  ie.  0.3.22      But it did not help...\r\n\r\nSo I thought of changing our logic rather than changing that ai.py file.. \r\n\r\nI have put changes between  ##############  and  @@@@@@@@@@@@@  in the above code. \r\nchanges are in ChatMessage module -include a content dictionary field\r\nappend method - passing content after wrapping it under the dictionary\r\niter_pair - have changed to make it work for new modifications\r\n\r\nYou can adjust functions accordingly and remove unnecessary lines...\r\n\r\nHope this will help...ðŸ™\r\n\r\n"
      },
      {
        "author": "jacopo-chevallard",
        "created_at": "2024-12-09T09:05:55+00:00",
        "body": "Perhaps @keenborder786 or @Jakolo121, who helped on #26967, can confirm whether this is a bug or not?"
      },
      {
        "author": "Jakolo121",
        "created_at": "2024-12-09T09:45:40+00:00",
        "body": "@jacopo-chevallard you mentioned that \r\n> Following the solution in https://github.com/langchain-ai/langchain/issues/26967, I created a new, clean venv and reinstalled the dependencies, but I still get the error.\r\n\r\nwhat helped on the #26967 was reinstalling python and poetry. So I guess this is not a bug. But I didn't test your code on my environment."
      },
      {
        "author": "jacopo-chevallard",
        "created_at": "2024-12-09T11:14:46+00:00",
        "body": "thanks for the feedback @Jakolo121 \r\n\r\n> So I guess this is not a bug. But I didn't test your code on my environment.\r\n\r\nOk, if you get the chance to test it, it would be great (and useful for the LangChain folks, I guess, to confirm the existence of a bug)"
      },
      {
        "author": "Jakolo121",
        "created_at": "2024-12-09T16:27:55+00:00",
        "body": "I tried your code in the environment from the #26967 issue and it did not work. I also tested your code in a new environment. With the same result. I created the new env with Python 3.13 and 3.12"
      }
    ],
    "url": "https://github.com/langchain-ai/langchain/issues/28531"
  },
  {
    "repository": "langchain-ai/langchain",
    "issue_number": 28530,
    "title": "langchain-cli: failed to download Grit CLI",
    "body": "### Checked other resources\n\n- [X] I added a very descriptive title to this issue.\n- [X] I searched the LangChain documentation with the integrated search.\n- [X] I used the GitHub search to find a similar question and didn't find it.\n- [X] I am sure that this is a bug in LangChain rather than my code.\n- [X] The bug is not resolved by updating to the latest stable version of LangChain (or the specific integration package).\n\n### Example Code\n\nRun the cli-tool In the base directory for the source code:\r\n\r\n```sh\r\nlangchain-cli migrate .\r\n```\n\n### Error Message and Stack Trace (if applicable)\n\n```\r\nDownloading Grit CLI from https://github.com/getgrit/gritql/releases/latest/download/marzano-x86_64-apple-darwin.tar.gz\r\nâ•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Traceback (most recent call last) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®\r\nâ”‚ /xxxxx/xxx/xxx/xxx/xxxxxxxxxxxxxxxxxx/sources/backend/.venv/lib/python3.11/site-packages/langcha â”‚\r\nâ”‚ in_cli/namespaces/migrate/main.py:68 in migrate                                                  â”‚\r\nâ”‚                                                                                                  â”‚\r\nâ”‚   65 â”‚   if diff:                                                                                â”‚\r\nâ”‚   66 â”‚   â”‚   args.append(\"--dry-run\")                                                            â”‚\r\nâ”‚   67 â”‚                                                                                           â”‚\r\nâ”‚ â± 68 â”‚   final_code = run.apply_pattern(                                                         â”‚\r\nâ”‚   69 â”‚   â”‚   \"langchain_all_migrations()\",                                                       â”‚\r\nâ”‚   70 â”‚   â”‚   args,                                                                               â”‚\r\nâ”‚   71 â”‚   â”‚   grit_dir=get_gritdir_path(),                                                        â”‚\r\nâ”‚                                                                                                  â”‚\r\nâ”‚ â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ locals â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®                                     â”‚\r\nâ”‚ â”‚        args = []                                         â”‚                                     â”‚\r\nâ”‚ â”‚         ctx = <click.core.Context object at 0x10a37b210> â”‚                                     â”‚\r\nâ”‚ â”‚        diff = False                                      â”‚                                     â”‚\r\nâ”‚ â”‚ interactive = False                                      â”‚                                     â”‚\r\nâ”‚ â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯                                     â”‚\r\nâ”‚                                                                                                  â”‚\r\nâ”‚ /xxxxx/xxx/xxx/xxx/xxxxxxxxxxxxxxxxxx/sources/backend/.venv/lib/python3.11/site-packages/gritql/ â”‚\r\nâ”‚ run.py:23 in apply_pattern                                                                       â”‚\r\nâ”‚                                                                                                  â”‚\r\nâ”‚   20 â”‚   if grit_dir:                                                                            â”‚\r\nâ”‚   21 â”‚   â”‚   final_args.append(\"--grit-dir\")                                                     â”‚\r\nâ”‚   22 â”‚   â”‚   final_args.append(grit_dir)                                                         â”‚\r\nâ”‚ â± 23 â”‚   return run_cli(final_args)                                                              â”‚\r\nâ”‚   24                                                                                             â”‚\r\nâ”‚   25 if __name__ == \"__main__\":                                                                  â”‚\r\nâ”‚   26 â”‚   run_cli(sys.argv[1:])                                                                   â”‚\r\nâ”‚                                                                                                  â”‚\r\nâ”‚ â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ locals â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•® â”‚\r\nâ”‚ â”‚            args = []                                                                         â”‚ â”‚\r\nâ”‚ â”‚      final_args = [                                                                          â”‚ â”‚\r\nâ”‚ â”‚                   â”‚   'apply',                                                               â”‚ â”‚\r\nâ”‚ â”‚                   â”‚   'langchain_all_migrations()',                                          â”‚ â”‚\r\nâ”‚ â”‚                   â”‚   '--grit-dir',                                                          â”‚ â”‚\r\nâ”‚ â”‚                   â”‚                                                                          â”‚ â”‚\r\nâ”‚ â”‚                   PosixPath('/xxxxx/xxx/xxx/xxx/xxxxxxxxxxxxxxxxxx/sources/backend/.venv/liâ€¦ â”‚ â”‚\r\nâ”‚ â”‚                   ]                                                                          â”‚ â”‚\r\nâ”‚ â”‚        grit_dir = PosixPath('/xxxxx/xxx/xxx/xxx/xxxxxxxxxxxxxxxxxx/sources/backend/.venv/liâ€¦ â”‚ â”‚\r\nâ”‚ â”‚ pattern_or_name = 'langchain_all_migrations()'                                               â”‚ â”‚\r\nâ”‚ â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯ â”‚\r\nâ”‚                                                                                                  â”‚\r\nâ”‚ /xxxxx/xxx/xxx/xxx/xxxxxxxxxxxxxxxxxx/sources/backend/.venv/lib/python3.11/site-packages/gritql/ â”‚\r\nâ”‚ run.py:9 in run_cli                                                                              â”‚\r\nâ”‚                                                                                                  â”‚\r\nâ”‚    6                                                                                             â”‚\r\nâ”‚    7 def run_cli(args: Any):                                                                     â”‚\r\nâ”‚    8 â”‚   \"\"\"Runs the Grit CLI\"\"\"                                                                 â”‚\r\nâ”‚ â±  9 â”‚   cli_path = find_install()                                                               â”‚\r\nâ”‚   10 â”‚   print(\"Running GritQL pattern with args:\", cli_path, args)                              â”‚\r\nâ”‚   11 â”‚                                                                                           â”‚\r\nâ”‚   12 â”‚   code = subprocess.run([cli_path, *args])                                                â”‚\r\nâ”‚                                                                                                  â”‚\r\nâ”‚ â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ locals â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•® â”‚\r\nâ”‚ â”‚ args = [                                                                                     â”‚ â”‚\r\nâ”‚ â”‚        â”‚   'apply',                                                                          â”‚ â”‚\r\nâ”‚ â”‚        â”‚   'langchain_all_migrations()',                                                     â”‚ â”‚\r\nâ”‚ â”‚        â”‚   '--grit-dir',                                                                     â”‚ â”‚\r\nâ”‚ â”‚        â”‚                                                                                     â”‚ â”‚\r\nâ”‚ â”‚        PosixPath('/xxxxx/xxx/xxx/xxx/xxxxxxxxxxxxxxxxxx/sources/backend/.venv/lib/python3.1â€¦ â”‚ â”‚\r\nâ”‚ â”‚        ]                                                                                     â”‚ â”‚\r\nâ”‚ â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯ â”‚\r\nâ”‚                                                                                                  â”‚\r\nâ”‚ /xxxxx/xxx/xxx/xxx/xxxxxxxxxxxxxxxxxx/sources/backend/.venv/lib/python3.11/site-packages/gritql/ â”‚\r\nâ”‚ installer.py:82 in find_install                                                                  â”‚\r\nâ”‚                                                                                                  â”‚\r\nâ”‚    79 â”‚   with httpx.Client() as client:                                                         â”‚\r\nâ”‚    80 â”‚   â”‚   download_response = client.get(download_url, follow_redirects=True)                â”‚\r\nâ”‚    81 â”‚   â”‚   if download_response.status_code != 200:                                           â”‚\r\nâ”‚ â±  82 â”‚   â”‚   â”‚   raise CLIError(f\"Failed to download Grit CLI from {download_url}\")             â”‚\r\nâ”‚    83 â”‚   â”‚   with open(temp_file, \"wb\") as file:                                                â”‚\r\nâ”‚    84 â”‚   â”‚   â”‚   for chunk in download_response.iter_bytes():                                   â”‚\r\nâ”‚    85 â”‚   â”‚   â”‚   â”‚   file.write(chunk)                                                          â”‚\r\nâ”‚                                                                                                  â”‚\r\nâ”‚ â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ locals â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•® â”‚\r\nâ”‚ â”‚              arch = 'x86_64'                                                                 â”‚ â”‚\r\nâ”‚ â”‚            client = <httpx.Client object at 0x10bb22790>                                     â”‚ â”‚\r\nâ”‚ â”‚          dir_name = PosixPath('/xxxxx/xxx/.cache/grit')                                      â”‚ â”‚\r\nâ”‚ â”‚ download_response = <Response [404 Not Found]>                                               â”‚ â”‚\r\nâ”‚ â”‚      download_url = 'https://github.com/getgrit/gritql/releases/latest/download/marzano-x86â€¦ â”‚ â”‚\r\nâ”‚ â”‚         file_name = 'marzano-x86_64-apple-darwin'                                            â”‚ â”‚\r\nâ”‚ â”‚         grit_path = None                                                                     â”‚ â”‚\r\nâ”‚ â”‚       install_dir = PosixPath('/xxxxx/xxx/.cache/grit/.install')                             â”‚ â”‚\r\nâ”‚ â”‚          platform = 'apple-darwin'                                                           â”‚ â”‚\r\nâ”‚ â”‚        target_dir = PosixPath('/xxxxx/xxx/.cache/grit/.install/bin')                         â”‚ â”‚\r\nâ”‚ â”‚       target_path = PosixPath('/xxxxx/xxx/.cache/grit/.install/bin/marzano')                 â”‚ â”‚\r\nâ”‚ â”‚         temp_file = PosixPath('/xxxxx/xxx/.cache/grit/.install/bin/marzano.tmp')             â”‚ â”‚\r\nâ”‚ â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯ â”‚\r\nâ•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯```\n\n### Description\n\nI am migrating from langchain `0.2.17` to `0.3.9`.  I followed the instructions on https://python.langchain.com/docs/versions/v0_3/#migrate-using-langchain-cli . When I attempted to start the migration, the described error occurs. There referenced URL does not exist.\r\n\n\n### System Info\n\n```\r\nSystem Information\r\n------------------\r\n> OS:  Darwin\r\n> OS Version:  Darwin Kernel Version 24.1.0: Thu Oct 10 21:03:15 PDT 2024; root:xnu-11215.41.3~2/RELEASE_ARM64_T6000\r\n> Python Version:  3.11.4 (main, Jun 15 2023, 07:29:58) [Clang 14.0.3 (clang-1403.0.22.14.1)]\r\n\r\nPackage Information\r\n-------------------\r\n> langchain_core: 0.3.21\r\n> langchain: 0.3.9\r\n> langchain_community: 0.3.9\r\n> langsmith: 0.1.147\r\n> langchain_cli: 0.0.33\r\n> langchain_openai: 0.2.11\r\n> langchain_text_splitters: 0.3.2\r\n> langserve: 0.3.0\r\n\r\nOther Dependencies\r\n------------------\r\n> aiohttp: 3.11.9\r\n> async-timeout: Installed. No version info available.\r\n> dataclasses-json: 0.6.7\r\n> fastapi: 0.105.0\r\n> gitpython: 3.1.43\r\n> gritql: 0.1.5\r\n> httpx: 0.27.2\r\n> httpx-sse: 0.4.0\r\n> jsonpatch: 1.33\r\n> langserve[all]: Installed. No version info available.\r\n> langsmith-pyo3: Installed. No version info available.\r\n> numpy: 1.26.4\r\n> openai: 1.56.2\r\n> orjson: 3.10.12\r\n> packaging: 23.2\r\n> pydantic: 2.10.3\r\n> pydantic-settings: 2.6.1\r\n> PyYAML: 6.0.2\r\n> requests: 2.32.3\r\n> requests-toolbelt: 1.0.0\r\n> SQLAlchemy: 2.0.36\r\n> sse-starlette: 1.8.2\r\n> tenacity: 8.5.0\r\n> tiktoken: 0.7.0\r\n> tomlkit: 0.13.2\r\n> typer[all]: Installed. No version info available.\r\n> typing-extensions: 4.12.2\r\n> uvicorn: 0.29.0\r\n```",
    "state": "closed",
    "created_at": "2024-12-05T13:46:59+00:00",
    "closed_at": "2024-12-05T13:58:29+00:00",
    "updated_at": "2024-12-05T14:04:26+00:00",
    "author": "codekie",
    "author_type": "User",
    "comments_count": 1,
    "reactions_count": 0,
    "labels": "ðŸ¤–:bug",
    "milestone": null,
    "closed_by": "codekie",
    "resolution_time_hours": 0.19166666666666668,
    "first_comments": [
      {
        "author": "codekie",
        "created_at": "2024-12-05T13:58:29+00:00",
        "body": "Duplicate of #27822 ."
      }
    ],
    "url": "https://github.com/langchain-ai/langchain/issues/28530"
  },
  {
    "repository": "langchain-ai/langchain",
    "issue_number": 28467,
    "title": "BadRequestError: Error code: 400 - {'error': {'message': 'Input required: specify \"prompt\"', 'code': 400}, 'user_id': 'user_2egJVLT61dw0Z9NPjilqjNjFkh9'}",
    "body": "### Checked other resources\n\n- [X] I added a very descriptive title to this issue.\n- [X] I searched the LangChain documentation with the integrated search.\n- [X] I used the GitHub search to find a similar question and didn't find it.\n- [X] I am sure that this is a bug in LangChain rather than my code.\n- [X] The bug is not resolved by updating to the latest stable version of LangChain (or the specific integration package).\n\n### Example Code\n\nfrom langchain_openai import OpenAI\r\n\r\nllm = OpenAI()\r\n\r\nresponse = llm.invoke(input=\"Hello how are you?\")  # æ­£ç¡®\r\n\n\n### Error Message and Stack Trace (if applicable)\n\n_No response_\n\n### Description\n\nfrom langchain_openai import OpenAI\r\n\r\nllm = OpenAI()\r\n\r\nresponse = llm.invoke(input=\"Hello how are you?\")  # æ­£ç¡®\r\n\n\n### System Info\n\nfrom langchain_openai import OpenAI\r\n\r\nllm = OpenAI()\r\n\r\nresponse = llm.invoke(input=\"Hello how are you?\")  # æ­£ç¡®\r\n",
    "state": "closed",
    "created_at": "2024-12-03T07:00:13+00:00",
    "closed_at": "2024-12-03T16:42:25+00:00",
    "updated_at": "2024-12-03T16:42:26+00:00",
    "author": "phpmac",
    "author_type": "User",
    "comments_count": 1,
    "reactions_count": 0,
    "labels": "ðŸ¤–:bug,investigate",
    "milestone": null,
    "closed_by": "ccurme",
    "resolution_time_hours": 9.703333333333333,
    "first_comments": [
      {
        "author": "ccurme",
        "created_at": "2024-12-03T16:42:25+00:00",
        "body": "Instead of specifying `input=`, you should just pass the string as a positional arg to `invoke`. See documentation [here](https://python.langchain.com/docs/integrations/llms/openai/). Let me know if this does not solve your problem.\r\n\r\nNote also the warning at the top of that page regarding the use of legacy text-completion models (`OpenAI`) vs. modern chat models (`ChatOpenAI`). In most cases you want to use `ChatOpenAI`."
      }
    ],
    "url": "https://github.com/langchain-ai/langchain/issues/28467"
  },
  {
    "repository": "langchain-ai/langchain",
    "issue_number": 28450,
    "title": "DOC: Managing Conversation History",
    "body": "### URL\n\nhttps://python.langchain.com/docs/tutorials/chatbot/#managing-conversation-history\n\n### Checklist\n\n- [X] I added a very descriptive title to this issue.\n- [X] I included a link to the documentation page I am referring to (if applicable).\n\n### Issue with current documentation:\n\n![image](https://github.com/user-attachments/assets/300c8115-57b4-4241-b659-8fed707c9691)\r\n\r\n\r\n\r\n**Problem in the Example:**  \r\n- **query1**: `\"What is my name?\"`\r\n  thread_id: `abc567`\r\n- **query2**: `\"What math problem did I ask?\"`\r\n  thread_id: `abc678`\r\n\r\nThe change in `thread_id` is incorrect, as both queries should belong to the same conversation.\r\n\r\n**Intended Behavior to Demonstrate:**  \r\n- After trimming the chat history, the context for `query1` is discarded. The model cannot answer it.  \r\n- The context for `query2` is retained, so the model can still respond to it.\r\n\r\n**Suggested Fix:**  \r\nUse the same `thread_id` for both queries to accurately reflect context retention within a single thread.\n\n### Idea or request for content:\n\n_No response_",
    "state": "closed",
    "created_at": "2024-12-02T15:29:09+00:00",
    "closed_at": "2024-12-03T23:29:39+00:00",
    "updated_at": "2024-12-03T23:29:39+00:00",
    "author": "CC-KEH",
    "author_type": "User",
    "comments_count": 2,
    "reactions_count": 0,
    "labels": "ðŸ¤–:docs",
    "milestone": null,
    "closed_by": "efriis",
    "resolution_time_hours": 32.00833333333333,
    "first_comments": [
      {
        "author": "asdkfjsd",
        "created_at": "2024-12-03T08:09:00+00:00",
        "body": "Hello, I am interested in taking on this task. However, I have a couple of questions to ensure I understand the scope and requirements correctly.\r\n\r\nIs it sufficient to only modify this particular text and unify the thread_id ( \"abc567\" and \"abc678\" ) across the context?\r\nDo I need to check all similar pages across the documentation to address this issue, or is the change limited to a specific page or section?\r\n\r\nI am eager to assist with this task and would appreciate any guidance you can provide, thank you very much."
      },
      {
        "author": "efriis",
        "created_at": "2024-12-03T23:29:39+00:00",
        "body": "hey! just ran the existing code, and it's working as expected. if we use the same thread id, then it'll duplicate some of the messages, so the intention is actually to create a new thread (with the same conversation history in `messages`) "
      }
    ],
    "url": "https://github.com/langchain-ai/langchain/issues/28450"
  },
  {
    "repository": "langchain-ai/langchain",
    "issue_number": 28436,
    "title": "DOC: HuggingFaceEmbeddings use `FP16` by nested loop",
    "body": "### URL\r\n\r\nhttps://python.langchain.com/docs/integrations/text_embedding/bge_huggingface/\r\n\r\n### Checklist\r\n\r\n- [X] I added a very descriptive title to this issue.\r\n- [X] I included a link to the documentation page I am referring to (if applicable).\r\n\r\n### Issue with current documentation:\r\n```py\r\nmodel_name = \"BAAI/bge-small-en\"\r\nmodel_kwargs={\r\n      \"device\": device,\r\n      \"torch_dtype\": torch.float16\r\n  },\r\nencode_kwargs = {\"normalize_embeddings\": True}\r\nhf = HuggingFaceBgeEmbeddings(\r\n    model_name=model_name, model_kwargs=model_kwargs, encode_kwargs=encode_kwargs\r\n)\r\n```\r\n\r\ncause to error\r\n\r\n```\r\n_embeddings_instance = HuggingFaceBgeEmbeddings(\r\n    |   File \"/Users/tae/mambaforge/envs/api/lib/python3.9/site-packages/langchain_huggingface/embeddings/huggingface.py\", line 58, in __init__\r\n    |     self._client = sentence_transformers.SentenceTransformer(\r\n    | TypeError: __init__() got an unexpected keyword argument 'torch_dtype'\r\n```\r\n\r\n### Idea or request for content:\r\n\r\n_No response_",
    "state": "closed",
    "created_at": "2024-12-01T09:42:50+00:00",
    "closed_at": "2024-12-27T18:24:49+00:00",
    "updated_at": "2024-12-27T18:24:49+00:00",
    "author": "MoosaTae",
    "author_type": "User",
    "comments_count": 2,
    "reactions_count": 0,
    "labels": "ðŸ¤–:docs",
    "milestone": null,
    "closed_by": "MoosaTae",
    "resolution_time_hours": 632.6997222222222,
    "first_comments": [
      {
        "author": "MoosaTae",
        "created_at": "2024-12-01T09:43:28+00:00",
        "body": "```py\r\nmodel_name = \"BAAI/bge-small-en\"\r\nmodel_kwargs={\r\n      \"device\": device,\r\n      \"model_kwargs\": {\"torch_dtype\": torch.float16},\r\n  },\r\nencode_kwargs = {\"normalize_embeddings\": True}\r\nhf = HuggingFaceBgeEmbeddings(\r\n    model_name=model_name, model_kwargs=model_kwargs, encode_kwargs=encode_kwargs\r\n)\r\n```\r\n\r\n\r\nit work right now, `but` it need to parse to be `nested loop`, should it fix document or code?"
      },
      {
        "author": "keenborder786",
        "created_at": "2024-12-01T20:35:59+00:00",
        "body": "@MoosaTae I have created a PR which resolves this issue."
      }
    ],
    "url": "https://github.com/langchain-ai/langchain/issues/28436"
  },
  {
    "repository": "langchain-ai/langchain",
    "issue_number": 28412,
    "title": "`.with_structured_output` fails with nested Pydantic model and Llama3.1",
    "body": "### Checked other resources\n\n- [X] I added a very descriptive title to this issue.\n- [X] I searched the LangChain documentation with the integrated search.\n- [X] I used the GitHub search to find a similar question and didn't find it.\n- [X] I am sure that this is a bug in LangChain rather than my code.\n- [X] The bug is not resolved by updating to the latest stable version of LangChain (or the specific integration package).\n\n### Example Code\n\n```python\r\nfrom langchain_core.prompts import ChatPromptTemplate\r\nfrom langchain_ollama import ChatOllama\r\nfrom langgraph.graph import MessagesState, StateGraph\r\nfrom pydantic import BaseModel, Field\r\n\r\n\r\n# define player\r\nclass Player(BaseModel):\r\n    name: str = Field(description=\"The name of the player\")\r\n\r\n# define player list\r\nclass PlayerList(BaseModel):\r\n    players: list[Player] = Field(description=\"The list of players\")\r\n\r\n# define llm\r\nllm = ChatOllama(model=\"llama3.1\")\r\n\r\n# define state\r\nclass State(MessagesState):\r\n    num_players: int = Field(description=\"The number of players\")\r\n    players: list[Player] = Field(description=\"The players\")\r\n\r\n# define generate_players node\r\ndef generate_players(state: State) -> State:\r\n    num_players = state[\"num_players\"]\r\n    prompt = ChatPromptTemplate.from_messages(\r\n        [\r\n            (\"human\", \"Please generate {num_players} players with unique names.\"),\r\n        ]\r\n    )\r\n    prompt = prompt.invoke({\"num_players\": num_players})\r\n\r\n    players = llm.with_structured_output(PlayerList).invoke(prompt)\r\n\r\n    return {\"players\": players}\r\n\r\n# define graph\r\ngraph = StateGraph(State)\r\ngraph.add_node(\"generate_players\", generate_players)\r\ngraph.set_entry_point(\"generate_players\")\r\ngraph = graph.compile()\r\n\r\ngraph.invoke({\"num_players\": 1})\r\n```\n\n### Error Message and Stack Trace (if applicable)\n\n---------------------------------------------------------------------------\r\nValidationError                           Traceback (most recent call last)\r\nCell In[3], [line 43](vscode-notebook-cell:?execution_count=3&line=43)\r\n     [40](vscode-notebook-cell:?execution_count=3&line=40) graph.set_entry_point(\"generate_players\")\r\n     [41](vscode-notebook-cell:?execution_count=3&line=41) graph = graph.compile()\r\n---> [43](vscode-notebook-cell:?execution_count=3&line=43) graph.invoke({\"num_players\": 1})\r\n\r\nFile ~/App/python-env/py311/lib/python3.11/site-packages/langgraph/pregel/__init__.py:1927, in Pregel.invoke(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, **kwargs)\r\n   [1925](https://vscode-remote+ssh-002dremote-002byu-002dlerner.vscode-resource.vscode-cdn.net/home/yuzhu/synology/projects/Agents/MisInfoAgent/code/~/App/python-env/py311/lib/python3.11/site-packages/langgraph/pregel/__init__.py:1925) else:\r\n   [1926](https://vscode-remote+ssh-002dremote-002byu-002dlerner.vscode-resource.vscode-cdn.net/home/yuzhu/synology/projects/Agents/MisInfoAgent/code/~/App/python-env/py311/lib/python3.11/site-packages/langgraph/pregel/__init__.py:1926)     chunks = []\r\n-> [1927](https://vscode-remote+ssh-002dremote-002byu-002dlerner.vscode-resource.vscode-cdn.net/home/yuzhu/synology/projects/Agents/MisInfoAgent/code/~/App/python-env/py311/lib/python3.11/site-packages/langgraph/pregel/__init__.py:1927) for chunk in self.stream(\r\n   [1928](https://vscode-remote+ssh-002dremote-002byu-002dlerner.vscode-resource.vscode-cdn.net/home/yuzhu/synology/projects/Agents/MisInfoAgent/code/~/App/python-env/py311/lib/python3.11/site-packages/langgraph/pregel/__init__.py:1928)     input,\r\n   [1929](https://vscode-remote+ssh-002dremote-002byu-002dlerner.vscode-resource.vscode-cdn.net/home/yuzhu/synology/projects/Agents/MisInfoAgent/code/~/App/python-env/py311/lib/python3.11/site-packages/langgraph/pregel/__init__.py:1929)     config,\r\n   [1930](https://vscode-remote+ssh-002dremote-002byu-002dlerner.vscode-resource.vscode-cdn.net/home/yuzhu/synology/projects/Agents/MisInfoAgent/code/~/App/python-env/py311/lib/python3.11/site-packages/langgraph/pregel/__init__.py:1930)     stream_mode=stream_mode,\r\n   [1931](https://vscode-remote+ssh-002dremote-002byu-002dlerner.vscode-resource.vscode-cdn.net/home/yuzhu/synology/projects/Agents/MisInfoAgent/code/~/App/python-env/py311/lib/python3.11/site-packages/langgraph/pregel/__init__.py:1931)     output_keys=output_keys,\r\n   [1932](https://vscode-remote+ssh-002dremote-002byu-002dlerner.vscode-resource.vscode-cdn.net/home/yuzhu/synology/projects/Agents/MisInfoAgent/code/~/App/python-env/py311/lib/python3.11/site-packages/langgraph/pregel/__init__.py:1932)     interrupt_before=interrupt_before,\r\n   [1933](https://vscode-remote+ssh-002dremote-002byu-002dlerner.vscode-resource.vscode-cdn.net/home/yuzhu/synology/projects/Agents/MisInfoAgent/code/~/App/python-env/py311/lib/python3.11/site-packages/langgraph/pregel/__init__.py:1933)     interrupt_after=interrupt_after,\r\n   [1934](https://vscode-remote+ssh-002dremote-002byu-002dlerner.vscode-resource.vscode-cdn.net/home/yuzhu/synology/projects/Agents/MisInfoAgent/code/~/App/python-env/py311/lib/python3.11/site-packages/langgraph/pregel/__init__.py:1934)     debug=debug,\r\n   [1935](https://vscode-remote+ssh-002dremote-002byu-002dlerner.vscode-resource.vscode-cdn.net/home/yuzhu/synology/projects/Agents/MisInfoAgent/code/~/App/python-env/py311/lib/python3.11/site-packages/langgraph/pregel/__init__.py:1935)     **kwargs,\r\n   [1936](https://vscode-remote+ssh-002dremote-002byu-002dlerner.vscode-resource.vscode-cdn.net/home/yuzhu/synology/projects/Agents/MisInfoAgent/code/~/App/python-env/py311/lib/python3.11/site-packages/langgraph/pregel/__init__.py:1936) ):\r\n   [1937](https://vscode-remote+ssh-002dremote-002byu-002dlerner.vscode-resource.vscode-cdn.net/home/yuzhu/synology/projects/Agents/MisInfoAgent/code/~/App/python-env/py311/lib/python3.11/site-packages/langgraph/pregel/__init__.py:1937)     if stream_mode == \"values\":\r\n   [1938](https://vscode-remote+ssh-002dremote-002byu-002dlerner.vscode-resource.vscode-cdn.net/home/yuzhu/synology/projects/Agents/MisInfoAgent/code/~/App/python-env/py311/lib/python3.11/site-packages/langgraph/pregel/__init__.py:1938)         latest = chunk\r\n\r\nFile ~/App/python-env/py311/lib/python3.11/site-packages/langgraph/pregel/__init__.py:1647, in Pregel.stream(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, subgraphs)\r\n   [1641](https://vscode-remote+ssh-002dremote-002byu-002dlerner.vscode-resource.vscode-cdn.net/home/yuzhu/synology/projects/Agents/MisInfoAgent/code/~/App/python-env/py311/lib/python3.11/site-packages/langgraph/pregel/__init__.py:1641)     # Similarly to Bulk Synchronous Parallel / Pregel model\r\n   [1642](https://vscode-remote+ssh-002dremote-002byu-002dlerner.vscode-resource.vscode-cdn.net/home/yuzhu/synology/projects/Agents/MisInfoAgent/code/~/App/python-env/py311/lib/python3.11/site-packages/langgraph/pregel/__init__.py:1642)     # computation proceeds in steps, while there are channel updates\r\n   [1643](https://vscode-remote+ssh-002dremote-002byu-002dlerner.vscode-resource.vscode-cdn.net/home/yuzhu/synology/projects/Agents/MisInfoAgent/code/~/App/python-env/py311/lib/python3.11/site-packages/langgraph/pregel/__init__.py:1643)     # channel updates from step N are only visible in step N+1\r\n   [1644](https://vscode-remote+ssh-002dremote-002byu-002dlerner.vscode-resource.vscode-cdn.net/home/yuzhu/synology/projects/Agents/MisInfoAgent/code/~/App/python-env/py311/lib/python3.11/site-packages/langgraph/pregel/__init__.py:1644)     # channels are guaranteed to be immutable for the duration of the step,\r\n   [1645](https://vscode-remote+ssh-002dremote-002byu-002dlerner.vscode-resource.vscode-cdn.net/home/yuzhu/synology/projects/Agents/MisInfoAgent/code/~/App/python-env/py311/lib/python3.11/site-packages/langgraph/pregel/__init__.py:1645)     # with channel updates applied only at the transition between steps\r\n   [1646](https://vscode-remote+ssh-002dremote-002byu-002dlerner.vscode-resource.vscode-cdn.net/home/yuzhu/synology/projects/Agents/MisInfoAgent/code/~/App/python-env/py311/lib/python3.11/site-packages/langgraph/pregel/__init__.py:1646)     while loop.tick(input_keys=self.input_channels):\r\n-> [1647](https://vscode-remote+ssh-002dremote-002byu-002dlerner.vscode-resource.vscode-cdn.net/home/yuzhu/synology/projects/Agents/MisInfoAgent/code/~/App/python-env/py311/lib/python3.11/site-packages/langgraph/pregel/__init__.py:1647)         for _ in runner.tick(\r\n   [1648](https://vscode-remote+ssh-002dremote-002byu-002dlerner.vscode-resource.vscode-cdn.net/home/yuzhu/synology/projects/Agents/MisInfoAgent/code/~/App/python-env/py311/lib/python3.11/site-packages/langgraph/pregel/__init__.py:1648)             loop.tasks.values(),\r\n   [1649](https://vscode-remote+ssh-002dremote-002byu-002dlerner.vscode-resource.vscode-cdn.net/home/yuzhu/synology/projects/Agents/MisInfoAgent/code/~/App/python-env/py311/lib/python3.11/site-packages/langgraph/pregel/__init__.py:1649)             timeout=self.step_timeout,\r\n   [1650](https://vscode-remote+ssh-002dremote-002byu-002dlerner.vscode-resource.vscode-cdn.net/home/yuzhu/synology/projects/Agents/MisInfoAgent/code/~/App/python-env/py311/lib/python3.11/site-packages/langgraph/pregel/__init__.py:1650)             retry_policy=self.retry_policy,\r\n   [1651](https://vscode-remote+ssh-002dremote-002byu-002dlerner.vscode-resource.vscode-cdn.net/home/yuzhu/synology/projects/Agents/MisInfoAgent/code/~/App/python-env/py311/lib/python3.11/site-packages/langgraph/pregel/__init__.py:1651)             get_waiter=get_waiter,\r\n   [1652](https://vscode-remote+ssh-002dremote-002byu-002dlerner.vscode-resource.vscode-cdn.net/home/yuzhu/synology/projects/Agents/MisInfoAgent/code/~/App/python-env/py311/lib/python3.11/site-packages/langgraph/pregel/__init__.py:1652)         ):\r\n   [1653](https://vscode-remote+ssh-002dremote-002byu-002dlerner.vscode-resource.vscode-cdn.net/home/yuzhu/synology/projects/Agents/MisInfoAgent/code/~/App/python-env/py311/lib/python3.11/site-packages/langgraph/pregel/__init__.py:1653)             # emit output\r\n   [1654](https://vscode-remote+ssh-002dremote-002byu-002dlerner.vscode-resource.vscode-cdn.net/home/yuzhu/synology/projects/Agents/MisInfoAgent/code/~/App/python-env/py311/lib/python3.11/site-packages/langgraph/pregel/__init__.py:1654)             yield from output()\r\n   [1655](https://vscode-remote+ssh-002dremote-002byu-002dlerner.vscode-resource.vscode-cdn.net/home/yuzhu/synology/projects/Agents/MisInfoAgent/code/~/App/python-env/py311/lib/python3.11/site-packages/langgraph/pregel/__init__.py:1655) # emit output\r\n\r\nFile ~/App/python-env/py311/lib/python3.11/site-packages/langgraph/pregel/runner.py:104, in PregelRunner.tick(self, tasks, reraise, timeout, retry_policy, get_waiter)\r\n    [102](https://vscode-remote+ssh-002dremote-002byu-002dlerner.vscode-resource.vscode-cdn.net/home/yuzhu/synology/projects/Agents/MisInfoAgent/code/~/App/python-env/py311/lib/python3.11/site-packages/langgraph/pregel/runner.py:102) t = tasks[0]\r\n    [103](https://vscode-remote+ssh-002dremote-002byu-002dlerner.vscode-resource.vscode-cdn.net/home/yuzhu/synology/projects/Agents/MisInfoAgent/code/~/App/python-env/py311/lib/python3.11/site-packages/langgraph/pregel/runner.py:103) try:\r\n--> [104](https://vscode-remote+ssh-002dremote-002byu-002dlerner.vscode-resource.vscode-cdn.net/home/yuzhu/synology/projects/Agents/MisInfoAgent/code/~/App/python-env/py311/lib/python3.11/site-packages/langgraph/pregel/runner.py:104)     run_with_retry(t, retry_policy, writer=writer)\r\n    [105](https://vscode-remote+ssh-002dremote-002byu-002dlerner.vscode-resource.vscode-cdn.net/home/yuzhu/synology/projects/Agents/MisInfoAgent/code/~/App/python-env/py311/lib/python3.11/site-packages/langgraph/pregel/runner.py:105)     self.commit(t, None)\r\n    [106](https://vscode-remote+ssh-002dremote-002byu-002dlerner.vscode-resource.vscode-cdn.net/home/yuzhu/synology/projects/Agents/MisInfoAgent/code/~/App/python-env/py311/lib/python3.11/site-packages/langgraph/pregel/runner.py:106) except Exception as exc:\r\n\r\nFile ~/App/python-env/py311/lib/python3.11/site-packages/langgraph/pregel/retry.py:40, in run_with_retry(task, retry_policy, writer)\r\n     [38](https://vscode-remote+ssh-002dremote-002byu-002dlerner.vscode-resource.vscode-cdn.net/home/yuzhu/synology/projects/Agents/MisInfoAgent/code/~/App/python-env/py311/lib/python3.11/site-packages/langgraph/pregel/retry.py:38) task.writes.clear()\r\n     [39](https://vscode-remote+ssh-002dremote-002byu-002dlerner.vscode-resource.vscode-cdn.net/home/yuzhu/synology/projects/Agents/MisInfoAgent/code/~/App/python-env/py311/lib/python3.11/site-packages/langgraph/pregel/retry.py:39) # run the task\r\n---> [40](https://vscode-remote+ssh-002dremote-002byu-002dlerner.vscode-resource.vscode-cdn.net/home/yuzhu/synology/projects/Agents/MisInfoAgent/code/~/App/python-env/py311/lib/python3.11/site-packages/langgraph/pregel/retry.py:40) task.proc.invoke(task.input, config)\r\n     [41](https://vscode-remote+ssh-002dremote-002byu-002dlerner.vscode-resource.vscode-cdn.net/home/yuzhu/synology/projects/Agents/MisInfoAgent/code/~/App/python-env/py311/lib/python3.11/site-packages/langgraph/pregel/retry.py:41) # if successful, end\r\n     [42](https://vscode-remote+ssh-002dremote-002byu-002dlerner.vscode-resource.vscode-cdn.net/home/yuzhu/synology/projects/Agents/MisInfoAgent/code/~/App/python-env/py311/lib/python3.11/site-packages/langgraph/pregel/retry.py:42) break\r\n\r\nFile ~/App/python-env/py311/lib/python3.11/site-packages/langgraph/utils/runnable.py:410, in RunnableSeq.invoke(self, input, config, **kwargs)\r\n    [408](https://vscode-remote+ssh-002dremote-002byu-002dlerner.vscode-resource.vscode-cdn.net/home/yuzhu/synology/projects/Agents/MisInfoAgent/code/~/App/python-env/py311/lib/python3.11/site-packages/langgraph/utils/runnable.py:408) context.run(_set_config_context, config)\r\n    [409](https://vscode-remote+ssh-002dremote-002byu-002dlerner.vscode-resource.vscode-cdn.net/home/yuzhu/synology/projects/Agents/MisInfoAgent/code/~/App/python-env/py311/lib/python3.11/site-packages/langgraph/utils/runnable.py:409) if i == 0:\r\n--> [410](https://vscode-remote+ssh-002dremote-002byu-002dlerner.vscode-resource.vscode-cdn.net/home/yuzhu/synology/projects/Agents/MisInfoAgent/code/~/App/python-env/py311/lib/python3.11/site-packages/langgraph/utils/runnable.py:410)     input = context.run(step.invoke, input, config, **kwargs)\r\n    [411](https://vscode-remote+ssh-002dremote-002byu-002dlerner.vscode-resource.vscode-cdn.net/home/yuzhu/synology/projects/Agents/MisInfoAgent/code/~/App/python-env/py311/lib/python3.11/site-packages/langgraph/utils/runnable.py:411) else:\r\n    [412](https://vscode-remote+ssh-002dremote-002byu-002dlerner.vscode-resource.vscode-cdn.net/home/yuzhu/synology/projects/Agents/MisInfoAgent/code/~/App/python-env/py311/lib/python3.11/site-packages/langgraph/utils/runnable.py:412)     input = context.run(step.invoke, input, config)\r\n\r\nFile ~/App/python-env/py311/lib/python3.11/site-packages/langgraph/utils/runnable.py:184, in RunnableCallable.invoke(self, input, config, **kwargs)\r\n    [182](https://vscode-remote+ssh-002dremote-002byu-002dlerner.vscode-resource.vscode-cdn.net/home/yuzhu/synology/projects/Agents/MisInfoAgent/code/~/App/python-env/py311/lib/python3.11/site-packages/langgraph/utils/runnable.py:182) else:\r\n    [183](https://vscode-remote+ssh-002dremote-002byu-002dlerner.vscode-resource.vscode-cdn.net/home/yuzhu/synology/projects/Agents/MisInfoAgent/code/~/App/python-env/py311/lib/python3.11/site-packages/langgraph/utils/runnable.py:183)     context.run(_set_config_context, config)\r\n--> [184](https://vscode-remote+ssh-002dremote-002byu-002dlerner.vscode-resource.vscode-cdn.net/home/yuzhu/synology/projects/Agents/MisInfoAgent/code/~/App/python-env/py311/lib/python3.11/site-packages/langgraph/utils/runnable.py:184)     ret = context.run(self.func, input, **kwargs)\r\n    [185](https://vscode-remote+ssh-002dremote-002byu-002dlerner.vscode-resource.vscode-cdn.net/home/yuzhu/synology/projects/Agents/MisInfoAgent/code/~/App/python-env/py311/lib/python3.11/site-packages/langgraph/utils/runnable.py:185) if isinstance(ret, Runnable) and self.recurse:\r\n    [186](https://vscode-remote+ssh-002dremote-002byu-002dlerner.vscode-resource.vscode-cdn.net/home/yuzhu/synology/projects/Agents/MisInfoAgent/code/~/App/python-env/py311/lib/python3.11/site-packages/langgraph/utils/runnable.py:186)     return ret.invoke(input, config)\r\n\r\nCell In[3], [line 33](vscode-notebook-cell:?execution_count=3&line=33)\r\n     [26](vscode-notebook-cell:?execution_count=3&line=26) prompt = ChatPromptTemplate.from_messages(\r\n     [27](vscode-notebook-cell:?execution_count=3&line=27)     [\r\n     [28](vscode-notebook-cell:?execution_count=3&line=28)         (\"human\", \"Please generate {num_players} players with unique names.\"),\r\n     [29](vscode-notebook-cell:?execution_count=3&line=29)     ]\r\n     [30](vscode-notebook-cell:?execution_count=3&line=30) )\r\n     [31](vscode-notebook-cell:?execution_count=3&line=31) prompt = prompt.invoke({\"num_players\": num_players})\r\n---> [33](vscode-notebook-cell:?execution_count=3&line=33) players = llm.with_structured_output(PlayerList).invoke(prompt)\r\n     [35](vscode-notebook-cell:?execution_count=3&line=35) return {\"players\": players}\r\n\r\nFile ~/App/python-env/py311/lib/python3.11/site-packages/langchain_core/runnables/base.py:3024, in RunnableSequence.invoke(self, input, config, **kwargs)\r\n   [3022](https://vscode-remote+ssh-002dremote-002byu-002dlerner.vscode-resource.vscode-cdn.net/home/yuzhu/synology/projects/Agents/MisInfoAgent/code/~/App/python-env/py311/lib/python3.11/site-packages/langchain_core/runnables/base.py:3022)             input = context.run(step.invoke, input, config, **kwargs)\r\n   [3023](https://vscode-remote+ssh-002dremote-002byu-002dlerner.vscode-resource.vscode-cdn.net/home/yuzhu/synology/projects/Agents/MisInfoAgent/code/~/App/python-env/py311/lib/python3.11/site-packages/langchain_core/runnables/base.py:3023)         else:\r\n-> [3024](https://vscode-remote+ssh-002dremote-002byu-002dlerner.vscode-resource.vscode-cdn.net/home/yuzhu/synology/projects/Agents/MisInfoAgent/code/~/App/python-env/py311/lib/python3.11/site-packages/langchain_core/runnables/base.py:3024)             input = context.run(step.invoke, input, config)\r\n   [3025](https://vscode-remote+ssh-002dremote-002byu-002dlerner.vscode-resource.vscode-cdn.net/home/yuzhu/synology/projects/Agents/MisInfoAgent/code/~/App/python-env/py311/lib/python3.11/site-packages/langchain_core/runnables/base.py:3025) # finish the root run\r\n   [3026](https://vscode-remote+ssh-002dremote-002byu-002dlerner.vscode-resource.vscode-cdn.net/home/yuzhu/synology/projects/Agents/MisInfoAgent/code/~/App/python-env/py311/lib/python3.11/site-packages/langchain_core/runnables/base.py:3026) except BaseException as e:\r\n\r\nFile ~/App/python-env/py311/lib/python3.11/site-packages/langchain_core/output_parsers/base.py:193, in BaseOutputParser.invoke(self, input, config, **kwargs)\r\n    [186](https://vscode-remote+ssh-002dremote-002byu-002dlerner.vscode-resource.vscode-cdn.net/home/yuzhu/synology/projects/Agents/MisInfoAgent/code/~/App/python-env/py311/lib/python3.11/site-packages/langchain_core/output_parsers/base.py:186) def invoke(\r\n    [187](https://vscode-remote+ssh-002dremote-002byu-002dlerner.vscode-resource.vscode-cdn.net/home/yuzhu/synology/projects/Agents/MisInfoAgent/code/~/App/python-env/py311/lib/python3.11/site-packages/langchain_core/output_parsers/base.py:187)     self,\r\n    [188](https://vscode-remote+ssh-002dremote-002byu-002dlerner.vscode-resource.vscode-cdn.net/home/yuzhu/synology/projects/Agents/MisInfoAgent/code/~/App/python-env/py311/lib/python3.11/site-packages/langchain_core/output_parsers/base.py:188)     input: Union[str, BaseMessage],\r\n    [189](https://vscode-remote+ssh-002dremote-002byu-002dlerner.vscode-resource.vscode-cdn.net/home/yuzhu/synology/projects/Agents/MisInfoAgent/code/~/App/python-env/py311/lib/python3.11/site-packages/langchain_core/output_parsers/base.py:189)     config: Optional[RunnableConfig] = None,\r\n    [190](https://vscode-remote+ssh-002dremote-002byu-002dlerner.vscode-resource.vscode-cdn.net/home/yuzhu/synology/projects/Agents/MisInfoAgent/code/~/App/python-env/py311/lib/python3.11/site-packages/langchain_core/output_parsers/base.py:190)     **kwargs: Any,\r\n    [191](https://vscode-remote+ssh-002dremote-002byu-002dlerner.vscode-resource.vscode-cdn.net/home/yuzhu/synology/projects/Agents/MisInfoAgent/code/~/App/python-env/py311/lib/python3.11/site-packages/langchain_core/output_parsers/base.py:191) ) -> T:\r\n    [192](https://vscode-remote+ssh-002dremote-002byu-002dlerner.vscode-resource.vscode-cdn.net/home/yuzhu/synology/projects/Agents/MisInfoAgent/code/~/App/python-env/py311/lib/python3.11/site-packages/langchain_core/output_parsers/base.py:192)     if isinstance(input, BaseMessage):\r\n--> [193](https://vscode-remote+ssh-002dremote-002byu-002dlerner.vscode-resource.vscode-cdn.net/home/yuzhu/synology/projects/Agents/MisInfoAgent/code/~/App/python-env/py311/lib/python3.11/site-packages/langchain_core/output_parsers/base.py:193)         return self._call_with_config(\r\n    [194](https://vscode-remote+ssh-002dremote-002byu-002dlerner.vscode-resource.vscode-cdn.net/home/yuzhu/synology/projects/Agents/MisInfoAgent/code/~/App/python-env/py311/lib/python3.11/site-packages/langchain_core/output_parsers/base.py:194)             lambda inner_input: self.parse_result(\r\n    [195](https://vscode-remote+ssh-002dremote-002byu-002dlerner.vscode-resource.vscode-cdn.net/home/yuzhu/synology/projects/Agents/MisInfoAgent/code/~/App/python-env/py311/lib/python3.11/site-packages/langchain_core/output_parsers/base.py:195)                 [ChatGeneration(message=inner_input)]\r\n    [196](https://vscode-remote+ssh-002dremote-002byu-002dlerner.vscode-resource.vscode-cdn.net/home/yuzhu/synology/projects/Agents/MisInfoAgent/code/~/App/python-env/py311/lib/python3.11/site-packages/langchain_core/output_parsers/base.py:196)             ),\r\n    [197](https://vscode-remote+ssh-002dremote-002byu-002dlerner.vscode-resource.vscode-cdn.net/home/yuzhu/synology/projects/Agents/MisInfoAgent/code/~/App/python-env/py311/lib/python3.11/site-packages/langchain_core/output_parsers/base.py:197)             input,\r\n    [198](https://vscode-remote+ssh-002dremote-002byu-002dlerner.vscode-resource.vscode-cdn.net/home/yuzhu/synology/projects/Agents/MisInfoAgent/code/~/App/python-env/py311/lib/python3.11/site-packages/langchain_core/output_parsers/base.py:198)             config,\r\n    [199](https://vscode-remote+ssh-002dremote-002byu-002dlerner.vscode-resource.vscode-cdn.net/home/yuzhu/synology/projects/Agents/MisInfoAgent/code/~/App/python-env/py311/lib/python3.11/site-packages/langchain_core/output_parsers/base.py:199)             run_type=\"parser\",\r\n    [200](https://vscode-remote+ssh-002dremote-002byu-002dlerner.vscode-resource.vscode-cdn.net/home/yuzhu/synology/projects/Agents/MisInfoAgent/code/~/App/python-env/py311/lib/python3.11/site-packages/langchain_core/output_parsers/base.py:200)         )\r\n    [201](https://vscode-remote+ssh-002dremote-002byu-002dlerner.vscode-resource.vscode-cdn.net/home/yuzhu/synology/projects/Agents/MisInfoAgent/code/~/App/python-env/py311/lib/python3.11/site-packages/langchain_core/output_parsers/base.py:201)     else:\r\n    [202](https://vscode-remote+ssh-002dremote-002byu-002dlerner.vscode-resource.vscode-cdn.net/home/yuzhu/synology/projects/Agents/MisInfoAgent/code/~/App/python-env/py311/lib/python3.11/site-packages/langchain_core/output_parsers/base.py:202)         return self._call_with_config(\r\n    [203](https://vscode-remote+ssh-002dremote-002byu-002dlerner.vscode-resource.vscode-cdn.net/home/yuzhu/synology/projects/Agents/MisInfoAgent/code/~/App/python-env/py311/lib/python3.11/site-packages/langchain_core/output_parsers/base.py:203)             lambda inner_input: self.parse_result([Generation(text=inner_input)]),\r\n    [204](https://vscode-remote+ssh-002dremote-002byu-002dlerner.vscode-resource.vscode-cdn.net/home/yuzhu/synology/projects/Agents/MisInfoAgent/code/~/App/python-env/py311/lib/python3.11/site-packages/langchain_core/output_parsers/base.py:204)             input,\r\n    [205](https://vscode-remote+ssh-002dremote-002byu-002dlerner.vscode-resource.vscode-cdn.net/home/yuzhu/synology/projects/Agents/MisInfoAgent/code/~/App/python-env/py311/lib/python3.11/site-packages/langchain_core/output_parsers/base.py:205)             config,\r\n    [206](https://vscode-remote+ssh-002dremote-002byu-002dlerner.vscode-resource.vscode-cdn.net/home/yuzhu/synology/projects/Agents/MisInfoAgent/code/~/App/python-env/py311/lib/python3.11/site-packages/langchain_core/output_parsers/base.py:206)             run_type=\"parser\",\r\n    [207](https://vscode-remote+ssh-002dremote-002byu-002dlerner.vscode-resource.vscode-cdn.net/home/yuzhu/synology/projects/Agents/MisInfoAgent/code/~/App/python-env/py311/lib/python3.11/site-packages/langchain_core/output_parsers/base.py:207)         )\r\n\r\nFile ~/App/python-env/py311/lib/python3.11/site-packages/langchain_core/runnables/base.py:1927, in Runnable._call_with_config(self, func, input, config, run_type, serialized, **kwargs)\r\n   [1923](https://vscode-remote+ssh-002dremote-002byu-002dlerner.vscode-resource.vscode-cdn.net/home/yuzhu/synology/projects/Agents/MisInfoAgent/code/~/App/python-env/py311/lib/python3.11/site-packages/langchain_core/runnables/base.py:1923)     context = copy_context()\r\n   [1924](https://vscode-remote+ssh-002dremote-002byu-002dlerner.vscode-resource.vscode-cdn.net/home/yuzhu/synology/projects/Agents/MisInfoAgent/code/~/App/python-env/py311/lib/python3.11/site-packages/langchain_core/runnables/base.py:1924)     context.run(_set_config_context, child_config)\r\n   [1925](https://vscode-remote+ssh-002dremote-002byu-002dlerner.vscode-resource.vscode-cdn.net/home/yuzhu/synology/projects/Agents/MisInfoAgent/code/~/App/python-env/py311/lib/python3.11/site-packages/langchain_core/runnables/base.py:1925)     output = cast(\r\n   [1926](https://vscode-remote+ssh-002dremote-002byu-002dlerner.vscode-resource.vscode-cdn.net/home/yuzhu/synology/projects/Agents/MisInfoAgent/code/~/App/python-env/py311/lib/python3.11/site-packages/langchain_core/runnables/base.py:1926)         Output,\r\n-> [1927](https://vscode-remote+ssh-002dremote-002byu-002dlerner.vscode-resource.vscode-cdn.net/home/yuzhu/synology/projects/Agents/MisInfoAgent/code/~/App/python-env/py311/lib/python3.11/site-packages/langchain_core/runnables/base.py:1927)         context.run(\r\n   [1928](https://vscode-remote+ssh-002dremote-002byu-002dlerner.vscode-resource.vscode-cdn.net/home/yuzhu/synology/projects/Agents/MisInfoAgent/code/~/App/python-env/py311/lib/python3.11/site-packages/langchain_core/runnables/base.py:1928)             call_func_with_variable_args,  # type: ignore[arg-type]\r\n   [1929](https://vscode-remote+ssh-002dremote-002byu-002dlerner.vscode-resource.vscode-cdn.net/home/yuzhu/synology/projects/Agents/MisInfoAgent/code/~/App/python-env/py311/lib/python3.11/site-packages/langchain_core/runnables/base.py:1929)             func,  # type: ignore[arg-type]\r\n   [1930](https://vscode-remote+ssh-002dremote-002byu-002dlerner.vscode-resource.vscode-cdn.net/home/yuzhu/synology/projects/Agents/MisInfoAgent/code/~/App/python-env/py311/lib/python3.11/site-packages/langchain_core/runnables/base.py:1930)             input,  # type: ignore[arg-type]\r\n   [1931](https://vscode-remote+ssh-002dremote-002byu-002dlerner.vscode-resource.vscode-cdn.net/home/yuzhu/synology/projects/Agents/MisInfoAgent/code/~/App/python-env/py311/lib/python3.11/site-packages/langchain_core/runnables/base.py:1931)             config,\r\n   [1932](https://vscode-remote+ssh-002dremote-002byu-002dlerner.vscode-resource.vscode-cdn.net/home/yuzhu/synology/projects/Agents/MisInfoAgent/code/~/App/python-env/py311/lib/python3.11/site-packages/langchain_core/runnables/base.py:1932)             run_manager,\r\n   [1933](https://vscode-remote+ssh-002dremote-002byu-002dlerner.vscode-resource.vscode-cdn.net/home/yuzhu/synology/projects/Agents/MisInfoAgent/code/~/App/python-env/py311/lib/python3.11/site-packages/langchain_core/runnables/base.py:1933)             **kwargs,\r\n   [1934](https://vscode-remote+ssh-002dremote-002byu-002dlerner.vscode-resource.vscode-cdn.net/home/yuzhu/synology/projects/Agents/MisInfoAgent/code/~/App/python-env/py311/lib/python3.11/site-packages/langchain_core/runnables/base.py:1934)         ),\r\n   [1935](https://vscode-remote+ssh-002dremote-002byu-002dlerner.vscode-resource.vscode-cdn.net/home/yuzhu/synology/projects/Agents/MisInfoAgent/code/~/App/python-env/py311/lib/python3.11/site-packages/langchain_core/runnables/base.py:1935)     )\r\n   [1936](https://vscode-remote+ssh-002dremote-002byu-002dlerner.vscode-resource.vscode-cdn.net/home/yuzhu/synology/projects/Agents/MisInfoAgent/code/~/App/python-env/py311/lib/python3.11/site-packages/langchain_core/runnables/base.py:1936) except BaseException as e:\r\n   [1937](https://vscode-remote+ssh-002dremote-002byu-002dlerner.vscode-resource.vscode-cdn.net/home/yuzhu/synology/projects/Agents/MisInfoAgent/code/~/App/python-env/py311/lib/python3.11/site-packages/langchain_core/runnables/base.py:1937)     run_manager.on_chain_error(e)\r\n\r\nFile ~/App/python-env/py311/lib/python3.11/site-packages/langchain_core/runnables/config.py:396, in call_func_with_variable_args(func, input, config, run_manager, **kwargs)\r\n    [394](https://vscode-remote+ssh-002dremote-002byu-002dlerner.vscode-resource.vscode-cdn.net/home/yuzhu/synology/projects/Agents/MisInfoAgent/code/~/App/python-env/py311/lib/python3.11/site-packages/langchain_core/runnables/config.py:394) if run_manager is not None and accepts_run_manager(func):\r\n    [395](https://vscode-remote+ssh-002dremote-002byu-002dlerner.vscode-resource.vscode-cdn.net/home/yuzhu/synology/projects/Agents/MisInfoAgent/code/~/App/python-env/py311/lib/python3.11/site-packages/langchain_core/runnables/config.py:395)     kwargs[\"run_manager\"] = run_manager\r\n--> [396](https://vscode-remote+ssh-002dremote-002byu-002dlerner.vscode-resource.vscode-cdn.net/home/yuzhu/synology/projects/Agents/MisInfoAgent/code/~/App/python-env/py311/lib/python3.11/site-packages/langchain_core/runnables/config.py:396) return func(input, **kwargs)\r\n\r\nFile ~/App/python-env/py311/lib/python3.11/site-packages/langchain_core/output_parsers/base.py:194, in BaseOutputParser.invoke.<locals>.<lambda>(inner_input)\r\n    [186](https://vscode-remote+ssh-002dremote-002byu-002dlerner.vscode-resource.vscode-cdn.net/home/yuzhu/synology/projects/Agents/MisInfoAgent/code/~/App/python-env/py311/lib/python3.11/site-packages/langchain_core/output_parsers/base.py:186) def invoke(\r\n    [187](https://vscode-remote+ssh-002dremote-002byu-002dlerner.vscode-resource.vscode-cdn.net/home/yuzhu/synology/projects/Agents/MisInfoAgent/code/~/App/python-env/py311/lib/python3.11/site-packages/langchain_core/output_parsers/base.py:187)     self,\r\n    [188](https://vscode-remote+ssh-002dremote-002byu-002dlerner.vscode-resource.vscode-cdn.net/home/yuzhu/synology/projects/Agents/MisInfoAgent/code/~/App/python-env/py311/lib/python3.11/site-packages/langchain_core/output_parsers/base.py:188)     input: Union[str, BaseMessage],\r\n    [189](https://vscode-remote+ssh-002dremote-002byu-002dlerner.vscode-resource.vscode-cdn.net/home/yuzhu/synology/projects/Agents/MisInfoAgent/code/~/App/python-env/py311/lib/python3.11/site-packages/langchain_core/output_parsers/base.py:189)     config: Optional[RunnableConfig] = None,\r\n    [190](https://vscode-remote+ssh-002dremote-002byu-002dlerner.vscode-resource.vscode-cdn.net/home/yuzhu/synology/projects/Agents/MisInfoAgent/code/~/App/python-env/py311/lib/python3.11/site-packages/langchain_core/output_parsers/base.py:190)     **kwargs: Any,\r\n    [191](https://vscode-remote+ssh-002dremote-002byu-002dlerner.vscode-resource.vscode-cdn.net/home/yuzhu/synology/projects/Agents/MisInfoAgent/code/~/App/python-env/py311/lib/python3.11/site-packages/langchain_core/output_parsers/base.py:191) ) -> T:\r\n    [192](https://vscode-remote+ssh-002dremote-002byu-002dlerner.vscode-resource.vscode-cdn.net/home/yuzhu/synology/projects/Agents/MisInfoAgent/code/~/App/python-env/py311/lib/python3.11/site-packages/langchain_core/output_parsers/base.py:192)     if isinstance(input, BaseMessage):\r\n    [193](https://vscode-remote+ssh-002dremote-002byu-002dlerner.vscode-resource.vscode-cdn.net/home/yuzhu/synology/projects/Agents/MisInfoAgent/code/~/App/python-env/py311/lib/python3.11/site-packages/langchain_core/output_parsers/base.py:193)         return self._call_with_config(\r\n--> [194](https://vscode-remote+ssh-002dremote-002byu-002dlerner.vscode-resource.vscode-cdn.net/home/yuzhu/synology/projects/Agents/MisInfoAgent/code/~/App/python-env/py311/lib/python3.11/site-packages/langchain_core/output_parsers/base.py:194)             lambda inner_input: self.parse_result(\r\n    [195](https://vscode-remote+ssh-002dremote-002byu-002dlerner.vscode-resource.vscode-cdn.net/home/yuzhu/synology/projects/Agents/MisInfoAgent/code/~/App/python-env/py311/lib/python3.11/site-packages/langchain_core/output_parsers/base.py:195)                 [ChatGeneration(message=inner_input)]\r\n    [196](https://vscode-remote+ssh-002dremote-002byu-002dlerner.vscode-resource.vscode-cdn.net/home/yuzhu/synology/projects/Agents/MisInfoAgent/code/~/App/python-env/py311/lib/python3.11/site-packages/langchain_core/output_parsers/base.py:196)             ),\r\n    [197](https://vscode-remote+ssh-002dremote-002byu-002dlerner.vscode-resource.vscode-cdn.net/home/yuzhu/synology/projects/Agents/MisInfoAgent/code/~/App/python-env/py311/lib/python3.11/site-packages/langchain_core/output_parsers/base.py:197)             input,\r\n    [198](https://vscode-remote+ssh-002dremote-002byu-002dlerner.vscode-resource.vscode-cdn.net/home/yuzhu/synology/projects/Agents/MisInfoAgent/code/~/App/python-env/py311/lib/python3.11/site-packages/langchain_core/output_parsers/base.py:198)             config,\r\n    [199](https://vscode-remote+ssh-002dremote-002byu-002dlerner.vscode-resource.vscode-cdn.net/home/yuzhu/synology/projects/Agents/MisInfoAgent/code/~/App/python-env/py311/lib/python3.11/site-packages/langchain_core/output_parsers/base.py:199)             run_type=\"parser\",\r\n    [200](https://vscode-remote+ssh-002dremote-002byu-002dlerner.vscode-resource.vscode-cdn.net/home/yuzhu/synology/projects/Agents/MisInfoAgent/code/~/App/python-env/py311/lib/python3.11/site-packages/langchain_core/output_parsers/base.py:200)         )\r\n    [201](https://vscode-remote+ssh-002dremote-002byu-002dlerner.vscode-resource.vscode-cdn.net/home/yuzhu/synology/projects/Agents/MisInfoAgent/code/~/App/python-env/py311/lib/python3.11/site-packages/langchain_core/output_parsers/base.py:201)     else:\r\n    [202](https://vscode-remote+ssh-002dremote-002byu-002dlerner.vscode-resource.vscode-cdn.net/home/yuzhu/synology/projects/Agents/MisInfoAgent/code/~/App/python-env/py311/lib/python3.11/site-packages/langchain_core/output_parsers/base.py:202)         return self._call_with_config(\r\n    [203](https://vscode-remote+ssh-002dremote-002byu-002dlerner.vscode-resource.vscode-cdn.net/home/yuzhu/synology/projects/Agents/MisInfoAgent/code/~/App/python-env/py311/lib/python3.11/site-packages/langchain_core/output_parsers/base.py:203)             lambda inner_input: self.parse_result([Generation(text=inner_input)]),\r\n    [204](https://vscode-remote+ssh-002dremote-002byu-002dlerner.vscode-resource.vscode-cdn.net/home/yuzhu/synology/projects/Agents/MisInfoAgent/code/~/App/python-env/py311/lib/python3.11/site-packages/langchain_core/output_parsers/base.py:204)             input,\r\n    [205](https://vscode-remote+ssh-002dremote-002byu-002dlerner.vscode-resource.vscode-cdn.net/home/yuzhu/synology/projects/Agents/MisInfoAgent/code/~/App/python-env/py311/lib/python3.11/site-packages/langchain_core/output_parsers/base.py:205)             config,\r\n    [206](https://vscode-remote+ssh-002dremote-002byu-002dlerner.vscode-resource.vscode-cdn.net/home/yuzhu/synology/projects/Agents/MisInfoAgent/code/~/App/python-env/py311/lib/python3.11/site-packages/langchain_core/output_parsers/base.py:206)             run_type=\"parser\",\r\n    [207](https://vscode-remote+ssh-002dremote-002byu-002dlerner.vscode-resource.vscode-cdn.net/home/yuzhu/synology/projects/Agents/MisInfoAgent/code/~/App/python-env/py311/lib/python3.11/site-packages/langchain_core/output_parsers/base.py:207)         )\r\n\r\nFile ~/App/python-env/py311/lib/python3.11/site-packages/langchain_core/output_parsers/openai_tools.py:298, in PydanticToolsParser.parse_result(self, result, partial)\r\n    [296](https://vscode-remote+ssh-002dremote-002byu-002dlerner.vscode-resource.vscode-cdn.net/home/yuzhu/synology/projects/Agents/MisInfoAgent/code/~/App/python-env/py311/lib/python3.11/site-packages/langchain_core/output_parsers/openai_tools.py:296)             continue\r\n    [297](https://vscode-remote+ssh-002dremote-002byu-002dlerner.vscode-resource.vscode-cdn.net/home/yuzhu/synology/projects/Agents/MisInfoAgent/code/~/App/python-env/py311/lib/python3.11/site-packages/langchain_core/output_parsers/openai_tools.py:297)         else:\r\n--> [298](https://vscode-remote+ssh-002dremote-002byu-002dlerner.vscode-resource.vscode-cdn.net/home/yuzhu/synology/projects/Agents/MisInfoAgent/code/~/App/python-env/py311/lib/python3.11/site-packages/langchain_core/output_parsers/openai_tools.py:298)             raise e\r\n    [299](https://vscode-remote+ssh-002dremote-002byu-002dlerner.vscode-resource.vscode-cdn.net/home/yuzhu/synology/projects/Agents/MisInfoAgent/code/~/App/python-env/py311/lib/python3.11/site-packages/langchain_core/output_parsers/openai_tools.py:299) if self.first_tool_only:\r\n    [300](https://vscode-remote+ssh-002dremote-002byu-002dlerner.vscode-resource.vscode-cdn.net/home/yuzhu/synology/projects/Agents/MisInfoAgent/code/~/App/python-env/py311/lib/python3.11/site-packages/langchain_core/output_parsers/openai_tools.py:300)     return pydantic_objects[0] if pydantic_objects else None\r\n\r\nFile ~/App/python-env/py311/lib/python3.11/site-packages/langchain_core/output_parsers/openai_tools.py:293, in PydanticToolsParser.parse_result(self, result, partial)\r\n    [288](https://vscode-remote+ssh-002dremote-002byu-002dlerner.vscode-resource.vscode-cdn.net/home/yuzhu/synology/projects/Agents/MisInfoAgent/code/~/App/python-env/py311/lib/python3.11/site-packages/langchain_core/output_parsers/openai_tools.py:288)         msg = (\r\n    [289](https://vscode-remote+ssh-002dremote-002byu-002dlerner.vscode-resource.vscode-cdn.net/home/yuzhu/synology/projects/Agents/MisInfoAgent/code/~/App/python-env/py311/lib/python3.11/site-packages/langchain_core/output_parsers/openai_tools.py:289)             f\"Tool arguments must be specified as a dict, received: \"\r\n    [290](https://vscode-remote+ssh-002dremote-002byu-002dlerner.vscode-resource.vscode-cdn.net/home/yuzhu/synology/projects/Agents/MisInfoAgent/code/~/App/python-env/py311/lib/python3.11/site-packages/langchain_core/output_parsers/openai_tools.py:290)             f\"{res['args']}\"\r\n    [291](https://vscode-remote+ssh-002dremote-002byu-002dlerner.vscode-resource.vscode-cdn.net/home/yuzhu/synology/projects/Agents/MisInfoAgent/code/~/App/python-env/py311/lib/python3.11/site-packages/langchain_core/output_parsers/openai_tools.py:291)         )\r\n    [292](https://vscode-remote+ssh-002dremote-002byu-002dlerner.vscode-resource.vscode-cdn.net/home/yuzhu/synology/projects/Agents/MisInfoAgent/code/~/App/python-env/py311/lib/python3.11/site-packages/langchain_core/output_parsers/openai_tools.py:292)         raise ValueError(msg)\r\n--> [293](https://vscode-remote+ssh-002dremote-002byu-002dlerner.vscode-resource.vscode-cdn.net/home/yuzhu/synology/projects/Agents/MisInfoAgent/code/~/App/python-env/py311/lib/python3.11/site-packages/langchain_core/output_parsers/openai_tools.py:293)     pydantic_objects.append(name_dict[res[\"type\"]](**res[\"args\"]))\r\n    [294](https://vscode-remote+ssh-002dremote-002byu-002dlerner.vscode-resource.vscode-cdn.net/home/yuzhu/synology/projects/Agents/MisInfoAgent/code/~/App/python-env/py311/lib/python3.11/site-packages/langchain_core/output_parsers/openai_tools.py:294) except (ValidationError, ValueError) as e:\r\n    [295](https://vscode-remote+ssh-002dremote-002byu-002dlerner.vscode-resource.vscode-cdn.net/home/yuzhu/synology/projects/Agents/MisInfoAgent/code/~/App/python-env/py311/lib/python3.11/site-packages/langchain_core/output_parsers/openai_tools.py:295)     if partial:\r\n\r\nFile ~/App/python-env/py311/lib/python3.11/site-packages/pydantic/main.py:214, in BaseModel.__init__(self, **data)\r\n    [212](https://vscode-remote+ssh-002dremote-002byu-002dlerner.vscode-resource.vscode-cdn.net/home/yuzhu/synology/projects/Agents/MisInfoAgent/code/~/App/python-env/py311/lib/python3.11/site-packages/pydantic/main.py:212) # `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\r\n    [213](https://vscode-remote+ssh-002dremote-002byu-002dlerner.vscode-resource.vscode-cdn.net/home/yuzhu/synology/projects/Agents/MisInfoAgent/code/~/App/python-env/py311/lib/python3.11/site-packages/pydantic/main.py:213) __tracebackhide__ = True\r\n--> [214](https://vscode-remote+ssh-002dremote-002byu-002dlerner.vscode-resource.vscode-cdn.net/home/yuzhu/synology/projects/Agents/MisInfoAgent/code/~/App/python-env/py311/lib/python3.11/site-packages/pydantic/main.py:214) validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)\r\n    [215](https://vscode-remote+ssh-002dremote-002byu-002dlerner.vscode-resource.vscode-cdn.net/home/yuzhu/synology/projects/Agents/MisInfoAgent/code/~/App/python-env/py311/lib/python3.11/site-packages/pydantic/main.py:215) if self is not validated_self:\r\n    [216](https://vscode-remote+ssh-002dremote-002byu-002dlerner.vscode-resource.vscode-cdn.net/home/yuzhu/synology/projects/Agents/MisInfoAgent/code/~/App/python-env/py311/lib/python3.11/site-packages/pydantic/main.py:216)     warnings.warn(\r\n    [217](https://vscode-remote+ssh-002dremote-002byu-002dlerner.vscode-resource.vscode-cdn.net/home/yuzhu/synology/projects/Agents/MisInfoAgent/code/~/App/python-env/py311/lib/python3.11/site-packages/pydantic/main.py:217)         'A custom validator is returning a value other than `self`.\\n'\r\n    [218](https://vscode-remote+ssh-002dremote-002byu-002dlerner.vscode-resource.vscode-cdn.net/home/yuzhu/synology/projects/Agents/MisInfoAgent/code/~/App/python-env/py311/lib/python3.11/site-packages/pydantic/main.py:218)         \"Returning anything other than `self` from a top level model validator isn't supported when validating via `__init__`.\\n\"\r\n    [219](https://vscode-remote+ssh-002dremote-002byu-002dlerner.vscode-resource.vscode-cdn.net/home/yuzhu/synology/projects/Agents/MisInfoAgent/code/~/App/python-env/py311/lib/python3.11/site-packages/pydantic/main.py:219)         'See the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.',\r\n    [220](https://vscode-remote+ssh-002dremote-002byu-002dlerner.vscode-resource.vscode-cdn.net/home/yuzhu/synology/projects/Agents/MisInfoAgent/code/~/App/python-env/py311/lib/python3.11/site-packages/pydantic/main.py:220)         stacklevel=2,\r\n    [221](https://vscode-remote+ssh-002dremote-002byu-002dlerner.vscode-resource.vscode-cdn.net/home/yuzhu/synology/projects/Agents/MisInfoAgent/code/~/App/python-env/py311/lib/python3.11/site-packages/pydantic/main.py:221)     )\r\n\r\nValidationError: 1 validation error for PlayerList\r\nplayers\r\n  Input should be a valid list [type=list_type, input_value='[{\"player_name\":\"John Doe\"}]', input_type=str]\r\n    For further information visit https://errors.pydantic.dev/2.10/v/list_type\n\n### Description\n\nThe expected output format should be a `PlayerList` pydantic model, however, it seems that llama3.1 fails to parse it.\r\n\r\nInterestingly, if I use ChatGPT, i.e., `llm = ChatOpenAI(model=\"gpt-4o-mini\")`, then the result is successfully parsed.\n\n### System Info\n\nSystem Information\r\n------------------\r\n> OS:  Linux\r\n> OS Version:  #20-Ubuntu SMP PREEMPT_DYNAMIC Mon Mar 18 17:38:55 UTC 2024\r\n> Python Version:  3.11.10 (main, Sep  7 2024, 18:35:41) [GCC 13.2.0]\r\n\r\nPackage Information\r\n-------------------\r\n> langchain_core: 0.3.19\r\n> langchain: 0.3.7\r\n> langchain_community: 0.3.7\r\n> langsmith: 0.1.144\r\n> langchain_anthropic: 0.3.0\r\n> langchain_experimental: 0.3.3\r\n> langchain_ollama: 0.2.0\r\n> langchain_openai: 0.2.9\r\n> langchain_text_splitters: 0.3.2\r\n> langgraph: 0.2.53\r\n\r\nOptional packages not installed\r\n-------------------------------\r\n> langserve\r\n\r\nOther Dependencies\r\n------------------\r\n> aiohttp: 3.10.10\r\n> anthropic: 0.40.0\r\n> async-timeout: Installed. No version info available.\r\n> dataclasses-json: 0.6.7\r\n> defusedxml: 0.7.1\r\n> httpx: 0.27.2\r\n> httpx-sse: 0.4.0\r\n> jsonpatch: 1.33\r\n> langgraph-checkpoint: 2.0.5\r\n> langgraph-sdk: 0.1.36\r\n> numpy: 1.26.3\r\n> ollama: 0.3.3\r\n> openai: 1.55.0\r\n> orjson: 3.10.11\r\n> packaging: 24.1\r\n> pydantic: 2.10.0\r\n> pydantic-settings: 2.6.1\r\n> PyYAML: 6.0.2\r\n> requests: 2.32.3\r\n> requests-toolbelt: 1.0.0\r\n> SQLAlchemy: 2.0.35\r\n> tenacity: 9.0.0\r\n> tiktoken: 0.8.0\r\n> typing-extensions: 4.12.2",
    "state": "closed",
    "created_at": "2024-11-29T04:07:29+00:00",
    "closed_at": "2024-12-05T15:56:06+00:00",
    "updated_at": "2025-01-28T14:31:26+00:00",
    "author": "XiaomoWu",
    "author_type": "User",
    "comments_count": 7,
    "reactions_count": 0,
    "labels": "ðŸ¤–:bug",
    "milestone": null,
    "closed_by": "XiaomoWu",
    "resolution_time_hours": 155.81027777777777,
    "first_comments": [
      {
        "author": "jooray",
        "created_at": "2024-11-29T10:01:52+00:00",
        "body": "Try using outputfixingparser for added robustness.\n\n"
      },
      {
        "author": "keenborder786",
        "created_at": "2024-11-30T18:22:31+00:00",
        "body": "@XiaomoWu Make sure you are running `llama3.1` on machine which have enough RAM. Do you have a machine which have more than 8GB of Ram? Can you please post your Machine Specification."
      },
      {
        "author": "XiaomoWu",
        "created_at": "2024-12-03T03:37:48+00:00",
        "body": "> @XiaomoWu Make sure you are running `llama3.1` on machine which have enough RAM. Do you have a machine which have more than 8GB of Ram? Can you please post your Machine Specification.\r\n\r\nThis is my machine specs:\r\n\r\nOS: Ubuntu 24.04\r\nGPU: Nvidia RTX 6000 Ada 48 GB\r\nCUDA: 12.1\r\nRAM: 256 GB\r\n\r\nI believe my machine should have more than enough RAM and VRAM to run llama3.1. Any machine that can run Ollama+llama3.1 should be able to reproduce the my minimal example.\r\n\r\nThe issue seems to be related to the nested output format `PlayerList` which is defined as `list[Player]`. LangSmith shows that the LLM generated a string:\r\n```python\r\n{\r\n  \"players\": \"[{\\\"name\\\":\\\"John\\\"}]\"\r\n}\r\n```\r\nwhich fails to be parsed as `PlayerList`."
      },
      {
        "author": "XiaomoWu",
        "created_at": "2024-12-03T03:42:34+00:00",
        "body": "> Try using outputfixingparser for added robustness.\r\n\r\nThank you for the advice. I checked the documentation of `outputfixingparser` but it's too dense to follow. It isn't mentioned in the tutorial or concepts guide either. Could you point me to some examples using this function?"
      },
      {
        "author": "michael-hoon",
        "created_at": "2024-12-05T06:58:08+00:00",
        "body": "you can check out [#28225](https://github.com/langchain-ai/langchain/pull/28225)"
      }
    ],
    "url": "https://github.com/langchain-ai/langchain/issues/28412"
  },
  {
    "repository": "langchain-ai/langchain",
    "issue_number": 28411,
    "title": "trim_first_node() and trim_last_node() removes nodes that should not be removed",
    "body": "### Checked other resources\r\n\r\n- [X] I added a very descriptive title to this issue.\r\n- [X] I searched the LangChain documentation with the integrated search.\r\n- [X] I used the GitHub search to find a similar question and didn't find it.\r\n- [X] I am sure that this is a bug in LangChain rather than my code.\r\n- [X] The bug is not resolved by updating to the latest stable version of LangChain (or the specific integration package).\r\n\r\n### Example Code\r\n\r\n```python\r\nfrom pydantic import BaseModel\r\nfrom langchain_core.runnables.graph import Graph\r\n\r\ndef test_trim() -> None:\r\n  class Scheme(BaseModel):\r\n    a: str\r\n  \r\n  graph = Graph()\r\n  start = graph.add_node(Scheme, id=\"__start__\")\r\n  a = graph.add_node(Scheme, id=\"a\")\r\n  end = graph.add_node(Scheme, id=\"__end__\")\r\n\r\n  graph.add_edge(start, a)\r\n  graph.add_edge(a, end)\r\n  graph.add_edge(start, end)\r\n\r\n  graph.trim_first_node() # should not remove __start__ since it has 2 outgoing edges\r\n  print(graph.first_node().id) # should be __start__ but prints 'a'\r\n\r\n  graph.trim_last_node() # should not remove the __end__ node since it has 2 incoming edges\r\n  print(graph.last_node().id) # should be __end__ but prints 'a'\r\n\r\nif __name__ == \"__main__\":\r\n  test_trim()\r\n```\r\n\r\n### Error Message and Stack Trace (if applicable)\r\n\r\n_No response_\r\n\r\n### Description\r\n\r\nThe trim_first_node() and trim_last_node() function trims nodes that have more than one outgoing/incoming edge. This seems to go against the described behaviours of the functions in their docstrings: \r\n\r\n```python\r\ndef trim_first_node(self) -> None:\r\n        \"\"\"Remove the first node if it exists and has a single outgoing edge,\r\n        i.e., if removing it would not leave the graph without a \"first\" node.\"\"\"\r\n\r\ndef trim_last_node(self) -> None:\r\n        \"\"\"Remove the last node if it exists and has a single incoming edge,\r\n        i.e., if removing it would not leave the graph without a \"last\" node.\"\"\"\r\n```\r\n\r\nThis issue was discovered while investigating issue [#1676](https://github.com/langchain-ai/langgraph/issues/1676) from the langgraph repository\r\n\r\n### System Info\r\n\r\nSystem Information\r\n------------------\r\n> OS:  Windows\r\n> OS Version:  10.0.19045\r\n> Python Version:  3.12.1 (tags/v3.12.1:2305ca5, Dec  7 2023, 22:03:25) [MSC v.1937 64 bit (AMD64)]\r\n\r\nPackage Information\r\n-------------------\r\n> langchain_core: 0.3.10\r\n> langchain: 0.3.3\r\n> langsmith: 0.1.134\r\n> langchain_text_splitters: 0.3.0\r\n\r\nOptional packages not installed\r\n-------------------------------\r\n> langgraph\r\n> langserve\r\n\r\nOther Dependencies\r\n------------------\r\n> aiohttp: 3.10.10\r\n> async-timeout: Installed. No version info available.\r\n> httpx: 0.27.2\r\n> jsonpatch: 1.33\r\n> numpy: 1.26.4\r\n> orjson: 3.10.7\r\n> packaging: 24.1\r\n> pydantic: 2.9.2\r\n> PyYAML: 6.0.2\r\n> requests: 2.32.3\r\n> requests-toolbelt: 1.0.0\r\n> SQLAlchemy: 2.0.35\r\n> tenacity: 8.5.0\r\n> typing-extensions: 4.12.2",
    "state": "closed",
    "created_at": "2024-11-29T03:58:47+00:00",
    "closed_at": "2024-12-09T04:45:29+00:00",
    "updated_at": "2024-12-09T04:45:30+00:00",
    "author": "Tasif1",
    "author_type": "User",
    "comments_count": 1,
    "reactions_count": 0,
    "labels": "ðŸ¤–:bug,â±­:  core",
    "milestone": null,
    "closed_by": "baskaryan",
    "resolution_time_hours": 240.77833333333334,
    "first_comments": [
      {
        "author": "Tasif1",
        "created_at": "2024-11-29T04:02:30+00:00",
        "body": "This is a potential fix for the functions:\r\n```python\r\ndef trim_first_node(self) -> None:\r\n        \"\"\"Remove the first node if it exists and has a single outgoing edge,\r\n        i.e., if removing it would not leave the graph without a \"first\" node.\"\"\"\r\n        first_node = self.first_node()\r\n        if first_node and len({e for e in self.edges if e.source == first_node.id}) == 1:\r\n            self.remove_node(first_node)\r\n\r\ndef trim_last_node(self) -> None:\r\n        \"\"\"Remove the last node if it exists and has a single incoming edge,\r\n        i.e., if removing it would not leave the graph without a \"last\" node.\"\"\"\r\n        last_node = self.last_node()\r\n        if last_node and len({e for e in self.edges if e.target == last_node.id}) == 1:\r\n            self.remove_node(last_node)\r\n```"
      }
    ],
    "url": "https://github.com/langchain-ai/langchain/issues/28411"
  },
  {
    "repository": "langchain-ai/langchain",
    "issue_number": 28406,
    "title": "Deprecated argument(s) removed when using `httpx==0.28.0`",
    "body": "### Checked other resources\n\n- [X] I added a very descriptive title to this issue.\n- [X] I searched the LangChain documentation with the integrated search.\n- [X] I used the GitHub search to find a similar question and didn't find it.\n- [X] I am sure that this is a bug in LangChain rather than my code.\n- [ ] The bug is not resolved by updating to the latest stable version of LangChain (or the specific integration package).\n\n### Example Code\n\n```python\r\nfrom langchain_openai import ChatOpenAI\r\n\r\nchat_oai = ChatOpenAI()\r\n```\n\n### Error Message and Stack Trace (if applicable)\n\n_No response_\n\n### Description\n\nAs for today, new `SyncAPIClient` class instances throw the exception:\r\n\r\n`TypeError: Client.__init__() got an unexpected keyword argument 'proxies'`\r\n\r\nAs stated [here](https://github.com/encode/httpx/releases/tag/0.28.0), some deprecated args, like `proxies`, has been removed.\r\n\r\nSo, a temporary override of `httpx<0.28.0` in project dependencies will work as expected.\n\n### System Info\n\nSystem Information\r\n------------------\r\n> OS:  Linux\r\n> OS Version:  #1 SMP Tue Jan 16 18:29:00 UTC 2024\r\n> Python Version:  3.11.2 (main, Mar 23 2023, 02:57:04) [GCC 10.2.1 20210110]\r\n\r\nPackage Information\r\n-------------------\r\n> langchain_core: 0.3.21\r\n> langchain: 0.3.9\r\n> langchain_community: 0.3.4\r\n> langsmith: 0.1.147\r\n> langchain_openai: 0.2.10\r\n> langchain_text_splitters: 0.3.2\r\n\r\nOptional packages not installed\r\n-------------------------------\r\n> langserve\r\n\r\nOther Dependencies\r\n------------------\r\n> aiohttp: 3.11.8\r\n> async-timeout: Installed. No version info available.\r\n> dataclasses-json: 0.6.7\r\n> httpx: 0.28.0\r\n> httpx-sse: 0.4.0\r\n> jsonpatch: 1.33\r\n> langsmith-pyo3: Installed. No version info available.\r\n> numpy: 1.26.4\r\n> openai: 1.55.3\r\n> orjson: 3.10.12\r\n> packaging: 24.2\r\n> pydantic: 2.10.2\r\n> pydantic-settings: 2.6.1\r\n> PyYAML: 6.0.2\r\n> requests: 2.32.3\r\n> requests-toolbelt: 1.0.0\r\n> SQLAlchemy: 2.0.36\r\n> tenacity: 9.0.0\r\n> tiktoken: 0.8.0\r\n> typing-extensions: 4.12.2",
    "state": "closed",
    "created_at": "2024-11-28T17:59:56+00:00",
    "closed_at": "2024-11-30T14:49:39+00:00",
    "updated_at": "2024-11-30T14:49:40+00:00",
    "author": "riccardogabellone",
    "author_type": "User",
    "comments_count": 4,
    "reactions_count": 0,
    "labels": "ðŸ¤–:bug,investigate",
    "milestone": null,
    "closed_by": "riccardogabellone",
    "resolution_time_hours": 44.82861111111111,
    "first_comments": [
      {
        "author": "hibana2077",
        "created_at": "2024-11-29T05:59:49+00:00",
        "body": "I also encounter same problem.\r\n\r\nAnd I found this code section on path : libs/partners/groq/langchain_groq/chat_models.py\r\n![image](https://github.com/user-attachments/assets/58d49d54-cc4e-4264-b29a-84b1b03477e1)\r\nIt's clear that langchain_groq doesn't pass any arugment call proxies.\r\n\r\nTherefore, I take a look into groq python library : _base_client.py\r\n![image](https://github.com/user-attachments/assets/d80d9b0d-9245-4d79-ac81-0e7e9aedf0d4)\r\nIn this section it passed proxies parameter to httpx.client\r\n\r\n![image](https://github.com/user-attachments/assets/8d13caae-afdb-4182-8e1f-cf525e5921fa)\r\nBut since 28th November, 2024 new release of httpx package (0.28.0) that \"proxies\"  has been removed.\r\n\r\nSo If you change to previous version of httpx, this bug should be resolved."
      },
      {
        "author": "ccurme",
        "created_at": "2024-11-29T13:57:43+00:00",
        "body": "@riccardogabellone would you mind sharing a full stack trace to help identify the problem?\r\n\r\nFrom what I can tell `ChatOpenAI` specifies `proxy` (not `proxies`) when creating the httpx client: https://github.com/langchain-ai/langchain/blob/2813e8640703b8066d8dd6c739829bb4f4aa634e/libs/partners/openai/langchain_openai/chat_models/base.py#L549"
      },
      {
        "author": "artpods56",
        "created_at": "2024-11-29T17:52:18+00:00",
        "body": "downgrading to httpx==0.27.0 fixed the problem for me"
      },
      {
        "author": "riccardogabellone",
        "created_at": "2024-11-30T14:49:39+00:00",
        "body": "> @riccardogabellone would you mind sharing a full stack trace to help identify the problem?\r\n> \r\n> From what I can tell `ChatOpenAI` specifies `proxy` (not `proxies`) when creating the httpx client:\r\n> \r\n> https://github.com/langchain-ai/langchain/blob/2813e8640703b8066d8dd6c739829bb4f4aa634e/libs/partners/openai/langchain_openai/chat_models/base.py#L549\r\n\r\nAfter depth investigating stack trace I found out issue was indeed the OpenAI lib, but a [new recent version](https://github.com/openai/openai-python/releases/tag/v1.55.3) fixed that, so, no need for manual downgrade anything. All last versions work normally, at least, for my use case."
      }
    ],
    "url": "https://github.com/langchain-ai/langchain/issues/28406"
  },
  {
    "repository": "langchain-ai/langchain",
    "issue_number": 28323,
    "title": "`Marqo.add_texts()` throws exception when using Marqo >= v2.0.0",
    "body": "### Checked other resources\r\n\r\n- [X] I added a very descriptive title to this issue.\r\n- [X] I searched the LangChain documentation with the integrated search.\r\n- [X] I used the GitHub search to find a similar question and didn't find it.\r\n- [X] I am sure that this is a bug in LangChain rather than my code.\r\n- [X] The bug is not resolved by updating to the latest stable version of LangChain (or the specific integration package).\r\n\r\n### Example Code\r\n\r\nrun container:\r\n```sh\r\ndocker pull marqoai/marqo:2.13 # or latest\r\ndocker rm -f marqo\r\ndocker run --name marqo -it -p 8882:8882 marqoai/marqo:2.13\r\n```\r\n\r\nrun script:\r\n```python\r\nfrom langchain_community.vectorstores import Marqo\r\nfrom langchain_core.documents import Document\r\nimport marqo\r\n\r\nmarqo_url = \"http://localhost:8882\"\r\nmarqo_api_key = \"\"\r\nclient = marqo.Client(url=marqo_url, api_key=marqo_api_key)\r\n\r\nindex_name = \"langchain-demo\"\r\ndocsearch = Marqo.from_texts([\"test\"], index_name=index_name)\r\nresult = docsearch.similarity_search(\"test\")\r\nprint(result)\r\n```\r\n\r\n### Error Message and Stack Trace (if applicable)\r\n\r\n```\r\nIndex langchain-demo exists.\r\nTraceback (most recent call last):\r\n  File \"./src.py\", line 13, in <module>\r\n    docsearch = Marqo.from_documents(docs, index_name=index_name)\r\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"./pyenv/lib/python3.12/site-packages/langchain_community/vectorstores/marqo.py\", line 364, in from_documents\r\n    return cls.from_texts(texts, metadatas=metadatas, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"./pyenv/lib/python3.12/site-packages/langchain_community/vectorstores/marqo.py\", line 452, in from_texts\r\n    instance.add_texts(texts, metadatas)\r\n  File \"./pyenv/lib/python3.12/site-packages/langchain_community/vectorstores/marqo.py\", line 112, in add_texts\r\n    if self._client.index(self._index_name).get_settings()[\"index_defaults\"][\r\n       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^\r\nKeyError: 'index_defaults'\r\n```\r\n\r\n### Description\r\n\r\nI'm following an example from: https://python.langchain.com/docs/integrations/vectorstores/marqo/.  Trying to create a vectorstore with some data on Marqo docker instance.\r\n\r\n**Expected behavior:**\r\nvector store gets created, with populated data\r\n\r\n**Actual behavior:**\r\nLangChain creates index and throws an exception in `add_texts()` (without populating data)\r\n\r\n**Reason:**\r\nsince Marqo 2.0 `index_defaults` is removed from index settings and `settings.index_defaults.treat_urls_and_pointers_as_images` should be referred in new version as `settings.treatUrlsAndPointersAsImages` (see https://docs.marqo.ai/latest/reference/api/indexes/create-index/#body-parameters and breaking changes in https://github.com/marqo-ai/marqo/blob/mainline/RELEASE.md#release-200)\r\n\r\n### System Info\r\n\r\nMarqo image version: 2.13.2\r\n\r\n`python -m langchain_core.sys_info`\r\n```\r\nSystem Information\r\n------------------\r\n> OS:  Linux\r\n> OS Version:  #1 SMP PREEMPT_DYNAMIC Tue Oct  8 03:24:49 UTC 2024\r\n> Python Version:  3.12.6 (main, Sep  8 2024, 13:18:56) [GCC 14.2.1 20240805]\r\n\r\nPackage Information\r\n-------------------\r\n> langchain_core: 0.3.21\r\n> langchain: 0.3.8\r\n> langchain_community: 0.3.8\r\n> langsmith: 0.1.145\r\n> langchain_text_splitters: 0.3.2\r\n\r\nOptional packages not installed\r\n-------------------------------\r\n> langserve\r\n\r\nOther Dependencies\r\n------------------\r\n> aiohttp: 3.11.7\r\n> async-timeout: Installed. No version info available.\r\n> dataclasses-json: 0.6.7\r\n> httpx: 0.27.2\r\n> httpx-sse: 0.4.0\r\n> jsonpatch: 1.33\r\n> numpy: 1.26.4\r\n> orjson: 3.10.12\r\n> packaging: 24.2\r\n> pydantic: 2.10.1\r\n> pydantic-settings: 2.6.1\r\n> PyYAML: 6.0.2\r\n> requests: 2.32.3\r\n> requests-toolbelt: 1.0.0\r\n> SQLAlchemy: 2.0.35\r\n> tenacity: 9.0.0\r\n> typing-extensions: 4.12.2\r\n```",
    "state": "closed",
    "created_at": "2024-11-24T01:22:47+00:00",
    "closed_at": "2024-12-18T12:00:29+00:00",
    "updated_at": "2024-12-18T12:00:29+00:00",
    "author": "tad1",
    "author_type": "User",
    "comments_count": 1,
    "reactions_count": 0,
    "labels": "â±­: vector store",
    "milestone": null,
    "closed_by": "tad1",
    "resolution_time_hours": 586.6283333333333,
    "first_comments": [
      {
        "author": "keenborder786",
        "created_at": "2024-11-25T23:44:50+00:00",
        "body": "@tad1 `get_setting` was still accounting for v1.5.x API but in the above PR accounted for later 2.x versions"
      }
    ],
    "url": "https://github.com/langchain-ai/langchain/issues/28323"
  },
  {
    "repository": "langchain-ai/langchain",
    "issue_number": 28284,
    "title": "PydanticUserError: `SQLDatabaseToolkit` is not fully defined; you should define `BaseCache`, then call `SQLDatabaseToolkit.model_rebuild()`.",
    "body": "### Checked other resources\n\n- [X] I added a very descriptive title to this issue.\n- [X] I searched the LangChain documentation with the integrated search.\n- [X] I used the GitHub search to find a similar question and didn't find it.\n- [X] I am sure that this is a bug in LangChain rather than my code.\n- [X] The bug is not resolved by updating to the latest stable version of LangChain (or the specific integration package).\n\n### Example Code\n\n```\r\nfrom langchain_community.agent_toolkits.sql.toolkit import SQLDatabaseToolkit\r\nfrom langchain_community.utilities.sql_database import SQLDatabase\r\nfrom langchain_openai import ChatOpenAI\r\nfrom langchain_community.agent_toolkits import create_sql_agent\r\n\r\ndb = SQLDatabase.from_uri(\"sqlite:///Chinook.db\")\r\nllm = llm_chat()# LLM of your choice\r\n\r\n# Initialize your SQLDatabaseToolkit\r\ntoolkit = SQLDatabaseToolkit(db=db, llm=llm)\r\nagent_executor = create_sql_agent(llm_chat(), toolkit=toolkit,max_iterations=11,verbose=True,handle_parsing_errors=True)\r\n\r\n```\n\n### Error Message and Stack Trace (if applicable)\n\n![image](https://github.com/user-attachments/assets/32d1a7f1-f936-4619-9c02-4ce3959c44bd)\r\n\n\n### Description\n\n1. I am trying to build a sql agent which can query over tabular data using create_sql_agent and SQLDatabaseToolkit from Langchain.\r\n2. My code was working fine till day before. Since yesterday SQLDatabasetoolkit function is throwing error.\r\n3. I will use the build agent to query database but right now cannot even built the toolkit. I am new to this, not sure where to make changes. I believe it is something related to update. \r\n\r\nAny help is highly appreciated :)\n\n### System Info\n\nSystem Information\r\n------------------\r\n> OS:  Linux\r\n> OS Version:  #1 SMP Tue Oct 22 16:38:23 UTC 2024\r\n> Python Version:  3.11.9 | packaged by conda-forge | (main, Apr 19 2024, 18:36:13) [GCC 12.3.0]\r\n\r\nPackage Information\r\n-------------------\r\n> langchain_core: 0.3.19\r\n> langchain: 0.3.7\r\n> langchain_community: 0.3.7\r\n> langsmith: 0.1.144\r\n> langchain_aws: 0.2.7\r\n> langchain_text_splitters: 0.3.2\r\n\r\nOptional packages not installed\r\n-------------------------------\r\n> langgraph\r\n> langserve\r\n\r\nOther Dependencies\r\n------------------\r\n> aiohttp: 3.11.7\r\n> async-timeout: Installed. No version info available.\r\n> boto3: 1.34.131\r\n> dataclasses-json: 0.6.7\r\n> httpx: 0.27.0\r\n> httpx-sse: 0.4.0\r\n> jsonpatch: 1.33\r\n> numpy: 1.26.4\r\n> orjson: 3.10.11\r\n> packaging: 23.2\r\n> pydantic: 2.10.1\r\n> pydantic-settings: 2.6.1\r\n> PyYAML: 6.0.1\r\n> requests: 2.31.0\r\n> requests-toolbelt: 1.0.0\r\n> SQLAlchemy: 2.0.31\r\n> tenacity: 8.4.1\r\n> typing-extensions: 4.12.2",
    "state": "closed",
    "created_at": "2024-11-22T10:01:27+00:00",
    "closed_at": "2024-11-23T12:51:52+00:00",
    "updated_at": "2024-11-24T11:36:37+00:00",
    "author": "yadav-shivani",
    "author_type": "User",
    "comments_count": 5,
    "reactions_count": 0,
    "labels": "ðŸ¤–:bug",
    "milestone": null,
    "closed_by": "yadav-shivani",
    "resolution_time_hours": 26.84027777777778,
    "first_comments": [
      {
        "author": "ksteimel",
        "created_at": "2024-11-22T16:54:22+00:00",
        "body": "I had a similar issue and downgrading pydantic to 2.9.2 resolved it for me."
      },
      {
        "author": "ahmed33033",
        "created_at": "2024-11-22T17:04:16+00:00",
        "body": "#28257 mentions a very similar issue! "
      },
      {
        "author": "eyurtsev",
        "created_at": "2024-11-22T20:04:32+00:00",
        "body": "Breaking due to new pydantic release: https://github.com/pydantic/pydantic/releases/tag/v2.10.1 investigating"
      },
      {
        "author": "yadav-shivani",
        "created_at": "2024-11-23T12:51:52+00:00",
        "body": "> I had a similar issue and downgrading pydantic to 2.9.2 resolved it for me.\r\n\r\nThanks a lot!. It is working for me now."
      },
      {
        "author": "Viicos",
        "created_at": "2024-11-24T11:36:35+00:00",
        "body": "As discussed on Slack: https://github.com/langchain-ai/langchain/pull/28297 fixed the issue, however there might be a more robust way to fix it.\r\n\r\nThe [`SQLDatabaseToolkit`](https://github.com/langchain-ai/langchain/blob/a83357dc5ab5fcbed8c2dd7606e9ce763e48d194/libs/community/langchain_community/agent_toolkits/sql/toolkit.py#L21) is a Pydantic model with a `llm` field annotated as `BaseLanguageModel`. This class, also a Pydantic model, has a `cache` field defined:\r\n\r\nhttps://github.com/langchain-ai/langchain/blob/a83357dc5ab5fcbed8c2dd7606e9ce763e48d194/libs/core/langchain_core/language_models/base.py#L95-L103\r\n\r\nBecause `BaseCache` is imported in an [`if TYPE_CHECKING:`](https://docs.python.org/3/library/typing.html#typing.TYPE_CHECKING) block, meaning Pydantic can't know about it (same for `Callbacks`, used in another field):\r\n\r\nhttps://github.com/langchain-ai/langchain/blob/a83357dc5ab5fcbed8c2dd7606e9ce763e48d194/libs/core/langchain_core/language_models/base.py#L31-L33\r\n\r\nFor every Pydantic model using `BaseLanguageModel`, the current fix from https://github.com/langchain-ai/langchain/pull/28297 added a `.model_rebuild()` call and imported the missing symbols. While this works, this is quite confusing as is. The reason it works is `model_rebuild()` will use the module namespace where is it called to resolve annotations that previously failed to resolve. Hence the added `BaseCache` and `Callbacks` imports in the aforementioned PR. Here are options I would consider instead:\r\n\r\n- For every `model_rebuild()` call, provide the missing annotations using the `_types_namespace` argument, e.g. `SQLDatabaseToolkit.model_rebuild(_types_namespace={'BaseCache': BaseCache, 'Callbacks': Callbacks})`. This is more explicit, as we at least know why the two imports were added and not used explicitly in the module (I believe that's why you had to use the `import BaseCache as BaseCache` form â€” initially used by typing stub files â€” to avoid having the import marked as unused).\r\n- Fix the root issue: instead of rebuilding every model making use of `BaseLanguageModel`, I would strongly recommend having `BaseLanguageModel` defined properly in the first place. It seems like moving `BaseCache` and `Callbacks` outside of the `if TYPE_CHECKING:` block does not cause any circular import issues.\r\n\r\n---\r\n\r\nAdditionally, it seems like `BaseToolkit` (which `SQLDatabaseToolkit` inherits from) is defined as a Pydantic model. `SQLDatabaseToolkit` has only one field that Pydantic can validate (`BaseLanguageModel`, itself a Pydantic model), meaning you had to use `arbitrary_types_allowed=True`. Is validation necessary for these kind of classes? Pydantic's primarily usage is to validate data, having most of the fields using arbitrary types (meaning no validation is performed) defeats the initial purpose of the library [^1].\r\n\r\n[^1]: Note that I'm not familiar with this library, there might be a valid reason for `BaseLanguageModel`/`SQLDatabaseToolkit` to be Pydantic models."
      }
    ],
    "url": "https://github.com/langchain-ai/langchain/issues/28284"
  },
  {
    "repository": "langchain-ai/langchain",
    "issue_number": 28283,
    "title": "can not install langchain when docling is installed",
    "body": "### Checked other resources\n\n- [X] I added a very descriptive title to this issue.\n- [X] I searched the LangChain documentation with the integrated search.\n- [X] I used the GitHub search to find a similar question and didn't find it.\n- [X] I am sure that this is a bug in LangChain rather than my code.\n- [X] The bug is not resolved by updating to the latest stable version of LangChain (or the specific integration package).\n\n### Example Code\n\n```sh\r\npoetry add docling==0.2.7 langchain=0.3.1\r\n```\n\n### Error Message and Stack Trace (if applicable)\n\nBecause no versions of langchain match >0.3.7,<0.4.0\r\n and langchain (0.3.7) depends on numpy (>=1.26.0,<2.0.0), langchain (>=0.3.7,<0.4.0) requires numpy (>=1.26.0,<2.0.0).\r\nBecause deepsearch-glm (0.26.1) depends on numpy (>=2.0.2,<3.0.0)\r\n and no versions of deepsearch-glm match >0.26.1,<0.27.0, deepsearch-glm (>=0.26.1,<0.27.0) requires numpy (>=2.0.2,<3.0.0).\r\nThus, langchain (>=0.3.7,<0.4.0) is incompatible with deepsearch-glm (>=0.26.1,<0.27.0).\r\nAnd because docling (2.7.0) depends on deepsearch-glm (>=0.26.1,<0.27.0)\r\n and no versions of docling match >2.7.0,<3.0.0, langchain (>=0.3.7,<0.4.0) is incompatible with docling (>=2.7.0,<3.0.0).\r\nSo, because nick-ai-batch depends on both docling (^2.7.0) and langchain (^0.3.7), version solving failed.\n\n### Description\n\nI would like to install docling and langchain simultaneously:\r\n\r\n```sh\r\n$ poetry add docling langchain\r\n```\r\n\r\nHowever, the following error occurs:\r\n\r\n```sh\r\nBecause no versions of langchain match >0.3.7,<0.4.0\r\n and langchain (0.3.7) depends on numpy (>=1.26.0,<2.0.0), langchain (>=0.3.7,<0.4.0) requires numpy (>=1.26.0,<2.0.0).\r\nBecause deepsearch-glm (0.26.1) depends on numpy (>=2.0.2,<3.0.0)\r\n and no versions of deepsearch-glm match >0.26.1,<0.27.0, deepsearch-glm (>=0.26.1,<0.27.0) requires numpy (>=2.0.2,<3.0.0).\r\nThus, langchain (>=0.3.7,<0.4.0) is incompatible with deepsearch-glm (>=0.26.1,<0.27.0).\r\nAnd because docling (2.7.0) depends on deepsearch-glm (>=0.26.1,<0.27.0)\r\n and no versions of docling match >2.7.0,<3.0.0, langchain (>=0.3.7,<0.4.0) is incompatible with docling (>=2.7.0,<3.0.0).\r\nSo, because nick-ai-batch depends on both docling (^2.7.0) and langchain (^0.3.7), version solving failed.\r\n```\r\n\r\nWhen I check the numpy version requirements for deepsearch-glm (a dependency of docling), it is defined as follows:\r\n\r\n```sh\r\n$ poetry show deepsearch-glm --tree | grep numpy\r\n\r\n>>>\r\nâ”‚   â”œâ”€â”€ numpy >=2.0.2,<3.0.0 (circular dependency aborted here)\r\nâ”‚   â”œâ”€â”€ numpy >=1.26.4,<2.0.0 (circular dependency aborted here)\r\n```\r\n\r\nHowever, for langchain, its dependency on numpy does not seem to handle circular dependency:\r\n\r\n```sh\r\n$ poetry show langchain --tree | grep numpy\r\n\r\n>>>\r\nâ”œâ”€â”€ numpy >=1.26.0,<2.0.0\r\n```\r\n\r\nOn the other hand, if I install llama-index, it succeeds. Checking its dependency definitions for numpy, it seems to handle circular dependencies:\r\n\r\n```sh\r\n$ poetry show llama-index --tree | grep numpy\r\n\r\n>>>\r\nâ”‚   â”‚   â”œâ”€â”€ numpy * \r\nâ”‚   â”‚   â”œâ”€â”€ numpy * \r\nâ”‚   â”œâ”€â”€ numpy * \r\nâ”‚   â”‚   â”œâ”€â”€ numpy * \r\nâ”‚       â”œâ”€â”€ numpy * \r\nâ”‚   â”œâ”€â”€ numpy * \r\nâ”‚   â”‚   â”œâ”€â”€ numpy >=1.26.0 (circular dependency aborted here)\r\nâ”‚   â”‚   â”œâ”€â”€ numpy * \r\nâ”‚   â”‚   â”œâ”€â”€ numpy * \r\nâ”‚   â”‚   â”‚   â”œâ”€â”€ numpy * \r\nâ”‚   â”‚   â”œâ”€â”€ numpy * \r\nâ”‚   â”‚   â”œâ”€â”€ numpy * \r\nâ”‚   â”‚   â”œâ”€â”€ numpy >=1.26.0 (circular dependency aborted here)\r\nâ”‚   â”‚   â”œâ”€â”€ numpy * \r\n```\r\n\r\nWould it be possible to address the circular dependency in langchain similarly?\n\n### System Info\n\nSystem Information\r\n------------------\r\n> OS:  Darwin\r\n> OS Version:  Darwin Kernel Version 23.4.0: Fri Mar 15 00:12:49 PDT 2024; root:xnu-10063.101.17~1/RELEASE_ARM64_T6020\r\n> Python Version:  3.12.7 (main, Nov 18 2024, 16:51:48) [Clang 15.0.0 (clang-1500.3.9.4)]\r\n\r\nPackage Information\r\n-------------------\r\n> langchain_core: 0.3.19\r\n> langsmith: 0.1.144\r\n\r\nOptional packages not installed\r\n-------------------------------\r\n> langgraph\r\n> langserve\r\n\r\nOther Dependencies\r\n------------------\r\n> httpx: 0.27.2\r\n> jsonpatch: 1.33\r\n> orjson: 3.10.11\r\n> packaging: 24.2\r\n> pydantic: 2.9.2\r\n> PyYAML: 6.0.2\r\n> requests: 2.32.3\r\n> requests-toolbelt: 1.0.0\r\n> tenacity: 8.5.0\r\n> typing-extensions: 4.12.2",
    "state": "closed",
    "created_at": "2024-11-22T09:00:11+00:00",
    "closed_at": "2024-11-25T06:28:56+00:00",
    "updated_at": "2024-11-25T06:28:56+00:00",
    "author": "ryotaro-ikeda0",
    "author_type": "User",
    "comments_count": 2,
    "reactions_count": 0,
    "labels": "investigate",
    "milestone": null,
    "closed_by": "ryotaro-ikeda0",
    "resolution_time_hours": 69.47916666666667,
    "first_comments": [
      {
        "author": "vagenas",
        "created_at": "2024-11-22T20:50:06+00:00",
        "body": "@ryotaro-ikeda0 for a workaround check out this comment: \r\nhttps://github.com/DS4SD/docling/issues/283#issuecomment-2465035868"
      },
      {
        "author": "ryotaro-ikeda0",
        "created_at": "2024-11-25T06:28:56+00:00",
        "body": "@vagenas \r\nThank you so much!! resolved!\r\n\r\n"
      }
    ],
    "url": "https://github.com/langchain-ai/langchain/issues/28283"
  },
  {
    "repository": "langchain-ai/langchain",
    "issue_number": 28282,
    "title": "SteamWebAPIWrapper issue",
    "body": "### Checked other resources\r\n\r\n- [X] I added a very descriptive title to this issue.\r\n- [X] I searched the LangChain documentation with the integrated search.\r\n- [X] I used the GitHub search to find a similar question and didn't find it.\r\n- [X] I am sure that this is a bug in LangChain rather than my code.\r\n- [X] The bug is not resolved by updating to the latest stable version of LangChain (or the specific integration package).\r\n\r\n### Example Code\r\n\r\nfrom langchain_community.utilities import SteamWebAPIWrapper\r\n\r\nsteam_search = SteamWebAPIWrapper()\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\thung\\miniconda3\\envs\\api-hub\\Lib\\site-packages\\langchain_community\\utilities\\steam.py\", line 48, in validate_environment\r\n    from steam import Steam\r\nImportError: cannot import name 'Steam' from 'steam' (C:\\Users\\thung\\miniconda3\\envs\\api-hub\\Lib\\site-packages\\steam\\__init__.py)\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Users\\thung\\miniconda3\\envs\\api-hub\\Lib\\site-packages\\pydantic\\v1\\main.py\", line 339, in __init__\r\n    values, fields_set, validation_error = validate_model(__pydantic_self__.__class__, data)\r\n                                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\thung\\miniconda3\\envs\\api-hub\\Lib\\site-packages\\pydantic\\v1\\main.py\", line 1100, in validate_model\r\n    values = validator(cls_, values)\r\n             ^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\thung\\miniconda3\\envs\\api-hub\\Lib\\site-packages\\langchain_community\\utilities\\steam.py\", line 50, in validate_environment\r\n    raise ImportError(\"python-steam-api library is not installed. \")\r\nImportError: python-steam-api library is not installed.\r\n>>> from langchain_core import sys_info\r\n>>> sys_info.print_sys_info()\r\n```\r\n\r\n![image](https://github.com/user-attachments/assets/ceb83b5b-33a7-455d-be84-060696dcaff5)\r\n\r\n\r\n### Error Message and Stack Trace (if applicable)\r\n\r\n_No response_\r\n\r\n### Description\r\n\r\nThe Steam tool seems to have trouble importing functions from the Steam package when there are some functions in Steam you call not exist in Steam package\r\n\r\n### System Info\r\n\r\nSystem Information\r\n------------------\r\n> OS:  Windows\r\n> OS Version:  10.0.22621\r\n> Python Version:  3.12.7 | packaged by Anaconda, Inc. | (main, Oct  4 2024, 13:17:27) [MSC v.1929 64 bit (AMD64)]\r\n\r\nPackage Information\r\n-------------------\r\n> langchain_core: 0.2.40\r\n> langchain: 0.2.1\r\n> langchain_community: 0.2.4\r\n> langsmith: 0.1.121\r\n> langchain_anthropic: 0.1.21\r\n> langchain_cli: 0.0.26\r\n> langchain_experimental: 0.0.59\r\n> langchain_google_genai: 1.0.8\r\n> langchain_groq: 0.1.6\r\n> langchain_openai: 0.1.19\r\n> langchain_pinecone: 0.1.2\r\n> langchain_text_splitters: 0.2.2\r\n> langchainhub: 0.1.20\r\n> langgraph: 0.2.22\r\n> langserve: 0.2.2\r\n\r\nOther Dependencies\r\n------------------\r\n> aiohttp: 3.9.5\r\n> aiosqlite: Installed. No version info available.\r\n> aleph-alpha-client: Installed. No version info available.\r\n> anthropic: 0.31.2\r\n> arxiv: 2.1.3\r\n> assemblyai: 0.25.0\r\n> async-timeout: Installed. No version info available.\r\n> atlassian-python-api: Installed. No version info available.\r\n> azure-ai-formrecognizer: Installed. No version info available.\r\n> azure-ai-textanalytics: Installed. No version info available.\r\n> azure-cognitiveservices-speech: Installed. No version info available.\r\n> azure-core: 1.30.2\r\n> azure-cosmos: Installed. No version info available.\r\n> azure-identity: Installed. No version info available.\r\n> azure-search-documents: Installed. No version info available.\r\n> beautifulsoup4: 4.12.3\r\n> bibtexparser: Installed. No version info available.\r\n> cassio: Installed. No version info available.\r\n> chardet: 5.2.0\r\n> clarifai: Installed. No version info available.\r\n> cohere: Installed. No version info available.\r\n> couchbase: Installed. No version info available.\r\n> dashvector: Installed. No version info available.\r\n> databricks-vectorsearch: Installed. No version info available.\r\n> dataclasses-json: 0.6.7\r\n> datasets: Installed. No version info available.\r\n> defusedxml: 0.7.1\r\n> dgml-utils: Installed. No version info available.\r\n> docarray[hnswlib]: Installed. No version info available.\r\n> esprima: Installed. No version info available.\r\n> faiss-cpu: Installed. No version info available.\r\n> faker: Installed. No version info available.\r\n> fastapi: 0.110.3\r\n> feedparser: 6.0.11\r\n> fireworks-ai: Installed. No version info available.\r\n> geopandas: Installed. No version info available.\r\n> gitpython: 3.1.43\r\n> google-cloud-documentai: Installed. No version info available.\r\n> google-generativeai: 0.7.2\r\n> gql: Installed. No version info available.\r\n> groq: 0.11.0\r\n> hologres-vector: Installed. No version info available.\r\n> html2text: Installed. No version info available.\r\n> httpx: 0.27.0\r\n> huggingface_hub: 0.24.2\r\n> javelin-sdk: Installed. No version info available.\r\n> jinja2: 3.1.4\r\n> jq: Installed. No version info available.\r\n> jsonpatch: 1.33\r\n> jsonschema: 4.23.0\r\n> langgraph-checkpoint: 1.0.10\r\n> langserve[all]: Installed. No version info available.\r\n> libcst: 1.4.0\r\n> lxml: 5.2.2\r\n> manifest-ml: Installed. No version info available.\r\n> markdownify: Installed. No version info available.\r\n> motor: Installed. No version info available.\r\n> msal: Installed. No version info available.\r\n> mwparserfromhell: Installed. No version info available.\r\n> mwxml: Installed. No version info available.\r\n> newspaper3k: Installed. No version info available.\r\n> nlpcloud: Installed. No version info available.\r\n> numexpr: 2.10.1\r\n> numpy: 1.26.4\r\n> openai: 1.37.1\r\n> openapi-pydantic: Installed. No version info available.\r\n> openlm: Installed. No version info available.\r\n> orjson: 3.10.6\r\n> packaging: 24.1\r\n> pandas: 2.2.2\r\n> pdfminer-six: Installed. No version info available.\r\n> pgvector: Installed. No version info available.\r\n> pillow: 10.4.0\r\n> pinecone-client: 3.2.2\r\n> praw: Installed. No version info available.\r\n> presidio-analyzer: Installed. No version info available.\r\n> presidio-anonymizer: Installed. No version info available.\r\n> psychicapi: Installed. No version info available.\r\n> py-trello: Installed. No version info available.\r\n> pydantic: 2.8.2\r\n> pymupdf: 1.24.9\r\n> pypdf: 4.3.1\r\n> pypdfium2: Installed. No version info available.\r\n> pyproject-toml: 0.0.10\r\n> pyspark: Installed. No version info available.\r\n> PyYAML: 6.0.1\r\n> qdrant-client: Installed. No version info available.\r\n> rank-bm25: Installed. No version info available.\r\n> rapidfuzz: 3.9.4\r\n> rapidocr-onnxruntime: Installed. No version info available.\r\n> rdflib: Installed. No version info available.\r\n> requests: 2.32.3\r\n> requests-toolbelt: 1.0.0\r\n> rspace_client: Installed. No version info available.\r\n> scikit-learn: 1.5.2\r\n> sentence-transformers: Installed. No version info available.\r\n> SQLAlchemy: 2.0.31\r\n> sqlite-vss: Installed. No version info available.\r\n> sse-starlette: 1.8.2\r\n> streamlit: Installed. No version info available.\r\n> sympy: 1.13.3\r\n> tabulate: 0.9.0\r\n> telethon: Installed. No version info available.\r\n> tenacity: 8.5.0\r\n> tiktoken: 0.7.0\r\n> timescale-vector: Installed. No version info available.\r\n> tomlkit: 0.12.5\r\n> torch: 2.4.1\r\n> tqdm: 4.66.4\r\n> transformers: Installed. No version info available.\r\n> typer: 0.9.4\r\n> typer[all]: Installed. No version info available.\r\n> types-requests: 2.32.0.20240712\r\n> typing-extensions: 4.12.2\r\n> upstash-redis: Installed. No version info available.\r\n> uvicorn: 0.23.2\r\n> vowpal-wabbit-next: Installed. No version info available.\r\n> xata: Installed. No version info available.\r\n> xmltodict: Installed. No version info available.",
    "state": "closed",
    "created_at": "2024-11-22T06:49:12+00:00",
    "closed_at": "2024-11-22T08:36:07+00:00",
    "updated_at": "2024-11-22T08:36:07+00:00",
    "author": "thunguyenuehk39",
    "author_type": "User",
    "comments_count": 0,
    "reactions_count": 0,
    "labels": "ðŸ¤–:bug",
    "milestone": null,
    "closed_by": "thunguyenuehk39",
    "resolution_time_hours": 1.7819444444444446,
    "first_comments": [],
    "url": "https://github.com/langchain-ai/langchain/issues/28282"
  },
  {
    "repository": "langchain-ai/langchain",
    "issue_number": 28281,
    "title": "Langchain-Ollama: ChatOllama stopped responding and AgentExecutor only performs actions",
    "body": "### Checked other resources\n\n- [X] I added a very descriptive title to this issue.\n- [X] I searched the LangChain documentation with the integrated search.\n- [X] I used the GitHub search to find a similar question and didn't find it.\n- [X] I am sure that this is a bug in LangChain rather than my code.\n- [X] The bug is not resolved by updating to the latest stable version of LangChain (or the specific integration package).\n\n### Example Code\n\nMain Example on GitHub\r\n\r\n``` \r\nfrom langchain_ollama import ChatOllama\r\n\r\nllm = ChatOllama(model=\"llama3.2\")\r\nllm.invoke(\"Sing a ballad of LangChain.\") \r\n```\n\n### Error Message and Stack Trace (if applicable)\n\nTypeError: 'NoneType' object is not iterable\n\n### Description\n\nTested with multiple people, the new version of Ollama must have changed output format but ChatOllama now cannot provide any text result. \n\n### System Info\n\nSystem Information\r\n------------------\r\n> OS:  Darwin\r\n> OS Version:  Darwin Kernel Version 23.5.0: Wed May  1 20:14:38 PDT 2024; root:xnu-10063.121.3~5/RELEASE_ARM64_T6020\r\n> Python Version:  3.11.3 (main, Apr 19 2023, 18:49:55) [Clang 14.0.6 ]\r\n\r\nPackage Information\r\n-------------------\r\n> langchain_core: 0.3.19\r\n> langchain: 0.1.17\r\n> langchain_community: 0.0.37\r\n> langsmith: 0.1.144\r\n> langchain_experimental: 0.0.57\r\n> langchain_ollama: 0.2.0\r\n> langchain_openai: 0.1.6\r\n> langchain_text_splitters: 0.0.1\r\n\r\nOptional packages not installed\r\n-------------------------------\r\n> langgraph\r\n> langserve\r\n\r\nOther Dependencies\r\n------------------\r\n> aiohttp: 3.9.3\r\n> aiosqlite: 0.18.0\r\n> aleph-alpha-client: Installed. No version info available.\r\n> anthropic: Installed. No version info available.\r\n> arxiv: Installed. No version info available.\r\n> assemblyai: Installed. No version info available.\r\n> async-timeout: 4.0.2\r\n> atlassian-python-api: Installed. No version info available.\r\n> azure-ai-documentintelligence: Installed. No version info available.\r\n> azure-ai-formrecognizer: Installed. No version info available.\r\n> azure-ai-textanalytics: Installed. No version info available.\r\n> azure-cognitiveservices-speech: Installed. No version info available.\r\n> azure-core: Installed. No version info available.\r\n> azure-cosmos: Installed. No version info available.\r\n> azure-identity: Installed. No version info available.\r\n> azure-search-documents: Installed. No version info available.\r\n> beautifulsoup4: 4.12.2\r\n> bibtexparser: Installed. No version info available.\r\n> cassio: Installed. No version info available.\r\n> chardet: 4.0.0\r\n> clarifai: Installed. No version info available.\r\n> cloudpickle: 2.2.1\r\n> cohere: Installed. No version info available.\r\n> couchbase: Installed. No version info available.\r\n> dashvector: Installed. No version info available.\r\n> databricks-vectorsearch: Installed. No version info available.\r\n> dataclasses-json: 0.6.5\r\n> datasets: 2.14.6\r\n> dgml-utils: Installed. No version info available.\r\n> docarray[hnswlib]: Installed. No version info available.\r\n> elasticsearch: Installed. No version info available.\r\n> esprima: Installed. No version info available.\r\n> faiss-cpu: 1.8.0\r\n> faker: Installed. No version info available.\r\n> feedparser: Installed. No version info available.\r\n> fireworks-ai: Installed. No version info available.\r\n> friendli-client: Installed. No version info available.\r\n> geopandas: Installed. No version info available.\r\n> gitpython: 3.1.43\r\n> google-cloud-documentai: Installed. No version info available.\r\n> gql: Installed. No version info available.\r\n> gradientai: Installed. No version info available.\r\n> hdbcli: Installed. No version info available.\r\n> hologres-vector: Installed. No version info available.\r\n> html2text: Installed. No version info available.\r\n> httpx: 0.27.0\r\n> httpx-sse: Installed. No version info available.\r\n> huggingface_hub: 0.26.2\r\n> javelin-sdk: Installed. No version info available.\r\n> jinja2: 3.1.3\r\n> jq: Installed. No version info available.\r\n> jsonpatch: 1.33\r\n> jsonschema: 4.17.3\r\n> lxml: 4.9.2\r\n> manifest-ml: Installed. No version info available.\r\n> markdownify: Installed. No version info available.\r\n> motor: Installed. No version info available.\r\n> msal: Installed. No version info available.\r\n> mwparserfromhell: Installed. No version info available.\r\n> mwxml: Installed. No version info available.\r\n> newspaper3k: Installed. No version info available.\r\n> nlpcloud: Installed. No version info available.\r\n> numexpr: 2.8.4\r\n> numpy: 1.24.3\r\n> nvidia-riva-client: Installed. No version info available.\r\n> oci: Installed. No version info available.\r\n> ollama: 0.4.0\r\n> openai: 1.26.0\r\n> openapi-pydantic: Installed. No version info available.\r\n> openlm: Installed. No version info available.\r\n> oracle-ads: Installed. No version info available.\r\n> oracledb: Installed. No version info available.\r\n> orjson: 3.9.15\r\n> packaging: 23.2\r\n> pandas: 1.5.3\r\n> pdfminer-six: 20231228\r\n> pgvector: Installed. No version info available.\r\n> praw: Installed. No version info available.\r\n> premai: Installed. No version info available.\r\n> presidio-analyzer: Installed. No version info available.\r\n> presidio-anonymizer: Installed. No version info available.\r\n> psychicapi: Installed. No version info available.\r\n> py-trello: Installed. No version info available.\r\n> pydantic: 2.10.1\r\n> pyjwt: 2.8.0\r\n> pymupdf: Installed. No version info available.\r\n> pypdf: 4.2.0\r\n> pypdfium2: 4.30.0\r\n> pyspark: Installed. No version info available.\r\n> PyYAML: 6.0.1\r\n> qdrant-client: Installed. No version info available.\r\n> rank-bm25: Installed. No version info available.\r\n> rapidfuzz: 3.9.0\r\n> rapidocr-onnxruntime: Installed. No version info available.\r\n> rdflib: Installed. No version info available.\r\n> requests: 2.32.3\r\n> requests-toolbelt: 1.0.0\r\n> rspace_client: Installed. No version info available.\r\n> scikit-learn: 1.2.2\r\n> sentence-transformers: 3.2.1\r\n> SQLAlchemy: 1.4.39\r\n> sqlite-vss: Installed. No version info available.\r\n> streamlit: Installed. No version info available.\r\n> sympy: 1.13.1\r\n> tabulate: 0.9.0\r\n> telethon: Installed. No version info available.\r\n> tenacity: 8.2.2\r\n> tidb-vector: Installed. No version info available.\r\n> tiktoken: 0.6.0\r\n> timescale-vector: Installed. No version info available.\r\n> torch: 2.5.1\r\n> tqdm: 4.67.0\r\n> transformers: 4.46.2\r\n> tree-sitter: Installed. No version info available.\r\n> tree-sitter-languages: Installed. No version info available.\r\n> typer: 0.13.0\r\n> typing-extensions: 4.12.2\r\n> upstash-redis: Installed. No version info available.\r\n> vdms: Installed. No version info available.\r\n> vowpal-wabbit-next: Installed. No version info available.\r\n> xata: Installed. No version info available.\r\n> xmltodict: Installed. No version info available.",
    "state": "closed",
    "created_at": "2024-11-22T05:37:44+00:00",
    "closed_at": "2024-11-25T21:23:53+00:00",
    "updated_at": "2024-11-26T08:30:02+00:00",
    "author": "miguelg719",
    "author_type": "User",
    "comments_count": 27,
    "reactions_count": 0,
    "labels": "ðŸ¤–:bug",
    "milestone": null,
    "closed_by": "miguelg719",
    "resolution_time_hours": 87.76916666666666,
    "first_comments": [
      {
        "author": "rtuin",
        "created_at": "2024-11-22T10:05:43+00:00",
        "body": "I'm also getting this error using `qwen2.5-coder:7b` model. Perhaps it helps to share the stack trace:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  [...redacted stack trace...]\r\n    response = llm.invoke(messages)\r\n               ^^^^^^^^^^^^^^^^^^^^\r\n  File \"my_anonimized_project_dir/.venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 286, in invoke\r\n    self.generate_prompt(\r\n  File \"my_anonimized_project_dir/.venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 786, in generate_prompt\r\n    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"my_anonimized_project_dir/.venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 643, in generate\r\n    raise e\r\n  File \"my_anonimized_project_dir/.venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 633, in generate\r\n    self._generate_with_cache(\r\n  File \"my_anonimized_project_dir/.venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 851, in _generate_with_cache\r\n    result = self._generate(\r\n             ^^^^^^^^^^^^^^^\r\n  File \"my_anonimized_project_dir/.venv/lib/python3.11/site-packages/langchain_ollama/chat_models.py\", line 644, in _generate\r\n    final_chunk = self._chat_stream_with_aggregation(\r\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"my_anonimized_project_dir/.venv/lib/python3.11/site-packages/langchain_ollama/chat_models.py\", line 558, in _chat_stream_with_aggregation\r\n    tool_calls=_get_tool_calls_from_response(stream_resp),\r\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"my_anonimized_project_dir/.venv/lib/python3.11/site-packages/langchain_ollama/chat_models.py\", line 70, in _get_tool_calls_from_response\r\n    for tc in response[\"message\"][\"tool_calls\"]:\r\nTypeError: 'NoneType' object is not iterable\r\n```\r\n\r\nEdit: mention that i use a different model"
      },
      {
        "author": "Ruslando",
        "created_at": "2024-11-22T11:08:06+00:00",
        "body": "Same thing happening with Llama3.1"
      },
      {
        "author": "Ruslando",
        "created_at": "2024-11-22T11:08:06+00:00",
        "body": "Same thing happening with Llama3.1"
      },
      {
        "author": "Fernando7181",
        "created_at": "2024-11-22T16:20:28+00:00",
        "body": "I was having this problem using llama3 but once switched to llma3.1 everything is working fine and using base_model "
      },
      {
        "author": "AlbertoFormaggio1",
        "created_at": "2024-11-22T17:05:36+00:00",
        "body": "@Fernando7181 can it be the case that llama3.1 was already downloaded in your system, while llama3 was freshly downloaded after the update?\r\nI am having this problem with every model I am using (all of them pulled today from ollama)\r\n"
      }
    ],
    "url": "https://github.com/langchain-ai/langchain/issues/28281"
  },
  {
    "repository": "langchain-ai/langchain",
    "issue_number": 28261,
    "title": "The system message in ChatCoze is not working",
    "body": "### Checked other resources\r\n\r\n- [X] I added a very descriptive title to this issue.\r\n- [X] I searched the LangChain documentation with the integrated search.\r\n- [X] I used the GitHub search to find a similar question and didn't find it.\r\n- [X] I am sure that this is a bug in LangChain rather than my code.\r\n- [X] The bug is not resolved by updating to the latest stable version of LangChain (or the specific integration package).\r\n\r\n### Example Code\r\n\r\nfrom langchain_community.chat_models import ChatCoze\r\nfrom langchain_core.messages import SystemMessage, HumanMessage\r\nimport os\r\nfrom config.config_llm import *\r\n\r\nchat = ChatCoze(\r\n    coze_api_key= coze_apikey,\r\n    bot_id=\"*******\",\r\n    user=\"*******\",\r\n    streaming=True\r\n)\r\n\r\n#### test1ï¼šä»…ä½¿ç”¨ HumanMessage\r\nmessages1 = [\r\n    HumanMessage(content=\"Who are you?\")\r\n]\r\nresponse1 = chat(messages1)\r\nprint(\"Test 1 - Only HumanMessage:\", response1)\r\n\r\n##### test2ï¼šä½¿ç”¨ SystemMessage + HumanMessage\r\nmessages2 = [\r\n    SystemMessage(content=\"You are a helpful math tutor who speaks like Shakespeare.\"),\r\n    HumanMessage(content=\"What is 2+2?\")\r\n]\r\nresponse2 = chat(messages2)\r\nprint(\"Test 2 - With SystemMessage:\", response2)\r\n\r\n### Error Message and Stack Trace (if applicable)\r\n\r\n(i:\\Diting\\diting_backend\\.conda) PS I:\\Diting\\diting_backend> python .\\test.py\r\nI:\\Diting\\diting_backend\\test.py:17: LangChainDeprecationWarning: The method `BaseChatModel.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\r\n  response1 = chat(messages1)\r\nTest 1 - Only HumanMessage: content=\"I'm Claude_DITING. How can I assist you today?\" additional_kwargs={} response_metadata={} id='run-996ef539-8e93-4318-ad80-6e9917ef3b99-0'\r\nTest 2 - With SystemMessage: content='Verily, the sum of 2 and 2 is 4.' additional_kwargs={} response_metadata={} id='run-97b81d24-8c14-4aca-9739-62721a2ebbdf-0'\r\n\r\n### Description\r\n\r\nThe system message in ChatCoze is not working\r\n\r\n### System Info\r\n\r\npython 3.11\r\n\r\nlangchain                                0.3.7\r\nlangchain-anthropic                      0.3.0\r\nlangchain-chroma                         0.1.4\r\nlangchain-cohere                         0.3.2\r\nlangchain-community                      0.3.7\r\nlangchain-core                           0.3.19\r\nlangchain-experimental                   0.3.3\r\nlangchain-ollama                         0.2.0\r\nlangchain-openai                         0.2.9\r\nlangchain-text-splitters                 0.3.2\r\nlangchainhub                             0.1.21\r\nlangsmith                                0.1.144\r\nlxml                                     5.3.0",
    "state": "closed",
    "created_at": "2024-11-21T15:25:52+00:00",
    "closed_at": "2024-11-28T09:44:14+00:00",
    "updated_at": "2024-11-28T09:46:30+00:00",
    "author": "lymanzhao",
    "author_type": "User",
    "comments_count": 3,
    "reactions_count": 0,
    "labels": "ðŸ¤–:bug",
    "milestone": null,
    "closed_by": "lymanzhao",
    "resolution_time_hours": 162.3061111111111,
    "first_comments": [
      {
        "author": "lymanzhao",
        "created_at": "2024-11-21T15:28:46+00:00",
        "body": "ref:\r\nhttps://python.langchain.com/docs/integrations/chat/coze/"
      },
      {
        "author": "keenborder786",
        "created_at": "2024-11-22T23:48:36+00:00",
        "body": "Please do this:\r\n\r\n```python\r\nfrom langchain_community.chat_models import ChatCoze\r\nfrom langchain_core.messages import SystemMessage, HumanMessage\r\nimport os\r\n\r\nchat = ChatCoze(\r\ncoze_api_key= \"\",\r\nbot_id=\"\",\r\nuser=\"\",\r\nstreaming=True\r\n)\r\n\r\n\r\nmessages1 = [\r\nHumanMessage(content=\"Who are you?\")\r\n]\r\nresponse1 = chat.invoke(messages1)\r\nprint(\"Test 1 - Only HumanMessage:\", response1)\r\n\r\n\r\nmessages2 = [\r\nSystemMessage(content=\"You are a helpful math tutor who speaks like Shakespeare.\"),\r\nHumanMessage(content=\"What is 2+2?\")\r\n]\r\nresponse2 = chat.invoke(messages2)\r\nprint(\"Test 2 - With SystemMessage:\", response2)\r\n\r\n\r\n```"
      },
      {
        "author": "lymanzhao",
        "created_at": "2024-11-28T09:46:29+00:00",
        "body": "The issue has been translated but remains unresolved; contacting Coze is recommended, as it appears to be their problem."
      }
    ],
    "url": "https://github.com/langchain-ai/langchain/issues/28261"
  },
  {
    "repository": "langchain-ai/langchain",
    "issue_number": 28237,
    "title": "Investigate tool message usage with mistral",
    "body": "### Privileged issue\r\n\r\n- [X] I am a LangChain maintainer, or was asked directly by a LangChain maintainer to create an issue here.\r\n\r\n### Issue Content\r\n\r\n```from langchain_core.messages import HumanMessage, ToolMessage\r\nfrom langchain_core.tools import tool\r\nfrom langchain_mistralai import ChatMistralAI\r\n\r\n\r\n@tool\r\ndef example_tool(action: str):\r\n    \"\"\"Perform an example action\"\"\"\r\n    print(f\"Performing action: {action}\")\r\n    return f\"Action {action} completed!\"\r\n\r\n\r\nllm = ChatMistralAI(model=\"mistral-large-latest\", api_key=\"\",\r\n                    temperature=0).bind_tools([example_tool])\r\n\r\nhuman_message = HumanMessage(role=\"user\", content=\"perform action A\")\r\n\r\nresult = llm.invoke([human_message])\r\n\r\ntool_call = result.additional_kwargs[\"tool_calls\"][0]\r\nprint(f\"Tool Call: {tool_call['function']['arguments']}\")\r\n\r\ntool_result = example_tool.invoke(\"A\")\r\n\r\ntool_message = ToolMessage(\r\n    content=tool_result,\r\n    tool_call_id=tool_call[\"id\"]\r\n)\r\n\r\nconversation = [\r\n    human_message,\r\n    result,\r\n    tool_message\r\n]\r\n\r\nresponse = llm.invoke(conversation)\r\n\r\nprint(response.content)\r\n```\r\n\r\n```\r\n    raise httpx.HTTPStatusError(\r\nhttpx.HTTPStatusError: Error response 500 while fetching https://api.mistral.ai/v1/chat/completions: {\"object\":\"error\",\"message\":\"Service unavailable.\",\"type\":\"internal_server_error\",\"param\":null,\"code\":\"3000\"} (edited) \r\n```",
    "state": "closed",
    "created_at": "2024-11-20T21:01:42+00:00",
    "closed_at": "2024-11-20T22:09:10+00:00",
    "updated_at": "2024-11-20T22:09:23+00:00",
    "author": "eyurtsev",
    "author_type": "User",
    "comments_count": 2,
    "reactions_count": 0,
    "labels": "",
    "milestone": null,
    "closed_by": "eyurtsev",
    "resolution_time_hours": 1.1244444444444444,
    "first_comments": [
      {
        "author": "eyurtsev",
        "created_at": "2024-11-20T21:54:46+00:00",
        "body": "https://github.com/langchain-ai/langchain/pull/28238/files"
      },
      {
        "author": "eyurtsev",
        "created_at": "2024-11-20T22:09:22+00:00",
        "body": "Will be released in langchain-mistral 0.2.2"
      }
    ],
    "url": "https://github.com/langchain-ai/langchain/issues/28237"
  },
  {
    "repository": "langchain-ai/langchain",
    "issue_number": 28231,
    "title": "DOC: Incorrect Code Sample for Hugging Face Embeddings",
    "body": "### URL\n\nhttps://python.langchain.com/docs/integrations/text_embedding/\n\n### Checklist\n\n- [X] I added a very descriptive title to this issue.\n- [X] I included a link to the documentation page I am referring to (if applicable).\n\n### Issue with current documentation:\n\nCurrently the code snippet for the hugging face model is \r\n\r\n```python\r\nfrom langchain_huggingface import HuggingFaceEmbeddings\r\n\r\nembeddings = HuggingFaceEmbeddings(model=\"sentence-transformers/all-mpnet-base-v2\")\r\n```\r\n\r\nThe `model` param should be `model_name` instead as per the langchain_huggingface [docs](https://python.langchain.com/api_reference/huggingface/embeddings/langchain_huggingface.embeddings.huggingface.HuggingFaceEmbeddings.html#langchain_huggingface.embeddings.huggingface.HuggingFaceEmbeddings) \n\n### Idea or request for content:\n\n_No response_",
    "state": "closed",
    "created_at": "2024-11-20T14:45:29+00:00",
    "closed_at": "2024-11-22T14:59:34+00:00",
    "updated_at": "2024-11-22T14:59:34+00:00",
    "author": "comsma",
    "author_type": "User",
    "comments_count": 0,
    "reactions_count": 0,
    "labels": "ðŸ¤–:docs",
    "milestone": null,
    "closed_by": "ccurme",
    "resolution_time_hours": 48.234722222222224,
    "first_comments": [],
    "url": "https://github.com/langchain-ai/langchain/issues/28231"
  },
  {
    "repository": "langchain-ai/langchain",
    "issue_number": 28229,
    "title": "`max_tokens` param not work in ChatPerplexity model",
    "body": "### Checked other resources\n\n- [X] I added a very descriptive title to this issue.\n- [X] I searched the LangChain documentation with the integrated search.\n- [X] I used the GitHub search to find a similar question and didn't find it.\n- [X] I am sure that this is a bug in LangChain rather than my code.\n- [X] The bug is not resolved by updating to the latest stable version of LangChain (or the specific integration package).\n\n### Example Code\n\n```\r\nchat_prompt = ...\r\n\r\nmodel = ChatPerplexity(\r\n    pplx_api_key=PERPLEXITY_API_KEY,\r\n    model=\"llama-3.1-sonar-small-128k-online\",\r\n    temperature=0.1,\r\n    max_tokens=200,\r\n)\r\n\r\nchain = chat_prompt | model\r\n\r\nresponse = chain.invoke(...)\r\n```\n\n### Error Message and Stack Trace (if applicable)\n\n_No response_\n\n### Description\n\n[Perplexity Docs Example : ](https://docs.perplexity.ai/api-reference/chat-completions)\r\n\r\n```\r\nimport requests\r\n\r\nurl = \"https://api.perplexity.ai/chat/completions\"\r\n\r\npayload = {\r\n    \"max_tokens\": 100,\r\n    \"model\": \"llama-3.1-sonar-small-128k-online\",\r\n    \"messages\": [\r\n        {\r\n            \"content\": \"read me a long fairy tale\",\r\n            \"role\": \"user\"\r\n        }\r\n    ]\r\n}\r\nheaders = {\r\n    \"Authorization\": ...,\r\n    \"Content-Type\": \"application/json\"\r\n}\r\n```\r\n\r\n---\r\n\r\nLangchainâ€™s Current Implementation :\r\n\r\nhttps://github.com/langchain-ai/langchain/blob/16918842bf86de1b493229310b0f6fc593d7a686/libs/community/langchain_community/chat_models/perplexity.py#L147-L156\r\n\r\nhttps://github.com/langchain-ai/langchain/blob/16918842bf86de1b493229310b0f6fc593d7a686/libs/community/langchain_community/chat_models/perplexity.py#L267-L275\r\n\r\n---\r\n\r\nSuggestion :\r\n```\r\n @property \r\n def _default_params(self) -> Dict[str, Any]: \r\n     \"\"\"Get the default parameters for calling PerplexityChat API.\"\"\" \r\n     return { \r\n         \"request_timeout\": self.request_timeout, \r\n         \"stream\": self.streaming, \r\n         \"temperature\": self.temperature, \r\n         **self.model_kwargs, \r\n     } \r\n```\r\n\r\n```\r\n @property \r\n def _invocation_params(self) -> Mapping[str, Any]: \r\n     \"\"\"Get the parameters used to invoke the model.\"\"\" \r\n     pplx_creds: Dict[str, Any] = { \r\n         \"api_key\": self.pplx_api_key, \r\n         \"api_base\": \"https://api.perplexity.ai\", \r\n         \"model\": self.model, \r\n         \"max_tokens\": self.max_tokens,\r\n     } \r\n     return {**pplx_creds, **self._default_params} \r\n```\n\n### System Info\n\nSystem Information\r\n------------------\r\n> OS:  Darwin\r\n> OS Version:  Darwin Kernel Version 24.1.0: Thu Oct 10 22:08:48 PDT 2024; root:xnu-11215.41.3~5/RELEASE_ARM64_T6000\r\n> Python Version:  3.10.14 (main, Mar 19 2024, 21:46:16) [Clang 15.0.0 (clang-1500.3.9.4)]\r\n\r\nPackage Information\r\n-------------------\r\n> langchain_core: 0.3.17\r\n> langchain: 0.2.15\r\n> langchain_community: 0.2.14\r\n> langsmith: 0.1.128\r\n> langchain_anthropic: 0.3.0\r\n> langchain_experimental: 0.0.57\r\n> langchain_google_genai: 2.0.4\r\n> langchain_google_vertexai: 1.0.4\r\n> langchain_mongodb: 0.2.0\r\n> langchain_openai: 0.0.5\r\n> langchain_text_splitters: 0.2.2\r\n\r\nOptional packages not installed\r\n-------------------------------\r\n> langgraph\r\n> langserve\r\n\r\nOther Dependencies\r\n------------------\r\n> aiohttp: 3.9.3\r\n> anthropic: 0.39.0\r\n> anthropic[vertexai]: Installed. No version info available.\r\n> async-timeout: 4.0.3\r\n> dataclasses-json: 0.6.7\r\n> defusedxml: 0.7.1\r\n> faker: Installed. No version info available.\r\n> google-cloud-aiplatform: 1.51.0\r\n> google-cloud-storage: 2.16.0\r\n> google-generativeai: 0.8.3\r\n> httpx: 0.27.2\r\n> jinja2: 3.1.2\r\n> jsonpatch: 1.33\r\n> numpy: 1.23.1\r\n> openai: 1.10.0\r\n> orjson: 3.10.2\r\n> packaging: 23.2\r\n> pandas: 2.2.0\r\n> pillow: 10.1.0\r\n> presidio-analyzer: Installed. No version info available.\r\n> presidio-anonymizer: Installed. No version info available.\r\n> pydantic: 2.9.2\r\n> pymongo: 4.8.0\r\n> PyYAML: 6.0.1\r\n> requests: 2.31.0\r\n> sentence-transformers: Installed. No version info available.\r\n> SQLAlchemy: 2.0.25\r\n> tabulate: 0.9.0\r\n> tenacity: 8.2.3\r\n> tiktoken: 0.5.2\r\n> typing-extensions: 4.9.0\r\n> vowpal-wabbit-next: Installed. No version info available.",
    "state": "closed",
    "created_at": "2024-11-20T08:21:54+00:00",
    "closed_at": "2024-12-10T00:23:32+00:00",
    "updated_at": "2024-12-10T00:23:32+00:00",
    "author": "YJU-KimJeongSu",
    "author_type": "User",
    "comments_count": 3,
    "reactions_count": 0,
    "labels": "ðŸ¤–:bug,investigate",
    "milestone": null,
    "closed_by": "efriis",
    "resolution_time_hours": 472.02722222222224,
    "first_comments": [
      {
        "author": "fanericcw",
        "created_at": "2024-11-26T01:12:46+00:00",
        "body": "Hello, we are a group of students from the University of Toronto Scarborough. We are interested in working on this issue, and we hope to have a working PR soon."
      },
      {
        "author": "arnavp103",
        "created_at": "2024-11-28T04:34:26+00:00",
        "body": "Hi, I'm part of the team investigating this. I was able to successfully reproduce this error and found that not only is `max_tokens` not respected, neither is `temperature` and I presume the other kwargs as well. Your suggested fix unfortunately didn't work, but we're investigating another fix and should have have it by tomorrow."
      },
      {
        "author": "trisolarion",
        "created_at": "2024-12-02T18:43:28+00:00",
        "body": "Thanks for the fix. I needed this for work"
      }
    ],
    "url": "https://github.com/langchain-ai/langchain/issues/28229"
  },
  {
    "repository": "langchain-ai/langchain",
    "issue_number": 28187,
    "title": "Inconsistent Cost Calculation with AzureChatOpenAI vs ChatOpenAI",
    "body": "### Checked other resources\n\n- [X] I added a very descriptive title to this issue.\n- [X] I searched the LangChain documentation with the integrated search.\n- [X] I used the GitHub search to find a similar question and didn't find it.\n- [X] I am sure that this is a bug in LangChain rather than my code.\n- [X] The bug is not resolved by updating to the latest stable version of LangChain (or the specific integration package).\n\n### Example Code\n\n### **Code**:\r\n```python\r\nfrom langchain.chat_models import AzureChatOpenAI\r\nfrom langchain_community.callbacks import get_openai_callback\r\n\r\n# Initialize the LLM\r\nllm = AzureChatOpenAI(\r\n    temperature=0,\r\n    model=\"gpt-4o-2024-08-06\",\r\n)\r\n\r\n# Use the callback to track token usage and costs\r\nwith get_openai_callback() as cb:\r\n    entity_response = _process_entity_extraction(llm, prompt, content, document_type)\r\n    validation_response = _process_validation(llm, prompt, entity_response, document_type)\r\n    page_info_response = _process_page_info(llm, prompt, content)\r\n\r\n    final_response = validation_response | page_info_response\r\n    final_response = _normalize_response(final_response)\r\n    print(f\"Input tokens: {cb.total_tokens}, Cost: ${cb.total_cost}\")\r\n```\r\n\n\n### Error Message and Stack Trace (if applicable)\n\n_No response_\n\n### Description\n\n### **Description**:\r\n\r\nWhen using the `AzureChatOpenAI` integration with LangChain, I observe a discrepancy in cost calculations compared to `ChatOpenAI`. While token usage is nearly identical between the two integrations, the reported cost for `AzureChatOpenAI` is approximately double.\r\n\r\n**Comparison of Logs**:\r\n\r\n1. **ChatOpenAI**:\r\n   ```\r\n   Input tokens: 11616\r\n   Output tokens: 723\r\n   Total cost: $0.036\r\n   ```\r\n\r\n2. **AzureChatOpenAI**:\r\n   ```\r\n   Input tokens: 11618\r\n   Output tokens: 760\r\n   Total cost: $0.069\r\n   ```\r\n\r\n**Expected Behavior**:  \r\nCosts should be consistent for the same model (`gpt-4o-2024-08-06`) across both integrations, as the pricing tables for OpenAI and Azure align.\r\n\r\n**Observed Behavior**:  \r\nThe cost reported for `AzureChatOpenAI` is significantly higher, despite negligible differences in token usage.\r\n\n\n### System Info\n\n### **System Info**:\r\n\r\n- **LangChain Versions**:\r\n  - `langchain`: 0.2.16\r\n  - `langchain-community`: 0.2.17\r\n  - `langchain-core`: 0.2.41\r\n  - `langchain-openai`: 0.1.25\r\n  - `langchain-text-splitters`: 0.2.4\r\n  - `langsmith`: 0.1.131\r\n- **Python Version**: 3.10.15\r\n- **Environment**:\r\n  - **Azure Region**: Global Standard \r\n  - **Model**: `gpt-4o-2024-08-06`\r\n  - **Callback Library**: `langchain_community.callbacks.get_openai_callback`\r\n\r\n### **Additional Context**:\r\n\r\n- This issue might stem from differences in how costs are calculated for Azure models compared to OpenAI's native API.\r\n- Requesting clarification on whether this behavior is expected or if adjustments to cost calculation logic are needed.\r\n",
    "state": "closed",
    "created_at": "2024-11-18T18:52:18+00:00",
    "closed_at": "2024-11-19T10:52:12+00:00",
    "updated_at": "2024-11-19T10:52:12+00:00",
    "author": "rafgonlop1",
    "author_type": "User",
    "comments_count": 0,
    "reactions_count": 0,
    "labels": "ðŸ¤–:bug,investigate",
    "milestone": null,
    "closed_by": "rafgonlop1",
    "resolution_time_hours": 15.998333333333333,
    "first_comments": [],
    "url": "https://github.com/langchain-ai/langchain/issues/28187"
  },
  {
    "repository": "langchain-ai/langchain",
    "issue_number": 28051,
    "title": "DOC: Fix typo in documentation for streaming modes, correcting â€œwittenâ€ to â€œwrittenâ€ in â€œEmit custom output witten using LangGraphâ€™s StreamWriter.â€",
    "body": "### URL\n\nhttps://python.langchain.com/docs/concepts/streaming/\n\n### Checklist\n\n- [X] I added a very descriptive title to this issue.\n- [X] I included a link to the documentation page I am referring to (if applicable).\n\n### Issue with current documentation:\n\nThe current documentation mentions the following:\r\n\r\n<h3>â€œEmit custom output witten using LangGraphâ€™s StreamWriter.â€</h3>\r\n\r\nThis statement contains a typo (â€œwittenâ€ should be â€œwrittenâ€).\n\n### Idea or request for content:\n\n_No response_",
    "state": "closed",
    "created_at": "2024-11-12T15:59:43+00:00",
    "closed_at": "2024-11-13T12:31:00+00:00",
    "updated_at": "2024-11-13T12:31:00+00:00",
    "author": "adityash97",
    "author_type": "User",
    "comments_count": 0,
    "reactions_count": 0,
    "labels": "ðŸ¤–:docs",
    "milestone": null,
    "closed_by": "adityash97",
    "resolution_time_hours": 20.52138888888889,
    "first_comments": [],
    "url": "https://github.com/langchain-ai/langchain/issues/28051"
  },
  {
    "repository": "langchain-ai/langchain",
    "issue_number": 28029,
    "title": "Document metadata returns Fragment object instead of Dict in _results_to_docs_and_scores ",
    "body": "### Checked other resources\n\n- [X] I added a very descriptive title to this issue.\n- [X] I searched the LangChain documentation with the integrated search.\n- [X] I used the GitHub search to find a similar question and didn't find it.\n- [X] I am sure that this is a bug in LangChain rather than my code.\n- [X] The bug is not resolved by updating to the latest stable version of LangChain (or the specific integration package).\n\n### Example Code\n\nI have agent graph code\r\n\r\n```Python\r\nfrom dotenv import load_dotenv\r\nfrom langchain_ollama import ChatOllama\r\nfrom langgraph.graph import StateGraph, START, END\r\nfrom langgraph.prebuilt import ToolNode\r\nfrom langgraph.checkpoint.memory import MemorySaver\r\nfrom langgraph.prebuilt import tools_condition\r\nfrom langgraph.graph import MessagesState\r\nfrom src.agents import tools\r\nimport os\r\n\r\nload_dotenv(\".env\")\r\nollama_url = os.getenv(\"OLLAMA_BASE_URL\")\r\nollama_model = os.getenv(\"OLLAMA_MODEL\")\r\n\r\nllm = ChatOllama(model=ollama_model, base_url=ollama_url)\r\nllm_with_tools = llm.bind_tools(tools.tools_list)\r\nmemory = MemorySaver() #checkpoint every node state\r\n\r\n# Node\r\ndef llm_call(state: MessagesState):\r\n    return {\"messages\": [llm_with_tools.invoke(state[\"messages\"])]}\r\n\r\n# Build graphÐ°\r\nbuilder = StateGraph(MessagesState)\r\nbuilder.add_node(\"llm_call\", llm_call)\r\nbuilder.add_node(\"tools\", ToolNode(tools.tools_list))\r\n\r\nbuilder.add_edge(START, \"llm_call\")\r\nbuilder.add_conditional_edges(\"llm_call\", tools_condition)\r\nbuilder.add_edge(\"tools\", \"llm_call\")\r\n\r\ngraph = builder.compile(memory)\r\n```\r\n\r\nI have tools code\r\n\r\n```Python\r\nfrom src.database.db import Database\r\nfrom src.parsers.habrnews import Habr\r\nfrom langchain.agents import tool\r\n\r\n@tool\r\ndef habr():\r\n    \"\"\"\r\n    - Returns the last IT news article on Habr\r\n    \"\"\"\r\n    return Habr.getNews()\r\n\r\n@tool\r\ndef news_database():\r\n    \"\"\"\r\n    - Returns data about news querry from the vector database\r\n    \"\"\"\r\n    db = Database()\r\n    return db.search(\"News for the last 24h\")\r\n\r\n# list of tools\r\ntools_list = [news_database]\r\n```\r\n\r\nI have Database code \r\n\r\n```Python\r\nfrom langchain_ollama import OllamaEmbeddings\r\nfrom langchain_ollama import ChatOllama\r\nfrom langchain_postgres import PGVector\r\nfrom langchain_core.documents import Document\r\nfrom dotenv import load_dotenv\r\nimport os\r\nimport json\r\n\r\nclass Database:\r\n\r\n    def __init__(self):\r\n        load_dotenv(\".env\")\r\n        name = os.getenv(\"POSTGRES_NAME\")\r\n        pwd = os.getenv(\"POSTGRES_PASSWORD\")\r\n        ollama_url = os.getenv(\"OLLAMA_BASE_URL\")\r\n        ollama_model = os.getenv(\"OLLAMA_MODEL\")\r\n        self.ollama = ChatOllama(model=ollama_model, base_url=ollama_url)\r\n\r\n        connection = f\"postgresql+psycopg://{name}:{pwd}@postgresql-pgvector:5432/feedconveyor\" \r\n        embeddings = OllamaEmbeddings(model=ollama_model, base_url=ollama_url)\r\n        \r\n        self.vector_database = PGVector(\r\n            collection_name=\"store\",\r\n            connection=connection,\r\n            embeddings=embeddings\r\n        )\r\n        self.vector_database.create_vector_extension()\r\n        self.vector_database.create_tables_if_not_exists()\r\n\r\n\r\n    async def store_data(self, data):\r\n        self.vector_database.add_documents(data, ids=[doc.metadata[\"id\"] for doc in data])\r\n\r\n    def search(self, search):\r\n        docs: dict[Document] = self.vector_database.similarity_search(search)\r\n        return docs\r\n\r\n```\r\n\r\nI have Docker compose code (LangGraph Studio and the main app are in the same network)\r\n\r\n```yaml\r\nservices:\r\n  bot:\r\n    build:\r\n      context: .\r\n    container_name: telegram-bot\r\n    develop:\r\n      watch:\r\n        - action: sync\r\n          path: ./src\r\n          target: /src\r\n          ignore:\r\n            - node_modules/\r\n        - action: rebuild \r\n          path: requirements.txt\r\n      depends_on:\r\n        - postgresql-pgvector\r\n    networks:\r\n      - local-network\r\n\r\n  postgresql-pgvector:\r\n      image: ankane/pgvector\r\n      container_name: postgresql-pgvector\r\n      env_file:\r\n        - \".env\"\r\n      restart: always\r\n      environment:\r\n        - POSTGRES_DB=feedconveyor\r\n        - POSTGRES_USER=${POSTGRES_NAME}\r\n        - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}\r\n      volumes:\r\n        - ./db:/var/lib/postgresql/data\r\n      networks:\r\n        - local-network\r\n      ports:\r\n        - 5432:5432\r\n        \r\n  pgadmin:\r\n    image: dpage/pgadmin4:latest\r\n    container_name: pgadmin\r\n    restart: always\r\n    env_file:\r\n      - \".env\"\r\n    environment:\r\n      - PGADMIN_DEFAULT_EMAIL=${PGADMIN_EMAIL}\r\n      - PGADMIN_DEFAULT_PASSWORD=${PGADMIN_PASSWORD}\r\n    ports:\r\n      - 8080:80\r\n    networks:\r\n      - local-network\r\n\r\nnetworks:\r\n  local-network:\r\n    driver: bridge\r\n\r\n```\r\n\r\n<img width=\"1512\" alt=\"issue\" src=\"https://github.com/user-attachments/assets/438ab168-6a75-45e2-81f0-c97aad72282d\">\r\n\r\n\r\n\n\n### Error Message and Stack Trace (if applicable)\n\nWhen I use Ollama:3.1:8b to answer the question by using RAG tools for searching the data I get \r\n\r\n```logs\r\n1 validation error for Document\r\nmetadata\r\n  Input should be a valid dictionary [type=dict_type, input_value=Fragment(buf=b'{\"id\": 1}'), input_type=Fragment]\r\n    For further information visit https://errors.pydantic.dev/2.9/v/dict_typeTraceback (most recent call last):\r\n\r\n\r\n  File \"/usr/local/lib/python3.12/site-packages/langchain_core/tools/base.py\", line 657, in run\r\n    response = context.run(self._run, *tool_args, **tool_kwargs)\r\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n\r\n\r\n  File \"/usr/local/lib/python3.12/site-packages/langchain_core/tools/structured.py\", line 80, in _run\r\n    return self.func(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n\r\n\r\n  File \"/deps/__outer_Feed-Conveyor/src/src/agents/tools.py\", line 18, in news_database\r\n    return db.search(\"News for the last 24h\")\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n\r\n\r\n  File \"/deps/__outer_Feed-Conveyor/src/src/database/db.py\", line 37, in search\r\n    docs: dict[Document] = self.vector_database.similarity_search(search)\r\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n\r\n\r\n  File \"/usr/local/lib/python3.12/site-packages/langchain_postgres/vectorstores.py\", line 943, in similarity_search\r\n    return self.similarity_search_by_vector(\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n\r\n\r\n  File \"/usr/local/lib/python3.12/site-packages/langchain_postgres/vectorstores.py\", line 1498, in similarity_search_by_vector\r\n    docs_and_scores = self.similarity_search_with_score_by_vector(\r\n                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n\r\n\r\n  File \"/usr/local/lib/python3.12/site-packages/langchain_postgres/vectorstores.py\", line 1043, in similarity_search_with_score_by_vector\r\n    return self._results_to_docs_and_scores(results)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n\r\n\r\n  File \"/usr/local/lib/python3.12/site-packages/langchain_postgres/vectorstores.py\", line 1063, in _results_to_docs_and_scores\r\n    Document(\r\n\r\n\r\n  File \"/usr/local/lib/python3.12/site-packages/langchain_core/documents/base.py\", line 285, in __init__\r\n    super().__init__(page_content=page_content, **kwargs)  # type: ignore[call-arg]\r\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n\r\n\r\n  File \"/usr/local/lib/python3.12/site-packages/langchain_core/load/serializable.py\", line 125, in __init__\r\n    super().__init__(*args, **kwargs)\r\n\r\n\r\n  File \"/usr/local/lib/python3.12/site-packages/pydantic/main.py\", line 212, in __init__\r\n    validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)\r\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n\r\n\r\npydantic_core._pydantic_core.ValidationError: 1 validation error for Document\r\nmetadata\r\n  Input should be a valid dictionary [type=dict_type, input_value=Fragment(buf=b'{\"id\": 1}'), input_type=Fragment]\r\n    For further information visit https://errors.pydantic.dev/2.9/v/dict_type\r\n```\n\n### Description\n\nIn https://github.com/langchain-ai/langchain/discussions/28027#discussioncomment-11217069 I discussed with AI about the issue:\r\n\r\nThe _results_to_docs_and_scores method is where the conversion of database results into Document objects occurs. The issue with metadata being returned as a Fragment object instead of a dictionary likely arises here. The metadata is being directly assigned from result.EmbeddingStore.cmetadata.\r\n\r\nTo resolve the issue, ensure that result.EmbeddingStore.cmetadata is properly deserialized into a dictionary. If cmetadata is stored as a JSON field in the database, it should be automatically deserialized by the ORM (e.g., SQLAlchemy) into a dictionary. However, if it's being returned as a Fragment, you might need to explicitly convert it to a dictionary. Here's a potential fix:\r\n\r\n```Python\r\ndef _results_to_docs_and_scores(self, results: Any) -> List[Tuple[Document, float]]:\r\n    \"\"\"Return docs and scores from results.\"\"\"\r\n    docs = [\r\n        (\r\n            Document(\r\n                id=str(result.EmbeddingStore.id),\r\n                page_content=result.EmbeddingStore.document,\r\n                metadata=dict(result.EmbeddingStore.cmetadata),  # Ensure it's a dictionary\r\n            ),\r\n            result.distance if self.embeddings is not None else None,\r\n        )\r\n        for result in results\r\n    ]\r\n    return docs\r\n```\r\n\r\nBy wrapping result.EmbeddingStore.cmetadata with dict(), you ensure that the metadata is explicitly converted to a dictionary, which should resolve the validation error.\r\n\r\n\n\n### System Info\n\nApple M1 14 Pro (15.1 24B83) 16 GB",
    "state": "closed",
    "created_at": "2024-11-11T17:22:19+00:00",
    "closed_at": "2024-11-18T12:55:51+00:00",
    "updated_at": "2024-11-18T12:55:51+00:00",
    "author": "simadimonyan",
    "author_type": "User",
    "comments_count": 1,
    "reactions_count": 0,
    "labels": "â±­: vector store",
    "milestone": null,
    "closed_by": "simadimonyan",
    "resolution_time_hours": 163.5588888888889,
    "first_comments": [
      {
        "author": "simadimonyan",
        "created_at": "2024-11-18T12:55:51+00:00",
        "body": "Fixed in: https://github.com/langchain-ai/langchain-postgres/pull/125"
      }
    ],
    "url": "https://github.com/langchain-ai/langchain/issues/28029"
  },
  {
    "repository": "langchain-ai/langchain",
    "issue_number": 28028,
    "title": "vectorstore.similarity_search error / Error: AttributeError: 'str' object has no attribute 'query'",
    "body": "### Checked other resources\n\n- [X] I added a very descriptive title to this issue.\n- [X] I searched the LangChain documentation with the integrated search.\n- [X] I used the GitHub search to find a similar question and didn't find it.\n- [X] I am sure that this is a bug in LangChain rather than my code.\n- [X] The bug is not resolved by updating to the latest stable version of LangChain (or the specific integration package).\n\n### Example Code\n\nThe following code:\r\n\r\n#Create the connection to Bedrock\r\nbedrock_runtime = boto3.client(service_name='bedrock-runtime',\r\n    region_name='us-east-1', \r\n    aws_access_key_id='aws_access_key_id', aws_secret_access_key='aws_secret_access_key'\r\n)\r\n\r\nos.environ['PINECONE_API_KEY']='pinecone_api_key'\r\nindex_name = \"whale\"\r\nembedding_model = BedrockEmbeddings(client=bedrock_runtime,model_id=\"amazon.titan-embed-text-v1\" )\r\n\r\nvectorstore = PineconeVectorStore(embedding = embedding_model, index=index_name)\r\n\r\nsimilar_search_results =  vectorstore.similarity_search(query = \"say something...\", k = 3)\r\n\r\n\r\nRaised this following error:\r\nAttributeError: 'str' object has no attribute 'query'\n\n### Error Message and Stack Trace (if applicable)\n\nembedding_model = BedrockEmbeddings(client=bedrock_runtime,model_id=\"amazon.titan-embed-text-v1\" )\r\nTraceback (most recent call last):\r\n  File \"c:\\Users\\91822\\Documents\\Python Scripts\\test.py\", line 22, in <module>\r\n    similar_search_results =  vectorstore.similarity_search(query = \"say something...\", k = 3)\r\n  File \"C:\\Users\\91822\\AppData\\Roaming\\Python\\Python310\\site-packages\\langchain_pinecone\\vectorstores.py\", line 379, in similarity_search\r\n    docs_and_scores = self.similarity_search_with_score(\r\n  File \"C:\\Users\\91822\\AppData\\Roaming\\Python\\Python310\\site-packages\\langchain_pinecone\\vectorstores.py\", line 321, in similarity_search_with_score\r\n    return self.similarity_search_by_vector_with_score(\r\n  File \"C:\\Users\\91822\\AppData\\Roaming\\Python\\Python310\\site-packages\\langchain_pinecone\\vectorstores.py\", line 338, in similarity_search_by_vector_with_score\r\n    results = self._index.query(\r\nAttributeError: 'str' object has no attribute 'query'\n\n### Description\n\nI'm trying to do a similarity search on my pinecone vector store through langchain's PineconeVectorStore module. The function is supposed to recieve a string as input  arg named \"query\". But it is throwing the mentioned error.\n\n### System Info\n\nSystem Information\r\n------------------\r\n> OS:  Windows\r\n> OS Version:  10.0.19045\r\n> Python Version:  3.10.2 (tags/v3.10.2:a58ebcc, Jan 17 2022, 14:12:15) [MSC v.1929 64 bit (AMD64)]\r\n\r\nPackage Information\r\n-------------------\r\n> langchain_core: 0.3.15\r\n> langchain: 0.3.7\r\n> langchain_community: 0.3.5\r\n> langsmith: 0.1.142\r\n> langchain_pinecone: 0.2.0\r\n> langchain_text_splitters: 0.3.2\r\n\r\nOptional packages not installed\r\n-------------------------------\r\n> langgraph\r\n> langserve\r\n\r\nOther Dependencies\r\n------------------\r\n> aiohttp: 3.9.5\r\n> async-timeout: 4.0.2\r\n> dataclasses-json: 0.6.7\r\n> httpx: 0.27.2\r\n> httpx-sse: 0.4.0\r\n> jsonpatch: 1.33\r\n> numpy: 1.23.2\r\n> orjson: 3.10.11\r\n> packaging: 24.2\r\n> pinecone-client: 5.0.1\r\n> pydantic: 2.9.2\r\n> pydantic-settings: 2.6.1\r\n> PyYAML: 6.0.2\r\n> requests: 2.28.2\r\n> requests-toolbelt: 1.0.0\r\n> SQLAlchemy: 2.0.35\r\n> tenacity: 8.2.1\r\n> typing-extensions: 4.12.2",
    "state": "closed",
    "created_at": "2024-11-11T16:55:09+00:00",
    "closed_at": "2024-12-14T02:13:06+00:00",
    "updated_at": "2024-12-14T02:13:06+00:00",
    "author": "naveen-vsamy",
    "author_type": "User",
    "comments_count": 4,
    "reactions_count": 0,
    "labels": "â±­: vector store,ðŸ¤–:bug,investigate",
    "milestone": null,
    "closed_by": "naveen-vsamy",
    "resolution_time_hours": 777.2991666666667,
    "first_comments": [
      {
        "author": "naveen-vsamy",
        "created_at": "2024-11-11T17:06:12+00:00",
        "body": "When defining \"vectorstore\" I passed \"index\" as arg instead of index_name. I changed the code and it worked fine"
      },
      {
        "author": "naveen-vsamy",
        "created_at": "2024-11-11T17:11:14+00:00",
        "body": "But still the error was misleading"
      },
      {
        "author": "myke11j",
        "created_at": "2024-11-17T09:51:16+00:00",
        "body": "In your initial code, when you passed a string in index parameter which should actually be pinecone.Index\r\n\r\n`vectorstore = PineconeVectorStore(embedding = embedding_model, index=index_name)`\r\n\r\nWhen you invoke any search function, it calls index.query which is a function from pinecone, but becauuse you passed a String, it tried to call \"index\" function on a string which is not part of string object. That's why you get the error \r\n\r\n`'str' object has no attribute 'query'`, which is not an error from lang-chain but it's a Python error.\r\n\r\nHere's the source code for reference https://github.com/langchain-ai/langchain/blob/master/libs/partners/pinecone/langchain_pinecone/vectorstores.py#L338"
      },
      {
        "author": "naveen-vsamy",
        "created_at": "2024-12-14T02:13:01+00:00",
        "body": "Got it, Thanks for the explanation"
      }
    ],
    "url": "https://github.com/langchain-ai/langchain/issues/28028"
  },
  {
    "repository": "langchain-ai/langchain",
    "issue_number": 27999,
    "title": "GCSDirectoryLoader error: Unable to get page count. Is poppler installed and in PATH?",
    "body": "### Checked other resources\n\n- [X] I added a very descriptive title to this issue.\n- [X] I searched the LangChain documentation with the integrated search.\n- [X] I used the GitHub search to find a similar question and didn't find it.\n- [X] I am sure that this is a bug in LangChain rather than my code.\n- [X] The bug is not resolved by updating to the latest stable version of LangChain (or the specific integration package).\n\n### Example Code\n\n```python\r\ntext_splitter = RecursiveCharacterTextSplitter()\r\n\r\ngcs_directory_loader = GCSDirectoryLoader(\r\n    project_name='project_name',\r\n    bucket='bucket_name',\r\n    loader_func=None\r\n)\r\n\r\ndocuments = gcs_directory_loader.load_and_split(text_splitter=self.state.text_splitter)\r\n```\n\n### Error Message and Stack Trace (if applicable)\n\nUnable to get page count. Is poppler installed and in PATH?\n\n### Description\n\n* This pytest unit test is running on macOS 14.7.1 (23H222)\n\n### System Info\n\nSystem Information\r\n------------------\r\n> OS:  Darwin\r\n> OS Version:  Darwin Kernel Version 23.6.0: Thu Sep 12 23:34:49 PDT 2024; root:xnu-10063.141.1.701.1~1/RELEASE_X86_64\r\n> Python Version:  3.9.19 (main, May  1 2024, 22:24:35) \r\n[Clang 15.0.0 (clang-1500.3.9.4)]\r\n\r\nPackage Information\r\n-------------------\r\n> langchain_core: 0.2.41\r\n> langchain: 0.2.9\r\n> langchain_community: 0.2.10\r\n> langsmith: 0.1.137\r\n> langchain_google_cloud_sql_pg: 0.6.1\r\n> langchain_google_community: 1.0.8\r\n> langchain_google_vertexai: 1.0.6\r\n> langchain_text_splitters: 0.2.4\r\n\r\nOptional packages not installed\r\n-------------------------------\r\n> langgraph\r\n> langserve\r\n\r\nOther Dependencies\r\n------------------\r\n> aiohttp: 3.10.10\r\n> anthropic[vertexai]: Installed. No version info available.\r\n> async-timeout: 4.0.3\r\n> beautifulsoup4: 4.12.3\r\n> black[jupyter]: Installed. No version info available.\r\n> cloud-sql-python-connector[asyncpg]: Installed. No version info available.\r\n> dataclasses-json: 0.6.7\r\n> db-dtypes: Installed. No version info available.\r\n> gapic-google-longrunning: Installed. No version info available.\r\n> google-api-core: 2.22.0\r\n> google-api-python-client: 2.149.0\r\n> google-auth-httplib2: 0.2.0\r\n> google-auth-oauthlib: Installed. No version info available.\r\n> google-cloud-aiplatform: 1.70.0\r\n> google-cloud-bigquery: 3.26.0\r\n> google-cloud-bigquery-storage: Installed. No version info available.\r\n> google-cloud-contentwarehouse: Installed. No version info available.\r\n> google-cloud-discoveryengine: Installed. No version info available.\r\n> google-cloud-documentai: Installed. No version info available.\r\n> google-cloud-documentai-toolbox: Installed. No version info available.\r\n> google-cloud-speech: Installed. No version info available.\r\n> google-cloud-storage: 2.18.2\r\n> google-cloud-texttospeech: Installed. No version info available.\r\n> google-cloud-translate: Installed. No version info available.\r\n> google-cloud-vision: 3.8.0\r\n> googlemaps: Installed. No version info available.\r\n> grpcio: 1.67.1\r\n> httpx: 0.27.2\r\n> isort: 5.13.2\r\n> jsonpatch: 1.33\r\n> mypy: 1.10.1\r\n> numpy: 1.26.4\r\n> orjson: 3.10.10\r\n> packaging: 24.1\r\n> pandas: 2.2.3\r\n> pgvector: 0.3.1\r\n> pyarrow: Installed. No version info available.\r\n> pydantic: 2.9.2\r\n> pytest: 8.3.3\r\n> pytest-asyncio: Installed. No version info available.\r\n> pytest-cov: Installed. No version info available.\r\n> PyYAML: 6.0.2\r\n> requests: 2.32.3\r\n> requests-toolbelt: 1.0.0\r\n> SQLAlchemy: 2.0.36\r\n> SQLAlchemy[asyncio]: Installed. No version info available.\r\n> tenacity: 8.3.0\r\n> typing-extensions: 4.12.2",
    "state": "closed",
    "created_at": "2024-11-08T20:13:08+00:00",
    "closed_at": "2024-11-11T16:46:09+00:00",
    "updated_at": "2024-11-11T16:46:10+00:00",
    "author": "ejstembler",
    "author_type": "User",
    "comments_count": 3,
    "reactions_count": 0,
    "labels": "ðŸ¤–:bug",
    "milestone": null,
    "closed_by": "ejstembler",
    "resolution_time_hours": 68.55027777777778,
    "first_comments": [
      {
        "author": "keenborder786",
        "created_at": "2024-11-09T23:35:02+00:00",
        "body": "Just install the following:\r\n\r\n```\r\npip install python-poppler\r\n```"
      },
      {
        "author": "keenborder786",
        "created_at": "2024-11-09T23:42:16+00:00",
        "body": "Since the `loader_func` is None and langchain by default uses `UnstructuredFileLoader` to load your pdf which might be requiring the above dependency!!! "
      },
      {
        "author": "ejstembler",
        "created_at": "2024-11-11T16:46:10+00:00",
        "body": "On macOS, I had several iterations to get it all working. Poppler took awhile to get it to completely install.  Basically something like this:\r\n\r\n- Make sure Xcode CommandLineTools are installed with header files in the right locations. `sudo rm -rf /Library/Developer/CommandLineTools && xcode-select --install`\r\n- Wait for Xcode dialog window to appear and click the install button. Takes 10-15 minutes.\r\n- `brew update && brew install poppler pkg-config`\r\n- `pip install python-poppler`"
      }
    ],
    "url": "https://github.com/langchain-ai/langchain/issues/27999"
  },
  {
    "repository": "langchain-ai/langchain",
    "issue_number": 27938,
    "title": "AIMessage enforces ToolCall model incompatible with OpenAI",
    "body": "### Checked other resources\r\n\r\n- [X] I added a very descriptive title to this question.\r\n- [X] I searched the LangChain documentation with the integrated search.\r\n- [X] I used the GitHub search to find a similar question and didn't find it.\r\n\r\n### Commit to Help\r\n\r\n- [X] I commit to help with one of those options ðŸ‘†\r\n\r\n### Example Code\r\n\r\n```python\r\n\r\nfrom langchain.messages import AIMessage, ToolCall\r\n\r\n#Â OpenAI compatible tool call format\r\ntool_call = {\r\n   \"id\":\"tc1\",\r\n   \"type\":\"function\",\r\n   \"function\":{\r\n      \"name\":\"SomeTool\",\r\n      \"arguments\":\"{\\\"some_argument\\\": \\\"some_value\\\"}\"\r\n   }\r\n}\r\n\r\n# line below works fine\r\nToolCall(**tool_call)\r\n\r\n#Â line below returns an error\r\nAIMessage(content=\"\", tool_calls=[ToolCall(**tool_call)])\r\n\r\n#Â error message:\r\n#Â pydantic.v1.error_wrappers.ValidationError: 1 validation error for AIMessage\r\n#Â tool_call() got an unexpected keyword argument 'function' (type=type_error)\r\n```\r\n\r\n\r\n### Description\r\n\r\nWhen you provide a value for the '_tool_calls_' argument of the _AIMessage_, _root_validator_ tries to parse the values in it to a _ToolCall_ object using the _langchain_core.messages.tool.tool_call_ function. Although instancing _ToolCall_ by calling it directly with OpenAI compatible tool_calls model is possible, the _tool_call_() function expects a specific set of variables to build the _ToolCall_, and does not accept extra keyword variables. This enforces a very specific data model when tool_calls is provided with _AIMessage_ and makes it impossible to create an AIMessage object with the desired _tool_calls_ formatting.\r\n\r\nIs this intended or could this be a bug? IIs there a different way of creating an AI message with tool_calls that I'm missing? \r\n\r\n\r\n\r\n### System Info\r\n\r\nlangchain==0.2.14\r\nlangchain-community==0.2.12\r\nlangchain-core==0.2.33\r\nlangchain-openai==0.1.22\r\nlangchain-text-splitters==0.2.2\r\n\r\nChecked the latest source code & documentation and the issue persists in the most recent version as well.\r\n\r\n_Originally posted by @yildirimgoks in https://github.com/langchain-ai/langchain/discussions/27919_",
    "state": "closed",
    "created_at": "2024-11-06T09:39:58+00:00",
    "closed_at": "2024-11-08T11:46:36+00:00",
    "updated_at": "2024-11-08T11:47:24+00:00",
    "author": "yildirimgoks",
    "author_type": "User",
    "comments_count": 2,
    "reactions_count": 0,
    "labels": "ðŸ¤–:bug",
    "milestone": null,
    "closed_by": "yildirimgoks",
    "resolution_time_hours": 50.11055555555556,
    "first_comments": [
      {
        "author": "keenborder786",
        "created_at": "2024-11-06T21:53:02+00:00",
        "body": "@yildirimgoks, this is intentional. `ToolCall` is an internal way for representation of AI Tool Call Message. At the end Tool Calling API for OpenAI is invoked with the compatible schema. But if you want to achieve the desire result:\r\n```python\r\n\r\n\r\n\r\nfrom langchain_core.messages import AIMessage, ToolCall\r\n\r\ntool_call = {\r\n   \"id\":\"tc1\",\r\n   \"type\":\"function\",\r\n   \"name\":\"some_function\",\r\n   \"args\":{\"some_argument\": \"some_value\"}\r\n}\r\n\r\n# line below works fine\r\nToolCall(**tool_call)\r\n\r\n# line below returns an error\r\nAIMessage(content=\"\", tool_calls=[ToolCall(**tool_call)])\r\n\r\n\r\n```"
      },
      {
        "author": "yildirimgoks",
        "created_at": "2024-11-08T11:46:36+00:00",
        "body": "When I initially did it like your suggestion I had received a BadRequest error from OpenAI complaining the tool call format was wrong - which was what actually prompted me to look into the data model LangChain was enforcing and all that.\r\n\r\nToday we had the opportunity to migrate the project to Langchain 0.3 and I can confirm your suggestion now works. I had looked into relevant parts of the codebase beforehand to check if 0.3 would make a difference and thought it wouldn't, must have overlooked something. \r\n\r\nThanks for the guidance & I'll close the issue."
      }
    ],
    "url": "https://github.com/langchain-ai/langchain/issues/27938"
  },
  {
    "repository": "langchain-ai/langchain",
    "issue_number": 27932,
    "title": "Supabase vectorstore does not support $in in a filter to match to a list of ids ",
    "body": "### Checked other resources\n\n- [X] I added a very descriptive title to this issue.\n- [X] I searched the LangChain documentation with the integrated search.\n- [X] I used the GitHub search to find a similar question and didn't find it.\n- [X] I am sure that this is a bug in LangChain rather than my code.\n- [X] The bug is not resolved by updating to the latest stable version of LangChain (or the specific integration package).\n\n### Example Code\n\n``` python\r\nfrom langchain_community.vectorstores import SupabaseVectorStore\r\n\r\nvector_store = SupabaseVectorStore(table_name=\"documents\",\r\n                                        query_name=\"match_documents\",\r\n                                        embedding=cached_embedding,\r\n                                        client=supabase\r\n)\r\npolicy_ids = ['20bac4b3-5964-47cf-a2a9-06e888c14add', '6004585c-624a-417c-91b5-ff4b9374b0fd']\r\nvector_store.similarity_search_with_relevance_scores(\r\n                    question,\r\n                    filter = {\"file_id\": {\"$in\": policy_ids}},\r\n                    )\n\n### Error Message and Stack Trace (if applicable)\n\nNone\n\n### Description\n\nI'm using Superbase as a vector store. I want to filter the similarity search to just the rows  where metadata-> file_id is in a list of file_ids\r\n\r\nAccording to the documentation I should be able to use filter = {\"file_id\": {\"$in\": policy_ids}}, however this results in no rows being returned.\r\n\r\nIf I use filter = {\"file_id\": policy_ids[0} I get the match for the first policy_id\r\n\r\nThe issue seems to be $in is not supported in Superbase\n\n### System Info\n\nSystem Information\r\n------------------\r\n> OS:  Linux\r\n> OS Version:  #1 SMP PREEMPT_DYNAMIC Wed, 23 Oct 2024 21:23:44 +0000\r\n> Python Version:  3.11.2 (main, Aug 26 2024, 07:20:54) [GCC 12.2.0]\r\n\r\nPackage Information\r\n-------------------\r\n> langchain_core: 0.3.10\r\n> langchain: 0.3.3\r\n> langchain_community: 0.3.2\r\n> langsmith: 0.1.134\r\n> langchain_anthropic: 0.2.3\r\n> langchain_openai: 0.2.2\r\n> langchain_text_splitters: 0.3.0\r\n\r\nOptional packages not installed\r\n-------------------------------\r\n> langgraph\r\n> langserve\r\n\r\nOther Dependencies\r\n------------------\r\n> aiohttp: 3.10.10\r\n> anthropic: 0.36.0\r\n> async-timeout: Installed. No version info available.\r\n> dataclasses-json: 0.6.7\r\n> defusedxml: 0.7.1\r\n> httpx: 0.27.2\r\n> jsonpatch: 1.33\r\n> numpy: 1.26.4\r\n> openai: 1.51.2\r\n> orjson: 3.10.7\r\n> packaging: 24.1\r\n> pydantic: 2.9.2\r\n> pydantic-settings: 2.5.2\r\n> PyYAML: 6.0.2\r\n> requests: 2.32.3\r\n> requests-toolbelt: 1.0.0\r\n> SQLAlchemy: 2.0.35\r\n> tenacity: 8.5.0\r\n> tiktoken: 0.8.0\r\n> typing-extensions: 4.12.2",
    "state": "closed",
    "created_at": "2024-11-06T01:22:30+00:00",
    "closed_at": "2025-01-29T14:24:20+00:00",
    "updated_at": "2025-01-29T14:24:20+00:00",
    "author": "Kevin-McIsaac",
    "author_type": "User",
    "comments_count": 1,
    "reactions_count": 0,
    "labels": "â±­: vector store",
    "milestone": null,
    "closed_by": "ccurme",
    "resolution_time_hours": 2029.0305555555556,
    "first_comments": [
      {
        "author": "shockValue666",
        "created_at": "2024-12-14T11:11:38+00:00",
        "body": "hey have you resolved the issue?"
      }
    ],
    "url": "https://github.com/langchain-ai/langchain/issues/27932"
  },
  {
    "repository": "langchain-ai/langchain",
    "issue_number": 27927,
    "title": "Return only Output in LLMChain",
    "body": "### Checked other resources\n\n- [X] I added a very descriptive title to this issue.\n- [X] I searched the LangChain documentation with the integrated search.\n- [X] I used the GitHub search to find a similar question and didn't find it.\n- [X] I am sure that this is a bug in LangChain rather than my code.\n- [X] The bug is not resolved by updating to the latest stable version of LangChain (or the specific integration package).\n\n### Example Code\n\n\r\nfrom langchain.llms import VertexAI\r\nfrom langchain import PromptTemplate, LLMChain\r\n\r\ntemplate = \"\"\"Given this text, decide what is the issue the customer is concerned about. Valid categories are these:\r\n* product issues\r\n* delivery problems\r\n* missing or late orders\r\n* wrong product\r\n* cancellation request\r\n* refund or exchange\r\n* bad support experience\r\n* no clear reason to be upset\r\n\r\nText: {email}\r\nCategory:\r\n\"\"\"\r\nprompt = PromptTemplate(template=template, input_variables=[\"email\"])\r\nllm = VertexAI()\r\nllm_chain = LLMChain(prompt=prompt, llm=llm, verbose=True)\r\nprint(llm_chain.run(customer_email))\r\n\r\n\r\n\n\n### Error Message and Stack Trace (if applicable)\n\n_No response_\n\n### Description\n\nFor the below code : \r\n\r\n1. I am getting full input string + output on printing results\r\n2. I want LLMChain to return output only ; for which I tested return_output_only=True; but the result remain same as in Step 1\r\n\r\n\r\n```\r\nfrom langchain.llms import VertexAI\r\nfrom langchain import PromptTemplate, LLMChain\r\n\r\ntemplate = \"\"\"Given this text, decide what is the issue the customer is concerned about. Valid categories are these:\r\n* product issues\r\n* delivery problems\r\n* missing or late orders\r\n* wrong product\r\n* cancellation request\r\n* refund or exchange\r\n* bad support experience\r\n* no clear reason to be upset\r\n\r\nText: {email}\r\nCategory:\r\n\"\"\"\r\nprompt = PromptTemplate(template=template, input_variables=[\"email\"])\r\nllm = VertexAI()\r\nllm_chain = LLMChain(prompt=prompt, llm=llm, verbose=True)\r\nprint(llm_chain.run(customer_email))\r\n\r\n\r\n```\n\n### System Info\n\naccelerate==1.0.1\r\naiohappyeyeballs==2.4.0\r\naiohttp==3.10.5\r\naiosignal==1.3.1\r\nannotated-types==0.7.0\r\nanyio==4.4.0\r\nargcomplete==1.10.3\r\nargon2-cffi==23.1.0\r\nargon2-cffi-bindings==21.2.0\r\narrow==1.3.0\r\nasttokens==2.4.1\r\nasync-lru==2.0.4\r\nasync-timeout==4.0.3\r\nattrs==24.2.0\r\nawscli==1.35.0\r\nbabel==2.16.0\r\nbeautifulsoup4==4.8.2\r\nbleach==6.1.0\r\nblis==1.0.1\r\nboto3==1.35.34\r\nbotocore==1.35.34\r\ncatalogue==2.0.10\r\ncertifi==2024.8.30\r\ncffi==1.17.1\r\nchardet==3.0.4\r\ncharset-normalizer==3.3.2\r\nclick==8.1.7\r\ncloudpathlib==0.19.0\r\ncolorama==0.4.6\r\ncomm==0.2.2\r\ncompressed-rtf==1.0.6\r\nconfection==0.1.5\r\ncryptography==43.0.1\r\ncymem==2.0.8\r\ndataclasses-json==0.6.7\r\ndebugpy==1.8.5\r\ndecorator==5.1.1\r\ndefusedxml==0.7.1\r\nDeprecated==1.2.14\r\ndirtyjson==1.0.8\r\ndiskcache==5.6.3\r\ndistro==1.9.0\r\ndocutils==0.16\r\ndocx2txt==0.8\r\neasyllm==0.6.2\r\nebcdic==1.1.1\r\nen_core_web_sm @ https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl\r\net-xmlfile==1.1.0\r\nexceptiongroup==1.2.2\r\nexecuting==2.1.0\r\nextract-msg==0.28.7\r\nfastjsonschema==2.20.0\r\nfilelock==3.16.1\r\nfire==0.6.0\r\nfonttools==4.53.1\r\nfqdn==1.5.1\r\nfrozenlist==1.4.1\r\nfsspec==2024.9.0\r\ngreenlet==3.1.0\r\nh11==0.14.0\r\nhttpcore==1.0.5\r\nhttpx==0.27.2\r\nhuggingface-hub==0.25.2\r\nidna==3.10\r\nIMAPClient==2.1.0\r\nInstructorEmbedding==1.0.1\r\nipykernel==6.29.5\r\nipython==8.27.0\r\nipywidgets==8.1.5\r\nisoduration==20.11.0\r\njedi==0.19.1\r\nJinja2==3.1.4\r\njiter==0.5.0\r\njmespath==1.0.1\r\njoblib==1.4.2\r\njson5==0.9.25\r\njsonpatch==1.33\r\njsonpointer==3.0.0\r\njsonschema==4.23.0\r\njsonschema-specifications==2023.12.1\r\njupyter==1.1.1\r\njupyter-console==6.6.3\r\njupyter-events==0.10.0\r\njupyter-lsp==2.2.5\r\njupyter_client==8.6.2\r\njupyter_core==5.7.2\r\njupyter_server==2.14.2\r\njupyter_server_terminals==0.5.3\r\njupyterlab==4.2.5\r\njupyterlab_pygments==0.3.0\r\njupyterlab_server==2.27.3\r\njupyterlab_widgets==3.0.13\r\nlangchain==0.3.3\r\nlangchain-community==0.3.2\r\nlangchain-core==0.3.10\r\nlangchain-text-splitters==0.3.0\r\nlangcodes==3.4.1\r\nlangsmith==0.1.135\r\nlanguage_data==1.2.0\r\nllama-cloud==0.0.11\r\nllama-extract==0.0.4\r\nllama-index==0.11.8\r\nllama-index-agent-openai==0.3.1\r\nllama-index-cli==0.3.1\r\nllama-index-core==0.11.9\r\nllama-index-embeddings-huggingface==0.3.1\r\nllama-index-embeddings-instructor==0.2.1\r\nllama-index-embeddings-openai==0.2.5\r\nllama-index-indices-managed-llama-cloud==0.3.1\r\nllama-index-legacy==0.9.48.post3\r\nllama-index-llms-huggingface==0.3.5\r\nllama-index-llms-huggingface-api==0.2.0\r\nllama-index-llms-llama-cpp==0.2.3\r\nllama-index-llms-ollama==0.3.4\r\nllama-index-llms-openai==0.2.7\r\nllama-index-multi-modal-llms-openai==0.2.1\r\nllama-index-program-evaporate==0.2.0\r\nllama-index-program-openai==0.2.0\r\nllama-index-question-gen-openai==0.2.0\r\nllama-index-readers-file==0.2.1\r\nllama-index-readers-llama-parse==0.3.0\r\nllama-parse==0.5.5\r\nllama_cpp_python==0.1.65\r\nllmsherpa==0.1.4\r\nlxml==5.3.0\r\nmarisa-trie==1.2.1\r\nmarkdown-it-py==3.0.0\r\nMarkupSafe==2.1.5\r\nmarshmallow==3.22.0\r\nmatplotlib-inline==0.1.7\r\nmdurl==0.1.2\r\nminijinja==2.2.0\r\nmistune==3.0.2\r\nmpmath==1.3.0\r\nmultidict==6.1.0\r\nmurmurhash==1.0.10\r\nmypy-extensions==1.0.0\r\nnanoid==2.0.0\r\nnbclient==0.10.0\r\nnbconvert==7.16.4\r\nnbformat==5.10.4\r\nnest-asyncio==1.6.0\r\nnetworkx==3.3\r\nnltk==3.9.1\r\nnotebook==7.2.2\r\nnotebook_shim==0.2.4\r\nnumpy==1.26.4\r\nnvidia-cublas-cu12==12.1.3.1\r\nnvidia-cuda-cupti-cu12==12.1.105\r\nnvidia-cuda-nvrtc-cu12==12.1.105\r\nnvidia-cuda-runtime-cu12==12.1.105\r\nnvidia-cudnn-cu12==9.1.0.70\r\nnvidia-cufft-cu12==11.0.2.54\r\nnvidia-curand-cu12==10.3.2.106\r\nnvidia-cusolver-cu12==11.4.5.107\r\nnvidia-cusparse-cu12==12.1.0.106\r\nnvidia-nccl-cu12==2.20.5\r\nnvidia-nvjitlink-cu12==12.6.68\r\nnvidia-nvtx-cu12==12.1.105\r\nolefile==0.47\r\nollama==0.3.3\r\nopenai==0.28.0\r\nopencv-python-headless==4.10.0.84\r\nopenpyxl==3.1.5\r\norjson==3.10.7\r\noverrides==7.7.0\r\npackaging==24.1\r\npandas==2.2.2\r\npandocfilters==1.5.1\r\nparso==0.8.4\r\npdf2docx==0.5.8\r\npdf2image==1.17.0\r\npdfminer.six==20191110\r\npdfplumber==0.11.4\r\npexpect==4.9.0\r\npillow==10.4.0\r\nplatformdirs==4.3.3\r\nplum-dispatch==1.7.4\r\npreshed==3.0.9\r\nprometheus_client==0.20.0\r\nprompt_toolkit==3.0.47\r\npsutil==6.0.0\r\nptyprocess==0.7.0\r\npure_eval==0.2.3\r\npyasn1==0.6.1\r\npycparser==2.22\r\npycryptodome==3.21.0\r\npydantic==2.9.2\r\npydantic-settings==2.5.2\r\npydantic_core==2.23.4\r\nPygments==2.18.0\r\nPyMuPDF==1.24.10\r\nPyMuPDFb==1.24.10\r\npypdf==4.3.1\r\nPyPDF2==3.0.1\r\npypdfium2==4.30.0\r\npyresparser==1.0.6\r\npyrsistent==0.20.0\r\npytesseract==0.3.13\r\npython-dateutil==2.9.0.post0\r\npython-decouple==3.8\r\npython-docx==1.1.2\r\npython-dotenv==1.0.1\r\npython-json-logger==2.0.7\r\npython-pptx==0.6.23\r\npytz==2024.2\r\nPyYAML==6.0.2\r\npyzmq==26.2.0\r\nreferencing==0.35.1\r\nregex==2024.9.11\r\nrequests==2.32.3\r\nrequests-toolbelt==1.0.0\r\nrfc3339-validator==0.1.4\r\nrfc3986-validator==0.1.1\r\nrich==13.9.2\r\nrpds-py==0.20.0\r\nrsa==4.7.2\r\ns3transfer==0.10.2\r\nsafetensors==0.4.5\r\nscikit-learn==1.5.2\r\nscipy==1.14.1\r\nSend2Trash==1.8.3\r\nsentence-transformers==2.7.0\r\nshellingham==1.5.4\r\nsix==1.12.0\r\nsmart-open==7.0.5\r\nsniffio==1.3.1\r\nsortedcontainers==2.4.0\r\nsoupsieve==2.6\r\nspacy==3.8.2\r\nspacy-legacy==3.0.12\r\nspacy-loggers==1.0.5\r\nSpeechRecognition==3.8.1\r\nSpire.Pdf==10.8.1\r\nSQLAlchemy==2.0.34\r\nsrsly==2.4.8\r\nstack-data==0.6.3\r\nstriprtf==0.0.26\r\nsympy==1.13.3\r\ntenacity==8.5.0\r\ntermcolor==2.4.0\r\nterminado==0.18.1\r\ntext-generation==0.7.0\r\ntextract==1.6.5\r\nthinc==8.3.2\r\nthreadpoolctl==3.5.0\r\ntiktoken==0.7.0\r\ntinycss2==1.3.0\r\ntokenizers==0.19.1\r\ntomli==2.0.1\r\ntorch==2.4.1\r\ntornado==6.4.1\r\ntqdm==4.66.5\r\ntraitlets==5.14.3\r\ntransformers==4.44.2\r\ntriton==3.0.0\r\ntyper==0.12.5\r\ntypes-python-dateutil==2.9.0.20240906\r\ntyping-inspect==0.9.0\r\ntyping_extensions==4.12.2\r\ntzdata==2024.1\r\ntzlocal==5.2\r\nuri-template==1.3.0\r\nurllib3==2.2.3\r\nwasabi==1.1.3\r\nwcwidth==0.2.13\r\nweasel==0.4.1\r\nwebcolors==24.8.0\r\nwebencodings==0.5.1\r\nwebsocket-client==1.8.0\r\nwidgetsnbextension==4.0.13\r\nwrapt==1.16.0\r\nxlrd==1.2.0\r\nXlsxWriter==3.2.0\r\nyarl==1.11.1\r\n",
    "state": "closed",
    "created_at": "2024-11-05T22:18:30+00:00",
    "closed_at": "2024-11-13T16:27:57+00:00",
    "updated_at": "2024-11-13T16:27:57+00:00",
    "author": "pn12",
    "author_type": "User",
    "comments_count": 1,
    "reactions_count": 0,
    "labels": "ðŸ¤–:bug",
    "milestone": null,
    "closed_by": "eyurtsev",
    "resolution_time_hours": 186.1575,
    "first_comments": [
      {
        "author": "keenborder786",
        "created_at": "2024-11-05T23:48:46+00:00",
        "body": "@pn12 LLMChain is the old way of constructing the Chain. Please try the following Langchain Expression Language to construct chains:\r\n\r\n```python\r\n\r\n\r\nfrom langchain.llms.vertexai import VertexAI\r\nfrom langchain_core.runnables import RunnableLambda\r\nfrom langchain import PromptTemplate\r\nimport os\r\n\r\n\r\n\r\ntemplate = \"\"\"Given this text, decide what is the issue the customer is concerned about. Valid categories are these:\r\n* product issues\r\n* delivery problems\r\n* missing or late orders\r\n* wrong product\r\n* cancellation request\r\n* refund or exchange\r\n* bad support experience\r\n* no clear reason to be upset\r\n\r\nText: {email}\r\nCategory:\r\n\"\"\"\r\nprompt = PromptTemplate(template=template, input_variables=[\"email\"])\r\nllm = VertexAI()\r\n\r\nchain = prompt | llm | RunnableLambda(lambda x:x.strip())\r\nprint(chain.invoke({'email':\"This is my email\"}))\r\n\r\n```"
      }
    ],
    "url": "https://github.com/langchain-ai/langchain/issues/27927"
  },
  {
    "repository": "langchain-ai/langchain",
    "issue_number": 27925,
    "title": "Built-in react graph does not stream responses token-by-token even though underlying model does (Ollama)",
    "body": "### Checked other resources\r\n\r\n- [X] I added a very descriptive title to this issue.\r\n- [X] I searched the [LangGraph](https://langchain-ai.github.io/langgraph/)/LangChain documentation with the integrated search.\r\n- [X] I used the GitHub search to find a similar question and didn't find it.\r\n- [X] I am sure that this is a bug in LangGraph/LangChain rather than my code.\r\n- [X] I am sure this is better as an issue [rather than a GitHub discussion](https://github.com/langchain-ai/langgraph/discussions/new/choose), since this is a LangGraph bug and not a design question.\r\n\r\n### Example Code\r\n\r\n```python\r\n# Reproduce on colab: https://colab.research.google.com/drive/1IOLnnXIj161xZ2DFzaxQXG7sCOXfBxlU?usp=sharing\r\n\r\nfrom langchain_core.tools import tool\r\nfrom langchain import hub\r\nfrom langchain_core.messages import AIMessageChunk, HumanMessage\r\n\r\nfrom langchain_ollama import ChatOllama\r\nmodel = ChatOllama(model=\"llama3.2\")\r\n\r\nfor chunk in model.stream(\"What is 2+2?\"):\r\n    print(chunk.content, end=\"|\", flush=True)\r\n\r\nprint(\"\\n\\n\")\r\n\r\n@tool\r\ndef magic_function(input: int) -> int:\r\n    \"\"\"Applies a magic function to an input.\"\"\"\r\n    return input + 2\r\n\r\ntools = [magic_function]\r\n\r\nfrom langgraph.prebuilt import create_react_agent\r\n\r\n# Choose the LLM that will drive the agent\r\nagent_executor = create_react_agent(model, tools)\r\n\r\nfor msg, metadata in agent_executor.stream({\"messages\": [(\"user\", \"what is 2+2?\")]}, stream_mode=\"messages\"):\r\n    if msg.content and not isinstance(msg, HumanMessage):\r\n        print(msg.content, end=\"|\", flush=True)\r\n```\r\n\r\n\r\n### Error Message and Stack Trace (if applicable)\r\n\r\n```shell\r\nOutput:\r\n\r\n2| +| |2| =| |4|.||\r\n\r\n\r\n6|The calculation of 2+2 results in 4, not 6.|\r\n```\r\n\r\nNotice that the model's direct response is streamed, but the majority of the React agent's response is not streamed.\r\n\r\n\r\n### Description\r\n\r\nI am trying to stream the response from the built-in react agent token-by-token.  Although streaming output from the model directly works fine, attempting to use `stream_mode=\"messages\"` as documented [here](https://langchain-ai.github.io/langgraph/how-tos/streaming-tokens/#streaming-llm-tokens) does not stream token by token, but rather message by message.\r\n\r\n[Reproduction on Google Colab](https://colab.research.google.com/drive/1IOLnnXIj161xZ2DFzaxQXG7sCOXfBxlU?usp=sharing)\r\n\r\n### System Info\r\n\r\nSystem Information\r\n------------------\r\n> OS:  Linux\r\n> OS Version:  langchain-ai/langgraph#1 SMP PREEMPT_DYNAMIC Thu Jun 27 21:05:47 UTC 2024\r\n> Python Version:  3.10.12 (main, Sep 11 2024, 15:47:36) [GCC 11.4.0]\r\n\r\nPackage Information\r\n-------------------\r\n> langchain_core: 0.3.15\r\n> langchain: 0.3.4\r\n> langsmith: 0.1.137\r\n> langchain_ollama: 0.2.0\r\n> langchain_text_splitters: 0.3.0\r\n> langgraph: 0.2.45\r\n\r\nOptional packages not installed\r\n-------------------------------\r\n> langserve\r\n\r\nOther Dependencies\r\n------------------\r\n> aiohttp: 3.10.10\r\n> async-timeout: 4.0.3\r\n> httpx: 0.27.2\r\n> jsonpatch: 1.33\r\n> langgraph-checkpoint: 2.0.2\r\n> langgraph-sdk: 0.1.35\r\n> numpy: 1.26.4\r\n> ollama: 0.3.3\r\n> orjson: 3.10.10\r\n> packaging: 24.1\r\n> pydantic: 2.9.2\r\n> PyYAML: 6.0.2\r\n> requests: 2.32.3\r\n> requests-toolbelt: 1.0.0\r\n> SQLAlchemy: 2.0.36\r\n> tenacity: 9.0.0\r\n> typing-extensions: 4.12.2",
    "state": "closed",
    "created_at": "2024-11-05T21:32:57+00:00",
    "closed_at": "2024-11-07T12:48:52+00:00",
    "updated_at": "2024-11-07T13:00:33+00:00",
    "author": "edmcman",
    "author_type": "User",
    "comments_count": 5,
    "reactions_count": 0,
    "labels": "",
    "milestone": null,
    "closed_by": "edmcman",
    "resolution_time_hours": 39.265277777777776,
    "first_comments": [
      {
        "author": "vbarda",
        "created_at": "2024-11-05T21:52:32+00:00",
        "body": "@edmcman looks like this is an issue with `ChatOllama` -- when it's used with tools, the output is not streamed correctly, i.e. these two code snippets behave differently. this is not an issue with langgraph, will transfer to LangChain\r\n\r\n```python\r\nfor chunk in model.stream(\"hi\"):\r\n    print(chunk)\r\n```\r\n\r\n```python\r\nfor chunk in model.bind_tools([magic_function]).stream(\"hi, don't use any tools\"):\r\n    print(chunk)\r\n```"
      },
      {
        "author": "edmcman",
        "created_at": "2024-11-06T13:03:50+00:00",
        "body": "@vbarda Thanks, didn't think to try that."
      },
      {
        "author": "siddhawan",
        "created_at": "2024-11-07T11:41:17+00:00",
        "body": "were you able to solve the issue?"
      },
      {
        "author": "edmcman",
        "created_at": "2024-11-07T12:48:52+00:00",
        "body": "Looks like an ollama limitation actually: https://github.com/ollama/ollama/issues/5796\r\n\r\nSo closing because it's not a langchain issue either."
      },
      {
        "author": "edmcman",
        "created_at": "2024-11-07T13:00:32+00:00",
        "body": "Also: https://github.com/ollama/ollama/pull/6066#issuecomment-2284563379"
      }
    ],
    "url": "https://github.com/langchain-ai/langchain/issues/27925"
  },
  {
    "repository": "langchain-ai/langchain",
    "issue_number": 27920,
    "title": "chain invoke method returns different sources from ainvoke",
    "body": "### Checked other resources\n\n- [X] I added a very descriptive title to this issue.\n- [X] I searched the LangChain documentation with the integrated search.\n- [X] I used the GitHub search to find a similar question and didn't find it.\n- [X] I am sure that this is a bug in LangChain rather than my code.\n- [X] The bug is not resolved by updating to the latest stable version of LangChain (or the specific integration package).\n\n### Example Code\n\nI have retriever like this\r\n```\r\nclass FilteringRetriever(VectorStoreRetriever):\r\n\r\n    def _get_relevant_documents(\r\n        self, query: str, *, run_manager: CallbackManagerForRetrieverRun\r\n    ) -> List[Document]:\r\n        docs = super()._get_relevant_documents(query=query, run_manager=run_manager)\r\n        # filter documents\r\n        newdocs = my_filter(docs)\r\n        return newdocs[:5] # I want only 5 documents\r\n\r\n```\n\n### Error Message and Stack Trace (if applicable)\n\n_No response_\n\n### Description\n\nWhen I call `chain.invoke` it correctly returns 5 documents. But when I invoke it asynchronously `chain.ainvoke` it returns all 100 (probably from initial `search_kwargs`). I'd expect it to be consistent, but I appreciate maybe that's not intended use of retriever. Can someone point me towards some workaround, as I need async calls in my application?\n\n### System Info\n\n```\r\nSystem Information\r\n------------------\r\n> OS:  Darwin\r\n> OS Version:  Darwin Kernel Version 23.6.0: Mon Jul 29 21:14:30 PDT 2024; root:xnu-10063.141.2~1/RELEASE_ARM64_T6000\r\n> Python Version:  3.11.7 | packaged by conda-forge | (main, Dec 23 2023, 14:38:07) [Clang 16.0.6 ]\r\n\r\nPackage Information\r\n-------------------\r\n> langchain_core: 0.1.16\r\n> langchain: 0.1.4\r\n> langchain_community: 0.0.16\r\n> langchain_openai: 0.0.5\r\n\r\nPackages not installed (Not Necessarily a Problem)\r\n--------------------------------------------------\r\nThe following packages were not found:\r\n\r\n> langgraph\r\n> langserve\r\n```",
    "state": "closed",
    "created_at": "2024-11-05T19:09:30+00:00",
    "closed_at": "2024-11-06T13:46:35+00:00",
    "updated_at": "2024-11-06T13:46:36+00:00",
    "author": "dokato",
    "author_type": "User",
    "comments_count": 1,
    "reactions_count": 0,
    "labels": "ðŸ¤–:bug",
    "milestone": null,
    "closed_by": "dokato",
    "resolution_time_hours": 18.618055555555557,
    "first_comments": [
      {
        "author": "dokato",
        "created_at": "2024-11-06T13:46:35+00:00",
        "body": "Ok, I figured that my custom implementation of retriever was lacking:\r\n\r\n```py\r\n    async def aget_relevant_documents(self, query: str, *, run_manager: CallbackManagerForRetrieverRun) -> List[Document]:\r\n        return await run_in_executor(None, self._get_relevant_documents, query, run_manager=run_manager)\r\n\r\n```\r\nJust wonder why this is not a default behaviour ðŸ¤” ."
      }
    ],
    "url": "https://github.com/langchain-ai/langchain/issues/27920"
  },
  {
    "repository": "langchain-ai/langchain",
    "issue_number": 27881,
    "title": "### **remote connection led the failure of bind_tools**",
    "body": "When I locally use Ollama, it calls the tools correctly.\r\nBut `langchain_ollama.ChatOllama` and `langchain_openai.ChatOpenAI` not work, while the\r\nOllama running remotely.\r\nSince not only one API function or LLM have this problem, the error may happen in a\r\ngeneral place. For example, `llm.bind_tools` cannot send the tool information to remote Ollama.\r\n**Details**\r\n**locally use works well**\r\n```\r\n# start ollama\r\nollama serve\r\nollama pull llama3.2\r\n```\r\n```\r\n# run agent\r\nfrom langchain_ollama import ChatOllama\r\nllm = ChatOllama(\r\n        model = \"llama3.2\", \r\n        temperature = 0.3,\r\n    )\r\n...\r\npart_3_assistant_runnable = assistant_prompt | llm.bind_tools(\r\n    part_3_safe_tools + part_3_sensitive_tools\r\n)\r\n```\r\n**The Agent output:**\r\n=================== Ai Message =================\r\nTool Calls:\r\n  list_selectable_catalogs \r\n Call ID: ..\r\n  Args:\r\n    catalogue_name: Gaia\r\n    catalogue_type: 0\r\n================== Tool Message =================\r\nName: list_selectable_catalogs\r\n{\r\n    \"code\": 200,\r\n...\r\n\r\n**remote ollama cannot see the tools**\r\n```\r\n# start remote ollama \r\nexport OLLAMA_HOST=0.0.0.0\r\nollama serve\r\n```\r\n```\r\n# run agent\r\nfrom langchain_ollama import ChatOllama\r\nllm = ChatOllama(\r\n        base_url=\"http://X.X.X.X:11434/\",\r\n        model = \"llama3.2\", \r\n        temperature = 0.3,\r\n    )\r\n# or use ChatOpenAI\r\nfrom langchain_openai import ChatOpenAI\r\nllm = ChatOpenAI(\r\n     api_key=\"ollama\",\r\n     base_url=\"http://X.X.X.X:11434/v1\",\r\n     model = \"llama3.2\", \r\n     temperature = 0.3,\r\n )\r\n...no any other change\r\n```\r\n**The Agent output:**\r\nTo assist you in finding the Gaia catalog, I will follow these steps:\r\n1. **Confirm the type of data required**: Please inform me of the specific type of data you need. For instance, if you are interested in the position, distance, spectral type, or any other particular attribute of stars, please provide detailed information.\r\n2.  **Query the database**: Use astronomical database tools (such as the SIMBAD Astronomical Database or the Gaia Data Release) to perform the search. These tools allow you to search and retrieve catalog data based on specific criteria.\r\n...\r\n\r\n**System Info**\r\nlangchain==0.3.0\r\nlangchain_community==0.3.0\r\nlanggraph==0.2.28\r\nlangchain_ollama==0.2.0\r\nplatform: Unbantu\r\nPython 3.12.4\r\n\r\n_Originally posted by @zhouhh2017 in https://github.com/langchain-ai/langchain/issues/21479#issuecomment-2453905504_\r\n            ",
    "state": "closed",
    "created_at": "2024-11-04T07:49:21+00:00",
    "closed_at": "2024-11-08T08:36:32+00:00",
    "updated_at": "2024-11-08T08:36:32+00:00",
    "author": "zhouhh2017",
    "author_type": "User",
    "comments_count": 1,
    "reactions_count": 0,
    "labels": "ðŸ¤–:bug,investigate,â±­:  core",
    "milestone": null,
    "closed_by": "zhouhh2017",
    "resolution_time_hours": 96.7863888888889,
    "first_comments": [
      {
        "author": "zhouhh2017",
        "created_at": "2024-11-08T08:36:32+00:00",
        "body": "Sorryï¼Œthe reason is the version of ollama on Linux.\r\nLinux do not install the latest version of ollama, 0.1.34 was installed.\r\nAfter update it to 0.4.0, all runs well."
      }
    ],
    "url": "https://github.com/langchain-ai/langchain/issues/27881"
  },
  {
    "repository": "langchain-ai/langchain",
    "issue_number": 27868,
    "title": "PipelinePromptTemplate doesn't support `partial`",
    "body": "### Checked other resources\r\n\r\n- [X] I added a very descriptive title to this issue.\r\n- [X] I searched the LangChain documentation with the integrated search.\r\n- [X] I used the GitHub search to find a similar question and didn't find it.\r\n- [X] I am sure that this is a bug in LangChain rather than my code.\r\n- [X] The bug is not resolved by updating to the latest stable version of LangChain (or the specific integration package).\r\n\r\n### Example Code\r\n\r\n```python\r\n      # Example from docs: https://python.langchain.com/v0.1/docs/modules/model_io/prompts/composition/#using-pipelineprompt\r\n      full_template = \"\"\"{introduction}\r\n      {example}\r\n      {start}\"\"\"\r\n      full_prompt = PromptTemplate.from_template(full_template)\r\n  \r\n      introduction_template = \"\"\"You are impersonating {person}.\"\"\"\r\n      introduction_prompt = PromptTemplate.from_template(introduction_template)\r\n      \r\n      example_template = \"\"\"Here's an example of an interaction:\r\n      Q: {example_q}\r\n      A: {example_a}\"\"\"\r\n      example_prompt = PromptTemplate.from_template(example_template)\r\n  \r\n      start_template = \"\"\"Now, do this for real!\r\n      Q: {input}\r\n      A:\"\"\"\r\n      start_prompt = PromptTemplate.from_template(start_template)\r\n  \r\n      input_prompts = [\r\n          (\"introduction\", introduction_prompt),\r\n          (\"example\", example_prompt),\r\n          (\"start\", start_prompt),\r\n      ]\r\n      pipeline_prompt = PipelinePromptTemplate(\r\n          final_prompt=full_prompt, pipeline_prompts=input_prompts\r\n      )\r\n  \r\n      pipeline_prompt.partial(person=\"Elon Musk\")\r\n```\r\n\r\n### Error Message and Stack Trace (if applicable)\r\n\r\n```python\r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n../../.venv/lib/python3.12/site-packages/langchain_core/prompts/base.py:277: in partial\r\n    return type(self)(**prompt_dict)\r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n\r\nself = PipelinePromptTemplate(input_variables=['person', 'input', 'example_q', 'example_a'], input_types={}, partial_variable...ariables=['input'], input_types={}, partial_variables={}, template='Now, do this for real!\\n    Q: {input}\\n    A:'))])\r\nargs = ()\r\nkwargs = {'final_prompt': PromptTemplate(input_variables=['example', 'introduction', 'start'], input_types={}, partial_variable...mple}\\n    {start}'), 'input_types': {}, 'input_variables': ['input', 'example_a', 'example_q'], 'metadata': None, ...}\r\n\r\n    def __init__(self, *args: Any, **kwargs: Any) -> None:\r\n        \"\"\"\"\"\"\r\n>       super().__init__(*args, **kwargs)\r\nE       pydantic_core._pydantic_core.ValidationError: 1 validation error for PipelinePromptTemplate\r\nE         Value error, Found overlapping input and partial variables: {'person'}\r\nE       For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT [type=value_error, input_value={'name': None, 'input_var... Q: {input}\\n    A:'))]}, input_type=dict]\r\nE           For further information visit https://errors.pydantic.dev/2.9/v/value_error\r\n\r\n../../.venv/lib/python3.12/site-packages/langchain_core/load/serializable.py:125: ValidationError\r\n```\r\n\r\n### Description\r\n\r\nIn the `partial` method the `PipelinePromptTemplate` is reinstantiated with a new set of `input_variables` and `partial_variables`. However, when creating a new instance the `get_input_variables` model validator is invoked, which recreates the original input variables based on the `pipeline_prompts`, overwriting the new `input_variables`. Therefore the validation error related to overlapping `input_variables` and `partial_variables`.\r\n\r\n\r\n\r\n### System Info\r\n\r\n```\r\nâ¯ python -m langchain_core.sys_info\r\n\r\nSystem Information\r\n------------------\r\n> OS:  Darwin\r\n> OS Version:  Darwin Kernel Version 23.5.0: Wed May  1 20:12:58 PDT 2024; root:xnu-10063.121.3~5/RELEASE_ARM64_T6000\r\n> Python Version:  3.12.7 (main, Oct  1 2024, 02:05:46) [Clang 15.0.0 (clang-1500.3.9.4)]\r\n\r\nPackage Information\r\n-------------------\r\n> langchain_core: 0.3.15\r\n> langchain: 0.3.7\r\n> langsmith: 0.1.139\r\n> langchain_aws: 0.2.6\r\n> langchain_text_splitters: 0.3.2\r\n\r\nOptional packages not installed\r\n-------------------------------\r\n> langgraph\r\n> langserve\r\n```",
    "state": "closed",
    "created_at": "2024-11-03T09:53:35+00:00",
    "closed_at": "2024-12-10T03:45:51+00:00",
    "updated_at": "2024-12-10T03:51:25+00:00",
    "author": "dmenini",
    "author_type": "User",
    "comments_count": 3,
    "reactions_count": 0,
    "labels": "ðŸ¤–:bug",
    "milestone": null,
    "closed_by": "efriis",
    "resolution_time_hours": 881.8711111111111,
    "first_comments": [
      {
        "author": "Aarya2004",
        "created_at": "2024-11-07T16:58:28+00:00",
        "body": "Hello @dmenini! My group and I investigated this error and we found the cause of this is due to the partial method not being overridden in the PipleinePromptTemplate class. We've come up with a potential fix but if this issue can be assigned to us, we can open a PR asap!"
      },
      {
        "author": "efriis",
        "created_at": "2024-12-10T03:45:51+00:00",
        "body": "Hey there! Agreed this is not handled well by the PipelinePromptTemplate, and it's largely because PipelinePromptTemplate doesn't adhere to the PromptTemplate interface in many ways.\r\n\r\nI would recommend doing something like the following instead\r\n\r\n```\r\nkeys_and_prompts: list[tuple[str, PromptTemplate]] = ...\r\n\r\n\r\nchain = RunnablePassthrough().assign(partial_var_1=x, partial_var_2=y)\r\nfor key, prompt in keys_and_prompts:\r\n  chain = chain | RunnablePassthrough.assign(**{key: prompt})\r\n\r\nchain = chain | lambda x: x[keys_and_prompts[-1][0]] # get the output of the final prompt\r\n\r\nchain.invoke({\"a\": \"b\"}) # all your prompt variables\r\n```"
      },
      {
        "author": "efriis",
        "created_at": "2024-12-10T03:51:24+00:00",
        "body": "better one not using LCEL:\r\n\r\n```\r\nmy_input = {\"key\": \"value\"}\r\nfor name, prompt in pipeline_prompts:\r\n    my_input[name] = prompt.invoke(my_input).to_string()\r\nmy_output = final_prompt.invoke(my_input)\r\n```"
      }
    ],
    "url": "https://github.com/langchain-ai/langchain/issues/27868"
  },
  {
    "repository": "langchain-ai/langchain",
    "issue_number": 27865,
    "title": "Fix \"NotImplementedError\" with the `atransform_documents()` method in `MarkdownifyTransformer` (this issue includes a suggested fix)",
    "body": "### Checked other resources\r\n\r\n- [X] I added a very descriptive title to this issue.\r\n- [X] I searched the LangChain documentation with the integrated search.\r\n- [X] I used the GitHub search to find a similar question and didn't find it.\r\n- [X] I am sure that this is a bug in LangChain rather than my code.\r\n- [X] The bug is not resolved by updating to the latest stable version of LangChain (or the specific integration package).\r\n\r\n### Example Code\r\n\r\nLinked to discussion: #27760\r\n\r\nCurrently, the MarkdownifyTransformer raises a NotImplementedError if a user calls the atransform_documents() method:\r\n\r\nhttps://github.com/langchain-ai/langchain/blob/3b956b3a97a5fde589fafe976e7f7a64cdb4ca3f/libs/community/langchain_community/document_transformers/markdownify.py#L78-L83\r\n\r\nHere's a minimum reproducible example:\r\n\r\n```python\r\nfrom langchain_community.document_transformers import MarkdownifyTransformer\r\nfrom langchain_core.documents import Document\r\n\r\ndocs = [\r\n    Document(\r\n        page_content=(\r\n            \"\"\"\r\n            <h1>The first doc</h1>\r\n            <p>Here's the first document.</p>\r\n            \"\"\"\r\n        ),\r\n        metadata={\"source\": \"doc1\"}\r\n    ),\r\n    Document(\r\n        page_content=(\r\n            \"\"\"\r\n            <h1>The second doc</h1>\r\n            <p>Here's the second document.</p>\r\n            \"\"\"\r\n        ),\r\n        metadata={\"source\": \"doc2\"}\r\n    )\r\n]\r\n\r\nmd = MarkdownifyTransformer()\r\nmd_docs = await md.atransform_documents(docs)\r\nprint(md_docs)\r\n```\r\n\r\nExpected result:\r\n\r\n```\r\n[Document(metadata={'source': 'doc1'}, page_content=\"# The first doc\\n\\nHere's the first document.\"), Document(metadata={'source': 'doc2'}, page_content=\"# The second doc\\n\\nHere's the second document.\")]\r\n```\r\n\r\nor, when formatted:\r\n\r\n```\r\n[\r\n    Document(\r\n        metadata={'source': 'doc1'},\r\n        page_content=\"# The first doc\\n\\nHere's the first document.\"\r\n    ),\r\n    Document(\r\n        metadata={'source': 'doc2'},\r\n        page_content=\"# The second doc\\n\\nHere's the second document.\"\r\n    )\r\n]\r\n```\r\n\r\nActual result:\r\n\r\n```\r\nNotImplementedError\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/ryan/langchain_test/.venv/lib/python3.11/site-packages/marimo/_runtime/executor.py\", line 131, in execute_cell_async\r\n    await eval(cell.body, glbls)\r\n  Cell marimo://langchain_markdownify_async.py#cell=cell-0\r\n, line 26, in <module>\r\n    md_docs = await md.atransform_documents(docs)\r\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/ryan/langchain_test/.venv/lib/python3.11/site-packages/langchain_community/document_transformers/markdownify.py\", line 83, in atransform_documents\r\n    raise NotImplementedError\r\nNotImplementedError\r\n```\r\n\r\n## Recommended resolution\r\nI implemented the `atransform_documents()` method [here](https://github.com/rparkr/langchain/blob/rparkr/async-markdownify-transform/libs/community/langchain_community/document_transformers/markdownify.py#L107-L122) and would be happy to contribute a PR.\r\n\r\n### Error Message and Stack Trace (if applicable)\r\n\r\n```\r\nNotImplementedError\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/ryan/langchain_test/.venv/lib/python3.11/site-packages/marimo/_runtime/executor.py\", line 131, in execute_cell_async\r\n    await eval(cell.body, glbls)\r\n  Cell marimo://langchain_markdownify_async.py#cell=cell-0\r\n, line 26, in <module>\r\n    md_docs = await md.atransform_documents(docs)\r\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/ryan/langchain_test/.venv/lib/python3.11/site-packages/langchain_community/document_transformers/markdownify.py\", line 83, in atransform_documents\r\n    raise NotImplementedError\r\nNotImplementedError\r\n```\r\n\r\n### Description\r\n\r\nThe `MarkdownifyTransformer` class in `langchain_community.document_transformers` has not yet implemented the `atransform_documents` method.\r\n\r\nI would like to use that method within an application that uses asynchronous methods for document loading, embedding generation, and semantic search.\r\n\r\nI created an example implementation [here](https://github.com/rparkr/langchain/blob/rparkr/async-markdownify-transform/libs/community/langchain_community/document_transformers/markdownify.py#L107-L122) and would be happy to contribute a PR with the changes.\r\n\r\n### System Info\r\n\r\nSystem Information\r\n------------------\r\n> OS:  Linux\r\n> OS Version:  #1 SMP PREEMPT_DYNAMIC Sun Oct 20 15:04:22 UTC 2024\r\n> Python Version:  3.11.10 (main, Sep  9 2024, 22:11:19) [Clang 18.1.8 ]\r\n\r\nPackage Information\r\n-------------------\r\n> langchain_core: 0.3.15\r\n> langchain: 0.3.7\r\n> langchain_community: 0.3.5\r\n> langsmith: 0.1.139\r\n> langchain_text_splitters: 0.3.2\r\n\r\nOptional packages not installed\r\n-------------------------------\r\n> langgraph\r\n> langserve\r\n\r\nOther Dependencies\r\n------------------\r\n> aiohttp: 3.10.10\r\n> async-timeout: Installed. No version info available.\r\n> dataclasses-json: 0.6.7\r\n> httpx: 0.27.2\r\n> httpx-sse: 0.4.0\r\n> jsonpatch: 1.33\r\n> numpy: 1.26.4\r\n> orjson: 3.10.11\r\n> packaging: 24.1\r\n> pydantic: 2.9.2\r\n> pydantic-settings: 2.6.1\r\n> PyYAML: 6.0.2\r\n> requests: 2.32.3\r\n> requests-toolbelt: 1.0.0\r\n> SQLAlchemy: 2.0.35\r\n> tenacity: 9.0.0\r\n> typing-extensions: 4.12.2",
    "state": "closed",
    "created_at": "2024-11-03T02:09:31+00:00",
    "closed_at": "2024-12-13T22:32:24+00:00",
    "updated_at": "2024-12-13T22:32:24+00:00",
    "author": "rparkr",
    "author_type": "User",
    "comments_count": 0,
    "reactions_count": 0,
    "labels": "ðŸ¤–:bug",
    "milestone": null,
    "closed_by": "efriis",
    "resolution_time_hours": 980.3813888888889,
    "first_comments": [],
    "url": "https://github.com/langchain-ai/langchain/issues/27865"
  },
  {
    "repository": "langchain-ai/langchain",
    "issue_number": 27831,
    "title": "langchain_ollama.embeddings.OllamaEmbeddings throws \"invalid input type\" error with ChromaDB ",
    "body": "### Checked other resources\r\n\r\n- [X] I added a very descriptive title to this issue.\r\n- [X] I searched the LangChain documentation with the integrated search.\r\n- [X] I used the GitHub search to find a similar question and didn't find it.\r\n- [X] I am sure that this is a bug in LangChain rather than my code.\r\n- [X] The bug is not resolved by updating to the latest stable version of LangChain (or the specific integration package).\r\n\r\n### Example Code\r\n\r\n``` python\r\nfrom langchain_ollama import OllamaLLM\r\nfrom langchain_core.output_parsers import StrOutputParser\r\n\r\nfrom langchain_chroma import Chroma as LangchainChroma\r\nfrom langchain_ollama.embeddings import OllamaEmbeddings\r\nembed_model = OllamaEmbeddings(model='mxbai-embed-large')\r\nvector_store = LangchainChroma(collection_name = 'some-collection, \r\n                               embedding_function = embed_model)\r\nllm = OllamaLLM(model='llama3')\r\nchain = (\r\n    {\r\n        \"docs\": vector_store.as_retriever()\r\n        \"question\": RunnablePassthrough(),\r\n    }\r\n    | SOME_PROMPT\r\n    | llm\r\n    | StrOutputParser()\r\n)\r\n```\r\n\r\n### Error Message and Stack Trace (if applicable)\r\n\r\nTraceback (most recent call last):\r\n  File \"/path/lib/python3.10/site-packages/chainlit/utils.py\", line 44, in wrapper\r\n    return await user_function(**params_values)\r\n  File \"/path/lib/python3.10/site-packages/chainlit/callbacks.py\", line 118, in with_parent_id\r\n    await func(message)\r\n  File \"/path/rag_poc/main.py\", line 25, in on_message\r\n    async for chunk in runnable.astream(\r\n  File \"/path/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 5537, in astream\r\n    async for item in self.bound.astream(\r\n  File \"/path/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 3432, in astream\r\n    async for chunk in self.atransform(input_aiter(), config, **kwargs):\r\n  File \"/path/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 3415, in atransform\r\n    async for chunk in self._atransform_stream_with_config(\r\n  File \"/path/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 2308, in _atransform_stream_with_config\r\n    chunk = cast(Output, await py_anext(iterator))\r\n  File \"/path/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 3385, in _atransform\r\n    async for output in final_pipeline:\r\n  File \"/path/lib/python3.10/site-packages/langchain_core/output_parsers/transform.py\", line 84, in atransform\r\n    async for chunk in self._atransform_stream_with_config(\r\n  File \"/path/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 2261, in _atransform_stream_with_config\r\n    final_input: Optional[Input] = await py_anext(input_for_tracing, None)\r\n  File \"/path/lib/python3.10/site-packages/langchain_core/utils/aiter.py\", line 76, in anext_impl\r\n    return await __anext__(iterator)\r\n  File \"/path/lib/python3.10/site-packages/langchain_core/utils/aiter.py\", line 125, in tee_peer\r\n    item = await iterator.__anext__()\r\n  File \"/path/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 1455, in atransform\r\n    async for ichunk in input:\r\n  File \"/path/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 1455, in atransform\r\n    async for ichunk in input:\r\n  File \"/path/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 3916, in atransform\r\n    async for chunk in self._atransform_stream_with_config(\r\n  File \"/path/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 2308, in _atransform_stream_with_config\r\n    chunk = cast(Output, await py_anext(iterator))\r\n  File \"/path/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 3903, in _atransform\r\n    chunk = AddableDict({step_name: task.result()})\r\n  File \"/path/.pyenv/versions/3.10.14/lib/python3.10/asyncio/futures.py\", line 201, in result\r\n    raise self._exception.with_traceback(self._exception_tb)\r\n  File \"/path/.pyenv/versions/3.10.14/lib/python3.10/asyncio/tasks.py\", line 232, in __step\r\n    result = coro.send(None)\r\n  File \"/path/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 3886, in get_next_chunk\r\n    return await py_anext(generator)\r\n  File \"/path/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 3415, in atransform\r\n    async for chunk in self._atransform_stream_with_config(\r\n  File \"/path/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 2308, in _atransform_stream_with_config\r\n    chunk = cast(Output, await py_anext(iterator))\r\n  File \"/path/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 3385, in _atransform\r\n    async for output in final_pipeline:\r\n  File \"/path/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 4943, in atransform\r\n    async for output in self._atransform_stream_with_config(\r\n  File \"/path/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 2261, in _atransform_stream_with_config\r\n    final_input: Optional[Input] = await py_anext(input_for_tracing, None)\r\n  File \"/path/lib/python3.10/site-packages/langchain_core/utils/aiter.py\", line 76, in anext_impl\r\n    return await __anext__(iterator)\r\n  File \"/path/lib/python3.10/site-packages/langchain_core/utils/aiter.py\", line 125, in tee_peer\r\n    item = await iterator.__anext__()\r\n  File \"/path/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 1473, in atransform\r\n    async for output in self.astream(final, config, **kwargs):\r\n  File \"/path/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 1018, in astream\r\n    yield await self.ainvoke(input, config, **kwargs)\r\n  File \"/path/lib/python3.10/site-packages/langchain_core/retrievers.py\", line 317, in ainvoke\r\n    raise e\r\n  File \"/path/lib/python3.10/site-packages/langchain_core/retrievers.py\", line 310, in ainvoke\r\n    result = await self._aget_relevant_documents(\r\n  File \"/path/lib/python3.10/site-packages/langchain_core/vectorstores/base.py\", line 1101, in _aget_relevant_documents\r\n    docs = await self.vectorstore.asimilarity_search(\r\n  File \"/path/lib/python3.10/site-packages/langchain_core/vectorstores/base.py\", line 651, in asimilarity_search\r\n    return await run_in_executor(None, self.similarity_search, query, k=k, **kwargs)\r\n  File \"/path/lib/python3.10/site-packages/langchain_core/runnables/config.py\", line 588, in run_in_executor\r\n    return await asyncio.get_running_loop().run_in_executor(\r\n  File \"/path/.pyenv/versions/3.10.14/lib/python3.10/asyncio/futures.py\", line 285, in __await__\r\n    yield self  # This tells Task to wait for completion.\r\n  File \"/path/.pyenv/versions/3.10.14/lib/python3.10/asyncio/tasks.py\", line 304, in __wakeup\r\n    future.result()\r\n  File \"/path/.pyenv/versions/3.10.14/lib/python3.10/asyncio/futures.py\", line 201, in result\r\n    raise self._exception.with_traceback(self._exception_tb)\r\n  File \"/path/.pyenv/versions/3.10.14/lib/python3.10/concurrent/futures/thread.py\", line 58, in run\r\n    result = self.fn(*self.args, **self.kwargs)\r\n  File \"/path/lib/python3.10/site-packages/langchain_core/runnables/config.py\", line 579, in wrapper\r\n    return func(*args, **kwargs)\r\n  File \"/path/lib/python3.10/site-packages/langchain_chroma/vectorstores.py\", line 582, in similarity_search\r\n    docs_and_scores = self.similarity_search_with_score(\r\n  File \"/path/lib/python3.10/site-packages/langchain_chroma/vectorstores.py\", line 679, in similarity_search_with_score\r\n    query_embedding = self._embedding_function.embed_query(query)\r\n  File \"/path/lib/python3.10/site-packages/langchain_ollama/embeddings.py\", line 164, in embed_query\r\n    return self.embed_documents([text])[0]\r\n  File \"/path/lib/python3.10/site-packages/langchain_ollama/embeddings.py\", line 159, in embed_documents\r\n    embedded_docs = self._client.embed(self.model, texts)[\"embeddings\"]\r\n  File \"/path/lib/python3.10/site-packages/ollama/_client.py\", line 262, in embed\r\n    return self._request(\r\n  File \"/path/lib/python3.10/site-packages/ollama/_client.py\", line 75, in _request\r\n    raise ResponseError(e.response.text, e.response.status_code) from None\r\nollama._types.ResponseError: invalid input type\r\n\r\n### Description\r\n\r\n```python\r\nfrom langchain_ollama.embeddings import OllamaEmbeddings\r\nfrom langchain_community.embeddings import OllamaEmbeddings\r\n```\r\n\r\nIt seems like the newer version of `OllamaEmbeddings` have issues with ChromaDB - throws exception. Swapping to the older version continues to work\r\n\r\nDoing some digging i found out that, with the same code but swapping just the embedding class from legacy to new, the submitted api to Ollama's `/api/embed` is different:\r\n\r\nwith: `langchain_community.embeddings.OllamaEmbeddings` the legacy code, you get the following payload:\r\n\r\n```\r\n{'json': {'model': 'mxbai-embed-large', \r\n          'prompt': \"query: {'input': 'What is OTEL SIP, what are its features and benefits?'}\", \r\n          'options': {'mirostat': None, 'mirostat_eta': None, 'mirostat_tau': None, 'num_ctx': None, 'num_gpu': None, 'num_thread': None, 'repeat_last_n': None, 'repeat_penalty': None, 'temperature': None, 'stop': None, 'tfs_z': None, 'top_k': None, 'top_p': None}}}\r\n```\r\n-> this works fine\r\n\r\nWith `langchain_ollama.embeddings.OllamaEmbeddings` - the newer one, you get \r\n\r\n```\r\n{'json': {'input': [{'input': 'Some question'}],\r\n          'keep_alive': None,\r\n          'model': 'mxbai-embed-large',\r\n          'options': {},\r\n          'truncate': True}}\r\n```\r\n-> this blows up\r\nnotice the different in `'input'` key vs `'prompt'`\r\n\r\n\r\n### System Info\r\n\r\nlangchain                                0.3.6\r\nlangchain-chroma                         0.1.4\r\nlangchain-cli                            0.0.24\r\nlangchain-community                      0.3.2\r\nlangchain-core                           0.3.15\r\nlangchain-ollama                         0.2.0\r\nlangchain-text-splitters                 0.3.0",
    "state": "closed",
    "created_at": "2024-11-01T19:17:46+00:00",
    "closed_at": "2024-11-04T18:43:40+00:00",
    "updated_at": "2024-11-04T18:43:40+00:00",
    "author": "simingy",
    "author_type": "User",
    "comments_count": 2,
    "reactions_count": 0,
    "labels": "â±­: vector store",
    "milestone": null,
    "closed_by": "simingy",
    "resolution_time_hours": 71.43166666666667,
    "first_comments": [
      {
        "author": "keenborder786",
        "created_at": "2024-11-01T21:16:16+00:00",
        "body": "I guess you are passing a dict as your input. Please see the code below, that might help (I tested it and it works):\r\n```python\r\n\r\nfrom langchain_chroma import Chroma as LangchainChroma\r\nfrom langchain_ollama.embeddings import OllamaEmbeddings\r\nfrom langchain_ollama.chat_models import ChatOllama\r\nfrom langchain_core.runnables import RunnablePassthrough\r\nfrom langchain_core.output_parsers import StrOutputParser\r\nfrom langchain_core.prompts import PromptTemplate\r\nembed_model = OllamaEmbeddings(model='mxbai-embed-large')\r\nvector_store = LangchainChroma.from_texts([\"Harrison Lives in New York\"], embedding=embed_model)\r\nllm = ChatOllama(model='llama3')\r\nSOME_PROMPT = PromptTemplate.from_template(\"\"\"\r\nAnswer the following question: {question}\r\nusing the given context:\r\n{docs}\r\n\"\"\")\r\nchain = (\r\n    {\"docs\": vector_store.as_retriever(), \"question\": RunnablePassthrough()}\r\n    | SOME_PROMPT\r\n    | llm\r\n    | StrOutputParser()\r\n)\r\n\r\n```"
      },
      {
        "author": "simingy",
        "created_at": "2024-11-04T18:43:40+00:00",
        "body": "thank you for the help\r\nit seems like the dict as input _used_ to work. Strange."
      }
    ],
    "url": "https://github.com/langchain-ai/langchain/issues/27831"
  },
  {
    "repository": "langchain-ai/langchain",
    "issue_number": 27822,
    "title": "Invalid download path for Grit Cli in langchain-cli",
    "body": "### Checked other resources\n\n- [X] I added a very descriptive title to this issue.\n- [X] I searched the LangChain documentation with the integrated search.\n- [X] I used the GitHub search to find a similar question and didn't find it.\n- [X] I am sure that this is a bug in LangChain rather than my code.\n- [X] The bug is not resolved by updating to the latest stable version of LangChain (or the specific integration package).\n\n### Example Code\n\n`langchain-cli migrate --diff`\r\nfor the rest see: [https://github.com/langchain-ai/langchain/issues/26740]\n\n### Error Message and Stack Trace (if applicable)\n\n_No response_\n\n### Description\n\nJust a new issue because I cannot reopen a closed one myself. It persists and is fixable by upgrading the langchain cli to grit version 0.2.0:\r\nfor the rest see: [https://github.com/langchain-ai/langchain/issues/26740]\n\n### System Info\n\n```\r\naiohappyeyeballs==2.4.3\r\naiohttp==3.10.10\r\naiosignal==1.3.1\r\nannotated-types==0.7.0\r\nanyio==4.6.2.post1\r\nattr==0.3.2\r\nattrs==24.2.0\r\nBabel==2.12.1\r\ncertifi==2024.8.30\r\ncharset-normalizer==3.4.0\r\nclick==8.1.7\r\ncolorama==0.4.6\r\ndataclasses-json==0.5.14\r\ndecorator==5.1.1\r\ndistro==1.9.0\r\nfastapi==0.115.4\r\nfrozenlist==1.5.0\r\nfuzzywuzzy==0.18.0\r\ngitdb==4.0.11\r\nGitPython==3.1.43\r\ngritql==0.2.0\r\nh11==0.14.0\r\nhttpcore==1.0.6\r\nhttptools==0.6.4\r\nhttpx==0.27.2\r\nhttpx-sse==0.4.0\r\nidna==3.10\r\nitsdangerous==2.2.0\r\nJinja2==3.1.4\r\njiter==0.7.0\r\njsonpatch==1.33\r\njsonpointer==3.0.0\r\nlancedb==0.1.9\r\nlangchain==0.3.6\r\nlangchain-cli==0.0.31\r\nlangchain-community==0.3.4\r\nlangchain-core==0.3.15\r\nlangchain-text-splitters==0.3.1\r\nlangserve==0.3.0\r\nlangsmith==0.1.139\r\nmarkdown-it-py==3.0.0\r\nMarkupSafe==3.0.2\r\nmarshmallow==3.23.0\r\nmdurl==0.1.2\r\nmultidict==6.1.0\r\nmypy-extensions==1.0.0\r\nnumexpr==2.10.1\r\nnumpy==1.26.4\r\nopenai==1.53.0\r\norjson==3.10.10\r\npackaging==24.1\r\npandas==2.2.3\r\npropcache==0.2.0\r\npy==1.11.0\r\npyarrow==18.0.0\r\npydantic==2.9.2\r\npydantic-settings==2.6.0\r\npydantic_core==2.23.4\r\nPygments==2.18.0\r\npylance==0.5.10\r\npython-dateutil==2.8.2\r\npython-dotenv==1.0.1\r\npytz==2024.2\r\nPyYAML==6.0.2\r\nratelimiter==1.2.0.post0\r\nregex==2024.9.11\r\nrequests==2.32.3\r\nrequests-toolbelt==1.0.0\r\nretry==0.9.2\r\nrich==13.9.3\r\nshellingham==1.5.4\r\nsix==1.16.0\r\nsmmap==5.0.1\r\nsniffio==1.3.1\r\nSQLAlchemy==2.0.36\r\nsse-starlette==1.8.2\r\nstarlette==0.41.2\r\ntenacity==8.5.0\r\ntiktoken==0.8.0\r\ntomlkit==0.12.5\r\ntqdm==4.66.6\r\ntyper==0.9.4\r\ntyping-inspect==0.9.0\r\ntyping_extensions==4.12.2\r\ntzdata==2024.2\r\nurllib3==2.2.3\r\nuvicorn==0.23.2\r\nuvloop==0.21.0\r\nwatchfiles==0.24.0\r\nwebsockets==11.0.3\r\nyarl==1.17.1\r\n\r\n```",
    "state": "closed",
    "created_at": "2024-11-01T10:25:52+00:00",
    "closed_at": "2025-01-02T04:02:47+00:00",
    "updated_at": "2025-01-02T04:02:47+00:00",
    "author": "hansvdam",
    "author_type": "User",
    "comments_count": 1,
    "reactions_count": 0,
    "labels": "",
    "milestone": null,
    "closed_by": "efriis",
    "resolution_time_hours": 1481.6152777777777,
    "first_comments": [
      {
        "author": "MicAnt64",
        "created_at": "2024-11-05T12:08:38+00:00",
        "body": "I also came across this issue. I notice on https://github.com/getgrit/gritql/releases that there is no file at https://github.com/getgrit/gritql/releases/latest/download/marzano-aarch64-apple-darwin.tar.gz to pull from. I went to the installer file in my environment <MY_CONDA_ENV_PATH>/lib/python3.11/site-packages/gritql/installer.py, and changed:\r\n\r\n```python\r\n#target_path = target_dir / \"marzano\"\r\ntarget_path = target_dir / \"grit\"\r\n#temp_file = target_dir / \"marzano.tmp\"\r\ntemp_file = target_dir / \"grit.tmp\"\r\n```\r\n\r\nand\r\n\r\n```python\r\n#file_name = f\"marzano-{arch}-{platform}\"\r\nfile_name = f\"grit-{arch}-{platform}\"\r\n```\r\n"
      }
    ],
    "url": "https://github.com/langchain-ai/langchain/issues/27822"
  },
  {
    "repository": "langchain-ai/langchain",
    "issue_number": 27799,
    "title": "Issue with agents and Gemini: \"Please ensure that function response turn comes immediately after a function call turn\"",
    "body": "### Checked other resources\n\n- [X] I added a very descriptive title to this issue.\n- [X] I searched the [LangGraph](https://langchain-ai.github.io/langgraph/)/LangChain documentation with the integrated search.\n- [X] I used the GitHub search to find a similar question and didn't find it.\n- [X] I am sure that this is a bug in LangGraph/LangChain rather than my code.\n- [X] I am sure this is better as an issue [rather than a GitHub discussion](https://github.com/langchain-ai/langgraph/discussions/new/choose), since this is a LangGraph bug and not a design question.\n\n### Example Code\n\n```python\nfrom langchain.agents import tool\r\nfrom langgraph.prebuilt import create_react_agent\r\nfrom langchain_core.messages import AIMessage\r\nfrom langchain_google_genai import ChatGoogleGenerativeAI\r\n\r\n\r\n\r\nmodel = ChatGoogleGenerativeAI(\r\n    model=\"gemini-1.5-pro\",\r\n)\r\n\r\n\r\n# Overwrite model\r\n\r\n@tool\r\ndef get_weather_of(country:str):\r\n    \"\"\"\r\n    Use this tool to get the current weather of a country.\r\n    \"\"\"\r\n\r\n    return (f\"The current weather of the country {country} is sunny.\")\r\n\r\n\r\n@tool\r\ndef search_info_about(person:str):\r\n    \"\"\"\r\n    Use this tool to know where a person won the world cup.\r\n    \"\"\"\r\n\r\n    return (f\"{person} won the world cup in Qatar.\")\r\n\r\n\r\ntools = [\r\n    get_weather_of,\r\n    search_info_about\r\n]\r\n\r\nagent = create_react_agent(\r\n    model,\r\n    tools,\r\n    debug=True\r\n)\r\n\r\ninput_message = f\"\"\"\r\nAnswer the following question using the appropriate tool:\r\n\"\"\"\r\n\r\nagent.invoke({\r\n    \"messages\": [\r\n        (\"human\", input_message + \"\\n\\nWhat is the current weather of the country where Messi won the world cup?\")\r\n    ]\r\n})\r\npython\n```\n\n\n### Error Message and Stack Trace (if applicable)\n\n```shell\nThe autoreload extension is already loaded. To reload it, use:\r\n  %reload_ext autoreload\r\n[-1:checkpoint] State at the end of step -1:\r\n{'messages': []}\r\n[0:tasks] Starting step 0 with 1 task:\r\n- __start__ -> {'messages': [('human',\r\n               '\\n'\r\n               'Answer the following question using the appropriate tool:\\n'\r\n               '\\n'\r\n               '\\n'\r\n               'What is the current weather of the country where Messi won the '\r\n               'world cup?')]}\r\n[0:writes] Finished step 0 with writes to 1 channel:\r\n- messages -> [('human',\r\n  '\\n'\r\n  'Answer the following question using the appropriate tool:\\n'\r\n  '\\n'\r\n  '\\n'\r\n  'What is the current weather of the country where Messi won the world cup?')]\r\n[0:checkpoint] State at the end of step 0:\r\n{'messages': [HumanMessage(content='\\nAnswer the following question using the appropriate tool:\\n\\n\\nWhat is the current weather of the country where Messi won the world cup?', additional_kwargs={}, response_metadata={}, id='9331ab3c-922d-4985-94e3-fb3b411fbc2e')]}\r\n[1:tasks] Starting step 1 with 1 task:\r\n- agent -> {'is_last_step': False,\r\n 'messages': [HumanMessage(content='\\nAnswer the following question using the appropriate tool:\\n\\n\\nWhat is the current weather of the country where Messi won the world cup?', additional_kwargs={}, response_metadata={}, id='9331ab3c-922d-4985-94e3-fb3b411fbc2e')],\r\n 'remaining_steps': 24}\r\n[1:writes] Finished step 1 with writes to 1 channel:\r\n- messages -> [AIMessage(content='', additional_kwargs={'function_call': {'name': 'get_weather_of', 'arguments': '{\"country\": \"Argentina\"}'}}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run-1cb45547-01ac-420e-9720-573e36fad6fc-0', tool_calls=[{'name': 'search_info_about', 'args': {'person': 'Messi'}, 'id': 'b55d68bc-9861-467a-98bc-f3a4a3567700', 'type': 'tool_call'}, {'name': 'get_weather_of', 'args': {'country': 'Argentina'}, 'id': '06cce656-f922-46df-923a-d897978c0118', 'type': 'tool_call'}], usage_metadata={'input_tokens': 120, 'output_tokens': 14, 'total_tokens': 134, 'input_token_details': {'cache_read': 0}})]\r\n[1:checkpoint] State at the end of step 1:\r\n{'messages': [HumanMessage(content='\\nAnswer the following question using the appropriate tool:\\n\\n\\nWhat is the current weather of the country where Messi won the world cup?', additional_kwargs={}, response_metadata={}, id='9331ab3c-922d-4985-94e3-fb3b411fbc2e'),\r\n              AIMessage(content='', additional_kwargs={'function_call': {'name': 'get_weather_of', 'arguments': '{\"country\": \"Argentina\"}'}}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run-1cb45547-01ac-420e-9720-573e36fad6fc-0', tool_calls=[{'name': 'search_info_about', 'args': {'person': 'Messi'}, 'id': 'b55d68bc-9861-467a-98bc-f3a4a3567700', 'type': 'tool_call'}, {'name': 'get_weather_of', 'args': {'country': 'Argentina'}, 'id': '06cce656-f922-46df-923a-d897978c0118', 'type': 'tool_call'}], usage_metadata={'input_tokens': 120, 'output_tokens': 14, 'total_tokens': 134, 'input_token_details': {'cache_read': 0}})]}\r\n[2:tasks] Starting step 2 with 1 task:\r\n- tools -> {'is_last_step': False,\r\n 'messages': [HumanMessage(content='\\nAnswer the following question using the appropriate tool:\\n\\n\\nWhat is the current weather of the country where Messi won the world cup?', additional_kwargs={}, response_metadata={}, id='9331ab3c-922d-4985-94e3-fb3b411fbc2e'),\r\n              AIMessage(content='', additional_kwargs={'function_call': {'name': 'get_weather_of', 'arguments': '{\"country\": \"Argentina\"}'}}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run-1cb45547-01ac-420e-9720-573e36fad6fc-0', tool_calls=[{'name': 'search_info_about', 'args': {'person': 'Messi'}, 'id': 'b55d68bc-9861-467a-98bc-f3a4a3567700', 'type': 'tool_call'}, {'name': 'get_weather_of', 'args': {'country': 'Argentina'}, 'id': '06cce656-f922-46df-923a-d897978c0118', 'type': 'tool_call'}], usage_metadata={'input_tokens': 120, 'output_tokens': 14, 'total_tokens': 134, 'input_token_details': {'cache_read': 0}})],\r\n 'remaining_steps': 23}\r\n[2:writes] Finished step 2 with writes to 1 channel:\r\n- messages -> [ToolMessage(content='Messi won the world cup in Qatar.', name='search_info_about', tool_call_id='b55d68bc-9861-467a-98bc-f3a4a3567700'),\r\n ToolMessage(content='The current weather of the country Argentina is sunny.', name='get_weather_of', tool_call_id='06cce656-f922-46df-923a-d897978c0118')]\r\n[2:checkpoint] State at the end of step 2:\r\n{'messages': [HumanMessage(content='\\nAnswer the following question using the appropriate tool:\\n\\n\\nWhat is the current weather of the country where Messi won the world cup?', additional_kwargs={}, response_metadata={}, id='9331ab3c-922d-4985-94e3-fb3b411fbc2e'),\r\n              AIMessage(content='', additional_kwargs={'function_call': {'name': 'get_weather_of', 'arguments': '{\"country\": \"Argentina\"}'}}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run-1cb45547-01ac-420e-9720-573e36fad6fc-0', tool_calls=[{'name': 'search_info_about', 'args': {'person': 'Messi'}, 'id': 'b55d68bc-9861-467a-98bc-f3a4a3567700', 'type': 'tool_call'}, {'name': 'get_weather_of', 'args': {'country': 'Argentina'}, 'id': '06cce656-f922-46df-923a-d897978c0118', 'type': 'tool_call'}], usage_metadata={'input_tokens': 120, 'output_tokens': 14, 'total_tokens': 134, 'input_token_details': {'cache_read': 0}}),\r\n              ToolMessage(content='Messi won the world cup in Qatar.', name='search_info_about', id='70a658fb-0777-4a0a-a621-5f5e15a194f4', tool_call_id='b55d68bc-9861-467a-98bc-f3a4a3567700'),\r\n              ToolMessage(content='The current weather of the country Argentina is sunny.', name='get_weather_of', id='99e0bf3d-6cd3-4380-ac5f-94f4b138d004', tool_call_id='06cce656-f922-46df-923a-d897978c0118')]}\r\n[3:tasks] Starting step 3 with 1 task:\r\n- agent -> {'is_last_step': False,\r\n 'messages': [HumanMessage(content='\\nAnswer the following question using the appropriate tool:\\n\\n\\nWhat is the current weather of the country where Messi won the world cup?', additional_kwargs={}, response_metadata={}, id='9331ab3c-922d-4985-94e3-fb3b411fbc2e'),\r\n              AIMessage(content='', additional_kwargs={'function_call': {'name': 'get_weather_of', 'arguments': '{\"country\": \"Argentina\"}'}}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run-1cb45547-01ac-420e-9720-573e36fad6fc-0', tool_calls=[{'name': 'search_info_about', 'args': {'person': 'Messi'}, 'id': 'b55d68bc-9861-467a-98bc-f3a4a3567700', 'type': 'tool_call'}, {'name': 'get_weather_of', 'args': {'country': 'Argentina'}, 'id': '06cce656-f922-46df-923a-d897978c0118', 'type': 'tool_call'}], usage_metadata={'input_tokens': 120, 'output_tokens': 14, 'total_tokens': 134, 'input_token_details': {'cache_read': 0}}),\r\n              ToolMessage(content='Messi won the world cup in Qatar.', name='search_info_about', id='70a658fb-0777-4a0a-a621-5f5e15a194f4', tool_call_id='b55d68bc-9861-467a-98bc-f3a4a3567700'),\r\n              ToolMessage(content='The current weather of the country Argentina is sunny.', name='get_weather_of', id='99e0bf3d-6cd3-4380-ac5f-94f4b138d004', tool_call_id='06cce656-f922-46df-923a-d897978c0118')],\r\n 'remaining_steps': 22}\r\n\r\n---------------------------------------------------------------------------\r\nInvalidArgument                           Traceback (most recent call last)\r\nFile ~/miniconda3/envs/lib/python3.11/site-packages/langchain_google_genai/chat_models.py:193, in _chat_with_retry.<locals>._chat_with_retry(**kwargs)\r\n    192 try:\r\n--> 193     return generation_method(**kwargs)\r\n    194 # Do not retry for these errors.\r\n\r\nFile ~/miniconda3/envs/lib/python3.11/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/client.py:830, in GenerativeServiceClient.generate_content(self, request, model, contents, retry, timeout, metadata)\r\n    829 # Send the request.\r\n--> 830 response = rpc(\r\n    831     request,\r\n    832     retry=retry,\r\n    833     timeout=timeout,\r\n    834     metadata=metadata,\r\n    835 )\r\n    837 # Done; return the response.\r\n\r\nFile ~/miniconda3/envs/lib/python3.11/site-packages/google/api_core/gapic_v1/method.py:131, in _GapicCallable.__call__(self, timeout, retry, compression, *args, **kwargs)\r\n    129     kwargs[\"compression\"] = compression\r\n--> 131 return wrapped_func(*args, **kwargs)\r\n\r\nFile ~/miniconda3/envs/lib/python3.11/site-packages/google/api_core/retry/retry_unary.py:293, in Retry.__call__.<locals>.retry_wrapped_func(*args, **kwargs)\r\n    290 sleep_generator = exponential_sleep_generator(\r\n    291     self._initial, self._maximum, multiplier=self._multiplier\r\n    292 )\r\n--> 293 return retry_target(\r\n    294     target,\r\n    295     self._predicate,\r\n    296     sleep_generator,\r\n    297     timeout=self._timeout,\r\n    298     on_error=on_error,\r\n    299 )\r\n\r\nFile ~/miniconda3/envs/lib/python3.11/site-packages/google/api_core/retry/retry_unary.py:153, in retry_target(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\r\n    151 except Exception as exc:\r\n    152     # defer to shared logic for handling errors\r\n--> 153     _retry_error_helper(\r\n    154         exc,\r\n    155         deadline,\r\n    156         sleep,\r\n    157         error_list,\r\n    158         predicate,\r\n    159         on_error,\r\n    160         exception_factory,\r\n    161         timeout,\r\n    162     )\r\n    163     # if exception not raised, sleep before next attempt\r\n\r\nFile ~/miniconda3/envs/lib/python3.11/site-packages/google/api_core/retry/retry_base.py:212, in _retry_error_helper(exc, deadline, next_sleep, error_list, predicate_fn, on_error_fn, exc_factory_fn, original_timeout)\r\n    207     final_exc, source_exc = exc_factory_fn(\r\n    208         error_list,\r\n    209         RetryFailureReason.NON_RETRYABLE_ERROR,\r\n    210         original_timeout,\r\n    211     )\r\n--> 212     raise final_exc from source_exc\r\n    213 if on_error_fn is not None:\r\n\r\nFile ~/miniconda3/envs/lib/python3.11/site-packages/google/api_core/retry/retry_unary.py:144, in retry_target(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\r\n    143 try:\r\n--> 144     result = target()\r\n    145     if inspect.isawaitable(result):\r\n\r\nFile ~/miniconda3/envs/lib/python3.11/site-packages/google/api_core/timeout.py:120, in TimeToDeadlineTimeout.__call__.<locals>.func_with_timeout(*args, **kwargs)\r\n    118     kwargs[\"timeout\"] = max(0, self._timeout - time_since_first_attempt)\r\n--> 120 return func(*args, **kwargs)\r\n\r\nFile ~/miniconda3/envs/lib/python3.11/site-packages/google/api_core/grpc_helpers.py:78, in _wrap_unary_errors.<locals>.error_remapped_callable(*args, **kwargs)\r\n     77 except grpc.RpcError as exc:\r\n---> 78     raise exceptions.from_grpc_error(exc) from exc\r\n\r\nInvalidArgument: 400 Please ensure that function response turn comes immediately after a function call turn. And the number of function response parts should be equal to number of function call parts of the function call turn.\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nChatGoogleGenerativeAIError               Traceback (most recent call last)\r\nCell In[3], line 50\r\n     40 agent = create_react_agent(\r\n     41     model,\r\n     42     tools,\r\n     43     debug=True\r\n     44 )\r\n     46 input_message = f\"\"\"\r\n     47 Answer the following question using the appropriate tool:\r\n     48 \"\"\"\r\n---> 50 agent.invoke({\r\n     51     \"messages\": [\r\n     52         (\"human\", input_message + \"\\n\\nWhat is the current weather of the country where Messi won the world cup?\")\r\n     53     ]\r\n     54 })\r\n\r\nFile ~/miniconda3/envs/lib/python3.11/site-packages/langgraph/pregel/__init__.py:1586, in Pregel.invoke(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, **kwargs)\r\n   1584 else:\r\n   1585     chunks = []\r\n-> 1586 for chunk in self.stream(\r\n   1587     input,\r\n   1588     config,\r\n   1589     stream_mode=stream_mode,\r\n   1590     output_keys=output_keys,\r\n   1591     interrupt_before=interrupt_before,\r\n   1592     interrupt_after=interrupt_after,\r\n   1593     debug=debug,\r\n   1594     **kwargs,\r\n   1595 ):\r\n   1596     if stream_mode == \"values\":\r\n   1597         latest = chunk\r\n\r\nFile ~/miniconda3/envs/lib/python3.11/site-packages/langgraph/pregel/__init__.py:1315, in Pregel.stream(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, subgraphs)\r\n   1304     # Similarly to Bulk Synchronous Parallel / Pregel model\r\n   1305     # computation proceeds in steps, while there are channel updates\r\n   1306     # channel updates from step N are only visible in step N+1\r\n   1307     # channels are guaranteed to be immutable for the duration of the step,\r\n   1308     # with channel updates applied only at the transition between steps\r\n   1309     while loop.tick(\r\n   1310         input_keys=self.input_channels,\r\n   1311         interrupt_before=interrupt_before_,\r\n   1312         interrupt_after=interrupt_after_,\r\n   1313         manager=run_manager,\r\n   1314     ):\r\n-> 1315         for _ in runner.tick(\r\n   1316             loop.tasks.values(),\r\n   1317             timeout=self.step_timeout,\r\n   1318             retry_policy=self.retry_policy,\r\n   1319             get_waiter=get_waiter,\r\n   1320         ):\r\n   1321             # emit output\r\n   1322             yield from output()\r\n   1323 # emit output\r\n\r\nFile ~/miniconda3/envs/lib/python3.11/site-packages/langgraph/pregel/runner.py:56, in PregelRunner.tick(self, tasks, reraise, timeout, retry_policy, get_waiter)\r\n     54 t = tasks[0]\r\n     55 try:\r\n---> 56     run_with_retry(t, retry_policy)\r\n     57     self.commit(t, None)\r\n     58 except Exception as exc:\r\n\r\nFile ~/miniconda3/envs/lib/python3.11/site-packages/langgraph/pregel/retry.py:29, in run_with_retry(task, retry_policy)\r\n     27 task.writes.clear()\r\n     28 # run the task\r\n---> 29 task.proc.invoke(task.input, config)\r\n     30 # if successful, end\r\n     31 break\r\n\r\nFile ~/miniconda3/envs/lib/python3.11/site-packages/langgraph/utils/runnable.py:410, in RunnableSeq.invoke(self, input, config, **kwargs)\r\n    408 context.run(_set_config_context, config)\r\n    409 if i == 0:\r\n--> 410     input = context.run(step.invoke, input, config, **kwargs)\r\n    411 else:\r\n    412     input = context.run(step.invoke, input, config)\r\n\r\nFile ~/miniconda3/envs/lib/python3.11/site-packages/langgraph/utils/runnable.py:176, in RunnableCallable.invoke(self, input, config, **kwargs)\r\n    174     context = copy_context()\r\n    175     context.run(_set_config_context, child_config)\r\n--> 176     ret = context.run(self.func, input, **kwargs)\r\n    177 except BaseException as e:\r\n    178     run_manager.on_chain_error(e)\r\n\r\nFile ~/miniconda3/envs/lib/python3.11/site-packages/langgraph/prebuilt/chat_agent_executor.py:533, in create_react_agent.<locals>.call_model(state, config)\r\n    532 def call_model(state: AgentState, config: RunnableConfig) -> AgentState:\r\n--> 533     response = model_runnable.invoke(state, config)\r\n    534     has_tool_calls = isinstance(response, AIMessage) and response.tool_calls\r\n    535     all_tools_return_direct = (\r\n    536         all(call[\"name\"] in should_return_direct for call in response.tool_calls)\r\n    537         if isinstance(response, AIMessage)\r\n    538         else False\r\n    539     )\r\n\r\nFile ~/miniconda3/envs/lib/python3.11/site-packages/langchain_core/runnables/base.py:3024, in RunnableSequence.invoke(self, input, config, **kwargs)\r\n   3022             input = context.run(step.invoke, input, config, **kwargs)\r\n   3023         else:\r\n-> 3024             input = context.run(step.invoke, input, config)\r\n   3025 # finish the root run\r\n   3026 except BaseException as e:\r\n\r\nFile ~/miniconda3/envs/lib/python3.11/site-packages/langchain_core/runnables/base.py:5354, in RunnableBindingBase.invoke(self, input, config, **kwargs)\r\n   5348 def invoke(\r\n   5349     self,\r\n   5350     input: Input,\r\n   5351     config: Optional[RunnableConfig] = None,\r\n   5352     **kwargs: Optional[Any],\r\n   5353 ) -> Output:\r\n-> 5354     return self.bound.invoke(\r\n   5355         input,\r\n   5356         self._merge_configs(config),\r\n   5357         **{**self.kwargs, **kwargs},\r\n   5358     )\r\n\r\nFile ~/miniconda3/envs/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:286, in BaseChatModel.invoke(self, input, config, stop, **kwargs)\r\n    275 def invoke(\r\n    276     self,\r\n    277     input: LanguageModelInput,\r\n   (...)\r\n    281     **kwargs: Any,\r\n    282 ) -> BaseMessage:\r\n    283     config = ensure_config(config)\r\n    284     return cast(\r\n    285         ChatGeneration,\r\n--> 286         self.generate_prompt(\r\n    287             [self._convert_input(input)],\r\n    288             stop=stop,\r\n    289             callbacks=config.get(\"callbacks\"),\r\n    290             tags=config.get(\"tags\"),\r\n    291             metadata=config.get(\"metadata\"),\r\n    292             run_name=config.get(\"run_name\"),\r\n    293             run_id=config.pop(\"run_id\", None),\r\n    294             **kwargs,\r\n    295         ).generations[0][0],\r\n    296     ).message\r\n\r\nFile ~/miniconda3/envs/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:786, in BaseChatModel.generate_prompt(self, prompts, stop, callbacks, **kwargs)\r\n    778 def generate_prompt(\r\n    779     self,\r\n    780     prompts: list[PromptValue],\r\n   (...)\r\n    783     **kwargs: Any,\r\n    784 ) -> LLMResult:\r\n    785     prompt_messages = [p.to_messages() for p in prompts]\r\n--> 786     return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\r\n\r\nFile ~/miniconda3/envs/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:643, in BaseChatModel.generate(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\r\n    641         if run_managers:\r\n    642             run_managers[i].on_llm_error(e, response=LLMResult(generations=[]))\r\n--> 643         raise e\r\n    644 flattened_outputs = [\r\n    645     LLMResult(generations=[res.generations], llm_output=res.llm_output)  # type: ignore[list-item]\r\n    646     for res in results\r\n    647 ]\r\n    648 llm_output = self._combine_llm_outputs([res.llm_output for res in results])\r\n\r\nFile ~/miniconda3/envs/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:633, in BaseChatModel.generate(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\r\n    630 for i, m in enumerate(messages):\r\n    631     try:\r\n    632         results.append(\r\n--> 633             self._generate_with_cache(\r\n    634                 m,\r\n    635                 stop=stop,\r\n    636                 run_manager=run_managers[i] if run_managers else None,\r\n    637                 **kwargs,\r\n    638             )\r\n    639         )\r\n    640     except BaseException as e:\r\n    641         if run_managers:\r\n\r\nFile ~/miniconda3/envs/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:851, in BaseChatModel._generate_with_cache(self, messages, stop, run_manager, **kwargs)\r\n    849 else:\r\n    850     if inspect.signature(self._generate).parameters.get(\"run_manager\"):\r\n--> 851         result = self._generate(\r\n    852             messages, stop=stop, run_manager=run_manager, **kwargs\r\n    853         )\r\n    854     else:\r\n    855         result = self._generate(messages, stop=stop, **kwargs)\r\n\r\nFile ~/miniconda3/envs/lib/python3.11/site-packages/langchain_google_genai/chat_models.py:1021, in ChatGoogleGenerativeAI._generate(self, messages, stop, run_manager, tools, functions, safety_settings, tool_config, generation_config, cached_content, tool_choice, **kwargs)\r\n    995 def _generate(\r\n    996     self,\r\n    997     messages: List[BaseMessage],\r\n   (...)\r\n   1008     **kwargs: Any,\r\n   1009 ) -> ChatResult:\r\n   1010     request = self._prepare_request(\r\n   1011         messages,\r\n   1012         stop=stop,\r\n   (...)\r\n   1019         tool_choice=tool_choice,\r\n   1020     )\r\n-> 1021     response: GenerateContentResponse = _chat_with_retry(\r\n   1022         request=request,\r\n   1023         **kwargs,\r\n   1024         generation_method=self.client.generate_content,\r\n   1025         metadata=self.default_metadata,\r\n   1026     )\r\n   1027     return _response_to_result(response)\r\n\r\nFile ~/miniconda3/envs/lib/python3.11/site-packages/langchain_google_genai/chat_models.py:211, in _chat_with_retry(generation_method, **kwargs)\r\n    208     except Exception as e:\r\n    209         raise e\r\n--> 211 return _chat_with_retry(**kwargs)\r\n\r\nFile ~/miniconda3/envs/lib/python3.11/site-packages/tenacity/__init__.py:336, in BaseRetrying.wraps.<locals>.wrapped_f(*args, **kw)\r\n    334 copy = self.copy()\r\n    335 wrapped_f.statistics = copy.statistics  # type: ignore[attr-defined]\r\n--> 336 return copy(f, *args, **kw)\r\n\r\nFile ~/miniconda3/envs/lib/python3.11/site-packages/tenacity/__init__.py:475, in Retrying.__call__(self, fn, *args, **kwargs)\r\n    473 retry_state = RetryCallState(retry_object=self, fn=fn, args=args, kwargs=kwargs)\r\n    474 while True:\r\n--> 475     do = self.iter(retry_state=retry_state)\r\n    476     if isinstance(do, DoAttempt):\r\n    477         try:\r\n\r\nFile ~/miniconda3/envs/lib/python3.11/site-packages/tenacity/__init__.py:376, in BaseRetrying.iter(self, retry_state)\r\n    374 result = None\r\n    375 for action in self.iter_state.actions:\r\n--> 376     result = action(retry_state)\r\n    377 return result\r\n\r\nFile ~/miniconda3/envs/lib/python3.11/site-packages/tenacity/__init__.py:398, in BaseRetrying._post_retry_check_actions.<locals>.<lambda>(rs)\r\n    396 def _post_retry_check_actions(self, retry_state: \"RetryCallState\") -> None:\r\n    397     if not (self.iter_state.is_explicit_retry or self.iter_state.retry_run_result):\r\n--> 398         self._add_action_func(lambda rs: rs.outcome.result())\r\n    399         return\r\n    401     if self.after is not None:\r\n\r\nFile ~/miniconda3/envs/lib/python3.11/concurrent/futures/_base.py:449, in Future.result(self, timeout)\r\n    447     raise CancelledError()\r\n    448 elif self._state == FINISHED:\r\n--> 449     return self.__get_result()\r\n    451 self._condition.wait(timeout)\r\n    453 if self._state in [CANCELLED, CANCELLED_AND_NOTIFIED]:\r\n\r\nFile ~/miniconda3/envs/lib/python3.11/concurrent/futures/_base.py:401, in Future.__get_result(self)\r\n    399 if self._exception:\r\n    400     try:\r\n--> 401         raise self._exception\r\n    402     finally:\r\n    403         # Break a reference cycle with the exception in self._exception\r\n    404         self = None\r\n\r\nFile ~/miniconda3/envs/lib/python3.11/site-packages/tenacity/__init__.py:478, in Retrying.__call__(self, fn, *args, **kwargs)\r\n    476 if isinstance(do, DoAttempt):\r\n    477     try:\r\n--> 478         result = fn(*args, **kwargs)\r\n    479     except BaseException:  # noqa: B902\r\n    480         retry_state.set_exception(sys.exc_info())  # type: ignore[arg-type]\r\n\r\nFile ~/miniconda3/envs/lib/python3.11/site-packages/langchain_google_genai/chat_models.py:205, in _chat_with_retry.<locals>._chat_with_retry(**kwargs)\r\n    202         raise ValueError(error_msg)\r\n    204 except google.api_core.exceptions.InvalidArgument as e:\r\n--> 205     raise ChatGoogleGenerativeAIError(\r\n    206         f\"Invalid argument provided to Gemini: {e}\"\r\n    207     ) from e\r\n    208 except Exception as e:\r\n    209     raise e\r\n\r\nChatGoogleGenerativeAIError: Invalid argument provided to Gemini: 400 Please ensure that function response turn comes immediately after a function call turn. And the number of function response parts should be equal to number of function call parts of the function call turn.\n```\n\n\n### Description\n\nWhile trying to do a simple agent, from times to times two `ToolMessage`s get together and seems to annoy Gemini. Since yesterday, the above code is not always working. \n\n### System Info\n\nPlatform: WSL\r\nPython: 3.11.10\r\n\r\n\r\nlangchain==0.3.5\r\nlangchain-core==0.3.13\r\nlangchain-google-genai==2.0.3\r\nlangchain-text-splitters==0.3.1\r\nlanggraph==0.2.39\r\nlanggraph-checkpoint==2.0.2\r\nlanggraph-checkpoint-sqlite==2.0.1\r\nlanggraph-sdk==0.1.35\r\nlangsmith==0.1.137",
    "state": "closed",
    "created_at": "2024-10-31T16:21:29+00:00",
    "closed_at": "2024-11-19T16:37:19+00:00",
    "updated_at": "2024-11-19T16:37:20+00:00",
    "author": "finiteautomata",
    "author_type": "User",
    "comments_count": 1,
    "reactions_count": 0,
    "labels": "ðŸ¤–:bug",
    "milestone": null,
    "closed_by": "vbarda",
    "resolution_time_hours": 456.2638888888889,
    "first_comments": [
      {
        "author": "vbarda",
        "created_at": "2024-11-19T16:37:20+00:00",
        "body": "This should resolved in langchain-google-genai == 2.0.5 -- please let me know if you run into this issue again!"
      }
    ],
    "url": "https://github.com/langchain-ai/langchain/issues/27799"
  },
  {
    "repository": "langchain-ai/langchain",
    "issue_number": 27781,
    "title": "[BUG] Inconsistent results with `NLTKTextSplitter`'s `add_start_index=True`",
    "body": "### Checked other resources\n\n- [X] I added a very descriptive title to this issue.\n- [X] I searched the LangChain documentation with the integrated search.\n- [X] I used the GitHub search to find a similar question and didn't find it.\n- [X] I am sure that this is a bug in LangChain rather than my code.\n- [X] The bug is not resolved by updating to the latest stable version of LangChain (or the specific integration package).\n\n### Example Code\n\n```python\r\nfrom langchain_text_splitters import NLTKTextSplitter\r\nfrom langchain.docstore.document import Document\r\n\r\n\r\ntext_splitter = NLTKTextSplitter(\r\n    chunk_size=500,\r\n    chunk_overlap=0,\r\n    add_start_index=True,\r\n    language='english',\r\n    separator=\"\",\r\n)\r\n\r\ntxt = \"\"\" Report on operations of the TIM Group Financial and Operating Highlights of the BUs of the TIM Group Brazil Business Unit 29  Additionally, in October 2022, the 7th Business Court of the Judicial District of Rio de Janeiro handed down a preliminary decision, determining the deposit in court by the Buyers of approximately 1.53 billion reais â€“ of which approximately 670 million reais by TIM S.A. â€“ in an account linked to the court-ordered reorganization process of Oi, where it will be safeguarded until a later decision by the arbitration court.  Further details are provided in the Note â€œDisputes and pending legal actions, other information, commitments and guaranteesâ€ to the Consolidated Financial Statements at December 31, 2022 of the TIM Group. Revenues Revenues for 2022 of the Brazil Business Unit (TIM Brasil group) amounted to 21,531 million reais (18,058 million reais in 2021, +19,2%). Excluding revenues from the mobile business of the Oi Group (Cozani, acquired on April 20, 2022) revenues for the year 2022 are 20,759 million reais.  The acceleration has been determined by service revenues (20,829 million reais vs 17,497 million reais in 2021, +19.0%) with mobile service revenues growing 19.8% on 2021. This performance is mainly related to the continuous recovery of the pre-paid and post-paid segments. Revenues from fixed services have grown by 7.6% compared to 2021, determined above all by the growth rate of TIM Live. Revenues from product sales totaled 702 million reais (561 million reais in 2021). Revenues in the fourth quarter of 2022 totaled 5,825 million reais, increased by 21.4% on the fourth quarter of 2021 (4,799 million reais). Excluding the revenues of Cozani, revenues of the fourth quarter of 2022 grew by 1,204 million reais (25.1%).  Mobile ARPU for 2022 was 26.1 reais (26.4 reais in 2021). The reduction is connected with the acquisition of the Oi Group customer base. Total mobile lines in place at December 31, 2022 amounted to 62.5 million, +10.4 million compared to December 31, 2021 (52.1 million), mainly following the acquisition of the Cozani customer base. This overall increase came from the pre-paid segment (+6.0 million), and the post-paid segment (+4.4 million) and connected with the acquisition of the Oi Group customer base. Post-paid customers represented 43.6% of the customer base as of December 31, 2022 (43.9% at December 2021). The TIM Live Broadband business recorded net positive growth on December 31, 2022 in the customer base of 31 thousand users compared to December 31, 2021. In addition, the customer base continues to be concentrated on high-speed connections, with more than 50% exceeding 100Mbps. EBITDA EBITDA in 2022 was 9,993 million reais (8,661 million reais in 2021, +15.4%) and the margin on revenues amounted to 46.4% (48.0% in 2021). EBITDA in 2022 reflects the non-recurring charges of 128 million reais mainly related to the development of non-recurring projects and the corporate reorganization processes. Organic EBITDA, net of the non-recurring items, increased by 16.4% and was calculated as follows: (million Brazilian reais) 2022 2021 Changes    absolute %      EBITDA  9,993 8,661 1,332 15.4 Non-recurring expenses (income) 128 36 92  ORGANIC EBITDA - excluding non-recurring items 10,121 8,697 1,424 16.4 The increase of EBITDA is due to the greater revenues as well as the consolidation of Cozani (579 million reais). The relative margin on revenues, in organic terms, comes to 47.0% (48.2% in 2021). EBITDA for the fourth quarter of 2022, amounted to 2,824 million reais, up 16.3% compared to the fourth quarter of 2021 (2,429 million reais).  Net of non-recurring charges, the margin on revenues for the fourth quarter of 2022 was 49.1% (50.9% in the fourth quarter of 2021). The changes in the main cost items are shown below:   (million euros) (million Brazilian reais)   2022 2021 2022 2021 Change  (a) (b) (c) (d) (c-d)       Acquisition of goods and services 1,562 1,037 8,490 6,592 1,898 Employee benefits expenses 311 237 1,690 1,506 184 Other operating expenses 367 282 1,992 1,798 194 Change in inventories (6) 7 (34) 44 (78)   \"\"\"\r\nnew_passages = text_splitter.split_documents([Document(page_content=txt)])\r\nfor p in new_passages:\r\n    print(p.metadata['start_index'])\r\n```\r\n\r\nI'm getting the following prints:\r\n```python\r\n1\r\n-1\r\n-1\r\n-1\r\n-1\r\n-1\r\n2826\r\n3001\r\n-1\r\n3781\r\n```\n\n### Error Message and Stack Trace (if applicable)\n\n_No response_\n\n### Description\n\nI'm trying to use `add_start_index=True` with the `NLTKTextSplitter`. However, as you can see from the previous snippet of code, I'm getting some \"-1\".\n\n### System Info\n\nPython 3.10.12\r\nlangchain==0.2.16\r\nlangchain-core==0.2.41\r\nlangchain-text-splitters==0.2.4\r\nnltk==3.9.1\r\n\r\n# note that the bug is also here with 0.3.X",
    "state": "closed",
    "created_at": "2024-10-31T07:37:16+00:00",
    "closed_at": "2024-12-16T19:53:16+00:00",
    "updated_at": "2024-12-16T19:53:16+00:00",
    "author": "antoniolanza1996",
    "author_type": "User",
    "comments_count": 0,
    "reactions_count": 0,
    "labels": "ðŸ¤–:bug",
    "milestone": null,
    "closed_by": "efriis",
    "resolution_time_hours": 1116.2666666666667,
    "first_comments": [],
    "url": "https://github.com/langchain-ai/langchain/issues/27781"
  },
  {
    "repository": "langchain-ai/langchain",
    "issue_number": 27770,
    "title": "Error when loading pdf with PyPDFLoader",
    "body": "### Checked other resources\n\n- [X] I added a very descriptive title to this issue.\n- [X] I searched the LangChain documentation with the integrated search.\n- [X] I used the GitHub search to find a similar question and didn't find it.\n- [X] I am sure that this is a bug in LangChain rather than my code.\n- [X] The bug is not resolved by updating to the latest stable version of LangChain (or the specific integration package).\n\n### Example Code\n\n```python\r\nfrom langchain_community.document_loaders import PyPDFLoader\r\nimport requests\r\nfrom pathlib import Path\r\n\r\n# Get data from url\r\nurl = 'https://camels.readthedocs.io/_/downloads/en/latest/pdf/'\r\nr = requests.get(url, stream=True)\r\ndocument_path = Path('data.pdf')\r\n\r\ndocument_path.write_bytes(r.content)\r\nloader = PyPDFLoader(document_path)\r\ndocuments = loader.load()\r\n```\n\n### Error Message and Stack Trace (if applicable)\n\n```\r\nTraceback (most recent call last):\r\n  File \"/home/tda/GenAICourse/RAG/ragbot_langchain/loadpdf.py\", line 12, in <module>\r\n    documents = loader.load()\r\n  File \"/home/tda/miniconda3/envs/ragbot2/lib/python3.10/site-packages/langchain_core/document_loaders/base.py\", line 31, in load\r\n    return list(self.lazy_load())\r\n  File \"/home/tda/miniconda3/envs/ragbot2/lib/python3.10/site-packages/langchain_community/document_loaders/pdf.py\", line 257, in lazy_load\r\n    yield from self.parser.parse(blob)\r\n  File \"/home/tda/miniconda3/envs/ragbot2/lib/python3.10/site-packages/langchain_core/document_loaders/base.py\", line 127, in parse\r\n    return list(self.lazy_parse(blob))\r\n  File \"/home/tda/miniconda3/envs/ragbot2/lib/python3.10/site-packages/langchain_community/document_loaders/parsers/pdf.py\", line 124, in lazy_parse\r\n    yield from [\r\n  File \"/home/tda/miniconda3/envs/ragbot2/lib/python3.10/site-packages/langchain_community/document_loaders/parsers/pdf.py\", line 126, in <listcomp>\r\n    page_content=_extract_text_from_page(page=page)\r\n  File \"/home/tda/miniconda3/envs/ragbot2/lib/python3.10/site-packages/langchain_community/document_loaders/parsers/pdf.py\", line 117, in _extract_text_from_page\r\n    return page.extract_text(\r\n  File \"/home/tda/miniconda3/envs/ragbot2/lib/python3.10/site-packages/pypdf/_page.py\", line 2393, in extract_text\r\n    return self._extract_text(\r\n  File \"/home/tda/miniconda3/envs/ragbot2/lib/python3.10/site-packages/pypdf/_page.py\", line 1868, in _extract_text\r\n    cmaps[f] = build_char_map(f, space_width, obj)\r\n  File \"/home/tda/miniconda3/envs/ragbot2/lib/python3.10/site-packages/pypdf/_cmap.py\", line 33, in build_char_map\r\n    font_subtype, font_halfspace, font_encoding, font_map = build_char_map_from_dict(\r\n  File \"/home/tda/miniconda3/envs/ragbot2/lib/python3.10/site-packages/pypdf/_cmap.py\", line 56, in build_char_map_from_dict\r\n    encoding, map_dict = get_encoding(ft)\r\n  File \"/home/tda/miniconda3/envs/ragbot2/lib/python3.10/site-packages/pypdf/_cmap.py\", line 129, in get_encoding\r\n    map_dict, int_entry = _parse_to_unicode(ft)\r\n  File \"/home/tda/miniconda3/envs/ragbot2/lib/python3.10/site-packages/pypdf/_cmap.py\", line 212, in _parse_to_unicode\r\n    return _type1_alternative(ft, map_dict, int_entry)\r\n  File \"/home/tda/miniconda3/envs/ragbot2/lib/python3.10/site-packages/pypdf/_cmap.py\", line 530, in _type1_alternative\r\n    map_dict[chr(i)] = v\r\nUnboundLocalError: local variable 'v' referenced before assignment\r\n```\n\n### Description\n\nI'm trying to load a pdf with PyPDFLoader for a RAG application but it raises an error. I tried both with a local file or requesting it from the url like in the example. It does not happen with other pdf files I tried.\n\n### System Info\n\n```\r\nSystem Information\r\n------------------\r\n> OS:  Linux\r\n> OS Version:  #134~20.04.1-Ubuntu SMP Tue Oct 1 15:27:33 UTC 2024\r\n> Python Version:  3.10.0 (default, Mar  3 2022, 09:58:08) [GCC 7.5.0]\r\n\r\nPackage Information\r\n-------------------\r\n> langchain_core: 0.3.14\r\n> langchain: 0.3.5\r\n> langchain_community: 0.3.3\r\n> langsmith: 0.1.138\r\n> langchain_chroma: 0.1.4\r\n> langchain_mistralai: 0.2.0\r\n> langchain_openai: 0.2.4\r\n> langchain_text_splitters: 0.3.1\r\n\r\nOptional packages not installed\r\n-------------------------------\r\n> langgraph\r\n> langserve\r\n\r\nOther Dependencies\r\n------------------\r\n> aiohttp: 3.10.10\r\n> async-timeout: 4.0.3\r\n> chromadb: 0.5.16\r\n> dataclasses-json: 0.6.7\r\n> fastapi: 0.115.4\r\n> httpx: 0.27.2\r\n> httpx-sse: 0.4.0\r\n> jsonpatch: 1.33\r\n> numpy: 1.26.4\r\n> openai: 1.53.0\r\n> orjson: 3.10.10\r\n> packaging: 24.1\r\n> pydantic: 2.9.2\r\n> pydantic-settings: 2.6.0\r\n> PyYAML: 6.0.2\r\n> requests: 2.32.3\r\n> requests-toolbelt: 1.0.0\r\n> SQLAlchemy: 2.0.36\r\n> tenacity: 9.0.0\r\n> tiktoken: 0.8.0\r\n> tokenizers: 0.20.1\r\n> typing-extensions: 4.12.2\r\n```",
    "state": "closed",
    "created_at": "2024-10-31T00:03:29+00:00",
    "closed_at": "2024-10-31T12:19:13+00:00",
    "updated_at": "2024-10-31T12:19:13+00:00",
    "author": "PabloVD",
    "author_type": "User",
    "comments_count": 1,
    "reactions_count": 0,
    "labels": "ðŸ¤–:bug",
    "milestone": null,
    "closed_by": "PabloVD",
    "resolution_time_hours": 12.262222222222222,
    "first_comments": [
      {
        "author": "PabloVD",
        "created_at": "2024-10-31T01:33:37+00:00",
        "body": "Seems to be a problem related to the last pypdf release 5.1.0. Uninstalling and installing version 5.0.1 fixed the issue."
      }
    ],
    "url": "https://github.com/langchain-ai/langchain/issues/27770"
  },
  {
    "repository": "langchain-ai/langchain",
    "issue_number": 27748,
    "title": "DOC: `How to split text by tokens` guide links to wrong `TokenTextSplitter` page",
    "body": "### URL\n\nhttps://python.langchain.com/docs/how_to/split_by_token/\n\n### Checklist\n\n- [X] I added a very descriptive title to this issue.\n- [X] I included a link to the documentation page I am referring to (if applicable).\n\n### Issue with current documentation:\n\nIn the start of the documentation, the line (just above the first codeblock) specifies:\r\n> [CharacterTextSplitter](https://python.langchain.com/api_reference/text_splitters/character/langchain_text_splitters.character.CharacterTextSplitter.html), [RecursiveCharacterTextSplitter](https://python.langchain.com/api_reference/text_splitters/character/langchain_text_splitters.character.RecursiveCharacterTextSplitter.html), and [TokenTextSplitter](https://python.langchain.com/api_reference/langchain_text_splitters/base/langchain_text_splitters.base.TokenTextSplitter.html) can be used with tiktoken directly.\r\n\r\nThe `TextTokenSplitter` links to the wrong the page (non-existent)\r\n\r\nThe correct redirect should be to this page: https://python.langchain.com/api_reference/text_splitters/base/langchain_text_splitters.base.TokenTextSplitter.html\r\n\n\n### Idea or request for content:\n\n_No response_",
    "state": "closed",
    "created_at": "2024-10-30T16:18:03+00:00",
    "closed_at": "2024-11-16T18:45:31+00:00",
    "updated_at": "2024-11-16T18:45:31+00:00",
    "author": "inclinedadarsh",
    "author_type": "User",
    "comments_count": 2,
    "reactions_count": 0,
    "labels": "ðŸ¤–:docs",
    "milestone": null,
    "closed_by": "inclinedadarsh",
    "resolution_time_hours": 410.4577777777778,
    "first_comments": [
      {
        "author": "DangerousPotential",
        "created_at": "2024-11-01T14:21:32+00:00",
        "body": "Proposed changes in #27824 "
      },
      {
        "author": "inclinedadarsh",
        "created_at": "2024-11-16T18:45:31+00:00",
        "body": "This issue has been fixed and merged in #27824."
      }
    ],
    "url": "https://github.com/langchain-ai/langchain/issues/27748"
  },
  {
    "repository": "langchain-ai/langchain",
    "issue_number": 27745,
    "title": "langchain_anthropic tool use cannot run because of the chat_models.py in langchain_anthropic  has problem with  \"args\": event.delta.partial_json, and   \"stop_reason\": event.delta.stop_reason",
    "body": "### Checked other resources\n\n- [X] I added a very descriptive title to this issue.\n- [X] I searched the LangChain documentation with the integrated search.\n- [X] I used the GitHub search to find a similar question and didn't find it.\n- [X] I am sure that this is a bug in LangChain rather than my code.\n- [X] The bug is not resolved by updating to the latest stable version of LangChain (or the specific integration package).\n\n### Example Code\n\nfrom dotenv import load_dotenv\r\nfrom langchain import hub\r\nfrom langchain.agents import (\r\n    AgentExecutor,\r\n    create_react_agent,\r\n)\r\nfrom langchain_core.tools import Tool\r\n\r\n# Load environment variables from .env file\r\nload_dotenv()\r\n\r\n\r\n# Define a very simple tool function that returns the current time\r\ndef get_current_time(*args, **kwargs):\r\n    \"\"\"Returns the current time in H:MM AM/PM format.\"\"\"\r\n    import datetime  # Import datetime module to get current time\r\n\r\n    now = datetime.datetime.now()  # Get current time\r\n    return now.strftime(\"%I:%M %p\")  # Format time in H:MM AM/PM format\r\n\r\ndef get_user_name(*args, **kwargs):\r\n    return \"laios\"\r\n\r\n# List of tools available to the agent\r\ntools = [\r\n    Tool(\r\n        name=\"Time\",  # Name of the tool\r\n        func=get_current_time,  # Function that the tool will execute\r\n        # Description of the tool\r\n        description=\"Useful for when you need to know the current time\",\r\n    ),\r\n]\r\n\r\n# Pull the prompt template from the hub\r\n# ReAct = Reason and Action\r\n# https://smith.langchain.com/hub/hwchase17/react\r\nprompt = hub.pull(\"hwchase17/react\")\r\n\r\n# Initialize a ChatAnthropic model\r\nfrom langchain_anthropic import ChatAnthropic\r\nANTHROPIC_API_KEY=\"sk-ZmdryaQOkdidlKX9hIvOEi4daloVTSmxRtwfVft0pPn9hy3k\"\r\nmodel=\"claude-3-5-sonnet-20241022\"\r\nllm = ChatAnthropic(model=model , api_key =  ANTHROPIC_API_KEY)\r\n\r\n# Create the ReAct agent using the create_react_agent function\r\nagent = create_react_agent(\r\n    llm=llm,\r\n    tools=tools,\r\n    prompt=prompt,\r\n    stop_sequence=True,\r\n)\r\n\r\n# Create an agent executor from the agent and tools\r\nagent_executor = AgentExecutor.from_agent_and_tools(\r\n    agent=agent,\r\n    tools=tools,\r\n    verbose=True,\r\n)\r\n\r\n# Run the agent with a test query\r\nresponse = agent_executor.invoke({\"input\": \"What time is it?\"})\r\n\r\n# Print the response from the agent\r\nprint(\"response:\", response)\r\n\r\nâ€œâ€â€œ\r\ncan not pass the simple tool use demo of agent_executor in langchain_anthropic  .\r\nthe chat_models.py in langchain_anthropic  has problem with  \"args\": event.delta.partial_json, and   \"stop_reason\": event.delta.stop_reason.\r\ncode 1223 and code 1235.\r\nâ€â€œâ€\n\n### Error Message and Stack Trace (if applicable)\n\nTraceback (most recent call last):\r\n  File \"D:\\.workspace\\web3\\0camp\\hackston\\ai\\tutorial\\langchain-crash-course\\5_agents_and_tools\\1_agent_and_tools_basics.py\", line 68, in <module>\r\n    response = agent_executor.invoke({\"input\": \"What time is it?\"})\r\n  File \"D:\\.workspace\\web3\\0camp\\hackston\\ai\\tutorial\\langchain-crash-course\\langchain-demo\\lib\\site-packages\\langchain\\chains\\base.py\", line 170, in invoke\r\n    raise e\r\n  File \"D:\\.workspace\\web3\\0camp\\hackston\\ai\\tutorial\\langchain-crash-course\\langchain-demo\\lib\\site-packages\\langchain\\chains\\base.py\", line 160, in invoke\r\n    self._call(inputs, run_manager=run_manager)\r\n  File \"D:\\.workspace\\web3\\0camp\\hackston\\ai\\tutorial\\langchain-crash-course\\langchain-demo\\lib\\site-packages\\langchain\\agents\\agent.py\", line 1629, in _call\r\n    next_step_output = self._take_next_step(\r\n  File \"D:\\.workspace\\web3\\0camp\\hackston\\ai\\tutorial\\langchain-crash-course\\langchain-demo\\lib\\site-packages\\langchain\\agents\\agent.py\", line 1335, in _take_next_step\r\n    [\r\n  File \"D:\\.workspace\\web3\\0camp\\hackston\\ai\\tutorial\\langchain-crash-course\\langchain-demo\\lib\\site-packages\\langchain\\agents\\agent.py\", line 1335, in <listcomp>\r\n    [\r\n  File \"D:\\.workspace\\web3\\0camp\\hackston\\ai\\tutorial\\langchain-crash-course\\langchain-demo\\lib\\site-packages\\langchain\\agents\\agent.py\", line 1363, in _iter_next_step\r\n    output = self._action_agent.plan(\r\n  File \"D:\\.workspace\\web3\\0camp\\hackston\\ai\\tutorial\\langchain-crash-course\\langchain-demo\\lib\\site-packages\\langchain\\agents\\agent.py\", line 464, in plan\r\n    for chunk in self.runnable.stream(inputs, config={\"callbacks\": callbacks}):\r\n  File \"D:\\.workspace\\web3\\0camp\\hackston\\ai\\tutorial\\langchain-crash-course\\langchain-demo\\lib\\site-packages\\langchain_core\\runnables\\base.py\", line 3407, in stream\r\n    yield from self.transform(iter([input]), config, **kwargs)\r\n  File \"D:\\.workspace\\web3\\0camp\\hackston\\ai\\tutorial\\langchain-crash-course\\langchain-demo\\lib\\site-packages\\langchain_core\\runnables\\base.py\", line 3394, in transform\r\n    yield from self._transform_stream_with_config(\r\n  File \"D:\\.workspace\\web3\\0camp\\hackston\\ai\\tutorial\\langchain-crash-course\\langchain-demo\\lib\\site-packages\\langchain_core\\runnables\\base.py\", line 2197, in _transform_stream_with_config\r\n    chunk: Output = context.run(next, iterator)  # type: ignore\r\n  File \"D:\\.workspace\\web3\\0camp\\hackston\\ai\\tutorial\\langchain-crash-course\\langchain-demo\\lib\\site-packages\\langchain_core\\runnables\\base.py\", line 3357, in _transform\r\n    yield from final_pipeline\r\n  File \"D:\\.workspace\\web3\\0camp\\hackston\\ai\\tutorial\\langchain-crash-course\\langchain-demo\\lib\\site-packages\\langchain_core\\runnables\\base.py\", line 1413, in transform\r\n    for ichunk in input:\r\n  File \"D:\\.workspace\\web3\\0camp\\hackston\\ai\\tutorial\\langchain-crash-course\\langchain-demo\\lib\\site-packages\\langchain_core\\runnables\\base.py\", line 5561, in transform\r\n    yield from self.bound.transform(\r\n  File \"D:\\.workspace\\web3\\0camp\\hackston\\ai\\tutorial\\langchain-crash-course\\langchain-demo\\lib\\site-packages\\langchain_core\\runnables\\base.py\", line 1431, in transform\r\n    yield from self.stream(final, config, **kwargs)\r\n  File \"D:\\.workspace\\web3\\0camp\\hackston\\ai\\tutorial\\langchain-crash-course\\langchain-demo\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 420, in stream\r\n    raise e\r\n  File \"D:\\.workspace\\web3\\0camp\\hackston\\ai\\tutorial\\langchain-crash-course\\langchain-demo\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 400, in stream\r\n    for chunk in self._stream(messages, stop=stop, **kwargs):\r\n  File \"D:\\.workspace\\web3\\0camp\\hackston\\ai\\tutorial\\langchain-crash-course\\langchain-demo\\lib\\site-packages\\langchain_anthropic\\chat_models.py\", line 716, in _stream\r\n    msg = _make_message_chunk_from_anthropic_event(\r\n  File \"D:\\.workspace\\web3\\0camp\\hackston\\ai\\tutorial\\langchain-crash-course\\langchain-demo\\lib\\site-packages\\langchain_anthropic\\chat_models.py\", line 1235, in _make_message_chunk_from_anthropic_event\r\n    \"stop_reason\": event.delta.stop_reason,\r\nAttributeError: 'NoneType' object has no attribute 'stop_reason'\n\n### Description\n\ncan not pass the simple tool use demo of agent_executor in langchain_anthropic.\r\nthe chat_models.py in langchain_anthropic  has problem with  \"args\": event.delta.partial_json, and   \"stop_reason\": event.delta.stop_reason.\r\ncode 1223 and code 1235.\r\n\r\nwhen you called this code\r\nresponse = agent_executor.invoke({\"input\": \"What time is it?\"})\r\nfinally they will goto the _make_message_chunk_from_anthropic_event() in models.py and  event.delta.partial_json donot have partial_json ;and event.delta is None.\r\n\r\nplease check it.\n\n### System Info\n\nwin10\r\npycharm\r\n\r\nrequirements.txt\r\nlangchain-openai>=0.2.2\r\nlangchain_ollama>=0.2.0\r\npython-dotenv>=1.0.1\r\nlangchain>=0.3.0\r\nlangchain-community>=0.3.0\r\nlangchain-anthropic>=0.2.0\r\nlangchain-google-genai>=1.1.0\r\nlangchain-google-firestore>=0.3.1\r\nfirestore>=0.0.8\r\nchromadb>=0.5.15\r\ntiktoken>=0.8.0\r\nsentence-transformers>=3.1.0\r\nbs4>=0.0.2\r\nfirecrawl-py>=0.0.14\r\nlangchainhub>=0.1.21\r\nwikipedia>=1.4.0\r\ntavily-python>=0.3.4",
    "state": "closed",
    "created_at": "2024-10-30T15:57:34+00:00",
    "closed_at": "2024-10-30T16:03:36+00:00",
    "updated_at": "2024-10-30T16:03:36+00:00",
    "author": "LaiosOvO",
    "author_type": "User",
    "comments_count": 0,
    "reactions_count": 0,
    "labels": "ðŸ¤–:bug,investigate",
    "milestone": null,
    "closed_by": "LaiosOvO",
    "resolution_time_hours": 0.10055555555555555,
    "first_comments": [],
    "url": "https://github.com/langchain-ai/langchain/issues/27745"
  },
  {
    "repository": "langchain-ai/langchain",
    "issue_number": 27735,
    "title": "when I using embeddings.embed_query(\"this is a test\") but the sever is not achieve the \"this is a test\"",
    "body": "### Checked other resources\n\n- [X] I added a very descriptive title to this issue.\n- [X] I searched the LangChain documentation with the integrated search.\n- [X] I used the GitHub search to find a similar question and didn't find it.\n- [X] I am sure that this is a bug in LangChain rather than my code.\n- [X] The bug is not resolved by updating to the latest stable version of LangChain (or the specific integration package).\n\n### Example Code\n\nwhen i using \r\n```python\r\nfrom langchain_community.embeddings import OllamaEmbeddings\r\nfrom langchain_openai import OpenAIEmbeddings\r\n\r\n embeddings.embed_query(\"this is a test\")\r\n```\r\n\n\n### Error Message and Stack Trace (if applicable)\n\nThe sever end should received the \r\n{'input': [\"this is a test\"], 'model': 'bge-m3', 'encoding_format': 'base64', 'dimensions': None} \r\nbut actually received the  \r\n{'input': [[576, 374, 264, 1296]], 'model': 'bge-m3', 'encoding_format': 'base64', 'dimensions': None}\n\n### Description\n\nI checked the \r\nclass DeterministicFakeEmbedding(Embeddings, BaseModel):\r\n    \"\"\"\r\n    Fake embedding model that always returns\r\n    the same embedding vector for the same text.\r\n    \"\"\"\r\n\r\n    size: int\r\n    \"\"\"The size of the embedding vector.\"\"\"\r\n\r\n    def _get_embedding(self, seed: int) -> List[float]:\r\n        import numpy as np  # type: ignore[import-not-found, import-untyped]\r\n\r\n        # set the seed for the random generator\r\n        np.random.seed(seed)\r\n        return list(np.random.normal(size=self.size))\r\n\r\n    def _get_seed(self, text: str) -> int:\r\n        \"\"\"\r\n        Get a seed for the random generator, using the hash of the text.\r\n        \"\"\"\r\n        return int(hashlib.sha256(text.encode(\"utf-8\")).hexdigest(), 16) % 10**8\r\n\r\n    def embed_documents(self, texts: List[str]) -> List[List[float]]:\r\n        return [self._get_embedding(seed=self._get_seed(_)) for _ in texts]\r\n\r\n    def embed_query(self, text: str) -> List[float]:\r\n        return self._get_embedding(seed=self._get_seed(text))\r\n  but i can not debug to this package I can not know why \"this is a test\" change to [576, 374, 264, 1296]\n\n### System Info\n\nSystem Information\r\n------------------\r\n> OS:  Linux\r\n> OS Version:  #31-Ubuntu SMP PREEMPT_DYNAMIC Sat Apr 20 00:40:06 UTC 2024\r\n> Python Version:  3.9.20 (main, Oct  3 2024, 07:27:41) \r\n[GCC 11.2.0]\r\n\r\nPackage Information\r\n-------------------\r\n> langchain_core: 0.1.52\r\n> langchain: 0.1.17\r\n> langchain_community: 0.0.36\r\n> langsmith: 0.1.137\r\n> langchain_chatchat: 0.3.1.3\r\n> langchain_experimental: 0.0.58\r\n> langchain_openai: 0.0.6\r\n> langchain_text_splitters: 0.0.2\r\n> langchainhub: 0.1.14\r\n",
    "state": "closed",
    "created_at": "2024-10-30T06:55:28+00:00",
    "closed_at": "2024-10-31T06:31:54+00:00",
    "updated_at": "2024-10-31T06:31:54+00:00",
    "author": "gabrielpondc",
    "author_type": "User",
    "comments_count": 1,
    "reactions_count": 0,
    "labels": "ðŸ¤–:bug",
    "milestone": null,
    "closed_by": "gabrielpondc",
    "resolution_time_hours": 23.607222222222223,
    "first_comments": [
      {
        "author": "gabrielpondc",
        "created_at": "2024-10-31T06:31:54+00:00",
        "body": "find the reason that langchain unsing cl100k_base encoder encoding the text"
      }
    ],
    "url": "https://github.com/langchain-ai/langchain/issues/27735"
  },
  {
    "repository": "langchain-ai/langchain",
    "issue_number": 27674,
    "title": "DOC: Docs said JSON output but uses the `.dict()` method",
    "body": "### URL\n\nhttps://python.langchain.com/docs/tutorials/classification/\n\n### Checklist\n\n- [X] I added a very descriptive title to this issue.\n- [X] I included a link to the documentation page I am referring to (if applicable).\n\n### Issue with current documentation:\n\n![image](https://github.com/user-attachments/assets/0dc7c21c-8274-47cc-bd7e-16945ae1c084)\r\n\r\n`res.dict()` is used instead of `res.json()`.\r\n\r\nJSON requires double quotes around its keys but the example show single quotes.`\n\n### Idea or request for content:\n\nReplace `res.dict()` with `res.json()`",
    "state": "closed",
    "created_at": "2024-10-28T07:27:40+00:00",
    "closed_at": "2024-10-29T15:03:45+00:00",
    "updated_at": "2024-10-29T15:03:45+00:00",
    "author": "DangerousPotential",
    "author_type": "User",
    "comments_count": 2,
    "reactions_count": 0,
    "labels": "ðŸ¤–:docs",
    "milestone": null,
    "closed_by": "DangerousPotential",
    "resolution_time_hours": 31.601388888888888,
    "first_comments": [
      {
        "author": "DangerousPotential",
        "created_at": "2024-10-28T08:17:50+00:00",
        "body": "In #27675 , I changed the word JSON to dictionary, but the team can choose whether to replace the `.dict()` to `.json()` if the intended use case is for JSON"
      },
      {
        "author": "DangerousPotential",
        "created_at": "2024-10-29T15:03:39+00:00",
        "body": "Fixed with #27675."
      }
    ],
    "url": "https://github.com/langchain-ai/langchain/issues/27674"
  },
  {
    "repository": "langchain-ai/langchain",
    "issue_number": 27669,
    "title": "Model using internal knowledge to answer the question when it is not supposed to in GraphSparqlQAChain",
    "body": "### Checked other resources\n\n- [X] I added a very descriptive title to this issue.\n- [X] I searched the LangChain documentation with the integrated search.\n- [X] I used the GitHub search to find a similar question and didn't find it.\n- [X] I am sure that this is a bug in LangChain rather than my code.\n- [X] The bug is not resolved by updating to the latest stable version of LangChain (or the specific integration package).\n\n### Example Code\n\nwhen i try to use the following code to answer the question: Where was barak obama born?\r\n```\r\ngraph = RdfGraph(\r\n    source_file=\"https://dbpedia.org/data/Barack_Obama.ttl\",\r\n    standard=\"rdf\",\r\n    local_copy=\"test.ttl\",\r\n)\r\n\r\ngraph.load_schema()\r\n\r\ngraph.get_schema\r\n\r\nimport os\r\n\r\nif \"OPENAI_API_KEY\" not in os.environ:\r\n    os.environ[\"OPENAI_API_KEY\"] = ''\r\n\r\nchain = GraphSparqlQAChain.from_llm(\r\n    ChatOpenAI(temperature=0), graph=graph, verbose=True, allow_dangerous_requests=True\r\n)\r\ninstruction = \"ADDITIONAL INSTRUCTION: When you make the final query, remove these ``` quotes and only have the query \\n\"\r\nquery = \"QUERY: Where was barak obama born?\"\r\nprompt = instruction + query\r\nprint(chain.run(prompt))\r\n```\r\n\n\n### Error Message and Stack Trace (if applicable)\n\n_No response_\n\n### Description\n\nThis is the SPARQL Query generated \r\n```\r\nPREFIX dbp: <http://dbpedia.org/ontology/>\r\nSELECT ?birthPlace\r\nWHERE {\r\n    dbp:Barack_Obama dbp:birthPlace ?birthPlace .\r\n}```\r\nQuery execution answer: []\r\n\r\nFinal answer from **print(chain.run(prompt))** : **Barack Obama was born in Honolulu, Hawaii.**\r\n\r\nI assume this is because of the [qa prompt](https://github.com/langchain-ai/langchain/blob/master/libs/community/langchain_community/chains/graph_qa/prompts.py#L192)\r\n\r\nMy suggestion would be to add one line  especially after:  Question: {prompt} to only answer from the available information as it would further reinforce the instructions. \n\n### System Info\n\nI first noted the problem on google collab, but i faced the same problem while testing it on my local machine. ",
    "state": "closed",
    "created_at": "2024-10-27T17:00:47+00:00",
    "closed_at": "2024-10-29T19:15:42+00:00",
    "updated_at": "2024-10-29T19:15:42+00:00",
    "author": "nadgeri14",
    "author_type": "User",
    "comments_count": 3,
    "reactions_count": 0,
    "labels": "ðŸ¤–:bug",
    "milestone": null,
    "closed_by": "nadgeri14",
    "resolution_time_hours": 50.24861111111111,
    "first_comments": [
      {
        "author": "Googlogogo",
        "created_at": "2024-10-28T23:22:37+00:00",
        "body": "Hi, we are a group of 4 students from University of Toronto, could we take a look at this issue? Thanks!"
      },
      {
        "author": "nadgeri14",
        "created_at": "2024-10-29T09:14:44+00:00",
        "body": "Hi, thanks for the help, I already have a solution. I am waiting for the bug to be assigned to me so that i can create a pull request. "
      },
      {
        "author": "Googlogogo",
        "created_at": "2024-10-29T14:50:29+00:00",
        "body": "Hi Abhishek,\r\n\r\nThank you for telling me that! In this case, good luck moving forward!\r\n\r\nBest,\r\nGogo\r\n\r\nOn Tue, Oct 29, 2024 at 5:15â€¯AM Abhishek Nadgeri ***@***.***>\r\nwrote:\r\n\r\n> Hi, thanks for the help, I already have a solution. I am waiting for the\r\n> bug to be assigned to me so that i can create a pull request.\r\n>\r\n> â€”\r\n> Reply to this email directly, view it on GitHub\r\n> <https://github.com/langchain-ai/langchain/issues/27669#issuecomment-2443656057>,\r\n> or unsubscribe\r\n> <https://github.com/notifications/unsubscribe-auth/AZV5KN4MXUUMVFIUA6GTRZ3Z55GZXAVCNFSM6AAAAABQV7SH2OVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZDINBTGY2TMMBVG4>\r\n> .\r\n> You are receiving this because you commented.Message ID:\r\n> ***@***.***>\r\n>\r\n"
      }
    ],
    "url": "https://github.com/langchain-ai/langchain/issues/27669"
  },
  {
    "repository": "langchain-ai/langchain",
    "issue_number": 27668,
    "title": "DOC: Improving Documentation for Tool Integration in LangChain",
    "body": "### URL\n\nhttps://python.langchain.com/docs/integrations/tools/\n\n### Checklist\n\n- [X] I added a very descriptive title to this issue.\n- [X] I included a link to the documentation page I am referring to (if applicable).\n\n### Issue with current documentation:\n\nThe LangChain integration documentation provides basic information about the tools, but gathering complete, up-to-date information requires additional steps. This includes details about subscriptions, regional API availability, and support for different models. I propose enhancing the documentation structure by adding filterable tables with key parameters (e.g., regional availability, cost) to simplify integration for developers.\n\n### Idea or request for content:\n\nI have prepared a [notebook](https://github.com/hherpa/LangChain-ToolDocs-Problem/blob/main/ToolDocsProblemEng.ipynb) with examples and an analysis of the current documentation structure, along with suggested improvements. I am ready for further discussion and improvement.",
    "state": "closed",
    "created_at": "2024-10-27T17:00:03+00:00",
    "closed_at": "2024-12-05T23:03:32+00:00",
    "updated_at": "2024-12-05T23:03:32+00:00",
    "author": "hherpa",
    "author_type": "User",
    "comments_count": 1,
    "reactions_count": 0,
    "labels": "ðŸ¤–:docs",
    "milestone": null,
    "closed_by": null,
    "resolution_time_hours": 942.0580555555556,
    "first_comments": [
      {
        "author": "efriis",
        "created_at": "2024-12-05T23:03:18+00:00",
        "body": "super thorough and appreciate the notebook! Going to convert this to a discussion and write some thoughts / solicit ideas on improvement. Fully agreed there's some low-hanging fruit to fix now, and would also love to come up with concrete issues that can be improved either by the OSS community or by the maintainers when we have more capacity"
      }
    ],
    "url": "https://github.com/langchain-ai/langchain/issues/27668"
  },
  {
    "repository": "langchain-ai/langchain",
    "issue_number": 27656,
    "title": "Multi-agent supervisor example not working with ChatOllama, breaks when attempting to use two system messages. Must be an issue with using two system prompts in ChatOllama.",
    "body": "### Checked other resources\r\n\r\n- [X] I added a very descriptive title to this issue.\r\n- [X] I searched the LangChain documentation with the integrated search.\r\n- [X] I used the GitHub search to find a similar question and didn't find it.\r\n- [X] I am sure that this is a bug in LangChain rather than my code.\r\n- [X] The bug is not resolved by updating to the latest stable version of LangChain (or the specific integration package).\r\n\r\n### Example Code\r\n\r\n```\r\npython\r\nfrom langchain_openai import ChatOpenAI\r\nfrom langchain_ollama import ChatOllama\r\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\r\nfrom langchain_core.messages import SystemMessage, HumanMessage\r\nfrom langchain_core.output_parsers import StrOutputParser\r\nfrom typing import Literal\r\nfrom pydantic import BaseModel\r\n\r\nclass routeResponse(BaseModel):\r\n    next: Literal[\"FINISH\", \"Researcher\", \"Coder\"]\r\n\r\nllm = ChatOpenAI(model=\"gpt-4o-mini\", api_key='')\r\nllm = ChatOllama(model=\"llama3.2:latest\")\r\n\r\nmembers = [\"Researcher\", \"Coder\"]\r\nsystem_prompt = (\r\n    \"You are a supervisor tasked with managing a conversation between the\"\r\n    \" following workers:  {members}. Given the following user request,\"\r\n    \" respond with the worker to act next. Each worker will perform a\"\r\n    \" task and respond with their results and status. When finished,\"\r\n    \" respond with FINISH.\"\r\n)\r\n# Our team supervisor is an LLM node. It just picks the next agent to process\r\n# and decides when the work is completed\r\noptions = [\"FINISH\"] + members\r\n\r\ntemplate = ChatPromptTemplate.from_messages(\r\n    [\r\n        SystemMessage(content=system_prompt),\r\n        MessagesPlaceholder(variable_name=\"messages\"),\r\n        SystemMessage(content=\"Given the conversation above, who should act next? Or should we FINISH? Select one of: {options}\"),\r\n    ]\r\n).partial(options=str([\"FINISH\", \"Researcher\", \"Coder\"]), members=\", \".join(members))\r\n\r\nstructured_llm = llm.with_structured_output(routeResponse)\r\nsupervisor_chain = template | structured_llm\r\nresult = supervisor_chain.invoke({\"messages\": [HumanMessage(content=\"What is 9 + 10>\")]})\r\nprint(result)\r\n\r\n### Error Message and Stack Trace (if applicable)\r\n\r\nTraceback (most recent call last):\r\n  File \"/Users/andres/Documents/Projects/Athena/app/frameworks/multiagent_supervisor.py\", line 112, in <module>\r\n    for s in graph.stream(\r\n  File \"/Users/andres/.pyenv/versions/3.11.9/lib/python3.11/site-packages/langgraph/pregel/__init__.py\", line 1315, in stream\r\n    for _ in runner.tick(\r\n  File \"/Users/andres/.pyenv/versions/3.11.9/lib/python3.11/site-packages/langgraph/pregel/runner.py\", line 56, in tick\r\n    run_with_retry(t, retry_policy)\r\n  File \"/Users/andres/.pyenv/versions/3.11.9/lib/python3.11/site-packages/langgraph/pregel/retry.py\", line 29, in run_with_retry\r\n    task.proc.invoke(task.input, config)\r\n  File \"/Users/andres/.pyenv/versions/3.11.9/lib/python3.11/site-packages/langgraph/utils/runnable.py\", line 412, in invoke\r\n    input = context.run(step.invoke, input, config)\r\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/andres/.pyenv/versions/3.11.9/lib/python3.11/site-packages/langgraph/utils/runnable.py\", line 184, in invoke\r\n    ret = context.run(self.func, input, **kwargs)\r\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/andres/.pyenv/versions/3.11.9/lib/python3.11/site-packages/langgraph/graph/graph.py\", line 93, in _route\r\n    result = self.path.invoke(value, config)\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/andres/.pyenv/versions/3.11.9/lib/python3.11/site-packages/langgraph/utils/runnable.py\", line 176, in invoke\r\n    ret = context.run(self.func, input, **kwargs)\r\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/andres/Documents/Projects/Athena/app/frameworks/multiagent_supervisor.py\", line 105, in <lambda>\r\n    workflow.add_conditional_edges(\"supervisor\", lambda x: x['next'], conditional_map)\r\n                                                           ~^^^^^^^^\r\nKeyError: 'next'\r\n\r\n### Description\r\n\r\nRunning the code above produces Error message when ran with ChatOllama, while if run with ChatOpenAI, it works no problem. Showing minimal reproducible version of Multi-agent example. Interestingly, if changing the second system message in the template to a HumanMessage, the example works but it works half way anyways. Must be an issue with using two system prompts in ChatOllama.\r\n\r\n### System Info\r\n\r\nSystem Information\r\n------------------\r\n> OS:  Darwin\r\n> OS Version:  Darwin Kernel Version 24.1.0: Thu Oct 10 21:02:26 PDT 2024; root:xnu-11215.41.3~2/RELEASE_ARM64_T8122\r\n> Python Version:  3.11.9 (main, Jul 15 2024, 14:34:44) [Clang 15.0.0 (clang-1500.3.9.4)]\r\n\r\nPackage Information\r\n-------------------\r\n> langchain_core: 0.3.12\r\n> langchain: 0.3.4\r\n> langchain_community: 0.3.3\r\n> langsmith: 0.1.137\r\n> langchain_chroma: 0.1.4\r\n> langchain_experimental: 0.3.2\r\n> langchain_ollama: 0.2.0\r\n> langchain_openai: 0.2.3\r\n> langchain_text_splitters: 0.3.0\r\n> langgraph: 0.2.39\r\n\r\nOptional packages not installed\r\n-------------------------------\r\n> langserve\r\n\r\nOther Dependencies\r\n------------------\r\n> aiohttp: 3.9.5\r\n> async-timeout: 4.0.3\r\n> chromadb: 0.5.15\r\n> dataclasses-json: 0.6.7\r\n> fastapi: 0.111.0\r\n> httpx: 0.27.0\r\n> jsonpatch: 1.33\r\n> langgraph-checkpoint: 2.0.2\r\n> langgraph-sdk: 0.1.34\r\n> numpy: 1.26.4\r\n> ollama: 0.3.3\r\n> openai: 1.52.2\r\n> orjson: 3.10.7\r\n> packaging: 23.2\r\n> pydantic: 2.9.2\r\n> pydantic-settings: 2.5.2\r\n> PyYAML: 6.0.2\r\n> requests: 2.32.3\r\n> requests-toolbelt: 1.0.0\r\n> SQLAlchemy: 2.0.35\r\n> tenacity: 8.5.0\r\n> tiktoken: 0.7.0\r\n> typing-extensions: 4.12.2\r\n\r\n\r\nEDIT: Modified code as I pasted wrong version",
    "state": "closed",
    "created_at": "2024-10-26T02:19:29+00:00",
    "closed_at": "2024-12-01T03:54:29+00:00",
    "updated_at": "2024-12-01T03:54:29+00:00",
    "author": "aguilarcarboni",
    "author_type": "User",
    "comments_count": 2,
    "reactions_count": 0,
    "labels": "",
    "milestone": null,
    "closed_by": "aguilarcarboni",
    "resolution_time_hours": 865.5833333333334,
    "first_comments": [
      {
        "author": "EmPajak21",
        "created_at": "2024-10-29T20:50:20+00:00",
        "body": "I also have the same issue and error message"
      },
      {
        "author": "aguilarcarboni",
        "created_at": "2024-12-01T03:54:29+00:00",
        "body": "Has to do with Ollama's API and how it handle's system prompts, not langchain."
      }
    ],
    "url": "https://github.com/langchain-ai/langchain/issues/27656"
  },
  {
    "repository": "langchain-ai/langchain",
    "issue_number": 27627,
    "title": "community: failing unit test with sqlalchemy 2.0.36",
    "body": "### Privileged issue\n\n- [X] I am a LangChain maintainer, or was asked directly by a LangChain maintainer to create an issue here.\n\n### Issue Content\n\nIf you poetry lock langchain-community so that the current latest sqlalchemy version is installed you get failing unit tests: https://github.com/langchain-ai/langchain/actions/runs/11508741758/job/32037584608\r\n\r\n```\r\n=========================== short test summary info ============================\r\nFAILED tests/unit_tests/test_sql_database_schema.py::test_table_info - sqlalchemy.exc.ProgrammingError: (duckdb.duckdb.CatalogException) Catalog Error: Type with name REGCLASS does not exist!\r\n[SQL: SELECT pg_catalog.pg_class.relname, pg_catalog.pg_description.description \r\nFROM pg_catalog.pg_class LEFT OUTER JOIN pg_catalog.pg_description ON pg_catalog.pg_class.oid = pg_catalog.pg_description.objoid AND pg_catalog.pg_description.objsubid = $1 AND pg_catalog.pg_description.classoid = CAST($2 AS REGCLASS) JOIN pg_catalog.pg_namespace ON pg_catalog.pg_namespace.oid = pg_catalog.pg_class.relnamespace \r\nWHERE pg_catalog.pg_class.relkind = ANY (ARRAY[$3, $4, $5]) AND pg_catalog.pg_table_is_visible(pg_catalog.pg_class.oid) AND pg_catalog.pg_namespace.nspname != $6 AND pg_namespace.nspname = $7 AND pg_catalog.pg_class.relname IN ($8)]\r\n[parameters: (0, 'pg_catalog.pg_class', 'r', 'p', 'f', 'pg_catalog', 'schema_a', 'user')]\r\n(Background on this error at: https://sqlalche.me/e/20/f405)\r\nFAILED tests/unit_tests/test_sql_database_schema.py::test_sql_database_run - sqlalchemy.exc.ProgrammingError: (duckdb.duckdb.CatalogException) Catalog Error: Type with name REGCLASS does not exist!\r\n[SQL: SELECT pg_catalog.pg_class.relname, pg_catalog.pg_description.description \r\nFROM pg_catalog.pg_class LEFT OUTER JOIN pg_catalog.pg_description ON pg_catalog.pg_class.oid = pg_catalog.pg_description.objoid AND pg_catalog.pg_description.objsubid = $1 AND pg_catalog.pg_description.classoid = CAST($2 AS REGCLASS) JOIN pg_catalog.pg_namespace ON pg_catalog.pg_namespace.oid = pg_catalog.pg_class.relnamespace \r\nWHERE pg_catalog.pg_class.relkind = ANY (ARRAY[$3, $4, $5]) AND pg_catalog.pg_table_is_visible(pg_catalog.pg_class.oid) AND pg_catalog.pg_namespace.nspname != $6 AND pg_namespace.nspname = $7 AND pg_catalog.pg_class.relname IN ($8)]\r\n[parameters: (0, 'pg_catalog.pg_class', 'r', 'p', 'f', 'pg_catalog', 'schema_a', 'user')]\r\n(Background on this error at: https://sqlalche.me/e/20/f405)\r\n========== 2 failed, 994 passed, 538 skipped, 704 warnings in 18.18s ===========\r\n```",
    "state": "closed",
    "created_at": "2024-10-24T23:15:13+00:00",
    "closed_at": "2024-12-10T17:45:50+00:00",
    "updated_at": "2024-12-10T17:45:50+00:00",
    "author": "baskaryan",
    "author_type": "User",
    "comments_count": 5,
    "reactions_count": 0,
    "labels": "ðŸ¤–:bug",
    "milestone": null,
    "closed_by": "baskaryan",
    "resolution_time_hours": 1122.5102777777777,
    "first_comments": [
      {
        "author": "Aarya2004",
        "created_at": "2024-10-24T23:44:37+00:00",
        "body": "Hello @baskaryan! I'm investigating this error now. I'll add a comment when I have some clarity on some possible causes. Do you have any idea why this error may be thrown? (Just to know if you have an idea how to approach the fix)"
      },
      {
        "author": "Aarya2004",
        "created_at": "2024-10-25T02:29:54+00:00",
        "body": "It appears that [this bug](https://github.com/sqlalchemy/sqlalchemy/issues/11961) [and the commit to fix it ](https://github.com/sqlalchemy/sqlalchemy/commit/d4d9fd0212de42c259e5badd397ce55fd5822af8) caused this code to error out (since it seems to be working fine with sqlalchemy v2.0.35). It seems that the REGCLASS typecasting done is not a type supported by duckdb and causes this error. \r\nI will be looking for workaround for this but if you have any suggestions on how to fix the issue, do let me know!\r\n\r\nEdit: There was an issue brought up in the sqlalchemy repo: https://github.com/sqlalchemy/sqlalchemy/discussions/12011"
      },
      {
        "author": "Aarya2004",
        "created_at": "2024-11-02T18:55:41+00:00",
        "body": "Update: Got a PR going with the fixes to this issue. Once it's been approved and merged, this issue should be resolved.\r\nThe PR - Mause/duckdb_engine#1147"
      },
      {
        "author": "Aarya2004",
        "created_at": "2024-11-07T16:54:54+00:00",
        "body": "Hello! We were able to merge our PR in the duckdb_engine repo (https://github.com/Mause/duckdb_engine/pull/1147). Once our updates are released, this issue should be fixed!"
      },
      {
        "author": "Aarya2004",
        "created_at": "2024-12-10T03:49:48+00:00",
        "body": "@efriis Sorry while you are closing issues, I think this could be closed as well? The fix has been released, the package just needs to be updated and the test should be fixed."
      }
    ],
    "url": "https://github.com/langchain-ai/langchain/issues/27627"
  },
  {
    "repository": "langchain-ai/langchain",
    "issue_number": 27626,
    "title": "MongoDBAtlasVectorSearch.from_connection_string. [SSL: CERTIFICATE_VERIFY_FAILED]",
    "body": "### Checked other resources\n\n- [X] I added a very descriptive title to this issue.\n- [X] I searched the LangChain documentation with the integrated search.\n- [X] I used the GitHub search to find a similar question and didn't find it.\n- [X] I am sure that this is a bug in LangChain rather than my code.\n- [X] The bug is not resolved by updating to the latest stable version of LangChain (or the specific integration package).\n\n### Example Code\n\n```\r\nvectorStore = MongoDBAtlasVectorSearch.from_connection_string(\r\n    \"xxxxxxxxxxx\",\r\n    dbName + \".\" + collectionName,\r\n    embeddings,\r\n    index_name=index,\r\n)\r\n```\n\n### Error Message and Stack Trace (if applicable)\n\nraise ServerSelectionTimeoutError(\r\npymongo.errors.ServerSelectionTimeoutError: langchain-shard-00-01.0xyal.mongodb.net:27017: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1000) (configured timeouts: socketTimeoutMS: 20000.0ms, connectTimeoutMS: 20000.0ms),langchain-shard-00-00.0xyal.mongodb.net:27017: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1000) (configured timeouts: socketTimeoutMS: 20000.0ms, connectTimeoutMS: 20000.0ms),langchain-shard-00-02.0xyal.mongodb.net:27017: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1000) (configured timeouts: socketTimeoutMS: 20000.0ms, connectTimeoutMS: 20000.0ms), Timeout: 30s, Topology Description: <TopologyDescription id: 671acee57ae8fd26080b2026, topology_type: ReplicaSetNoPrimary, servers: [<ServerDescription ('langchain-shard-00-00.0xyal.mongodb.net', 27017) server_type: Unknown, rtt: None, error=AutoReconnect('langchain-shard-00-00.0xyal.mongodb.net:27017: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1000) (configured timeouts: socketTimeoutMS: 20000.0ms, connectTimeoutMS: 20000.0ms)')>, <ServerDescription ('langchain-shard-00-01.0xyal.mongodb.net', 27017) server_type: Unknown, rtt: None, error=AutoReconnect('langchain-shard-00-01.0xyal.mongodb.net:27017: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1000) (configured timeouts: socketTimeoutMS: 20000.0ms, connectTimeoutMS: 20000.0ms)')>, <ServerDescription ('langchain-shard-00-02.0xyal.mongodb.net', 27017) server_type: Unknown, rtt: None, error=AutoReconnect('langchain-shard-00-02.0xyal.mongodb.net:27017: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1000) (configured timeouts: socketTimeoutMS: 20000.0ms, connectTimeoutMS: 20000.0ms)')>]>\n\n### Description\n\nLooks like on mac (big sur) somehow we cannot connect to AtlasMongoDB how can i pass tls to \r\n\r\nMongoDBAtlasVectorSearch.from_connection_string\n\n### System Info\n\nMac\r\nlangchain 0.3",
    "state": "closed",
    "created_at": "2024-10-24T22:58:15+00:00",
    "closed_at": "2024-10-24T23:01:18+00:00",
    "updated_at": "2024-10-24T23:01:47+00:00",
    "author": "codenamics",
    "author_type": "User",
    "comments_count": 1,
    "reactions_count": 0,
    "labels": "â±­: vector store,investigate",
    "milestone": null,
    "closed_by": "codenamics",
    "resolution_time_hours": 0.050833333333333335,
    "first_comments": [
      {
        "author": "codenamics",
        "created_at": "2024-10-24T23:01:46+00:00",
        "body": "install certificates on mac application/python install certs"
      }
    ],
    "url": "https://github.com/langchain-ai/langchain/issues/27626"
  },
  {
    "repository": "langchain-ai/langchain",
    "issue_number": 27602,
    "title": "ChatDeepInfra JSONDecode error when tool call args is empty string",
    "body": "### Checked other resources\n\n- [X] I added a very descriptive title to this issue.\n- [X] I searched the LangChain documentation with the integrated search.\n- [X] I used the GitHub search to find a similar question and didn't find it.\n- [X] I am sure that this is a bug in LangChain rather than my code.\n- [X] The bug is not resolved by updating to the latest stable version of LangChain (or the specific integration package).\n\n### Example Code\n\n```python\r\nfrom langchain_community.chat_models.deepinfra import _convert_dict_to_message\r\n\r\nresponse_dict = {\r\n    \"role\": \"assistant\",\r\n    \"content\": \"\",\r\n    \"tool_calls\": [{\"function\": {\"name\": \"get_something\", \"arguments\": \"\"}}],\r\n}\r\n\r\nmessage = _convert_dict_to_message(response_dict)\r\n```\n\n### Error Message and Stack Trace (if applicable)\n\nJSONDecodeError                           Traceback (most recent call last)\r\nCell In[19], [line 9](vscode-notebook-cell:?execution_count=19&line=9)\r\n      [1](vscode-notebook-cell:?execution_count=19&line=1) from langchain_community.chat_models.deepinfra import _convert_dict_to_message\r\n      [3](vscode-notebook-cell:?execution_count=19&line=3) response_dict = {\r\n      [4](vscode-notebook-cell:?execution_count=19&line=4)     \"role\": \"assistant\",\r\n      [5](vscode-notebook-cell:?execution_count=19&line=5)     \"content\": \"\",\r\n      [6](vscode-notebook-cell:?execution_count=19&line=6)     \"tool_calls\": [{\"function\": {\"name\": \"get_something\", \"arguments\": \"\"}}],\r\n      [7](vscode-notebook-cell:?execution_count=19&line=7) }\r\n----> [9](vscode-notebook-cell:?execution_count=19&line=9) message = _convert_dict_to_message(response_dict)\r\n\r\nFile ~/sap/.venv/lib/python3.12/site-packages/langchain_community/chat_models/deepinfra.py:132, in _convert_dict_to_message(_dict)\r\n    [129](https://vscode-remote+wsl-002bubuntu.vscode-resource.vscode-cdn.net/home/shawn/sap/~/sap/.venv/lib/python3.12/site-packages/langchain_community/chat_models/deepinfra.py:129)     content = _dict.get(\"content\", \"\") or \"\"\r\n    [130](https://vscode-remote+wsl-002bubuntu.vscode-resource.vscode-cdn.net/home/shawn/sap/~/sap/.venv/lib/python3.12/site-packages/langchain_community/chat_models/deepinfra.py:130)     tool_calls_content = _dict.get(\"tool_calls\", []) or []\r\n    [131](https://vscode-remote+wsl-002bubuntu.vscode-resource.vscode-cdn.net/home/shawn/sap/~/sap/.venv/lib/python3.12/site-packages/langchain_community/chat_models/deepinfra.py:131)     tool_calls = [\r\n--> [132](https://vscode-remote+wsl-002bubuntu.vscode-resource.vscode-cdn.net/home/shawn/sap/~/sap/.venv/lib/python3.12/site-packages/langchain_community/chat_models/deepinfra.py:132)         _parse_tool_calling(tool_call) for tool_call in tool_calls_content\r\n    [133](https://vscode-remote+wsl-002bubuntu.vscode-resource.vscode-cdn.net/home/shawn/sap/~/sap/.venv/lib/python3.12/site-packages/langchain_community/chat_models/deepinfra.py:133)     ]\r\n    [134](https://vscode-remote+wsl-002bubuntu.vscode-resource.vscode-cdn.net/home/shawn/sap/~/sap/.venv/lib/python3.12/site-packages/langchain_community/chat_models/deepinfra.py:134)     return AIMessage(content=content, tool_calls=tool_calls)\r\n    [135](https://vscode-remote+wsl-002bubuntu.vscode-resource.vscode-cdn.net/home/shawn/sap/~/sap/.venv/lib/python3.12/site-packages/langchain_community/chat_models/deepinfra.py:135) elif role == \"system\":\r\n\r\nFile ~/sap/.venv/lib/python3.12/site-packages/langchain_community/chat_models/deepinfra.py:100, in _parse_tool_calling(tool_call)\r\n     [98](https://vscode-remote+wsl-002bubuntu.vscode-resource.vscode-cdn.net/home/shawn/sap/~/sap/.venv/lib/python3.12/site-packages/langchain_community/chat_models/deepinfra.py:98) name = tool_call[\"function\"].get(\"name\", \"\")\r\n     [99](https://vscode-remote+wsl-002bubuntu.vscode-resource.vscode-cdn.net/home/shawn/sap/~/sap/.venv/lib/python3.12/site-packages/langchain_community/chat_models/deepinfra.py:99) print(tool_call[\"function\"][\"arguments\"])\r\n--> [100](https://vscode-remote+wsl-002bubuntu.vscode-resource.vscode-cdn.net/home/shawn/sap/~/sap/.venv/lib/python3.12/site-packages/langchain_community/chat_models/deepinfra.py:100) args = json.loads(tool_call[\"function\"][\"arguments\"])\r\n    [101](https://vscode-remote+wsl-002bubuntu.vscode-resource.vscode-cdn.net/home/shawn/sap/~/sap/.venv/lib/python3.12/site-packages/langchain_community/chat_models/deepinfra.py:101) id = tool_call.get(\"id\")\r\n    [102](https://vscode-remote+wsl-002bubuntu.vscode-resource.vscode-cdn.net/home/shawn/sap/~/sap/.venv/lib/python3.12/site-packages/langchain_community/chat_models/deepinfra.py:102) return create_tool_call(name=name, args=args, id=id)\r\n\r\nFile /usr/lib/python3.12/json/__init__.py:346, in loads(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\r\n    [341](https://vscode-remote+wsl-002bubuntu.vscode-resource.vscode-cdn.net/usr/lib/python3.12/json/__init__.py:341)     s = s.decode(detect_encoding(s), 'surrogatepass')\r\n    [343](https://vscode-remote+wsl-002bubuntu.vscode-resource.vscode-cdn.net/usr/lib/python3.12/json/__init__.py:343) if (cls is None and object_hook is None and\r\n    [344](https://vscode-remote+wsl-002bubuntu.vscode-resource.vscode-cdn.net/usr/lib/python3.12/json/__init__.py:344)         parse_int is None and parse_float is None and\r\n    [345](https://vscode-remote+wsl-002bubuntu.vscode-resource.vscode-cdn.net/usr/lib/python3.12/json/__init__.py:345)         parse_constant is None and object_pairs_hook is None and not kw):\r\n--> [346](https://vscode-remote+wsl-002bubuntu.vscode-resource.vscode-cdn.net/usr/lib/python3.12/json/__init__.py:346)     return _default_decoder.decode(s)\r\n    [347](https://vscode-remote+wsl-002bubuntu.vscode-resource.vscode-cdn.net/usr/lib/python3.12/json/__init__.py:347) if cls is None:\r\n    [348](https://vscode-remote+wsl-002bubuntu.vscode-resource.vscode-cdn.net/usr/lib/python3.12/json/__init__.py:348)     cls = JSONDecoder\r\n\r\nFile /usr/lib/python3.12/json/decoder.py:337, in JSONDecoder.decode(self, s, _w)\r\n    [332](https://vscode-remote+wsl-002bubuntu.vscode-resource.vscode-cdn.net/usr/lib/python3.12/json/decoder.py:332) def decode(self, s, _w=WHITESPACE.match):\r\n    [333](https://vscode-remote+wsl-002bubuntu.vscode-resource.vscode-cdn.net/usr/lib/python3.12/json/decoder.py:333)     \"\"\"Return the Python representation of ``s`` (a ``str`` instance\r\n    [334](https://vscode-remote+wsl-002bubuntu.vscode-resource.vscode-cdn.net/usr/lib/python3.12/json/decoder.py:334)     containing a JSON document).\r\n    [335](https://vscode-remote+wsl-002bubuntu.vscode-resource.vscode-cdn.net/usr/lib/python3.12/json/decoder.py:335) \r\n    [336](https://vscode-remote+wsl-002bubuntu.vscode-resource.vscode-cdn.net/usr/lib/python3.12/json/decoder.py:336)     \"\"\"\r\n--> [337](https://vscode-remote+wsl-002bubuntu.vscode-resource.vscode-cdn.net/usr/lib/python3.12/json/decoder.py:337)     obj, end = self.raw_decode(s, idx=_w(s, 0).end())\r\n    [338](https://vscode-remote+wsl-002bubuntu.vscode-resource.vscode-cdn.net/usr/lib/python3.12/json/decoder.py:338)     end = _w(s, end).end()\r\n    [339](https://vscode-remote+wsl-002bubuntu.vscode-resource.vscode-cdn.net/usr/lib/python3.12/json/decoder.py:339)     if end != len(s):\r\n\r\nFile /usr/lib/python3.12/json/decoder.py:355, in JSONDecoder.raw_decode(self, s, idx)\r\n    [353](https://vscode-remote+wsl-002bubuntu.vscode-resource.vscode-cdn.net/usr/lib/python3.12/json/decoder.py:353)     obj, end = self.scan_once(s, idx)\r\n    [354](https://vscode-remote+wsl-002bubuntu.vscode-resource.vscode-cdn.net/usr/lib/python3.12/json/decoder.py:354) except StopIteration as err:\r\n--> [355](https://vscode-remote+wsl-002bubuntu.vscode-resource.vscode-cdn.net/usr/lib/python3.12/json/decoder.py:355)     raise JSONDecodeError(\"Expecting value\", s, err.value) from None\r\n    [356](https://vscode-remote+wsl-002bubuntu.vscode-resource.vscode-cdn.net/usr/lib/python3.12/json/decoder.py:356) return obj, end\r\n\r\nJSONDecodeError: Expecting value: line 1 column 1 (char 0)\n\n### Description\n\nI am using ChatDeepInfra to call the sql_db_list_tables tool from langchain's SQLDatabaseToolkit. This tool is invoked without any arguments, so tool_call[\"function\"][\"arguments\"] is an empty string \"\", which causes an error when parsing the JSON.\r\n\r\nError parsing should be added, or we can just switch to something from langchain_core.output_parsers.\n\n### System Info\n\n\r\nSystem Information\r\n------------------\r\n> OS:  Linux\r\n> OS Version:  #1 SMP Fri Mar 29 23:14:13 UTC 2024\r\n> Python Version:  3.12.3 (main, Sep 11 2024, 14:17:37) [GCC 13.2.0]\r\n\r\nPackage Information\r\n-------------------\r\n> langchain_core: 0.3.12\r\n> langchain: 0.3.4\r\n> langchain_community: 0.3.3\r\n> langsmith: 0.1.136\r\n> langchain_text_splitters: 0.3.0\r\n> langgraph: 0.2.39\r\n\r\nOptional packages not installed\r\n-------------------------------\r\n> langserve\r\n\r\nOther Dependencies\r\n------------------\r\n> aiohttp: 3.10.10\r\n> async-timeout: Installed. No version info available.\r\n> dataclasses-json: 0.6.7\r\n> httpx: 0.27.2\r\n> jsonpatch: 1.33\r\n> langgraph-checkpoint: 2.0.1\r\n> langgraph-sdk: 0.1.33\r\n> numpy: 1.26.4\r\n> orjson: 3.10.9\r\n> packaging: 24.1\r\n> pydantic: 2.9.2\r\n> pydantic-settings: 2.6.0\r\n> PyYAML: 6.0.2\r\n> requests: 2.32.3\r\n> requests-toolbelt: 1.0.0\r\n> SQLAlchemy: 2.0.36\r\n> tenacity: 9.0.0\r\n> typing-extensions: 4.12.2\r\n",
    "state": "closed",
    "created_at": "2024-10-24T01:46:35+00:00",
    "closed_at": "2024-11-07T21:47:20+00:00",
    "updated_at": "2024-11-07T21:47:20+00:00",
    "author": "ShawnLJW",
    "author_type": "User",
    "comments_count": 0,
    "reactions_count": 0,
    "labels": "ðŸ¤–:bug",
    "milestone": null,
    "closed_by": "efriis",
    "resolution_time_hours": 356.0125,
    "first_comments": [],
    "url": "https://github.com/langchain-ai/langchain/issues/27602"
  },
  {
    "repository": "langchain-ai/langchain",
    "issue_number": 27577,
    "title": "ChatBedrockConverse bugs with content being list and max_tokens not working.",
    "body": "### Checked other resources\n\n- [X] I added a very descriptive title to this issue.\n- [X] I searched the LangChain documentation with the integrated search.\n- [X] I used the GitHub search to find a similar question and didn't find it.\n- [X] I am sure that this is a bug in LangChain rather than my code.\n- [X] The bug is not resolved by updating to the latest stable version of LangChain (or the specific integration package).\n\n### Example Code\n\npython...\r\n\r\ndef load_chat_model() -> BaseChatModel:\r\n    \"\"\"Load a chat model from a fully specified name.\r\n\r\n    Args:\r\n        fully_specified_name (str): String in the format 'provider/model'.\r\n    \"\"\"\r\n    model = os.getenv(\"LLM\") or \"anthropic.claude-3-haiku-20240307-v1:0\"\r\n    return ChatBedrockConverse(\r\n        name= os.getenv(\"API_TRACKING\"),\r\n        client=init_bedrock_client(),\r\n        model=model,\r\n        region_name=\"us-west-2\",\r\n        max_tokens=200\r\n    )\n\n### Error Message and Stack Trace (if applicable)\n\nAbsolutely, I'd be happy to recommend some tasty pizza options for you! Based on your request and the available menu items, I think you'd really enjoy our Panne Pizza. It's an Italian-style dish made with penne pasta and a creamy rose sauce, topped with mozzarella cheese and oregano. The combination of the pasta, sauce, and cheese makes for a delightfully rich and satisfying pizza-like experience. \r\n\r\nAnother great option is our Vegan Shawarma, which features a flavorful vegan \"meat\" wrapped in a tortilla with fresh veggies like onion, tomato, and pickled cucumber. It's a lighter, healthier twist on a pizza that I think you'd really appreciate.\r\n\r\nLet me know if either of those sound good to you, or if you'd like any other recommendations! I'm here to help you find the perfect dish.\n\n### Description\n\ni use the ChatBedrockConverse with max tokens to force the message to be short, and it is not working.\r\nalso when use langserve i saw that ChatOpenAI yield content correctly and ChatBedrockConverse not, and i found the content from the event_stream retruns list and not string.\n\n### System Info\n\nSystem Information\r\n------------------\r\n> OS:  Darwin\r\n> OS Version:  Darwin Kernel Version 24.0.0: Tue Sep 24 23:37:36 PDT 2024; root:xnu-11215.1.12~1/RELEASE_ARM64_T6020\r\n> Python Version:  3.12.4 (main, Jun  6 2024, 18:26:44) [Clang 15.0.0 (clang-1500.3.9.4)]\r\n\r\nPackage Information\r\n-------------------\r\n> langchain_core: 0.3.10\r\n> langchain: 0.3.2\r\n> langchain_community: 0.3.2\r\n> langsmith: 0.1.129\r\n> langchain_aws: 0.2.3\r\n> langchain_mongodb: 0.2.0\r\n> langchain_openai: 0.2.2\r\n> langchain_text_splitters: 0.3.0\r\n> langgraph: 0.2.35\r\n> langserve: 0.3.0\r\n\r\nOther Dependencies\r\n------------------\r\n> aiohttp: 3.10.8\r\n> async-timeout: Installed. No version info available.\r\n> boto3: 1.35.31\r\n> dataclasses-json: 0.6.7\r\n> fastapi: 0.115.2\r\n> httpx: 0.27.2\r\n> jsonpatch: 1.33\r\n> langgraph-checkpoint: 2.0.1\r\n> numpy: 1.26.4\r\n> openai: 1.51.2\r\n> orjson: 3.10.7\r\n> packaging: 24.1\r\n> pydantic: 2.9.2\r\n> pydantic-settings: 2.5.2\r\n> pymongo: 4.10.1\r\n> PyYAML: 6.0.2\r\n> requests: 2.32.3\r\n> SQLAlchemy: 2.0.35\r\n> sse-starlette: 2.1.3\r\n> tenacity: 8.5.0\r\n> tiktoken: 0.8.0\r\n> typing-extensions: 4.12.2",
    "state": "closed",
    "created_at": "2024-10-23T11:12:47+00:00",
    "closed_at": "2024-10-27T14:35:13+00:00",
    "updated_at": "2024-10-27T14:35:13+00:00",
    "author": "idotr7",
    "author_type": "User",
    "comments_count": 1,
    "reactions_count": 0,
    "labels": "ðŸ¤–:bug,investigate",
    "milestone": null,
    "closed_by": "idotr7",
    "resolution_time_hours": 99.37388888888889,
    "first_comments": [
      {
        "author": "idotr7",
        "created_at": "2024-10-23T11:21:19+00:00",
        "body": "also i was unable to use the: meta.llama3-2-3b-instruct-v1:0 model_id claiming it is not on-demand throughput, although it is.\r\nbotocore.errorfactory.ValidationException: An error occurred (ValidationException) when calling the ConverseStream operation: Invocation of model ID meta.llama3-2-3b-instruct-v1:0 with on-demand throughput isnâ€™t supported. Retry your request with the ID or ARN of an inference profile that contains this model.\r\nwith other model id it works well."
      }
    ],
    "url": "https://github.com/langchain-ai/langchain/issues/27577"
  },
  {
    "repository": "langchain-ai/langchain",
    "issue_number": 27572,
    "title": "DOC: Should we prefer langchain_aws for BedrockEmbeddings in doc?",
    "body": "### URL\n\nhttps://python.langchain.com/v0.1/docs/integrations/platforms/aws/#embedding-models\n\n### Checklist\n\n- [X] I added a very descriptive title to this issue.\n- [X] I included a link to the documentation page I am referring to (if applicable).\n\n### Issue with current documentation:\n\nI notice that the [Integrations > AWS > Embedding Models](https://python.langchain.com/v0.1/docs/integrations/platforms/aws/#embedding-models) page currently references `from langchain_community.embeddings import BedrockEmbeddings`, whereas several other sections already refer to the `langchain-aws` module (e.g. for `ChatBedrock`).\n\n### Idea or request for content:\n\nShould we be preferring [langchain_aws.BedrockEmbeddings](https://github.com/langchain-ai/langchain-aws/tree/main/libs/aws#embeddings), similarly to the chat/LLM models?",
    "state": "closed",
    "created_at": "2024-10-23T09:06:14+00:00",
    "closed_at": "2024-10-24T05:46:50+00:00",
    "updated_at": "2024-10-24T05:46:50+00:00",
    "author": "athewsey",
    "author_type": "User",
    "comments_count": 1,
    "reactions_count": 0,
    "labels": "ðŸ¤–:docs",
    "milestone": null,
    "closed_by": "athewsey",
    "resolution_time_hours": 20.676666666666666,
    "first_comments": [
      {
        "author": "athewsey",
        "created_at": "2024-10-24T05:46:50+00:00",
        "body": "Ah - just realised this is an outdated doc for v0.1x... v0.3 seems to have clarified things already - but it wasn't ranking for our queries on Google so we were looking at the old doc."
      }
    ],
    "url": "https://github.com/langchain-ai/langchain/issues/27572"
  },
  {
    "repository": "langchain-ai/langchain",
    "issue_number": 27571,
    "title": "Option field llm is not necessary for loadEvaluator with type embedding_distance",
    "body": "### Checked other resources\n\n- [X] I added a very descriptive title to this issue.\n- [X] I searched the LangChain documentation with the integrated search.\n- [X] I used the GitHub search to find a similar question and didn't find it.\n- [X] I am sure that this is a bug in LangChain rather than my code.\n- [X] The bug is not resolved by updating to the latest stable version of LangChain (or the specific integration package).\n\n### Example Code\n\n\r\n```js\r\nconst {loadEvaluator} = require(\"langchain/evaluation\");\r\nconst {AzureOpenAI, AzureChatOpenAI, AzureOpenAIEmbeddings} = require(\"@langchain/openai\");\r\n...\r\nconst chain = await loadEvaluator(\"embedding_distance\", {\r\n        embedding: azure_embedding,\r\n        distanceMetric: \"euclidean\",\r\n    });\r\n...\r\n```\n\n### Error Message and Stack Trace (if applicable)\n\nError: OpenAI or Azure OpenAI API key or Token Provider not found\r\n    at new ChatOpenAI (C:\\Users\\ouki_wang\\Desktop\\Team\\sandbox-companion\\node_modules\\@langchain\\openai\\dist\\chat_models.cjs:1032:19)\r\n    at loadEvaluator (C:\\Users\\ouki_wang\\Desktop\\Team\\sandbox-companion\\node_modules\\langchain\\dist\\evaluation\\loader.cjs:21:9)\r\n    at main (C:\\Users\\ouki_wang\\Desktop\\Team\\sandbox-companion\\05-sandbox.js:78:29)\r\n    at Object.<anonymous> (C:\\Users\\ouki_wang\\Desktop\\Team\\sandbox-companion\\05-sandbox.js:131:1)\r\n    at Module._compile (node:internal/modules/cjs/loader:1364:14)\r\n    at Module._extensions..js (node:internal/modules/cjs/loader:1422:10)\r\n    at Module.load (node:internal/modules/cjs/loader:1203:32)\r\n    at Module._load (node:internal/modules/cjs/loader:1019:12)\r\n    at Function.executeUserEntryPoint [as runMain] (node:internal/modules/run_main:128:12)\r\n    at node:internal/main/run_main_module:28:49\r\n\n\n### Description\n\nwe have tried to call loadEvaluator to get euclidean distance with embedding model.\r\nHowever get error message without llm missing in option fields of loadEvaluator.\r\nI think llm field is not necessary when call with type embedding_distance.\n\n### System Info\n\nnode test.js",
    "state": "closed",
    "created_at": "2024-10-23T06:34:15+00:00",
    "closed_at": "2024-10-23T06:44:28+00:00",
    "updated_at": "2024-10-23T06:44:28+00:00",
    "author": "trend-ouki-wang",
    "author_type": "User",
    "comments_count": 1,
    "reactions_count": 0,
    "labels": "ðŸ¤–:bug",
    "milestone": null,
    "closed_by": "trend-ouki-wang",
    "resolution_time_hours": 0.17027777777777778,
    "first_comments": [
      {
        "author": "trend-ouki-wang",
        "created_at": "2024-10-23T06:44:28+00:00",
        "body": "sorry, wrong repo"
      }
    ],
    "url": "https://github.com/langchain-ai/langchain/issues/27571"
  },
  {
    "repository": "langchain-ai/langchain",
    "issue_number": 27557,
    "title": "GoogleScholarAPIWrapper issue when initializing, pydantic_core._pydantic_core.ValidationError",
    "body": "### Checked other resources\r\n\r\n- [X] I added a very descriptive title to this issue.\r\n- [X] I searched the LangChain documentation with the integrated search.\r\n- [X] I used the GitHub search to find a similar question and didn't find it.\r\n- [X] I am sure that this is a bug in LangChain rather than my code.\r\n- [X] The bug is not resolved by updating to the latest stable version of LangChain (or the specific integration package).\r\n\r\n### Example Code\r\n\r\nimport os\r\nfrom langchain_community.tools.google_scholar import GoogleScholarQueryRun\r\nfrom langchain_community.utilities.google_scholar import GoogleScholarAPIWrapper\r\n\r\n\r\nos.environ[\"SERP_API_KEY\"] = \"my-key\"\r\n\r\n\r\nnew_scholar = GoogleScholarAPIWrapper()\r\n\r\n\r\ntool = GoogleScholarQueryRun(api_wrapper=new_scholar)\r\nresult = tool.run(\"LLM Models\")\r\nprint(result)\r\n\r\n### Error Message and Stack Trace (if applicable)\r\n\r\nTraceback (most recent call last):\r\n  File \"googlescholar.py\", line 9, in <module>\r\n    new_scholar = GoogleScholarAPIWrapper()\r\n  File \".venv/lib/python3.13/site-packages/pydantic/main.py\", line 212, in __init__\r\n    validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)\r\npydantic_core._pydantic_core.ValidationError: 2 validation errors for GoogleScholarAPIWrapper\r\nSERP_API_KEY\r\n  Extra inputs are not permitted [type=extra_forbidden, input_value='2d2dd691e41cc3135c34c8ea...e40d9edf183c92213960ef2', input_type=str]\r\n    For further information visit https://errors.pydantic.dev/2.9/v/extra_forbidden\r\ngoogle_scholar_engine\r\n  Extra inputs are not permitted [type=extra_forbidden, input_value=<class 'serpapi.google_sc...ch.GoogleScholarSearch'>, input_type=type]\r\n    For further information visit https://errors.pydantic.dev/2.9/v/extra_forbidden\r\n\r\n\r\n### Description\r\n\r\nI am trying to use the Google Scholar API Wrapper for testing.\r\n\r\nI am following these links: \r\nhttps://python.langchain.com/docs/integrations/tools/google_scholar/\r\n\r\nhttps://api.python.langchain.com/en/latest/utilities/langchain_community.utilities.google_scholar.GoogleScholarAPIWrapper.html\r\n\r\nI am stuck initializing it as it says \"extra inputs are not permitted\". I don't want to use my own class as I will be using this in a jupter notebook for testing and learning.\r\n\r\n### System Info\r\n\r\naiohappyeyeballs==2.4.3\r\naiohttp==3.10.10\r\naiosignal==1.3.1\r\nannotated-types==0.7.0\r\nanyio==4.6.2.post1\r\nattrs==24.2.0\r\ncachetools==5.5.0\r\ncertifi==2024.8.30\r\ncharset-normalizer==3.4.0\r\ndataclasses-json==0.6.7\r\ndocstring_parser==0.16\r\nfrozenlist==1.4.1\r\ngoogle-api-core==2.21.0\r\ngoogle-auth==2.35.0\r\ngoogle-cloud-aiplatform==1.70.0\r\ngoogle-cloud-bigquery==3.26.0\r\ngoogle-cloud-core==2.4.1\r\ngoogle-cloud-resource-manager==1.12.5\r\ngoogle-cloud-storage==2.18.2\r\ngoogle-crc32c==1.6.0\r\ngoogle-resumable-media==2.7.2\r\ngoogle_search_results==2.4.2\r\ngoogleapis-common-protos==1.65.0\r\ngrpc-google-iam-v1==0.13.1\r\ngrpcio==1.67.0\r\ngrpcio-status==1.67.0\r\nh11==0.14.0\r\nhttpcore==1.0.6\r\nhttpx==0.27.2\r\nhttpx-sse==0.4.0\r\nidna==3.10\r\njsonpatch==1.33\r\njsonpointer==3.0.0\r\nlangchain==0.3.4\r\nlangchain-community==0.3.3\r\nlangchain-core==0.3.12\r\nlangchain-google-vertexai==2.0.5\r\nlangchain-text-splitters==0.3.0\r\nlangsmith==0.1.136\r\nmarshmallow==3.23.0\r\nmultidict==6.1.0\r\nmypy-extensions==1.0.0\r\nnumpy==1.26.4\r\norjson==3.10.10\r\npackaging==24.1\r\npropcache==0.2.0\r\nproto-plus==1.24.0\r\nprotobuf==5.28.2\r\npyasn1==0.6.1\r\npyasn1_modules==0.4.1\r\npydantic==2.9.2\r\npydantic-settings==2.6.0\r\npydantic_core==2.23.4\r\npython-dateutil==2.9.0.post0\r\npython-dotenv==1.0.1\r\nPyYAML==6.0.2\r\nrequests==2.32.3\r\nrequests-toolbelt==1.0.0\r\nrsa==4.9\r\nshapely==2.0.6\r\nsix==1.16.0\r\nsniffio==1.3.1\r\nSQLAlchemy==2.0.36\r\ntenacity==9.0.0\r\ntyping-inspect==0.9.0\r\ntyping_extensions==4.12.2\r\nurllib3==2.2.3\r\nyarl==1.16.0\r\n",
    "state": "closed",
    "created_at": "2024-10-22T19:52:40+00:00",
    "closed_at": "2025-01-23T15:03:03+00:00",
    "updated_at": "2025-01-23T15:03:03+00:00",
    "author": "speralta-burwood",
    "author_type": "User",
    "comments_count": 0,
    "reactions_count": 0,
    "labels": "ðŸ¤–:bug",
    "milestone": null,
    "closed_by": "ccurme",
    "resolution_time_hours": 2227.1730555555555,
    "first_comments": [],
    "url": "https://github.com/langchain-ai/langchain/issues/27557"
  }
]